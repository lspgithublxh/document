---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。
>学习要有战略和格局：过于零散、孤立、末支、叶子的知识点就没必要化太多时间，懂得放弃。聚焦在根节点、枢纽节点、产生分支的节点、主干知识点、枢纽知识点、关联到很多知识点的知识点上。
 >对于主干知识点：要有全新的认识视角和进行精细的描述，大量的提问和好奇和进行必要的实验以获取支撑观点/导出观点的数据。
 >规范是工程最独特的特征.
 >慢慢读：
>一种新技术的学习：
 >它面对的情况和问题、它的世界观、它的方案、它的方案验证/论证/能处理的解决的所有情况及能成功处理的理由/功能边界
  >所有的软件：都可以看作是向上封装一层接口，根据自己的世界观封装底层而向上/对外提供统一的(统一的更简单的更直观的更业务的更少底层信息的)接口，底层包含一系列的第三方的插件/构件/组件；内部则去做兼容和调用(对底层)(对上层则做逻辑分解和底层实现)。
>知识混乱就是因为没有组织：
	>组织就是关键字树：几个单词就是每层上的每个节点的内容；
	>组织也可以看作逻辑树：有逻辑关系，逻辑顺序，逻辑联系的关键字的层层集合。层层囊括更精细的范围，层层划分范围。
>推进理解的属性发展拓展、问题延展：重要方式；
>什么是架构：架构也是从抽象到具体的考虑和描述；树形延展开来，可以写满非常大的黑板和巨大的脑图！！sharding-jdbc,dubbo,spring都可以这样方式来展现它的架构！！它的抽象到具体的考虑---本身才是架构！！！而不是什么模块、模式之类！！
>抽象设计：则某一层就不管上一层的含义和下一层的含义，即更抽象的含义或者更具体的含义；而是实现本层的含义；完成本层的含义指定的功能；。如网络协议的架构设计；	
>面向设计来理解，面向架构设计来理解，面向架构问题一层一层来理解它：面向设计来理解，所以按照面向对象设计的方式，看其中的对象、行为属性、流程环节逻辑。	
>找不到知识/描述 所对应的问题 ， 那么看书将没有条理纲领，变得零散琐碎没有组织。	
>不是按概念方式组织，而是按架构、问题方式来组织 笔记，书本内容。架构顺序，问题层次顺序。	
>架构不是设计出来的，也不是演进出来的(甚至不是迭代出来的--尽可能避免迭代)：而是问出来的。	
>每个方法方案都从属于一颗树，所以找到一个方法巧妙方法仅仅是第一步--找到从属的层次树 有更大的价值；(无论是谁想到的方法/概念，都要这样更进一步)
>解决问题的办法就是提出问题：类似递归和动态规划；。。权衡就是线性规划；	优势劣势在一定场景下也是劣势优势；
>一个词，到一句话，两句话，一段话，一篇文章；这个就是抽象总结，层次总结；越简洁，站得越高
--在网络、搜索引擎、推荐系统 三方面的专家；作为系统方面的独特优势/拔高优势；(网络-查询-推荐)
	
----有且只有响应，通信端才知道连接是否成功；。浏览器自动扩展。
----维持连接，并发连接，都是软件的实现，物理上看都是一条出口；从响应就是维护连接的角度看，不存在需要维护什么连接，维护就是维护连接数据而已；只要发送响应，连接就活了；在网络端口出口，可以连续发送不同目的地的响应报文，这就是并行；所以完全可以用队列来接收请求数据包；而用队列缓存发送响应数据包；多核使用起来，来并行大批量的发送和接收；不存在要维护和持续占用“端口”网络出口这种概念---完全没必要，用完即走 就可；	
	>或者不存在连接这个概念：所有的事情就是接收数据包和发送数据包(接送/发送缓冲区)。(连接 是 软件臆造出来的概念，不要和物理对应；和物理对应就会束缚思想，就会很多事情理解不了不知道原因)
	>连接的状态转移图；
	>应用的固定端口：实际上是建立新TCP连接的请求的处理的端口，请求到达这个端口--后面建立一个独立的TCP连接---来负责和客户端通信-交互数据；
----UML：为什么类继承关系图---因为这就是具体到一般的概念对象抽象过程。自上而下是能力顺序，能力组合；	
----说话和介绍：语速不要快，快就是掩盖问题，掩盖过程步骤；直接导致别人认为思路不清晰，表达不清楚，东拉西扯；也不利于自己思路的成长和扩展和自己主动发现问题，且必然导致不简洁--废话很多；
	>介绍需要先纲目后具体：抽象到具体；而不是张口就是罗列枚举---内容没有结构--全是线性结构；
	>描述更精简：一个字一个词，一句话，两句话，一段话；
----大事和吏治：大事 就像西天取经；吏治就像管理四人；


--计划：nginx/tomcat-->计算机系统-->架构-->自己的系统架构方案:专题研究、大提问、大总结。大简化/模型图化；
>一个进程看作一个消息，代码计划/任务计划；；都是异步隔离；	
	>程序启动点/执行点：可以多个，看作是并行的；(一个机器上，多个程序文件里)；可以留下执行点/新增执行点；可以删除执行点/减少执行点；
	>函数式编程为什么好：因为每个精细环节清晰描述确定了下来；使得充分配置和指明动作；
	>如何看待对象的方法：所有的对象都是被动的；主动的只有cpu/并行点；
	>程序的执行要想象为人在执行；多线程则是交接执行权给其他人执行；
	>抽象编程与具体填充：假设编程和实现假设。面向对象中一个对象属性就假设已经填充好了，一个接口的实例就假设已经在容器中有了。代码编码层面和虚拟机执行层面，都规定/定义/设计为 将 接口和实例分开，系统启动时/甚至具体接口调用时才去容器里找接口的实例(启动时 一方面建立实例容器，另一方面 对接口寻找匹配的实例和链接到实例---进行连接和关联)。资源填充 和 接口调用(抽象调用)，资源--接口的映射对。领域抽象，资源抽象，功能抽象。
----未知和迷惑的地方：痛点	；
----关键和核心的地方：要点

--混乱的答案，宁可不说；只回答真正掌握的；。没有逻辑，因为没有进行抽象；没有找到所在的流程环节、模型中的位置
--系统、中间件的介绍，不是一来就是组成结构；这层次已经太细了太具体了太里面了，必须要从最简洁最抽象最上层开始；最表面最近开始；务实，不僵硬，不突然，要自然，不要忽视和没注意没意识和跳过很多步；而是从问题出发、从困难出发、从疑惑出发
	>从问题出发：先明确问题；先明确表达出问题、疑惑点、黑箱、痛点、矛盾点、难点，表述的范围可以很大(完全不知道是什么怎么办)后面逐渐具体问；无论多少问题，先明确下来；尤其要全部且完整的描述下来；
		>问题的提出：先明确背景，自然衍生、过渡、转折、演化，逻辑关系上，什么时候什么事情什么人，事情什么阶段遇到的什么问题、阻碍、阻挠、缺陷、瓶颈、不够简洁、不够简便、不够方便、不够优雅、离目标远、离理想情况远、离期望/极限效果远；不够抽象的地方；把它们充分描述完整叙述结构式组织起来。
			>问题抽象：归结为一类问题；去除具体和细节而明确问题模型；
		>问题产生原因：过程；条件；	
		>问题导致的恶果：阻碍、损失；
		>理想的方案特征/效果/必做必不做的动作&事情/应当改变的环节: 
			>这种特征/必做必不做的前提、必要条件、必然要求、必然说明、必然指示、必然可以确定的更多的事情/结论：
				>一系列结论、约束得到之后(结合条件/问题/情况本身)逐渐可以清晰看到/归结出该具体问题符合的/满足的通用/一般的/一类的问题模型/函数模型/服务模型/IO模型/请求响应模型的轮廓：若干个具体模型
					>方案的装饰/补充: 补充可靠性/稳定性/高性能(从而高可用/高并发)：因为暂时只是一个裸机、容易受到伤害、有功能但没有抵抗力(仅为打火机的火而不是熊熊大火)
							>方案的用法规则：在请求缓存前使用
		>能将具体方案进行分类的维度/情况/模型/环节/流程/抽象表述 的确定：然后使用 抽象-具体 的方法来得到新方案；					
		>普通的方案：已有的方案；方案的抽象，方案的取值选择评价；方案的表象缺陷、劣势；。。模型、数据结构和算法、协议约定分担 维度 上考虑；
	>任何事情/事物都有顺序/逻辑：且几乎都是几种常见逻辑中的一种: 时间先后、空间远近、因果环扣、程度递增
--大总结：含义包括：重新 深刻理解：
--通用的建模架构能力+Flink深度强化学习的推荐系统。。。。而不是做简单的业务逻辑开发；用深度强化学习来做应用/解决实际问题(用户的识别问题和抉择问题)；用抽象建模架构出逻辑完整的方案(工具方案/服务方案)；
--一体两翼的发展模型：底座：增强操作系统、网络、搜索、推荐能力； 两翼：普通项目：则架构建模；特殊项目：则深度强化学习；。。四大基础+两大实践(应用/使用)。。基础：是为了解决自己的问题；实践：为了解决别人的问题(用户的问题/大众)；
--一次彻底弄懂，而不是 反复低温加热。看架构书和源码书，不看使用书。
--对话中胜利：一个是提问，二个是不断的输出-高能输出。
--牢固的观念: 系统都是被使用的。
--任何一个系统、产品、服务、方案、东西、事情，它要解决的核心问题是什么？理想的形态效果影响应该是什么？市面上对应的哪些产品达到了或者没达到或者很难达到？没达到是为什么是否有我们的机会？是否还有我们可以满足的缺口。找市场缺口。
--从缝纫机原理看方案所属的分类和方案内的环节。抽象出分类和环节。
--把自己当做cpu,调/使用各个服务/接口/类/系统。
--全新学习掌握方法：先定问题体系，先提问，定逻辑路径，后开始找答案--推导和思考和查阅资料。
	>旧方法：还是盲目阅读，还是从头到尾的阅读一遍的阅读。
--所有系统和方案的理解/研究/制定的出发点：都是 出发顶点/问题顶点/概念顶点，找到了顶点才能出发，出发要从顶点出发，才顺畅，没有后顾之忧
--计算机解决问题的方法：采集，记录，计算，展示，通知，跟踪 数据/信息。

--商业经营才能：(全新的解决问题的方法：提出问题)
--什么问题 是可以用互联网来解决的？ 只要满足什么特征，该问题就可以用互联网方式来解决? 该问题的本质是什么问题(如是信息传递问题？)该问题抽象一般化后是什么问题(资管、记录、通知、计算、采集识别调动、代理/中介/数据中心/信息中心/信息模拟线下过程中心、信息传递)(信息展示/发布/搜索/推荐 平台；信息池；信息可以是映射描述的现实事物的信息(如商品/价值物/人)，也可以是纯虚拟信息(游戏/知识/新闻/视频))
	>能用信息发布展示/活动追踪-信息记录/通知/协作/推荐/搜索 解决的问题：信息可以是 编辑的信息/映射的信息/等价现实的信息/等价现实任何事任何物的信息。内涵BAT的老业务(电商/聊天/游戏/信息流/搜索)。
	>能用数据驱动决策抉择与调度分配/识别与推理/观测采集与调动机电-生物 而解决的问题：如alphazero/图像识别/语音识别/视频识别/云计算/智慧城市/自动驾驶。内涵BAT新业务(云计算/视频语音识别-下棋打游戏智慧体/智能音箱/自动驾驶)
--码农之家和脚本之家、异步社区。从 面向对象-->模式-->架构：由小到大。
--给其他非软件行业的工程师/人员开发软件。三高三可(高可用高并发高性能,可扩展性可维护性可重用性)。
--最佳：一边看书，一边独立思考；(比起纯看书和纯思考都好)。因为 看书 要有沉淀，积累，独立思考 结论，总结。而不是流经书，翻翻而已，左进右出。
--说话：社交场合，最重要的是：要有自己的认知、决策和行动；对形势的判断和分析和预测。而不是被别人牵着走，或者置身事外。
--什么是最重要的? 就是你想做出一个什么东西...  这是最有价值的事情,是最重要的目标(比起学位和金钱和看多少书都重要);
	>不讲在公司的项目: 只讲自己 业余时间 做出的东西; ..  和 理解的东西... 总结起来 就是 四个方面: 架构 + 源码 + 算法 + 产品/作品
--不必指责和愤怒: 只需 战胜 和提防.
--莫忘理性决策：详细的明确和计算，一点也不能含糊。看看这次搬家时机的决策失误和搬家房间选择的失误，造成了金钱和时间上的巨大浪费。又比如以前自大没有投资股市---导致损失很大！！	拒绝感情用事和感动别人。
	>不能武断：不能一厢情愿。不能只见树木不见森林。
--兴趣不仅决定能力，而且决定性格，比经历更加影响性格；总结和思考 是兴趣带来的根本也是兴趣导致的根本活动；而性格能很大程度上影响命运---作。韩信(仰仗自己聪明而希望别人只记得我的好不记得我的坏)、赵括(自己负责精彩，麻烦交给别人--希望别人帮自己解决；只知顶层逻辑不知道低层逻辑)、李广(总觉得别人对自己不好，发现不了自己的错，不按程序办事，爱自己来，当别人按程序来--总觉得别人是让自己受气，受不了别人的指责)	
--在大部分有钱人和机构都将资本+利润存入股市的时候：说明股市是会升值的，股市就是最大的公司。多余的所有钱+利润 只有在股市才能分享。
	>既要有信心，也要有担忧： 激情和忧虑 都有 ，才是 理性的人。而不是两者都没有的麻木不仁、冷血和装。
	>做人不要尖酸刻薄，说话要说别人的好，坏处知道 但是不要说出来。有的人不喜欢被戴高帽，或者认为是反讽，这只是 好 没有说到点上而已。
	>面对别人指出的问题: 改就完了,且深刻反思和各种改进, 大量的收获和扩展.更强!!!错误的反思总结得到的东西让人更强!!!
--理论观点/方法缺乏层次系统的组织，还是在等别人分配赚钱任务。概念、观点 还是片段、不连续的。
	>基本认知方式：提炼、总结、简化、抽象、封装 。
	>问题为 元素 进行组织：这次不以概念为元素，来更方便于实际使用、进行设计训练---工作也当作是设计训练。
	>填充所有的逻辑漏洞：
	>分层问题森林\分层问题 群: 一个n层的问题,其顶元素 只是 另一个n层问题的一个元素. 因此 k个 分层问题  构成 了 一个最顶层分层问题 , 整体 是一个 问题森林.
--领域、问题、逻辑。(路径/方案/思路)(条件/目标/路径)	范畴。顶级思路/顶层思路。顶层领域，顶层问题。范畴与逻辑。范畴关系(同一范畴内、不同范畴内)、比较关系、规则关系、联动关系。
	>不能只以概念入手，也要从问题、思路入手。
	>谈话永远不能泛泛而谈：回答而已，别总想着怎么回答会好-显得聪明有创意有见解之类。必须要有目标，要预见和引导 话题走势走向，进而从当前条件 寻找 路径/思路。
>举动-痛点：分布式存储的架构过程。逻辑结构。自底向上的 对 (从最底层的)具体组件/功能的 抽象和封装(兼容处理各种情况各层情况，增强功能扩展适应范围)。自顶向下的 对 大范围概念 的 一步步一层层的 约束 和 定形(造形)(拆解和细化)。都保留中间过程，看清楚演化过程和演化逻辑，并方便进行旁路抽象封装 和  旁路 的约束定形(要求能做什么不能做什么都很明确) ，从而扩展出对新范围的描述和处理。	
	>解法/解决方案：我来设计 
		>分布式存储系统：
			>领域结构：职责系统
				>存储节点：专门负责存储数据(某种特定格式)和提供查询方法(某种特定格式)，变更原因只有 有新数据到来/新数据存储方式等内部原因为主。
					>存储管理节点：专门负责管理存储节点、分配数据到节点并记录这个分配、心跳各个存储节点-宕机摘除再重新分配再对外提供服务-新进节点获取信息再分配任务再对外提供服务、...(可以抽取责任出来为新的角色 来专门负责)
						>风险兼容存管节点：对外提供一样的同存储管理节点一样的功能、对内封装存储管理节点而 负责监控 管理节点崩溃，切换到备用节点。
							>解析模块：内部封装风险兼容存管节点 对外提供更原始格式的数据存储/查询方法：为减少直接依赖，根据DIP 而 建立一个新类，解析模块依赖它，存储节点也依赖它。
								>缓存解析模块： 封装 解析模块，缓存解析结果。
			>问题结构：好的问题(层层要解决哪些问题-哪些关键问题)
				>提供一个怎样的服务？:低成本存储海量数据 且方便快速查询(概括-主谓宾状明确)
					>低成本如何实现？：普通PC机
						>普通PC机的风险是什么？：
					>存储方式怎么选择的？：
					>海量如何实现？：存储节点可扩展
						>新增存储节点后如何被存储系统使用？：
						>集群节点扩展到一定规模后会遇到什么风险？：	
							>机器故障容错问题？：
						>数据如何分布？：一致性哈希 
						>自动负载均衡问题？：
					>数据类型有什么要求？：定长块、Blob对象、大文件。
						>数据如何再节点之间分布？：一致性哈希
							>
						>解压缩数据算法问题？：
					>查询如何实现？：
						>理想中的查询应该是怎样的？
						>自动负载均衡问题？：	
					>快速如何实现？：高性能如何实现？：并发读写。	
					>服务质量如何保证？：
						>服务可用性怎么保证？：自动迁移、自动容错、自动负载均衡。
						>服务存储容量扩展怎么实现？：
						>数据一致性如何保证？：
							>分布式事务问题？：
					>服务使用是否方便？：监控、运维工具是什么、功能怎么样？：是否可以与其他系统集成？：
					--重点问题：分布问题、一致性问题、故障机器容错问题、自动负载均衡问题、分布式事务问题、解压缩数据算法问题	
		>单机存储系统：
			>单机存储引擎：哈希表、B树、LSM(顺序append写入-定期合并-内存hash索引(value位置/key位置)) 在磁盘上的实现。
			>单机存储系统：对单机存储引擎的封装， 对外提供 文件、键值、表格、关系模型。
			>CPU架构：
				>超线程技术：一个核心同时执行两个线程。
				>主流服务器架构：NUMA--非一致存储访问；一个NUMA节点为一个SMP结构。
			>IO总线：南北桥架构。
				>北桥芯片--前端总线--CPU：挂接内存模块、PCI-E设备。
				>北桥芯片-DMI--南桥芯片：挂接网卡、硬盘
			>网络拓扑：	三层结构--接入层/汇聚层/核心层,同一个集群内：20480台服务器最多。
				>机房内网络来回：0.5ms; 异地机房之间网络来回：30-100ms; 磁盘顺序读取带宽：100MB，
			>数据结构：
				>B+树：mysql内存缓存页，缓存算法：两级 LRU,LIRS
				>LSM树：增量写入-大小过限合并。
					>LevelDB 存储引擎： 内存中memTable ,不可变memTable, 磁盘上：Current当前文件，Manifest清单文件，Commit log 操作日志文件，SSTable 文件。
						>写入：先顺序写入Commit log , 再应用到memTable ,这样才算成功。所以可以从 日志+SSTable位置 恢复内存中的memTable。
						>冻结转储：当memTable大小达到极限时，生成新的memTable和日志文件，新的写入请求则在这里处理；老的memTable形成不可变memTable 先对内容排序 后 转储到磁盘，形成一个新的SSTable文件。Compaction
							>SSTable元数据存储：文件名称、主键范围、层级 在 清单文件中--清单文件也会随着SSTable的变更新增而变更乃至产生新的清单文件。当前文件 则是 当前清单文件。
						>查询：内存中memTable + 磁盘上全部SSTable 来查找：可能可以借助清单文件中记录的范围来辅助查找。
						>合并：包括memTable转不可变memTable时的合并---minor compaction; 某一层级的SSTable数量达到限制时 将本层的SSTable合并 然后和上一层的合并---即直接晋升几个SSTable到上一层中；---Major compaction; 接着对上一层进行合并判断处理。
			>数据模型：
				>文件模型：POSIX本机系统；分布式环境下，则大多选择没有目录/一级目录；让对象一次写入系统，只能删除整个对象，对其他部分不能操作；避免操作目录引起的数据不一致；
				>关系模型：
					>SQL过程：from 后的元组 组合形成所有可能的元组-->where筛选元组-->按照group by分组-->having筛选分组-->select选择出属性/分组key/分组统计结果形成形的元组-->order by 对新的元组排序
					>索引：提高where/from/having筛选性能，减少筛选步骤；提高order by 性能；
					>事务：保证操作的ACID：一组操作并行下的ACID：
				>关系弱化的表格模型：HBase, 单表的简单操作；
					>HBase: 支持操作：insert/delete/update/get/scan; 主键扫描+按主键获取，按列过滤、排序、分组。不支持二级索引---事务也弱。
				>键值模型：	
			>事务与并发控制：
				>MVCC: COW--写时复制，来避免 写 阻塞读，因为互联网场景下 读事务 远高于 写事务。
				>两阶段事务的原因：第一阶段的检查 就是为了 避免 网络中断 而 还进入第二阶段；所以第一阶段本质上就是 通往各个节点的网络是否通畅的检查；如果都通畅，那么就可以认为可以像执行本地事务中的多个操作一样执行 分散在各个节点上的多个操作；(这个是基于网络通畅 和 事务执行成功 之间的关系 的认知)。如果第一阶段都失败了，即网络出故障了，那么不必进入第二阶段，省了很多事。如果第一阶段成功，而第二阶段时网络突然断了--那么显然 会影响到的范围 就是：第二阶段马上就会成功的事务 到 第二阶段刚刚开始  这期间的事务，所以画图可知影响的事务范围为阶段2时长内的事务。 这说明了阶段2越短越好。这个时长内的事务是白做的(超时回滚)/或者会导致数据不一致的(超时提交)。
					>可能需要：对记录的进入了阶段1而失败在阶段2的事务：进行邮件短信提醒 并人工介入修复。
				>隔离性：锁来保证 读写 的 隔离性：避免 读到 不一致的数据。
					>所有隔离级别：都不会出现回滚了其他事务的已经成功的结果。CAS式的回滚。
					>事务并发导致死锁的原因：事务所需的资源不是一次获取的。事务不超时。事务不让步/取消。
					>写时复制：无需锁。但是代价高。先复制-后修改-再提交替换数据根指针。且写操作互斥，一个写必须等另一个写写完了再读才能成功；和CAS类似。
					>MVCC：每行数据多2列：修改版本号/删除版本号。select/update/delete/insert的事务号  和 这两列的版本号进行比较。Delete标记为删除 就是 将 删除版本号列 更新为 当前事务号。
						>update时：先复制一行，将修改版本号更新为 当前事务的事务号；再添加到数据版本链表上。select时 有 自己的版本号， 和 update/delete列的版本号比较+事务隔离级别 来确定 读取哪个版本数据。
				>一致性：事务执行后的结果 和 事务操作前希望的结果 是一致的。
			>故障恢复：
				>对数据的修改 先 在内存中修改，后定期刷到磁盘：但同时 修改内存也追加增加 操作日志 到 磁盘；UNDO/REDO 日志： <T,X,5>, <T,X,15>....
					>成组提交技术：Redo日志先在内存中---存储系统的日志缓冲区中，再在 缓冲区数据量>512KB 或者 距离上次刷入磁盘超过一定时间入10ms则开始 写磁盘；
					>检查点技术：定期镜像内存数据到磁盘，以便故障恢复时不用重放 REDO所有的日志，而是重放到某个位置。系统也限制只能回放一定时间内的全部日志--内容太多内存无法装下也是一个问题。
						>镜像内存数据需要时间：所以过程需要保证数据一致性：日志中记录开始START CKPT, 然后内存镜像数据开始以一种易于加载的方式转储 到 磁盘，形成checkPoint文件---连带记录镜像时刻的REDO日志位置--用于故障恢复时的重放开始位置，然后日志中写入END CKPT
							>可以镜像：也可以先暂停修改。
						>恢复过程：	读取checkpoint文件加载到内存--往往只是索引数据，读取其中的REDO 位置，读取REDO该位置开始的日志，回放应用到内存数据中。
			>数据压缩：
				>压缩比和压缩执行效率：LZ算法系列(窗口内重复数据-数据字典)。霍夫曼编码。列式存储 可以进行压缩数据。找重复数据/数据规律--核心。
					>Huffman 编码：前缀编码：要求一个字符编码不能时另一个字符编码的前缀；。Huffman编码 则是  一种前缀编码方式--使得编码的长度最短的编码方式；
					>LZ算法：动态创建字典的算法，并将字典保存在压缩信息里；是一种动态复制粘贴 --复制前面出现过的某个连续子串  或者 输入新的字符 来 粘贴到 结果字符串后面 的过程 来复原。
						>Zippy/BMDiff: 压缩比不高但速度快。Zippy往后找--而不一定从开始位置找匹配的模式。
						>列式数据库：有数据压缩的优势：	
		>分布式系统：
			>数据分布：数据均匀存储到各个节点。标准：资源使用情况。
				>哈希分布/一致性哈希：传统的哈希分布---对机器上下线敏感---会带来大量数据迁移问题。一致性hash/带虚拟节点的一致性hash 同样 有 迁移数据的问题。
					>支持：随机读写。但不能顺序扫描。
				>顺序分布：数据范围+数据子表；
			>复制：保证可靠性和可用性，将数据复制多个副本。
				>复制协议：同步操作日志。主副本发送操作日志-->从副本回放操作日志-->从副本返回成功。都成功主副本才返回成功--则是保证了强一致性---但可用性差；。异步复制，则一致性差。
					>基于主副本的复制协议：
					>基于写多个存储节点的复制协议：NWR复制协议。
				--一致性和可用性的权衡：最大保护模式---强同步模式--主库至少同步到一个备库才返回成功；最大性能模式---主库成功就成功，异步复制；最大可用性模式--正常时最大保护模式/网络异常时最大性能模式。
			>一致性： 多个副本带来数据的一致性问题。。
				>强一致性：更新成功则立刻能读到最新的。
				>弱一致性：更新成功则不保证一定立刻能读到最新的，也不承诺时间。
				>最终一致性：更新成功则不保证一定立刻能读到最新的，但承诺时间。
			>容错：PC服务器性能很好但是故障率很高，所以硬件出问题了要在软件层面兼容这个故障、容错，检测故障并迁移数据；
				>故障检测：机器故障检测---租约协议Lease,。
					>一般：总控机 定期 向 工作机 发送 心跳。
					>租约机制：工作机 需要 拿到 总控机 的 租约， 然后再租约快过期的时候向 总控机续约。保证AB之间网络故障时根据自动过期机制 B能够主动不再提供服务。
				>服务复制/迁移：选举从副本为主副本，
			>分布式协议：paxos协议用于多个节点之间达成一致，总控节点的选举。两阶段提交协议--用于保证跨多个节点操作的原子性--要么全部成功要么全部失败。
				>Paxos 协议： 解决 网络故障 发生 网络分区 情况下的  脑裂问题--避免选出多个leader。
					>过程概要：1.获取最新，可能为空；2.提交最新，或者是自己的。	一个时期内 并发安全的更新超半数节点数据为首次二阶段成功的值---这个值就成为了共识/共识值/认同值。
					>paxos协议功能扼要：分布式提交者彼此不知且平等情况下 产生共识值(提交者共识)。一个节点既是proposor 也是 acceptor
			>异常：服务器节点和网络是不可靠的。
				>服务器宕机：内存错误、服务器停电、进程重启。
				>网络故障：网络分区 
				>磁盘故障：磁盘损坏--只能转移数据；磁盘数据错误--用校验和机制来解决。
				>超时/失败：支持幂等 则 重试；
			>衡量指标：
				>性能：QPS每秒处理的读操作数，和  系统的响应延迟 是 矛盾的。
				>可用性：正常服务的时间的比例。
				>一致性：可能影响性能和可用性。
				>可扩展性： 通过扩展集群服务器规模来提高系统存储容量、计算量和 性能的能力。理想的情况是：线性扩展。
			>性能分析：判断设计方案是否存在瓶颈点，权衡多种设计方案，也是后续性能优化的根据。	性能优化是后续。
				>估算：可能成为瓶颈的单机模块，找出可能出现的资源瓶颈。
			>可扩展性：数据分片，增加副本。	
				>异构系统：存储节点内是  多个分片的副本。每个存储节点 显然 是 有 一组组合不同的 分片的。
			>跨机房问题：
				>机房之间：网络延迟较大 且 不稳定。
					>数据同步：
					>服务切换：
				>跨机房部署方案：
					>集群整体切换：当主机房不可用时：备机房 可能等待主机房恢复 或者  允许丢失数据 而切换到备机房。
					>单个集群跨机房：主控 节点 和 备 控节点分在不同的机房。
					>Paxos选主副本：
		>分布式文件系统：目标：存储文档、图像、视频 之类的 Blob数据；		
			>顶级思路/顶层思路：：1.大文件拆分为固定大小块存储(分割存储)、分片；2.块有副本，分散存储、负载均衡；3.节点故障检测、副本其他节点恢复。
				>职责划分、任务拆解、角色定义、明确 对应负责 哪块 职责/功能的 组件模块：
					>文件切分、块句柄分配、数据块分发、数据块-文件对应关系维护、数据块-所在节点对应关系的维护  划归 在一起： 由 GFS Master 主控服务器 模块/组件/实体 负责；
					>chunk数据块租约管理/chunk复制/垃圾回收无用chunk 也交由 Master主控 服务器 负责: 此外， 心跳给 Client 来交换信息 自然也是它负责。
					>封装出给用户使用的API接口 划归 在一起 ：由 GFS Client负责。此外，扩展功能建设 还负责：缓存 Master的元数据，
					>GFS数据追加 功能 ：由于Master职责已经比较重，所以交给 每个节点上的 ChunkServer 来 负责； 为保证能负起责任 则 连带 要求 chunkServer 要先从 Master获取租约(这种功能/权利的租约)--一般为60s。 主ChunkServer, 备ChunkServer;
						>可靠性建设：追加失败，重试。
						>追加流程：先Master取得 chunk-chukServer映射，再 2阶段类似：客户端先发数据给每个备ChunkServer-但不提交，然后客户端请求主ChunkServer提交， 主副本将排序请求写入本地而 再提交写请求给 所有备ChunkServer, 然后备CHunkServer就执行提交并应答主副本；再回客户端；如果备ChunkServer有的失败了则客户端重试。
							>性优化：追加流程 --流水线、分离数据流、控制流。
			>完整思路：封装 顶层思路、在各种情形下的判定处理--能处理的独立处理不能的 会 调用 顶层思路 来处理；各种需求怎么 对应处理实现。
				>剩余需求方面：尽管核心需求 解决了(实现了功能和可靠性)，但是 新增的需求 如何 在此基础上解决。
				>性能优化方面：
				>稳定性建设方面： 
					>容错机制：
						>Master容错：操作日志+内存+checkpoint(转储到磁盘)+Shadow Master 实时热备(主备之间 Chubby服务进行选主操作)；
						>ChunServer容错：所有副本写入成功才成功；副本丢失则Master重新分配/转移；每个Chunk64MB,划分为1024个Block--每个64KB
					>负载均衡： 
						>chunk副本初始位置: 所在的ChunkServer的磁盘利用率低于平均水平。每个chunk的所有副本不能在一个机架上；限制chunkServer最近创建的数量---避免全部转移到一个空机器上。
						>master定期扫描副本分布：磁盘使用量大的机器上的副本进行迁移；--rebalance
					>删除文件：延迟删除。定期 执行 实际 删除。
					>快照机制：写时复制；	
				>安全建设方面：
				>扩展能力建设：层层封装。
					>
			>TFS系统：目标：存储Blob数据
				>顶级思路：1.小文件组合为块Block，多个逻辑图片文件共享一个物理文件；2.块有副本、分散存储、负载均衡。
					>职责划分、任务拆解、连带功能、强壮建设：
						>元数据管理(文件-块-DataServer映射)、DataServer节点状态心跳检测：交给 NameServer 负责， 高可用部署-主备模式。
						>每个挂载点的管理：一个挂载点 交给一个 dsp进程；一个挂载点一般对应一个独立磁盘；。从而一个DataServer利用 dsp进程 可以管理 多块磁盘；
						>同步追加数据 ：交给 DataServer; 同步流程：先从Master获取可写主副本位置--->写主副本-->再写备副本-->再通知master修改Block版本号-->成功，返回所在的Block id + 文件在Block中的偏移量；<BlockId, Block offset>标志的数据 在所有副本中是一致的。 
					>强壮建设： 
						>图片去重：第三方服务。如Tair
			>FaceBook Haystack: 目标：存储相册数据，每秒更新4000次以上；
				>概念定义：
				>顶层思路：1.多个小逻辑文件共享一个物理文件。2.数据分为：目录、存储、缓存。3.一个物理卷轴对应一个物理文件，100GB,多个物理存储节点上的物理卷轴 组成一个逻辑卷轴--用于备份；
					>职责划分、任务拆解、连带功能、强壮建设：
						>构造URL、维护逻辑卷轴-物理卷轴 之间的关系: 交给 Haystack 目录；
						>根据URL 获取图片：依次交给 CDN, Haystack 缓存，Haystack 存储节点；<逻辑卷轴，物理卷轴，偏移量>
			>CDN: 目标：边缘节点存储 网页图片数据。
				>顶级思路：1.DNS解析时转交给  智能DNS负载均衡系统，后者返回 边缘节点IP 2.边缘节点内部解析DNS得到真正ip并向服务器发起请求，返回的数据进行缓存。3.真正图片服务器：Nginx web集群，也缓存了图片，没有才请求TFS;
					>详细思路/设计/决策、职责划分、任务拆解、连带功能、强壮建设：职责详细定义--业务逻辑--方案路径。
						>单个CND节点内的架构：Lvs -->Haproxy -->squid(SSD/SAS/SATA三级)-->源服务器
		>分布式键值系统：目标：单个k-v的增删查改 
			>Amazon Dynamo: 目标： 简单存储、分布式。
				>各方面的决策和创新和设计：
					>数据分布： 虚拟节点的一致性哈希 。客户端也缓存 整个集群的信息；
					>复制协议：NWR复制写协议；
					>数据冲突处理：向量时钟 
					>临时故障处理：数据回传机制  。T内可以重启的节点的数据 由其他节点归还。	
					>永久故障后的恢复： Merkle哈希树 。T内不能继续服务的节点 则从其他副本进行数据同步。
					>成员资格和错误检测：基于gossip的成员资格和错误检测协议。每个节点 定期向其他节点交换集群信息--来更新自己的集群信息中旧的部分。
				>职责划分、任务拆解、连带功能、强壮建设：
					>写入过程：客户端 根据 数据 计算 出 该key所在的 所有存储节点，选择其中一个为协调者，然后并发向每个存储节点发起写请求；2. 其他存储节点写入成功后回复协调者，失败则协调者重试它，当成功了W-1个时 向 客户端 返回写入成功；
					>读取过程：客户端 根据 数据 计算 出 该key所在的 所有存储节点，选择其中一个为协调者 2. 协调者并发 向R 个存储节点发起读请求， 其他存储节点 读成功 回复 协调者，协调者选择R个副本，如果数据不一致 则选择最新的数据，返回客户端。		
			>Tair: 目标： 分布式kv存储引擎，可持久化；
				>名词定义/概念设计：桶--一个节点上多个桶--每个桶有副本；
				>职责划分、任务拆解、连带功能、强壮建设：将数据节点的状态信息专门抽取出来 维护 
					>数据节点状态维护、心跳检测、维护数据所在的DataServer 映射 ：交给 ConfigServer.部署方式：主备；
					>对外提供数据服务：交给 DataServer;
					>强壮建设： 
						>数据迁移：A上多个桶迁移--完成前在A完成后在B,迁移中有修改则日志保留后一并发给B;
						>路由表更新：ConfigServer推送给客户端，或者DataServer返回给客户端客户端重新从ConfigServer获取；
		>分布式表格系统：
			>Google BigTable ： 目标：1.基于GFS/Chubby 构建(在之上封装)的结构化/半结构化数据存储系统；
				>概念设计：
					>表格：多行；每行多列；每行每列的Cell多版本数据；
						>行主键：64KB以内的字符串；
					>角色划分：
						>客户端：Client  提供bigTable的应用程序接口
						>主控服务器：Master。
						>子表服务器：Tablet Server:
				>职责划分、任务拆解、连带功能、强壮建设：
					>子表分配、子表合并、子表迁移、子表分裂、监控子表服务器状态、子表服务器之间进行负载均衡和故障恢复：Master
					>提供bigTable的应用程序接口:客户端程序库 Client 。缓存子表位置信息。
					>子表格内容的读写、子表的合并和分裂、子表的装载/卸出、操作日志、子表的ssTable数据存储：Tablet Server 。根表-元数据表-用户表：形成类似B+树结构；
					>主控服务器的选举\BigTable 系统引导信息、配合主控服务器发现子表服务器加入和下线、获取BigTable表格的schema信息和访问控制信息：Chubby --核心为 Paxos算法；
				>强壮建设： 
					>复制与一致性：子表 在Chubby上有的互斥锁
					>容错与迁移：Chubby维护了 Tablet Server的信息，迁移数据：操作日志+SSTable .操作日志：<表格id, 行key,日志序列号>
					>负载均衡与子表迁移：子表迁移无需回放日志--因为已经合并；
					>子表分裂与合并：子表-子表锁，子表：memTable+多个SSTable
					>单机存储：插入删除更新增加 是一回事，Merge-dump引擎，先写操作日志再 内存memTable  再 SSTable磁盘刷入； SSTable再合并 为新的SSTable,Minor compaction 对memTable合并，Major compaction 对 SSTable合并
			>Google Megastore: 目标：1.在BigTable上封装出更友好的数据库功能支持；
				>背景/需求/规格指标/结论-路径-思路方向：应用可以按照 用户进行拆分，同一个用户内部 的操作 需要保证强一致性 如 支持事务，多个用户之间则只需要最终一致性；；所以根据用户将数据拆分为不同的子集 分布到 不同的机器上；
					>思路具化-扼要到点/预备概念/概念设计/名词定义：
						>实体组：可扩展性和数据库语义 之间的 一种权衡；同时获得NoSQL\RDBMS的优势。
							>根表： 
								>根实体：根表的一行；存放 用户数据 + Megastore事务 + 复制操作所需的元数据 + 操作日志 。
									>支持单行事务：同一个实体组的数据操作是原子的；同一个实体组的数据包含：根表数据+子表数据，连续存放；所以一般在一个BigTable 子表上，在一个BigTable Tablet Server机器上；除非这个实体组数据行太多 则可能分布在多个BigTable 子表上；
							>实体组子表：
				>顶层思路：顶层逻辑 
					>分层目标-条件-思路/完整思路/每层认知-决策/每层逻辑/业务规则设计/封装下层/分权分责： 
						>底层行动/底层决策/底层逻辑/具体情况-具体行为逻辑：
				>职责划分、职责分配到责任实体、职责进一步丰富-增加、实体内部功能拓展/实体之间联系建设、实体自身和系统整体的强壮建设：定义责任实体-根据顶层思路/分层逻辑，每层都有若干责任实体/决策实体/指挥实体，最底层也有具体的行为实体/行动实体；
					>应用程序到Megastore的接口、映射Megastore操作到BigTable、事务和并发控制、基于Paxos的复制、通过协调者实现快速读、将请求分送给复制服务器：交给 客户端库负责。
						>接受SQL请求并转换为 对底层 BigTable的操作：根据用户定义的Megastore数据模型 转化。
							>写入流程：先写根表操作日志(REDO日志)，再(回放REDO日志)写子表<rowkey,subkey>多行数据；
								>每个实体组的操作日志 同步 到 多个机房/数据中心：paxos协议 方式 实现，保证强一致性。
								>实体组之间：通过 分布式队列 的方式 保证最终一致性 ，或者两阶段提交协议的 方式  实现 分布式事务，保证 最终一致性。 
							>并发控制： 
								>读出流程： 
									>最新读取：单个实体组内完成。读取的是已提交数据-会等待。不会读到未提交事务的数据。
									>快照读取：单个实体组内完成。读取的是最近完整提交的事务版本的数据。不会读到未提交事务的数据。
									>非一致性读取：忽略日志状态，直接读取最新数据，可能是未提交的数据；
								>写事务流程：
									>读最后一次提交的事务的时间戳和日志位置：
									>将写操作聚集到日志缓冲区：对同一个实体组进行，则一个日志位置只有一个事务能写成功，其他事务将中止并重试-写下一个日志位置。客户端提交日志是并行的，但是后台执行日志回放串行的---从而对同一个实体组的事务备串行化了--但是对同一个实体组的事务同时发生的概率很低--所以实际上冲突导致的重试很少；
									>将缓冲区中的操作日志追加到多个机房的Bigtable 集群，通过Paxos协议保证一致性；
										>机器宕机时停止写服务的问题：自动切换 保证 不停 写服务。因为写操作要求强制写到多个集群都成功才成功；所以当master挂了，其他slave判断master挂了则每个都开始发起修改操作？--选举新Master?同时修改这一条数据？---paxos保证只有一个修改成功；			
									>应用操作日志、更新BigTable中的实体和索引：
									>删除不再需要的数据：
							>索引： 
								>局部索引：单个实体组内 寻找数据的索引。REDO日志写成功后，就更新数据+局部索引。。。某列增加索引---等于 实体组中增加 <rowkey, subkey, colum>的主键的行；
								>全局索引：跨多个实体组。在一个新的索引表；新增的行=<tag,rowkey, subkey>
					>跨机房链接数过多的问题-接受客户端请求并转放到所在机房的BigTable实例： 交给  复制服务器 实现。
					>存储每个机房的本地实体组是否处于最新状态的信息-实现快速读： 交给 协调者 复制；
						>记录本机房的每个实体组 是否最新：如果是最新，则读取本地即可，无需 跨机房读取超半数，实现快速读：。写操作 就需要在 更新 协调者的 这个实体组 为 状态失效  后 才写成功。
						>可用性： 
						>竞争条件：
						>追赶：无操作的写---就是用Paxos机制 来让更多的节点的数据更新为集群中最新的，更新的是日志，然后回放日志。
						>可扩展性：目标：线性扩展。
			>Windows Azure Storage: 目标：云存储系统(Blob/table/queue)
				>背景/需求/规格指标/结论-路径-思路方向：
					>预备概念/概念设计/名词定义：概念层次结构、功能结构、职责结构
				>顶级思路/顶层思路：顶层领域，顶层问题。范畴与逻辑。范畴关系	
					>分层目标-条件-思路/完整思路/每层认知-决策/每层逻辑/业务规则设计/封装下层/分权分责： 
						>底层行动/底层决策/底层逻辑/具体情况-具体行为逻辑：底层功能， 层层封装。
				>选任担当/职责划分、任务拆解、连带功能、强壮建设：职责分配到责任实体、职责进一步丰富-增加、实体内部功能拓展/实体之间联系建设、实体自身和系统整体的强壮建设
					>封装过程：自底向上 。开头是 各种具体的小功能 /未协调/未整合/未组织/但它能做什么已经明确了。归纳
						>硬件资源：最底层，各个硬件功能已经很明确、已经设计好了、已经在起作用了。
						>结构控制器：管理硬件资源--依赖硬件资源；提供更加友好的硬件整体服务；调度硬件更加方便。功能包括：
							>功能包括：节点管理、网络配置、健康检查；服务启动、关闭、升级、部署。获取网络拓扑信息、集群物理部署 以及 存储节点硬件配置信息。
						>定位服务|存储区：
							>定位服务：管理存储区、管理用户-存储区映射关系、
							>存储区：是一个集群，10-20个机架组成；一个机架 18个存储节点；
								>三层：
									>文件流层：分布式文件系统 。写操作只是追加。
										>流管理器|extent存储节点：
									>分区层：分区服务器服务 。提供 	强一致性 	并保证事务操作顺序。
									>前端层： 无状态web服务器--分区名到分区服务器的映射 、权限验证等功能。
					>塑造过程：自顶向下 。开头都是一坨泥。演绎。
		>分布式数据库: 目标： 关系数据库的可扩展性
			>背景/需求/规格指标/结论-路径-思路方向：
				>预备概念/概念设计/名词定义：概念层次结构、功能结构、职责结构
			>顶级思路/顶层思路、顶层领域，顶层问题、范畴与逻辑、范畴关系: 1.应用层划分数据，Mysql Sharding ,2. 关系数据库内部自动分片：Microsoft SQL Azure, 3.存储引擎层开始的新分布式数据库：Google Spanner
				>分层目标-条件-思路/完整思路/每层认知-决策/每层逻辑/业务规则设计/封装下层/分权分责： 
					>底层行动/底层决策/底层逻辑/具体情况-具体行为逻辑：底层功能， 层层封装。
			>职责分配到责任实体、职责进一步丰富-增加、实体内部功能拓展/实体之间联系建设、实体自身和系统整体的强壮建设:
				>封装过程： 
				>塑造过程： 
			>数据库中间层：1.目标：数据拆分为分片，分片独立存储在数据库节点上。
				>背景/需求/规格指标/结论-路径-思路方向：
					>概念设计/名词定义：
						>应用端：
						>LVS: 	对客户端进行负载均衡 
						>中间层dbproxy 集群：借助 元数据服务器  获取拆分规则 而 确定SQL语句的执行计划。
						>数据库组：dbgroup ;主备1:n模式； 主机负载写事务和 强一致读；
							>复制：将操作日志以binlog形式复制到备机。
						>元数据服务器：	dbgroup拆分规则 和 dbgroup 选主；。本身的高可用。
				>顶层思路-完整思路： 
					>扩容问题：先数据同步-异步复制，后停写服务，再等同步完成，切换服务规则，恢复写服务。
			>Microsoft SQL Azure: 目标：云关系型数据库，通过 分布式技术 提升 传统 关系型 数据库 SQL Server 的 可扩展性  和 容错能力。
				>背景/需求/规格指标/结论-路径-思路方向：
					>预备概念：
				>顶层思路-完整思路：1.数据划分为多个分区，限制事务只在一个分区执行；2.主备复制协议 来复制数据为多个副本分散存储。
					>概念设计/名词定义：逻辑模型 ：
						>表格组：一个逻辑数据库。
							>行组：表格组总 划分主键 相同的行集合--可以来自 多张表 。同一个行组 可以应用事务。也只支持行组事务。只读事务可以跨行组；写事务只限制单个行组；
							>分区：含多个行组，是 复制、迁移、负载均衡的基本单位。同一个机架同一个故障域，所以分区的副本分布在不同的故障域。
								>主副本：接收读写事务。以操作日志形式将 事务同步 到 备副本。
								>备副本：接收主副本发送的事务日志 并应用 到 本地数据库。
								>分区分裂：对半分裂。
							>全局分区管理器：维护 分区--SQLServer实例 的映射。本身7个副本。
							>协议网关：调用全局分区管理器，获取 分区对应的 SQLServer实例 , 来将用户链接请求转发给 主分区。
							>分布式基础组件：维护机器上下状态；每台服务器上都运行。
					>封装过程： 各层的情况、目标、条件、路径策略决策和采纳。
						>复制协议的封装： Quorum Commit复制协议：存储三份至少写成功2份才算成功；即超半数成功；类似TCC过程；两阶段过程；。写包括：如果落后太多则发送 快照 + 都会发送的 操作日志；逻辑操作日志---而不是 redo&undo日志；
							>消息：都有做校验。
						>容错封装： 数据节点故障，则由 全局分区管理器  转移 分区 ，可能还有 主分区选举。	
						>复制均衡： 节点 负载 过重/过轻 都需要  迁移分区--同时控制迁移频率；也可能需要 主备副本切换--避免原主副本 服务器负载过高。
						>多租户: 资源限制，超过都拒绝服务。
					>塑造过程：
			>Google Spanner: 目标：1.全球级分布式数据库，可扩展到数百个数据中心，数百万台服务器，上万亿行记录；2.外部一致性(同步复制/多版本控制)，支持跨数据中心事务。
				>背景/需求/规格指标/结论-路径-思路方向：通过分布式技术实现高可扩展性 ，并呈现给用户类似关系数据库的数据模型。
					>预备概念： 
				>顶级思路/顶层思路、顶层领域，顶层问题、范畴与逻辑、范畴关系：1.层次化的表。2.基于Colossus--GFS的延续--改进了实时性；3.底层通过分布式技术实现可扩展性，上层通过关系数据库的模型和接口将功能提供给用户；
					>概念设计/名词定义：概念层次结构，从底层到高层。
						>目录表：
							>目录：一个用户的信息 +  这个用户下的所有 业务数据信息；。同一个目录的数据存放在一起，其副本甚至也在同一个机器上；针对目录的读写事务 基本不设计跨机操作；
						>Universe: 一个Spanner部署实例；开发测试线上各一个；支持多数据中心部署。
							>Master: 监控Zone级别的状态信息
							>Placement Driver: 跨Zone数据迁移功能。
							>Zones: 一个数据中心多个Zone,一个Zone内部通信代价低，之间通信代价高。
								>zoneMaster:
								>Location Proxy: 提供数据位置信息服务，维护 数据--spanner server 映射关系。目录--SpannerServer 映射关系；
								>SpannerServer: 提供存储服务，功能上相当于 Bigtable 系统中的 Tablet Server;
									>服务多个子表：每个子表 包含多个目录；
									>读写简要过程：先从Location Proxy 获取目录所在的SpannerServer, 再 从 Spanner Server读写数据；	
									>子表：一个机器上100-1000个，在多个数据中心有副本；。每个子表一个 Paxos状态机---paxos协议会选择一个副本作为主副本-10s寿命，实现跨数据中心的多个副本之间的一致性；
										>目录： 
									>锁表：主副本所在的SpannerServer上有， 用于 并发控制；读写事务 操作 子表上的目录 ，就通过 锁表  来避免 事务之间干扰；
										>实现了 单个Paxos组内的单机事务：
									>事务管理器： 主副本上；一个事务跨多个Paxos组，就需要事务管理器来协调；(实现跨多个Paxos组的分布式事务---通过2pc实现：一个paxos组的主副本为协调者 其他paxos组的主副本为参与者)
						>全球时钟同步机制：TrueTime--基于GPS/原子钟
							>主时钟服务器：每个数据中心若干个---GPS/原子钟，其他机器上部署从时钟进程 来每隔30s从若干个主时钟服务器同步时钟信息，
					>封装过程：
						>强壮建设封装： 
							>复制与一致性： 
								>目录是数据分区、复制、迁移的基本单位：一个paxos组多个目录，目录可以在多个paxos组之间 移动；
								>spanner移动目录的原因：paxos组负载太大--需要切分；2.将数据移动到跟用户更近的地方--减少访问延迟；3.把经常一起访问的目录放进同一个paxos组；
							>并发控制： 
								>支持事务：单个paxos组
									>读写事务： 获取 时间戳-->执行读写操作-->提交事务 其中事务版本为 获取的时间戳。
									>只读事务： 
									>快照读：客户端提供时间戳 或者时间范围。
										>指定了读事务的版本：而不是取 系统 的当前时间戳；
								>支持事务： 多个paxos组 ;执行两阶段提交协议；
									>读写事务： 客户端发送数据到多个Paxos组的主副本-->协调者主副本发起prepare，参与者则锁住要操作的数据--->协调者主副本 发起 Commit协议，参与者提交数据 并解锁 操作的数据；事务的提交版本--为 协调者的当前时间戳；
									>只读事务： 阻塞读，保证读已提交。
									--延迟提交：考虑TrueTime后等e后提交；
					>塑造过程：
		>OceanBase 架构 初探：目标：可扩展的关系数据库 ，支持数百TB数据量 和 数十万 TPS\数百万 QPS的访问量。2.支持跨行跨表事务；
			>背景/需求/规格指标/结论-路径-思路方向：
				>预备概念/概念设计/名词定义/观念宇宙/领域概念/世界认知/独特的理解模型/独特的塑造模型：
					>对数据的认识和处理：按时间线 划分 为基线数据  和  增量数据；
						>基线数据：只读
						>增量数据：所有的修改 更新 到 增量数据中；系统内部通过 定期 合并操作 将 增量数据 融合 到基线数据中；
						>查询流程：基线数据 + 增量 数据
					>表格：	主键 B+树索引，叶子节点为范围数据--子表一个
						>子表：256MB, 按主键范围划分的数据分区。3个副本，分散在多个ChunkServer存储；。可以分裂和合并。负载均衡和任务调度的基本单元。
							>根表：一级索引 主键-子表-ChunkServer  。查询时 先定位到 子表；
							>SSTable: 多个。数据按主键有序存储；所以查找数据按照二分查找；.2级索引：块索引和行索引。列式存储,存的是列组. 可能一个列属于多个列组...可以提高存储压缩比
								>块：一个SSTable多个块4KB-64KB。数据压缩单位。支持压缩算法：LZO,Snappy
					>增量数据的结构：内存中的B+树，也会SSTable化而保持到SSD			
			>顶层思路-完整思路-共性问题的回答/特有问题的回答：1.老思路：分库分表----缺点：添加机器需要人工介入/范围查询吃力/未充分利用SSD能力
				>概念设计/名词定义：基线与增量，划分与合并
					>主控服务器：RootServer 。主备部署。主动和 MergeServer&ChunkServer保持心跳；
					>更新服务器：UpdateServer。 单台部署/主备部署(租约机制选择唯一的主UpdateServer)，记录一段时间的修改增量；从而在没用分布式事务的情况下 实现 跨行跨表事务；修改增量 定期 分发 到 多台基线数据服务器中；
					>基线数据服务器：ChunkServer 
						>存储多个子表：提供读取服务。定期合并和数据分发。
					>合并服务器：MegeServer 。解析请求 生成 逻辑查询计划+ 物理查询计划：得出要查哪些子表 分别在哪些 ChunkServer, 然后请求这些ChunkServer, 写操作则还要转发给UpdateServer ,
						>解析其实容易实现： 就是 笛卡尔直积 + 过滤分组排序 。
							>执行计划猜测：1.先确定 表 + 主键范围/索引列范围--->发送给子表对应的UpdateServer/ChunkServer -->单表数据 利用子表索引+列选择 得到 数据 并 基线数据和更新数据合并 ， 且用 得到 笛卡尔直积 的 两张表的数据，然后执行连接 --->然后执行过滤、分组-统计、筛选列、排序--->最终结果再返回客户端。
								>子表数据的完整查询：在ChunkServer上	有 CS-SQL 模块 独立计算；并且 ChunkServer自己 从 UpdateServer读取子表的增量。
									>分组|去重：group by | distinct 都可以用 排序 来间接实现。这样 一个序列 无需 经过任何容器就可以实现分组和去重。
									>连接：inner join | left join 都可以用 排序 来简化实现。先对两个表分别对连接key 进行内部排序，然后开始连接。hash join ,则是计算列的hash K然后放到 桶k中，这样两张表的数据可以在同一个K桶中关联起来，
									>子查询：往往内外两张表 是有key关联关系的，从而可以转为 连接 。
								>多个子表数据的合并：MergeServer完成。MS-SQL
							>查询优化器：逻辑查询计划的改进和物理查询计划的选择；
							>多版本并发控制：并发修改：并行提交修改任务到队列---类似日志占位，然后单线程 拉取任务来执行更新。
							>事务：已提交行操作链、未提交行操作链。
								>事务版本号：只读事务 读取 它的版本号最近的已提交版本号的行数据.
								>锁机制:行锁.	
						>无状态设计：
				>职责划分、任务拆解、连带功能、强壮建设：
					>管理集群中所有服务器、子表数据分布(主键范围-子表-chunkServer映射)、副本管理(定期负载均衡)、故障检测(ChunkServer)： 交给 RootServer, 主备部署--HA实现高可用性--共享VIP--IP漂移；强同步  
					>存储增量更新数据： 交给 UpdateServer。主备部署，共 RootServer 部署 一台机器。
					>存储基线数据：交割给 ChunkServer. 存2-3份。
					>客户端访问系统的接口实现、接收和解析用户SQL请求(词法分析、语法分析、抽象语法树构建、查询优化) 后转发 执行计划 给 ChunkServer/UpdateServer 如果多个 则 要合并结果: 交给  MergeServer 
					>强壮建设： 
						>多机房部署：每个机房 一套 R U C M .主备方式。
							>数据同步： 主集群的 主UpdateServer 往备集群 同步 增量更新操作日志 实现；
						>客户端读写请求：先请求RootServer 获取 集群中的 MergeServer的地址列表，再定位到 MergeServer ，失败则选择另一个MergeServer重试，
							>MergeServer读取过程：先确定子表所在的ChunkServer, 接着 发送请求，得到基线数据，接着请求UpdateServer得到增量数据，更新基线数据后，得到最终结果；
							>写请求专门：找到主UpdateServer, 主UpdateServer 先写日志、再内存表，再转SSTable快照文件，再合并SSTable, 持久化；再发送日志同步给备UpdateServer
						>升级备集群程序版本：
						>定期合并、数据分发：增量分发到 ChunkServer, 
							>过程：UpdateServer 产生快照--->通知RootServer数据版本发生了变化-->RootServer通过心跳通知ChunkServer--->ChunkServer从UpdateServer获取每个子表对应的增量更新数据；ChunkServer应用更新数据到本地内存中；--->定期多路归并：ChunkServer将基线数据和更新数据合并并放到新的SSTable 
						>强一致性和跨行跨表事务：修改：操作日志先备，后本地磁盘后本地内存；
							>单点部署：所以支持跨行跨表。
						>数据正确性：数据存储校验、数据传输校验	、镜像校验、副本校验
						>
		>分布式存储引擎：目标：1.数据分布、负载均衡、容错、一致性协议。2.支持根据主键 更新/插入/删除/随机读取/范围查找；
			>全局内存池：可以统计每个模块内存使用情况。
		>数据库功能：
		>质量保证、运维及实践：
			>系统表:
			>压力测试: 2倍数据和QPS
			>Benchmark测试: 基准测试,基本的语句的性能并发.
			>兼容测试: 兼容以前的接口.以前的接口能正常工作.C++开发的.不适合存储图片\视频等非结构化数据.
			>应用: 淘宝收藏夹\天猫评论,   功能特征: 1 item - select n subitem key1,key2 - sort key3,key4
				>例如: userid -- products -- sort by 价格,     product id --- comments --- sort by date ;    search words --- products -- sort by 价格 
		>云存储: 目标: 1.将 大量普通的存储设备的存储资源/数据服务 组织起来 通过 网络 以统一的接口  按 需 授权给 用户.
			>Amazon S3 : 对象存储服务.存储 照片\视频\音乐
			>多租户共享:
			>一层层封装技术问题\技术需求: 
				>容错问题\扩展问题\可靠性问题\可用性问题:  机器扩展问题\机器故障问题\网络故障问题: EC2 , EBS,S3,RDS,SQS,CloudFront 
			>云平台: 应用运行平台 + 云存储 
				>结构: DNS-->CDN节点 -->路由负载均衡-->计算实例组/计算实例--->存储访问层-->分布式存储层(分布式文件系统/对象系统,分布式键值系统，分布式表格系统，分布式数据库)；  公共服务：消息、缓存、用户、权限、安全、计费、监控、运维
		>大数据:
			>重点可能：不是数据挖掘，而是 展现 有趣 的内容，解决用户的问题/需求。而是生成 内容；
			>Google Tenzing: 将SQL查询转换为MR任务。
			>Google Pregel : 图模型 迭代计算
			>Google Dremel: 实时分析系统