---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。
>学习要有战略和格局：过于零散、孤立、末支、叶子的知识点就没必要化太多时间，懂得放弃。聚焦在根节点、枢纽节点、产生分支的节点、主干知识点、枢纽知识点、关联到很多知识点的知识点上。
 >对于主干知识点：要有全新的认识视角和进行精细的描述，大量的提问和好奇和进行必要的实验以获取支撑观点/导出观点的数据。
 >规范是工程最独特的特征.
 >慢慢读：
>一种新技术的学习：
 >它面对的情况和问题、它的世界观、它的方案、它的方案验证/论证/能处理的解决的所有情况及能成功处理的理由/功能边界
  >所有的软件：都可以看作是向上封装一层接口，根据自己的世界观封装底层而向上/对外提供统一的(统一的更简单的更直观的更业务的更少底层信息的)接口，底层包含一系列的第三方的插件/构件/组件；内部则去做兼容和调用(对底层)(对上层则做逻辑分解和底层实现)。

>阿里巴巴的观念：
	>自己遇到的解决的痛点，也是别人会遇到的需要解决的痛点。则将自己的经验能力提取出来，做成通用的服务，互联网服务，通过网络都可以享用到这个服务。从支付服务，服务自己，到服务别人--服务更多的商人--通过卖服务能力赚钱，且首先这个服务能力先服务自己，服务做好自己体验好之后，再拿出来服务更多的人。这就是阿里巴巴的服务理念。让大家通过网络来享用到自己亲自体验过的强大的丰富的各种服务能力，这就是阿里巴巴的服务理念--梦想和信念。通过网络，也可以说通过手机、pc浏览器；甚至说屏幕。因为网络也是服务能力，手机也是服务能力，因为可以当作服务的中间媒介---通过这个媒介享用到真正要享用的服务。
  
>举动-痛点：企业IT架构转型。业务特性:复杂的商业系统，海量的请求和数据。业务要求：很高的稳定性(不能停止服务和服务不稳定)和可靠性(不能丢失结果)。不能错、不能停、不能单机。
	>主要挑战：高可用、海量、复杂的业务逻辑。大规模数据的线性可扩展问题：存储和访问两个方面。
	>主要挑战2：复杂业务系统的解耦问题。迭代速度、发布速度提高。保持业务相对隔离来让工程师大规模并行工作，同时要满足海量访问和高性能的要求。
	>主要挑战3：全球进行分布式部署、99.999%以上的高可用、容灾等。应用需要在运行时可以全程进行动态感知和管理，要有全部的监控能力，和根据业务流量进行业务的优雅降级，确保系统的高可用。
	>解法/处理思想：解决上述挑战和问题：发展了一系列的中间件来支撑这种新的架构。实用的技术方案。
		>电商架构：烟囱式架构-->分布式架构-->共享式架构。
		>避免不断重复建设：数据可以重复使用，服务也要可以重复使用。来提升生产效率。
		>系统的建设：从生产型模型到运营型模型，从版本模型到迭代模型；来沉淀所有的积累。生产型的模型的逻辑来自：几个人的脑子，而运营型模型的逻辑则来自：无数客户、供应商、工程师的脑子。
		>企业级互联网架构：重构企业IT架构(如短期内重构供应链、SCRM等平台)(打造企业全渠道分销平台)
			>企业业务模式：
			>共享服务理念：
				>共享服务体系:	技术平台支撑。项目落地的过程。服务重用。
					>技术框架的选择：
						>服务化框架：
							>需求背景：单机war功能太多-协调问题大业务响应慢，内容越来越复杂、错误难于隔离--变化大的模块和小的模块都在一起, 数据库连接能力难以扩展---每个应用实例的连接池大小压缩到10个--但实例多而给数据库的压力超过5000/s ---过高的连接数使得数据库处于不稳定状态。应用扩展成本高---增加实例来分担服务负载--有的模块调用量高有的其实很低。
							>服务的切分/剥离/改造：某个模块如果业务逻辑相对独立、简单、复用率也高，如用户中心。接着剥离 交易中心、类目中心、商品中心、店铺中心。
								>切分规则：不同模块间进行清晰、稳定的服务契约定义，保证对外服务的接口定义不发生变化。降低不同模块开发团队间的协同成本，内部迭代开发速度也更快。降低系统之间的耦合度，业务更单一专一--提供更加专业和稳定的服务。避免了个别模块的错误给整体带来的影响。解放了对单数据库连接数的能力依赖--因为有独立专门的数据库。精细粒度的扩容，增大集群。
								>SOA特征：面向服务的分布式计算；服务间松散耦合；支持服务的组装；服务注册和自动发现；以服务契约方式定义服务交互方式；。。可以实现异构系统之间的交互。
									>点对点稳定性的解决：负载均衡，多版本支持。
									>中心化：需要连接ESB，ESB又要转发，网络开销大;网络带宽要求也大;升级麻烦；雪崩效应--一台企业服务总线实例出问题拒绝服务则剩下的实例压力更大。淘宝：立即下单---后端调用了200多个服务(主动调用或者发送消息)：但订单创建成功只用了200-300ms
							>分布式服务框架：HSF:High Speed Framework
								>架构设计：
									>服务提供者：真正服务功能的实现的应用实例。
										>集群部署；保障高可用性。
										>运行在：阿里巴巴优化定制后的Tomcat容器中。Tomcat运行在虚拟机/docker容器中; 一对一对一的关系。
											>定制Tomcat容器：容器层已经集成HSF服务框架对服务提供者或者服务调用者进行配置服务器的发现、服务注册、订阅、失效转移等相关功能；不管进行服务提供者开发 还是  服务调用者开发，只需要进行服务相关的配置操作，应用中无需引入任何HSF相关的jar依赖包。
									>服务调用者：也是war包运行在定制的Tomcat容器中。
										>对于C/C++,PHP,Node.js等语言开发的服务调用者：。
									>地址服务器：给服务提供者和服务调用者提供部署环境中所有配置服务器和diamond服务器的服务器列表信息，通过Nginx来提供这个服务能力。配置服务器集群、diamond服务器集群信息都设置在地址服务器上。
										>部署多台：提供负载均衡和高可用服务。
										>访问方式：通过统一域名的方式访问这些地址服务器，通过DNS轮询，实现地址服务器访问的高可用性。
									>配置服务器：记录环境中所有的服务发布(服务提供者的ip地址+端口信息)和服务订阅(服务调用者的ip地址和端口)，并将服务相关信息推送到服务节点上。
										>推送效率：将服务发布和订阅信息保存在内存中。
										>和服务提供者和调用者之间的连接方式：长连接。心跳监控运行状况。当服务提供者故障时自动推送更新后的服务提供者列表给相关的服务调用者端。。
											>推送特性：保障淘宝平台实现单元化(某个客户访问淘宝时，请求路由到某个淘宝机房后，在淘宝上的所有业务操作都可以在这个机房完成，无需访问其他机房的服务)、异地多活。
										>部署多台：用于服务发布、订阅、推送的负载均衡。多台之间：实时数据同步，保障服务发布和订阅信息尽快同步到各服务节点上。
									>Diamond服务器：类似zk,统一配置管理服务。给应用提供统一的配置设置和推送服务。
										>保存配置规则：服务调用安全管控规则、服务路由权重、服务QPS阈值。持久化到后端Mysql中。
										>生产环境：多台，提供负载均衡服务。
										>场景：
											>白名单设置：设置某些服务或者服务中的方法只能让特定IP地址的服务器调用。
											>控制服务是否能够调用：通过用户认证的方式。
											>服务器权重设置：服务调用者对多个服务提供者服务节点的访问。
											>设置服务的QPS能力上限值：一旦该服务的QPS达到该阈值，则拒绝服务的继续调用。实现服务限流。保障平台稳定性。
										>服务节点定时从Daimond服务器上同步相关配置信息。使得规则立即在服务运行环境中生效。
								>工作原理：规则：
									>服务节点对配置服务器列表的获取：服务调用/提供者 如何获取到 配置服务器 和 Diamond服务器的列表？ 就是通过在Tomcat容器启动后 以 域名 的方式 先获取到可用的地址服务器，再向地址服务器发送请求服务器列表信息，从而在容器启动完成时，就已经在该服务节点上获取了配置服务器和Diamond服务器的ip列表信息。
									>服务的注册发布：服务提供者节点，获取到配置服务器ip列表后，向配置服务器发送 当前应用中包含的服务提供者信息(在应用的配置文件中：服务的接口类全名、服务版本、所属服务组；当前服务器ip:port), 进行注册发布。启动后就注册完成了。
									>服务的订阅：服务调用者节点，获取到配置服务器ip列表后，向配置服务器发送 服务消费者相关信息(服务的接口全名、服务版本、所属服务组)，配置服务器收到后则从内存中匹配对应的服务提供者的ip:port列表，返回给调用者。
									>服务规则的发送：先在Diamond服务器提供的规则设置页面you，对指定的服务提供者和调用者设置相关的规则，保存后，就会推送到相关的服务节点上。
									>服务交互：服务调用者从已经在启动时获取而保存在本地的服务提供者列表：随机选择其中一台进行服务请求的发送，调用。
								>特性实现：
									>高效交互：
										>网络通信框架：Netty。多路复用的TCP长连接。一个连接交替传输不同请求的字节快。避免反复建立连接的开销，也避免了连接的等待闲置，从而减少了系统的连接总数，同时避免TCP顺序传输中的线头阻塞head-of-line blocking问题。
										>数据序列化协议：Hession。数据量较小时，性能表现出众。精简、高效，跨语言。充分利用Web容器的成熟功能，处理大量用户访问时有优势。比Java序列化开销缩短20倍，。正确率和准确性。性能和稳定性。TPS>10w性能和效率高。
									>容错机制：
										>错误类型：服务器故障(宕机、网络问题)
										>调用者调用时的处理：设置到期时间，收到调用失败的反馈，然后从服务器列表的剩下的服务器中选择一个来进行请求。重试其他服务器。而服务提供者本身的故障会被配置服务器感知，而配置服务器和调用者之间会进行服务调用者同步--推送的方式进行同步(来避免下次重新调用有故障的机器)。。。		
									>高可用：
									>扩展能力：服务能力的可扩展性。服务的业务处理能力随着服务器资源的增加而线性增长。
										>传统架构：程度不一的出现当服务节点数量达到一定量后，出现阻碍平台服务能力扩展的问题。原因在于：网络传输瓶颈，或者服务节点接入数量上的限制。ESB雪崩为例子。
										>扩展条件：当服务面对较大的服务调用压力，服务提供者的各个服务器的水位(cpu/mem/io)比较高时，就需要通过增加服务节点数量的方式提升该服务的服务处理能力。
										>扩展动作：增加该服务的提供者实例数, 新增的实例启动完成，则会几秒内注册发布到配置服务器，配置服务器又同步推送到服务调用者。例子：阿里巴巴共享服务的每个服务中心，双11各自部署的服务实例节点数量超过2000.
								>微服务架构的典型特征：
									>分布式服务组成的系统：多个分布式的服务组成，系统之间的交互：采用服务调用的方式。
									>按照业务而不是技术来划分组织: 
									>做有生命的产品而不是项目: 做产品的方式让服务在业务发展过程中快速演化。
									>智能化服务端点与傻瓜式服务编排：能力向服务端迁移而不是集中在ESB中。
									>自动化运维：对整体服务的运维管控和平台高可用性和稳定性方面的提出的要求。
									>系统容错：
									>服务快速演化：
									-容器化的好处：便于移植。
									-划分边界并非越多越好：运营和维护麻烦。
								>微服务化的应用架构如何进行有效的服务管控：服务链路跟踪、链路分析、实时业务指标的监控等问题。
									>分布式事务难题：需要针对业务的需求在事务一致性和性能之间做好平衡。一套成熟、稳定的分布式事务解决方案 在微服务框架构建中是需要首先考虑的技术方向问题。
									>自动化运维和平台稳定性：服务器数量和服务交互复杂程度都上升到了新的级别，
								>微服务的服务设计：
									>服务边界划分：从业务的维度,以什么样的服务颗粒度定义服务？
									>以什么样的数据模型支撑服务能力的线性扩展？
									>如何保障设计出的服务具有很好的业务前瞻性？
									>高效满足现有业务需求的前提，保持整个服务能力的通用性，为接下来其他业务的服务接入提供业务的扩展能力。
								>原有组织架构是否满足微服务架构持续发展的需要：服务强调持续的演变，服务为中心的持续运营。
									>建设微服务体系架构：难以靠一个项目就实现，服务能力在业务发展过程中不断沉淀，沉淀到一个阶段则体现出长远价值。。。质量控制：是开发测试软件+工业软件+稳定性和演练平台？
							>共享服务中心建设原则：
								>服务能力的两个层次：
									>底层Pass能力: 解决大型架构在分布式、可靠性、可用性、容错、监控以及运维层面上的通用需求
									>业务服务能力：提供云化的核心业务支撑能力，它的好与坏决定了是否能真正支持上层业务达到敏捷、稳定、高效。
								>有关建设共享服务中心的问题：
									>淘宝的各个能力中心怎么建设的、服务中心的边界是什么、划分的原则和标准、服务中心多大合适、对应的组织团队和流程应该怎么保障、服务中心里的服务数量和粒度
										>四大服务中心：独立发展，必然最后还会拆分出新的中心-只是业务发展时间问题。
											>用户中心：统一用户体系，统一用户数据、存储和服务接口。此服务被上层调用最频繁。复杂程度和重要性。
												>解耦后：方便用户大数据分析，简化了使用。提升了响应需求的效率，服务的稳定性提高和扩展能力提高。
											>商品中心：商品数据量大。
												>商品描述能力：商品的描述数据模型：类目属性体系-SPU-SKU等，统一的灵活的易于使用的商品数据模型。商品的存储模型--商品数据在数据库中的存储结构；三是对外提供的接口，操作商品数据的接口。
												>商品发布能力：B端商家通过OpenAPI直接对接现在企业的商品存销系统；C端小商家通过浏览器进行发布，C端大商家用c/s客户端的方式发布；无线端用APP或者手机扫码发布。服务中心提供：服务接口和发布工具。
												>商品管理能力：量10亿级别，每天更新，量也大；商品类目常变化。
												>商品巡检能力：判定为非活跃的不再经营店铺的卖家 从活跃商品库中剔除，节省计算和存储资源，用户体验。商品描述词，要发现违规的描述和随意违反认知的文字。
												>商品数据分析能力：针对商品的大数据分析。
												>商品评价的能力：评论商品和卖家，识别正确的评价，剔除恶意差评和好评，建立公平的评价体系。		
											>交易中心：交易业务。购物车、交易流程、订单管理、支持、结算、营销等。独立出库存中心和营销中心。
											>店铺中心：卖家店铺管理、店铺装修、店铺生命周期管理、店铺日常管理。第三方店铺装修市场。
											-物流中心：
											-营销中心：
											-数据服务中心：
										>划分原则：业务拆分降低系统的复杂性；业务共享提供可重用性；服务化达到业务支持的敏捷性；统一的数据架构消除数据交互的屏障。
											>设计、运营、工程：
											>高内聚、低耦合：剥离出来的中心如果业务还没有发展到丰富到不止是 增删改查 而已，则不建议一开始就独立出来服务中心。
											>数据完整性原则：数据模型层面的 高内聚和低耦合。
											>业务可运营性原则：服务中心承载业务逻辑、沉淀业务数据、产生业务价值的业务单元。上层的业务需求，业务内部的创新想法-淘宝基于大数据分析生长起来的商品巡检技术、前台类目自动聚合推荐。数据模型统一之后，把大数据技术引入服务中心的架构中，让数据来源、数据分析、业务生产自然形成闭环。
											>渐进性的建设原则：敏捷实践。但要降低风险和实施难度。
										>新的问题：分布式事务、问题排查是新的问题
									>数据拆分实现数据库能力线性扩展：数据库最容易产生性能瓶颈。数据库能力的扩展。
										>数据库分库分表：利用分布式数据库平台解决数据库瓶颈问题。数据库改造过程中 如何进行数据分库分表设计的最佳实践。
											>数据垂直分区：
											>数据库的读写分离：主数据库处理事务性的增、删、改；从数据库专门负责处理查询操作。而数据库的后台则会把事务性的操作导致的主数据库中的数据变更同步到集群中的从数据库。
												>但是写入能力：没有提高。且单表数据量有限，过多的数据会导致数据库性能显著下降。此时开始：水平拆分/分区；
											>数据库水平分区/拆分：按照用户ID将用户数据hash取模的方式实现用户数据平均分布在若干个数据库中。单个数据库中的数据量从而是能提供良好的读写性能的。
												>拆分数据/路由数据不复杂：查询时带条件筛选也并行，而困难在：排序、分页、join跨库连表、事务操作。数据的统计排序。
													>分布式数据库：配套的有：平台和工具。
										>阿里巴巴分布式数据层平台：
											>数据访问层：做功能的封装和管控；分库分表的逻辑和数据的跨库操作都交给应用的开发人员。
												>Cobar: 关系型数据的分布式处理系统。不支持跨库的连接、分页、排序、子查询。不支持savepoint,jdbc不支持重写批量语句；
												>TDDL: 分布式数据层框架。分布式数据层中间件。支撑阿里巴巴上千个应用，平均每天SQL调用超千亿次。定位在：数据库和应用之间，增加对SQL的解析-实现更精准的路由控制，以及对跨库join、统计等计算的支持。跨库数据聚合、子查询、group by 、 order by。是一个完整支持SQL语法兼容的平台。spring jdbc template --> TDDL matrix Ds --> TDDL Group Ds ---> TDDL Atom Ds(with jboss ds) ---> JDBC Driver --->Mysql/Oracle
													>三层：每层都按JDBC规范实现，使得对前端应用没有任何代码侵入。
													>Matrix层实现分库分表逻辑：底下持有多个GroupDS实例。
													>Group层实现数据库的主备/读写分离逻辑，底下持有多个AtomDs实例。
													>Atom层实现数据库连接等信息的动态推送，持有原子的数据源。
													--优点：
														>数据库主备和动态切换
														>带权重的读写分离
														>单线程读重试
														>集中式数据源信息管理和动态变更
														>支持Mysql和Oracle数据库
														>基于JDBC规范，很容易扩展支持实现JDBC规范的数据源。
														>无Server|client.jar形式存在，应用直连数据库
														>读写次数，并发度流程控制，动态变更。
														>可分析的日志打印，日志流控，动态变更。
												>新一代分布式数据库产品：DRDS。相比TDDL，在业务场景的支持、故障的定位、运维管控等方面又有了一个全面的升级。阿里云上用于解决关系型数据库线性扩展问题的标准产品。
													>数据平均拆分：否则产生数据访问热点。
														>订单数据表的结构：一个用户买了2个商家的两个产品。订单ID取模拆分？
															>主订单表：主订单id,买家Id，状态，金额，创建时间,,,
															>子订单表：主订单id,子订单id,金额，状态,,,,(可能精确到商家/卖家)
															>订单详情表：订单id,卖家id,商品id,金额，数量，...(优惠信息)
													>尽量减少事务边界：次优先考虑。因为i单机数量大了更不好处理。
														>DRDS: 对每条sql都带有分库分表键：根据sql中的参数？取模而路由？而转发到对应的数据库。
														>事务边界：单个SQL语句在后端数据库上同时执行的数量。边界数量大的弊端：
															>系统的锁冲突概率更高：多个这种SQL并发执行。
															>系统难以扩展：这种SQL导致整个平台的数据库连接数量是取决于后端单个数据库的连接能力---而不是后端数据库各个之和。
															>整体性能越低：分布式数据层，如果聚合性质的工作太多，会占用较大的内存和CPU计算资源。导致整体分布式服务的处理性能受到影响。条件一定是高频全表，低频不必。
																>异构索引手段：避免全表查，跨表join;提升数据库服务性能、处理吞吐能力。或者考虑其他平台：Hadoop离线分析，或者内存数据库/HBase平台。
																	>具体实现：(同一个语句的数据，先后使用不同键分库分表-即存2份)(来应对不同的查询，如查某个用户的所有订单和查某个订单)就是之前redis阅读时了解的：一个用户的购买的所有商品都保存在一个数据库里，而按照用户Id分片保存在多个数据库。这就是以空间换时间的做法。例如按照订单ID取模来分散存储到不同数据库时，同时按照订单中的买家ID 取模建立索引表--以买家ID为分库分表键。用两次效率最高的SQL请求代替之前需要全表扫描的问题。但是不建议数据全复制--每个索引都保存完整的数据。
																		>数据库层采用数据复制方式实现异构索引：精卫项目。数据的异构复制。精卫是一个基于Mysql的实时数据复制框架，通过图形界面配置实现异构数据复制的需求。这里用来同步异构索引数据。也认为是一个Mysql数据触发器+分发管道。
																			>精卫：数据实时同步复制的统一平台。平台，所以大家都可以申请接入。web配置页面-配置数据同步的事件类型(增删改)-不同数据源之间数据同步(表字段可以映射)。
																				>抽取器Extractor: 订单在Mysql中创建成功，则binlog中有日志记录--因为binlog会对更新或插入的sql记录下来--二进制形式保存在磁盘；精卫获取日志并转换为event对象
																				>管道：管道中有过滤器对数据进行一些过滤操作，如字段过滤、转换等 或基于接口开发的过滤器来对event对象中的数据进行处理，最终通过分发器将结果转换分发给DRDS的SQL语句。
																				>分发器：将结果转发给DRDS的SQL语句。
																				-精卫的实现功能：
																					>数据的同步：从单线程管道任务模式，到多线程管道。管道更多了。可以索引更多字段的分发。如果顺序要求不需要，那么还可以一个字段就分发到很多管道。
																						>顺序的保证：因为Extractor只有一个，所以只需要在Extractor上对event进行对线程数取模，来找到要分发给那个线程；那么一条记录的所有修改，都会经过同样的线程。
																					>数据的安全：同步任务效率最大化，同时保证服务的稳定和数据的安全。	
																						>平台稳定性保障：避免因为资源抢占和某些同步任务异常影响周边任务，支持多个服务节点作为任务执行的集群，通过统一的任务调度系统(zk集群)将任务分配到集群中的各个节点并行执行。设计规范2：每个同步任务为独立java进程，出现异常该任务自动停止。任务调度系统定期轮询任务列表，将还没有执行的任务调度起来。
																						>心跳+报警：服务节点和zk之间心跳：zk来感知节点运行状态和任务完成的位点，如果两者有异常则会触发报警。抽取器和分发器发生任何错误复制任务立即转变为STANDBY状态，而集群中的机器上的服务感知后会重新将任务启动，继续复制任务。
																						>Mysql主备切换：采用手工的方式进行同步任务的恢复。查看新主库的当前位点PR状态，查看老主库拉取的新主库的位点PR状态，如果PR>PA，直接采用新主库的PA切换到新主库上读取。
																							>自动化的方式：binlog里的serverId和时间戳，发现dump的binlog中的serverId发生变化记录变化时间戳，后再在给定Mysql服务器上查找同样变化的的数据库，根据探测到的serverId发生变化的时间戳进行回溯，在新的机器符合条件的位点进行dump。
																						>Mysql异常挂掉：利用数据库上binlog文件的修改时间，手动进行整个文件的回溯。
																							>在服务器上找到服务挂掉的时间点。
																							>到新的主机上查看找到服务挂掉时间点之前最近的binlog文件。
																							>从这个文件的位点开始进行回溯。
																					-任务管控：定时轮询zk集群中对应任务的节点。
																						>心跳监控：
																						>延迟堆积监控：
																						>任务状态、数据监控(TPS、异常等)：
																		>应用层实现异构索引数据的创建：带来分布式事务的问题。
																>倒排索引手段：比如淘宝商品搜索、高级搜索。不能再全表查询，因为全部用户都会进行、调用频繁，对数据库整体性能和数据库连接资源有巨大压力。
																	>将多条件频繁查询引入搜索引擎平台：即不采用数据库的方式，而是专业的搜索引擎平台。将数据离线dumper到indexer,再searcher来搜索。全文索引平台。
																		>OpenSearch:类似solr,es；但数据同步(从数据库到搜索引擎)、索引创建算法、查询执行计划、排序算法等方面针对商品搜索场景 做了调整和功能增强。
																		
																		
							>异步化和缓存：
								>业务流程异步化：服务异步调用，来让业务流程中业务逻辑允许同步执行的服务被同时调用，来解决大量远程服务线性/顺序调用带来的性能问题。
									>业务请求的组合调用：淘宝交易订单-调用200个服务，不能都在一个JVM中顺序调用。只能顺序的则顺序执行，可以异步化的异步执行。最好最后一步全是可以同步执行的。
									>消息中间件(代理分发)实现业务异步化：对于两部分逻辑处理没有事务关系的，即第一个的成功，不要求第二部分的所有一定成功后才成功。
										>贷款按月还款的场景：加入一个贷款人有500个借款人，则一次还款有1500次数据变更记录//但毕竟还款人之间是可以并行的。
											-用户点击还款。生成还款启动消息发送到消息中间件。
											>还款开始：找到未还款计划，写入还款请求，发送“还款计划计算消息”
											>还款计算：还款总额的计算，占款，发送支付流程的消息。
											>还款计划分派：给平台的账号转账，发送分期支付消息；(这里的消息是对所有的还款人生成还款计划处理消息，并行的500个事务)
											>还款计划处理：收到详单支付请求；详单查找，计算还款详单，从借款人账户扣占款，发送还款详单处理的消息。
											>详单处理：给还款人账户加钱，更新详单表信息。
											-程序异常：对业务的回滚或者重试。
								>数据库事务异步化：上述还款过程就是事务异步化。
								>数据分库分表后、数据进行异步操作：采用什么事务处理方式来实现事务一致性和数据库处理性能的平衡。
									>业务事务一致性问题：
										>CAP: 对于一个分布式系统。使用价值体现在读与写。这里的CAP：都是强一致、高可用、分区高度容忍.即不可能三个指标都是100分满分。
											>一致性：服务端写入一致和客户端读出一致。写成功则读出一致的新的值；写失败则读出一致的原值。体现和实现。系统特征-表象。
												>实现：数据同步。成功标准/条件：决定读的标准/条件。全部节点同步成功才成功则强一致。超半数成功则成功。
											>可用性：客户端读数据可以得到及时的响应。系统能够不间断使用的表征表象。从设计到实施都能够提供可持续的操作---不管遇到了操作冲突或者硬件软件升级失败。时效性。
											>分区容错性：分布式系统某节点故障，网络分区故障 的时候，仍然能够对外提供满足一致性或可用性的服务。除了整个网络的故障外，其他的故障都不能导致整个系统无法正确响应。A节点无法和B节点通信---分区就形成了。
											-数据都在一个节点上：有一致性。(都访问这一个节点，所以可用性不高--可能耗时久)
											-数据分库分表/分片到多个节点上：有可用性和一致性。(分片了流量分散了所以有可用性)(但一个节点挂了，分布式系统形成分区，那么该节点上面的数据将无法被访问，即还是没有分区容忍性)
											-数据库分库分表/分片到多个节点 且 每个分片都有多个副本(且副本不在一个数据中心才有意义)：有分区容忍性。(增添的副本如果要满足一致性则数据同步，数据同步时间则不可用牺牲可用性；如果满足可用性则不能大量数据同步，无数据同步则牺牲了一致性)
											----从而上述三种方法：都没有实现CAP三个特征的同时实现。
										>几种组合：子集特征组合的分布式系统。
											>传统：分片没有副本。数据在单个数据中心里。所以时CA系统。
											>放弃可用性：等待副本数据一致才算成功。等待期间无法对外提供服务(全部，而不是仅仅那些还没有同步完成的节点不可用)。
											>放弃一致性：不必等待副本数据同步完成就算成功--只需要主节点持久化或者超半数节点持久化。对于已经同步完成的节点就可以对外提供服务，而只有没有同步完成的节点处于不可用状态。那么在同步期间，从全局范围看，不变性约束受到了破坏。
										>放弃强一致性：但是进行优化。可以优化为最终一致性。这种方案成为BASE理论。折中方案。
											>基本可用：系统故障时，牺牲部分可用性，保障核心可用(基本功能可用，不是基本上可用)。即发生函数降级，用户被引导至降级页面，服务层提供降级服务。因为损失了部分可用性，所以是基本可用。
											>柔性状态: 系统存在中间状态；这个状态期间：副本数据不一致，但一致的副本数量在增加，最终全部副本一致。处于此状态不影响系统的可用性--即访问也没有逻辑错误合理不合理问题。
											>最终一致性：副本最终会都一致的特点。
										>互联网应用的核心诉求：最核心需求：高可用。服务稳定可靠。因为时间就是交易量。而不可用则给用户体验差-损失信心。
											>高可用=系统构建在多机=分布式系统 
											>高性能=分布式系统的副产品。
											>分布式系统内和单机系统内通信区别：单机系统总线不会丢失消息，而网络会。一台机器向另一台机器通信结果：收到、未收到、不知道收到没收到。消息不可靠，从而带来了状态在多机之间同步的成本很高，数据同步也是同理。
												>paxos协议：在多机通信之间不存在伪造和篡改(和丢失?)，则经过paxos协议而达成一致。写数据，系统内各个节点达成一致的数据(Acceptor)。强一致。
												>最终一致：一定会产生柔性状态。
										>传统分布式事务：	
											>两阶段提交：事务管理器询问准备好了？---->收到全部的准备好了、或者只读；没有没准备好---->发送提交
												>标准系统参考模型：X/Open事务模型、以及不同组件间与事务协调相关的接口。
													>客户端与事务管理器之间：TX接口：开启、提交、回滚。确定事务的边界和结果。
													>客户端与资源管理器之间：JMS接口、JDBC接口。
													>事务管理器与资源管理器之间：XA接口。将资源管理器加入事务，并控制两阶段提交。
													-单机锁：微秒。跨多机的锁：毫秒。1000倍差距，意味着锁冲突概率增加。尤其并发量大到一定程度时，就发生了事务积压和死锁。系统性能和处理吞吐率严重下滑。系统处理的吞吐率与资源上的时间消耗成反比，XN结果是资源总量，阿姆达尔定理。
										>柔性事务解决分布式事务问题：
											>引入日志和补偿机制：
												>柔性事务的原子性主要有日志保证：事务日志记录事务的开始、结束状态，可能还包含事务参与者的信息。参与者也需要 根据 重做、回滚需求 记录REDO/UNDO日志。当事务重试、回滚时，可以根据这些日志最终将数据恢复到一致状态。
													>事务日志：记录在分布式节点上。避免单点。
													>数据REDO/UNDO日志：记录在业务数据库上，(利用数据库事务)可以保证日志和业务操作同时成功/失败。
												>柔性事务的对日志的使用：通过日志记录找回事务的当前执行状态，根据状态是重试异常步骤(正向补偿)还是回滚前序步骤(反向补偿)。
												>实现中的常见问题：对于事务中的异常处理和补偿回滚支持不足。
											>可靠消息传递：
												>网络通信危险期：节点之间消息传递有：成/败/不知成败 三种状态。可靠消息传递为了解决 不知成败 的问题，是一个服务平台。是分布式系统除单点可靠性之外的问题。
													>对第三种状态的处理：消息投递的两种模式：
														>消息只投递一次：结果是可能没有收到。
														>消息至少投递一次：结果是可能投递多次。
														-业务一致性要求高，则只能第二种消息投递模式。第二种模式可能重复投递，所以消息处理需要实现幂等(同一操作执行多次结果不变)。
															>实现幂等的方法：根据业务流水号写日志-排重表。
											>实现无锁：影响数据库性能和吞吐率瓶颈往往是因为强事务带来的资源锁。如何很好的解决数据库锁问题是实现高性能的关键。
												>方案1：放弃锁，但不放弃隔离性。
													>实现事务隔离：
														>避免事务进入回滚：业务不管出现什么情况，只能继续朝事务处理流程的顺向继续处理。事务不会回滚，不会导致脏读。
														>辅助业务变化明细表：记录对某条数据的操作。比如某个商品的库存的减少操作。如果要查询剩余量，则用该商品的记录库存总数-操作记录的库存总数。这里只是读，所以没有锁冲突。
														>乐观锁：悲观锁排他性太强，产生数据库处理瓶颈的原因之一。
															>基于数据版本记录机制实现：商品记录上增加一个版本号字段。先获取版本号后更新操作前需要和版本号比较，如果一致则则成功，否则重试或者放弃事务。具备一定的局限性。如果在应用层实现，那么必须有服务中心，更上层的应用则从服务中心更新，而不允许单独直接读取操作数据。
										>阿里内部的三种柔性事务实现的分布式事务：
											>消息分布式事务：对两个数据库进行事务处理(事务中的操作是对两个数据库进行的)。
												>A发事务消息到MQ且收到成功响应：事务消息就是消费方暂时收不到。
												>A执行第一个数据库的事务：开启、提交、回滚。
												#如果成功：
													>将上次发送到MQ中的消息的状态更新为正常的消息状态：
												#如果没有及时反馈：比如挂了。
													>在MQ对事务消息的定时扫描中：检查到了这条事务消息在堆栈中保存了一段时间(如超过5min)，则发送一个请求到A的某个实例，让A去检查之前执行的本地事务成功还是失败。
													>A实例检查之后：发现：事务没有执行---则发送消息给MQ告知可扔弃该事务消息；事务成功---则发送消息更新MQ上该消息状态为正常状态；事务失败则也扔弃？。。
												-这样：只有当本地事务成功 且 发给MQ消息成功，消息的消费者才会收到这条消息。保证了本地事务和消息发送的事务性。
												-此时如果消息已经更新为正常状态：
												>B获取到正常状态的消息：根据消息里的事务和业务信息，开始执行第二个数据库的本地事务。
												#如果B执行成功：则实现了两个数据库上的事务同时成功。
												#如果B执行失败：发送回滚消息给MQ,MQ转发给A，A回滚对第一个数据库的事务。另一种机制：正向补偿，即不断重试或者人工干预 让事务链路 继续朝前执行。避免出现事务回滚。
												-因为同时成功或者失败：所以事务一致。因为无锁：所以数据库整体的吞吐率和性能超过了传统分布式事务。
												-新一代分布式事务平台TXC。
												-应用场景：淘宝订单交易。
													>下单：生成订单流水号。
													>发布交易创建事务消息：即会检查库存预减流水是否成功。
													>库存预减：写库存预减流水。(加一条数据)
													-如果上述成功：则订单创建消息状态为正常。
													>消费订单创建消息：创建交易详单；创建支付详单。
													>付款：客户付款拉起并支付。
													>支付宝转账、更新支付详单、写扣款流水-发布支付事务消息：其中发布支付事务消息后MQ 就会检查 支付扣款流水 是否成功。
													-如果上述成功：则支付流水创建成功。且支付事务消息状态为正常。
													>消费支付消息：实减库存：先实际更新库存量，后写减库存流水(对应预见库存流水)。修改订单状态：为支付成功。
													-订单创建或者支付异常：比如消费方-实减库存方失败、付款超时，会通过发送消息让消息发布方进行订单状态修改、支付订单状态更新、退款、预减库存回撤等操作---这种回滚回撤只需要删除那条预减记录即可。
												-应用场景2：异构索引数据的同步。总之，跨库数据更新的场景都采用消息驱动的方式实现走完整个流程。
												-缺点：如果事务上下文超过两个事务，则回滚逻辑非常复杂不可控。这样就只能正向事务补偿。
											>支付宝XTS框架：类似两阶段提交，基于BASE，保障在分布式环境下高可用、高可靠性的同时兼顾数据一致性的要求。同时支持正向和反向补偿。
												>TCC型事务：补偿型事务。try-confirm-cancel
													>try: 业务系统检测和资源预留。
													>confirm: 业务系统确认提交，不做业务检查，真正执行业务。满足幂等。
													>cancel: 业务执行错误 需要回滚的状态，执行业务取消，预留资源释放。满足幂等。
												>典型业务处理模式：
													>业务约束检查：
													>业务数据处理：
													>开启循环-一次或多次执行：账务处理--->账务服务：约束检查、记账-->根据账务处理结果进一步处理业务数据
													>完成业务处理：
												>主要组件：
													>业务服务：
													>账务操作：账务前置、账务核心 
													>主事务管理器：负责主事务的开启、提交、回滚。
													>分支事务管理器：负责账务服务操作的准备、确认、取消。
													>事务恢复：定时运行，恢复已经处于准备但 指定时间阈值内尚未确认或者取消的事务。
												>组件之间的协作过程：
													>准备阶段：
														>业务服务请求开启主事务：
														>主事务管理器启动本地事务：生成一个本次处理的txid,记录事务日志
														>业务服务向账务前置发送账务处理请求：主事务管理器能拦截此次请求--加上主事务id
														>账务前置进行前置约束检查：事务id有效，业务不重复，。检查前，相关账务锁定。
														>账务前置调用账务核心进行账务约束检查：账务状态正确 、账户资金足够；其他账务约束满足。
														>账务前置调用账务核心进行资金冻结：但不记录资金冻结日志，只是freeze_amount中增加这笔冻结资金，确保账务确认阶段能够使用这笔资金。
														>账务前置调用分支事务管理器记录分支事务日志：本次账务处理的内容和冻结的金额，以便确认阶段根据日志内容完成解冻和实际 的账务处理。
														>账务前置向业务服务返回账务处理结果。
														>业务服务根据账务处理的结果继续进行业务的处理。
													>确认阶段：
														>业务服务请求主事务管理器提交事务。
														>主事务管理器完成本地事务的提交。
														>主事务管理器向业务系统返回事务提交的结果。
														>主事务管理器向分支事务管理器确认分支事务的结果。
														>分支事务管理器顺序处理本次分布式事务的每一条分支事务日志，对每一条分支事务日志，调用账务前置确认该次处理。
														>账务前置首先请求账务核心解冻预冻结的资金。
														>账务前置请求账务核心进行账务处理。
														>账务核心对本次账务处理进行约束检查，对于特定的检查(比如账户状态是否有效等)是否需要做，视业务而定。
														>账务核心进行账务处理：记录账务日志并更新账户余额。
														>账务核心向前置返回处理结果。
														>前置向分支事务管理器返回账务确认的结果，分支事务管理器提交本地事务。
														>分支事务管理器请求主事务管理器勾对主事务：删除主事务记录或者为主事务记录打上标记。
													>回滚阶段：
														>业务服务向主事务管理器请求回滚事务：
														>主事务管理器回滚本地事务：
														>主事务管理器向业务系统返回回滚结果：
														>主事务管理器向分支事务管理器请求取消分支事务：
														>分支事务管理器针对每条分支事务明细，向账务前置请求取消账务处理：
														>账务前置向账务核心请求解冻预冻结资金:
														>分支事务管理器清除分支事务日志：
											>阿里巴巴AliWare TXC事务服务：
												>问题背景：无论基于消息或者基于XTS：在业务出现异常时，都需要自行实现事务的补偿或回滚。
												>TXC设计要求：
													>事务服务高可用，事务服务最终一致性。
													>无需自行开发事务回滚和补偿代码，平台支持按事务中操作的顺序依次回滚和补偿。
													>易用、事务完整、事务重试和自定义事务模式。
													>基于两阶段：全面支持分布式数据库事务、多库事务、消息事务、服务链路调用事务及其各种组合场景下的事务。
												>TXC过程：
													>事务发起者注册事务：在TXC服务器上注册事务。
													>注册分支事务：
													>实际数据库上执行SQL操作：同时进行undo和Redo日志的生成。undo:update之前查询出的要修改的数据；redo: 查询方式获取的修改后的数据；---用于业务回滚前脏数据的校验。
													>分支事务提交并在TXC服务器上更新该分支事务的状态。
													>第二个服务调用：如果出现对数据库修改的操作，会再次注册该分支事务到TXC服务器上。
													>分支事务提交，更新分支事务状态：提交时，带上undo/redo日志。
													-事务回滚：
														>对比当前数据库中的数据和Redo中是否一样：一样则恢复为undo日志上生成的undo SQL执行；不一致，则抛出异常，TXC server发出警告，引入人工干预。
													-性能测试：
														>测试场景：2query+1update+1insert 
															>性能测试：数据库qps峰值：23000较好。
															>稳定性测试：100并发，10小时，完成1亿次分布式事务，全部成功，没有业务异常。
															>数据一致性测试：金融业务测试。出异常、数据冲突时数据的严格一致。
												>一般建议：
													>应用程序做幂等实现：对数据库进行数据修改操作时。
													>远程模块之间用异步消息来驱动：异步消息还可以起到检查点的作用。
								>商品秒杀场景技术架构：缓存在此扮演的重要角色。
									>使用场景：满足业务对数据库90%的请求。
										>实现应用分布式session---来避免session在会话之间的复制。
										>业务去重判断：
										>交易快照：
										>图片索引：
										>数据缓存和查询：
											>缓存的添加和过期：1.分布式事务-添加操作和删除操作和更新操作时同时处理；2.数据库添加成功之后查询binlog日志将修改型数据提取分发更新缓存。
										>大促、秒杀场景：	
									>淘宝分布式缓存Tair:开源产品Redis
										>设计要求：
											>应用架构设计合理：
											>平台的稳定性保障：
											>极强的系统扩展能力：
											>不会出现业务问题：
											>高性能读取：
											-一般特征：显然不可能串行让所有用户走完，必然是让用户在流程中，在走的过程中；异步架构；整个流程走下来-多个步骤-第一个步骤-扣减量-第二个步骤-创建单号非常简单快速-并行。
										>小库存商品秒杀典型架构: 某个商品-共n个
											>秒杀开始前：商品详情页因为刷新频率高，因此必须缓存起来， 无需访问后端数据库。而设置网页返回头力的Expires和Last-Modified进一步有效控制。
											>下单付款之后：实际修改库存信息，同时修改缓存中商品的库存信息，这样用户在详情页和下单页看到的就是更新后的库存信息。
											>商品定时上架：时间到达之前，服务端收到的对应商品的下单请求都会拒绝。
											>商品库存的扣减的乐观锁实现：更新时带上页面上的库存剩余量，失败了进行重试-(库存量参数自减1)。
											>商品库存控制业务流：
												>用户查看详细页面-->购买-->确认购买、付款：
												>后端查询库存量-->更新库存量(先IC后ICDB)--->失效库存缓存：失效缓存的最好办法就是键的值对象包含是否失效 这个状态量。失效库存缓存而不是直接将新的值更新到缓存里。
												>缓存失效之后--->重新读取数据库里的数据缓存到Tair：这样各个页面读取的新的库存都是最新的值。
										>大库存商品大促架构：缓存为库存操作提供事务支持。
											>秒杀开始前：将秒杀商品从ICDB加载到IC再加载到缓存Tair:
											>商品详情页-下单页-付款页：到下单页的时候就已经后端创建了增加了一条订单记录(这样即便此时缓存崩溃，那么查看IC中的商品库存+订单记录 则 可以恢复出缓存中应该有的剩余库存量)，此时订单无状态，接着发送一条订单创建消息到消息服务器：消息消费者则更新缓存中的库存数量，然后再更新订单状态为下单状态；付款页付款请求到达后端-后端请求付款成功 后 再实际更新库存缓存---这样，缓存中的数据先减少但IC中数据最终会一致。
												>下单：缓存就减，但直到支付成功，数据库库存才真正减少。
							>数字化运营：
								>情景：每天发生的几千亿次服务调用出现报错时快速定位问题？实时监控服务的运行状态是否正常？给运营团队关注的业务指标提供实时的呈现以供他们进行实时的精准营销：
								>问题：微服务之后：海量日志分布在各个节点上。如何按照业务调用过程-恢复一条条完整的日志记录---即各个节点上的日志按照按照业务行一条条连接起来。
								>分布式日志引擎：	业务架构师-设计满足业务需求的服务调用链路。
									>整个链路的依赖路径：最为核心的是哪些？出错了怎么处理？
									>一次业务请求的时间花在了哪里？每一个服务调用的耗时都有记录。
									>哪些服务出错率比较高？哪些服务是业务链路的处理瓶颈？
								>分布式服务调用链路跟踪平台：鹰眼；zipkin,dapper
									>典型类比：高速公路网络-收费站记录每辆车到达的时间、车牌；最后可以查看一辆车的行驶路径。
										>每个服务是一个收费站：每个请求当作一辆汽车；则记录的就是每个请求的调用链路。
									>架构：
										>基础日志数据：每个服务进行了远程服务调用、缓存、数据库访问等操作，生成相关的访问日志并保存在应用所在的服务器上。生成：由应用调用鹰眼埋点的中间件生成。
											>日志特征：每个url请求都有一个独立的ID-TraceID，且调用其他服务时会发送这个ID,从而一个业务链路上的一次请求的所有节点上生成的日志都有这个ID，那么发送给鹰眼Storm集群后，就可以通过ID而查询出一个请求的所有调用服务出来--按照时间的先后顺序。ID-调用方名ip-被调用方IDip-调用开始时间-调用结束时间
											>日志的实际生成组件：服务框架层和各资源的访问驱动层 中嵌入响应的代码来实现发送-接收-统计和日志生成。从而在中间件层面统一实现鹰眼的上下文创建和日志埋点功能。调用的终点为：数据库、缓存节点、消息服务。这样，连服务服务之间的两次网络耗时也计算可知。上下文信息从开始接收到就放到ThreadLocal里，那么过程没有任何侵入，当服务开始调用其他服务时，中间件又从ThreadLocal里读取出请求上下文，来发送给下游服务。
											>TraceId:包含的信息：ip,创建时间---存储时用于分区，顺序数---用于链路采样。
											>RCPID: 异步调用相关。
										>日志数据汇聚：在每个应用所在的服务器上运行一个代理程序，专门负责实时的读取生成的日志文件增量 并发送到鹰眼的处理集群上。
										>日志处理：鹰眼Storm集群/flink集群收到--->分发给HDFS,HDFS上运行许多的MapReduce--->计算结果被鹰眼服务器读取，发送给HBase集群。。比如:对于日志信息要进行批量的统计和分析，如链路分析功能，则发送给HDFS来对日志数据计算和分析。
											>路线2：Storm集群直接分发给Hbase--->对日志信息进行实时业务统计的需求。比如：某一服务实时的QPS值、交易金额的实时变化等场景。
									>应用场景：
										>服务实时监控：QPS,请求耗时99%
										>服务调用链跟踪：在web界面清晰还原出每一次业务请求所产生的服务链调用情况。
											>跟踪过程：客服报问题，运维人员从日志中搜集到对应的TraceId(服务出现异常，自动将traceId打印到日志文件中)；也可以通过条件筛选：应用、ip地址、服务名称、时间段搜索找到服务调用链记录。
											>展示：调用链是一棵树形图：每个节点是一个树上的节点。每个节点的同一行的：右边列：是调用类型、ip、耗时、状态结果、
										>服务调用链分析：业务架构师关心服务间的依赖关系和服务运行的持续稳定和优化。
											>按一定时间区域(一个月/三个月/半年等)对服务调用数据进行统计和分析的功能：如这个调用链的服务调用树、服务方法、资源类型、QPS,峰值QPS,调用次数、平均耗时、超时比例、耗时占调用链路总量比、强依赖弱依赖--服务的重要性必须性---强的要求稳定性可用性--而弱的可以降级限流
										>业务全息排查：增加调用链对应操作的业务信息，对于系统异常的定位会更加精准和快捷。
											>主要：一个请求Trace 要关联到 订单ID/会员ID/商品ID， 调用的服务方法及其入参---这个很关键。从而知道某次异常是谁进行了什么动作调用什么服务什么入参导致的。
										>业务实时监控：实时变化和跳动的业务指标：实时交易金额、移动端比例、top10的热销商品、top10的商家销售排名，...
											>数据来源方式：
												>一种：直接查询在线数据库，分析统计计算---显然对在线交易数据库产生大的压力--牺牲在线数据库的正常服务---为指标而不值得。
												>第二种：先用ETL方式将数据从在线数据库同步出来到数据仓库或其他的数据库中，业务展现大屏则从这个数据仓库获取业务指标和统计数据。可以准实时，分钟级别，同步时间较长。
												>第三种：利用分布式日志处理平台TLog的能力。因为实时记录了存储了业务事件日志：订单成功创建-支付成功-用户登录等，采用图形化日志处理流程，处理任务下发到JStorm流式引擎，解析分类处理后 存放到后端在线数据库HBase或数据库平台中，最后通过API方式给前展示大屏提供数据。这样就没有影响在线数据库，且秒级变化体现。这样：实时、精准营销策略或活动计划等，提供有价值的参考数据。
								>海量日志分布式处理平台：TLog 零业务侵入、高性能、实时性强。
									>根据用户定制的处理流程：持续不断对目标机器生成的日志数据进行解析、计算、入库等操作。
									>接入：
										>日志采集配置：目标应用、机器IP、日志路径。
										>可视化日志数据处理流程自定义：采用Google Blockly 可视化编程工具--自定义日志处理流程编辑体验。可以对任意格式的日志信息进行切分、持久化、聚合等步骤的流程配置。
											>配置保存后：Tlog将流程部署在流式处理引擎上，转换为执行单元--分配到JStorm流式引擎不同组件中，即Spout和Bolt 。。从而日志被Tlog从机器节点上抓取下来，通过报表的方式展现在大盘或者监控平台上。
										>API方式数据获取：就像点餐一样。
											>按不同关键字对数据排序和统计。
										-采样率和性能损耗。	
							>打造平台稳定性能力: 共享服务中台的平台稳定性能力
								>问题背景：服务中心如何具备业务能力的持续扩展？以满足不停的有新业务的接入。是否在每天几千亿次服务调用中保持稳定的服务能力？整个平台平稳如山的度过峰值？机房断电或通信电缆故障如何保障平台持续稳定运行？
								>稳定性体系：
									>包含的范围：机房的布线、网络通信、硬件部署、应用架构、数据容灾等方面。应用架构设计和中间件平台维度为介绍：限流和降级、流量调度、业务开关、容量压测和评估、全链路压测平台、业务一致性平台等。
										>限流和降级：服务怎么被调用很难控制，必须自身做好保护；避免服务节点被请求占满 而产生访问服务超时，平台无法响应和崩溃。
											>限流：系统能处理100w的并发，更多的就直接返回“系统繁忙，稍后重试”，否则连这100w的请求也要受影响-全部不可用直接崩溃。
												>压力测试：评估服务实例部署量。
												>线上压测工具：更方便和准确的对服务的容量进行评估，即获取到该服务所能提供的最大处理能力。
												-掌握服务的容量后：开始针对服务资源的使用情况进行监控。但cpu/mem/io的使用情况和系统本身的处理能力之间没有清晰的对应关系，所以应该采用QPS作为限流的关键判断指标。
												>平台限流的实现：从一个典型的服务化应用架构的角度看：
													>用户请求先通过前端接入层(Nginx)分发到后端的应用集群上：限流拦截点最优在Nginx,也兼顾安全问题，通过一些安全策略防止对平台的恶意攻击。在Nginx上实现的扩展组件TMD实现了接入层限流的主要工作，TMD系统可以通过域名类限流、cookie限流、黑名单以及一些安全措施来实现接入层的限流措施。
														>TMD： 包含nginx-http-sysguard: 当访问负载和内存达到一定的阈值，会执行响应的动作--比如直接返回503,504或者其他URL请求返回码，一直等到负载回到阈值范围内站点恢复可用。Tengine平台有介绍。可视化的配置管理界面。
													>没有成功：则返回限流页面：页面包含跳转到y引导页面。形成用户体验和业务处理流程的闭环。
													>服务层的限流：Spring AOP 对调用的方法添加一个Advisor, 当当前调用方法QPS超过设定值 或者 线程数超过阈值，则返回访问限流的异常信息。
														>缺点：只能对一个应用内一类接口限流。扩展性差。硬编码--则配置管理差。有时线程多可能由于业务和下游服务引起，所以不能仅仅服务端限流。缺乏统一的监控平台，对当前的服务限流情况没有全局管控。限流算法简单，双十一会看到毛刺现象，需要一种更平滑的限流算法---而不是暴涨暴跌 而不稳定。
												>限流平台Sentinel: 为整个服务化体系的稳定运行行使着警戒任务，是对资源调用的控制平台，涵盖 授权、限流、降级、调用统计监控 四大功能模块。
													>授权：通过白名单/黑名单方式对HSF的接口和方法进行调用权限的控制。
													>限流：对特定资源进行调用的保护，防止资源的过度调用。
													>降级：判断依赖的资源的响应情况，当依赖的资源响应时间过长时进行自动降级，并且在指定的时间后自动恢复调用。
													>监控：全面的运行状态监控；实时监控资源的调用情况(QPS,响应时间，限流降级等信息)
													--概念和架构：
														>概念：资源和策略。对特定的资源采取不同的控制策略，保障应用稳定性。
															>默认切入点：服务调用时、数据库-缓存资源访问时，覆盖大部分应用场景，保证对应用的低侵入性，也支持硬编码和自定义AOP
														>架构：
															>sentinel客户端：获取规则(从Diamond服务器)，对服务调用入口：授权、限流；对服务调用出口--调用其他服务：限流、降级。统计QPS，响应时间等。
															>sentinel控制台：从客户端拉取QPS，响应时间等展现在控制台的监控界面上。控制台给运维人员提供 针对 服务、缓存、数据库等资源访问设置各种限流规则，并将设置好的规则发送到Diamond的规则配置中心，Diamond将规则推送给sentinel客户端。
																>降级功能：对于服务调用链路中的弱依赖服务，即平滑下线，去掉那些弱依赖的服务调用；或者依赖的非核心服务 因其处理性能和服务响应时间较长，则停止调用，保证当前服务的处理效率。
																>降级规则配置功能：某个服务的方法的响应时间超过阈值，意味着这个服务已经出现了处理性能的问题，则自动切换到降级模式，降级持续的时间可以自定义配置。
													--服务化平台建设：限流和降级平台至关重要。
										>流量调度：应用平台运行在云平台上。超配方式增加机器的利用率：一台物理机上创建的虚拟机CPU核数总和 会 超过 物理机实际的CPU核数。因为长尾效应，有的应用流量非常低。		
											>问题背景：一次网络抖动可能导致这台机器实时服务能力下降。造成单点故障、或者局部应用出现故障。弥补了应用局部可用性问题带来的问题和隐患。
												>问题类型：超卖问题带来资源争抢；部分应用启动的时候容易出现机器负载飙高，导致这部分机器响应时间变长；jvm假死，VM假死;受宿主机影响，负载飙高问题;jvm垃圾回收影响请求响应时间的问题；网络抖动导致RT抖动。
											>流量调度平台：根据机器的实时服务能力来分配机器的实时流量。实时服务能力好的多分配流量，差的少分配，不可用的迁移流量。屏蔽所有机器的软硬件差异，避免单点影响整体服务体系问题性的隐患。
												>原理：状态信息探测收集模块---比较计算匹配决策模块----执行模块恢复模块----服务状态视图、日志、报表、权限、监控
													>需要信息：秒级获取服务器系统的运行指标和业务指标;服务器上有暴露指标信息的接口。流量调度服务器大约1/5s来调用指标信息接口；
														>具体：CPU、Load等；HTTP响应时间、HSF服务调用响应时间、HTTPQPS HSFQPS\Tomcat线程池使用信息、HSF线程池使用信息；
													>行动根据/判定规则：流量调度平台设置的决策算法和规则；
													>动作逻辑/执行规则要求的动作：当发现满足规则条件的指标状态发生时，对线上环境的服务器下线等操作；以屏蔽单点或局部出现故障的应用实例对整体平台产生的扩展式的影响。
														>HSF客户端获取的服务提供者列表：带上了各个服务提供者的服务路由权重；权重大的则调用频率高。
														>流量调度平台通过发送信息给config server:让它更新某个服务的某些实例节点的服务路由权重：从而客户端自动重新拉取新的带权重的服务提供者列表。
														>接收服务实例的server: config server/ vip server;
												>例子：当A服务发布时，负载飙高响应时间高几倍，流量调度探测到异常，迁移掉这台机器的所有流量；
										>业务开关：分散在代码中带来运维问题；需要一套统一标准和规范的业务开关管理Switch平台。
											>稳定性开关平台功能架构：
												>客户端: 注册开关、获取开关、更新开关
													>嵌入在客户端的SDK：注解的方式在代码里使用。
												>控制台：开关管理、运行态监控、
													>将开关推送到内存：类似一个分布式配置管理平台。
													>监控集群开关状态：当不一致了则告警，提醒操作人调整开关值，保持集群状态一致。
										>容量压测及评估规划：
											>问题背景：对平台性能测试看能否满足预估的系统处理峰值，业务分析-测试模型-性能测试工具在测试环境模拟压测-分析测试结果，判断系统的最大负载能力。
												>缺点：测试场景有准确和全面的问题。
											>分布式应用系统容量压测和评估的自动化平台：通过对生产环境的流量模型引流到压测服务器上，获取到服务实例单机的最大处理能力，结合不同型号服务器处理能力以及生产环境的水位监控信息，对服务集群所需部署的服务器数量进行容量评估和预测。
												>实用性：准确容量预测、也提供了完整的测试场景、测试方法；建立了系统的性能基线；	
												>准确性：生产环境实时变化的复杂流量场景，压测流量模拟就更真实、全面。流量建模、容量压测、分析及预测，建立在生产环境上。
												>高效性：建模、压测、分析、预测基于同一个平台，同一种监控方式。
											>容量压测：先设置好目标机器的关键指标(cpu利用率，系统整体负载、QPS、响应时间等)达到阈值水位后，自动停止压测，此时该服务实例在系统资源达到阈值的单机服务实例所能提供的最大QPS处理值 ，就是目标值。
											>容量分析预测：容量规划平台：结合单机QPS数据---结合对各种服务器机型的处理能力的差异化分析----得出部署多少个服务实例可以满足大促活动的结论。避免大量冗余服务器运行。
										>全链路压测平台：上述仅仅对单一服务进行，有局限性。全系统每个环节都参与的实战演习则缺乏。需要对：0点峰值流量ji进行评估、以及对网站承压能力进行测试，是双十一之前为系统查漏补缺的重要一环。
											>基础数据抽取：模拟双十一。卖家、买家、商品的数据构造是真实性保障的重要一环。以线上数据为数据源。进行采样、过滤和脱敏。	
											>链路与模型构造：同一条链路需要构造海量的参数集合，代表不同用户的不同行为。链路范围、链路的访问量级、链路的参数集合、基础数据的特性一起构造了压测的业务模型。
											>链路的验证：每条链路都要让全链路压测引擎跑通，链路验证功能集成到平台中。自动化完成对压测链路的验证。
											>业务改造：全链路压测几个小时，链路的幂等性需要。下游写流量的拦截、防止污染BI报表和线上推荐算法等场景，需要对业务系统改造。
											>数据平台：沉淀了一系列自动化和智能化的基础设施。
											>流量平台：
												>全链路压测操控中心：进行压测的配置和操控、数据的监控和对压测引擎集群的管控。
												>压测引擎：由控制台统一管控，部署在外网CDN集群，进行登录、session同步、发送各种协议的压测请求、状态统计。
											>影子表：数据的隔离,在同一数据库的实例上对数据库表建立同样结构的影子表来进行数据的隔离。
											>中间件改造：中间件的协议要识别除压测流量，使得压测标识能够随着调用传递下去，使得下游的应用、基础中间件、存储都能识别压测流量。
											>安全机制：
												>安全的监控和保护：非法流量的监控机制，用户访问不了测试数据，测试账户访问不了正式数据。
												>对压测流量的安全过滤：放松安全策略，使得压测流量不被判别为攻击流量。
										>业务一致性平台：BCP
											>问题背景：远程服务调用失败，数据库保存失败，MQ消息发送失败：丢失数据或者各服务中心数据不一致。即便调用没有出错，但是逻辑问题，导致业务数据异常。比如优惠券扣减问题，多扣减了。
												>业务与数据不一致问题：业务稳定性保障，需要实时检测业务不一致的问题，及时报警。
												>实时业务审计平台：采用规范与标准化业务规则的方式，统一解决平台服务化后的业务一致性问题。
													>目标：
														>高实时性的发现脏数据和错误的逻辑实现。第一时间发现并通知技术保障人员。
														>方便地接入各种业务规则，通过脚本规则编写的方式，让各应用快速接入业务审计平台。
														>整合修订工具、形成规范的脏数据订正流程。
														>业务上线的实时监控：新上线业务可以很方便进行校验。
													>实现：业务层则侵入而不灵活，性能影响也大；
														>事件模式：把业务数据变化触发的消息(精卫、MQ等平台消息)转换为相应业务类型的事件，放入到事件执行队列进行规则检查。
														>事件监听框架：实现MQ对于数据库的对接，是对于数据变更日志信息接入到了BCP平台中。
															>数据变更的消息发送到MQ, 业务规则平台订阅消息收到消息，自动触发规则执行流程。一个事件经过多个规则的过滤处理，来判定是否是脏数据；是脏数据，一个报警一个存到数据库。Groovy脚本。
															>多数据源：先缓存，后多个消息到达后才进行对账。
									>共享服务中心对内和对外的协作共享：
										>问题：高效实现应用对服务中心的服务对接：
										>阿里巴巴共享服务平台：SPAS
											>服务化建设野蛮发展带来的问题：
										>服务治理：王坚<在线>	
										>共享服务化平台：进行高效的服务治理。
										>产品服务化：通过市场配置资源。
										>服务化改造过程：
											>API 接口提取明确：
											>API 组件化开发实现：
											>服务治理： 
											--服务共享中心：负责信息的监听和记录，配置的创建、接收、同步和下发。
											--岗位轮换：业务中台和前端应用开发交换：
												>业务沉淀到中台：中台独立沉淀，或者共建沉淀。
												>业务中台：服务稳定和业务创新。
											--淘宝开放平台：
					>技术平台：				
						>BCP业务校验保障平台：使用业务规则的方式，对交易进行业务和逻辑上的校验。对业务不一致的订单进行处理。
						>业务指标的关注--大数据平台：基于各个服务中心的数据。从各个维度展示各种业务指数。技术功底+精通业务。
					>各个服务中心：
						>组织：每个服务中心的组织构成：业务架构师(业务负责人)、开发人愿、UED工程师、运维工程师、DBA等。
						>对前端业务的支持和对接：
							>收到前端业务方对服务中心能力增加的需求：需要过滤，避免都引入到服务中心层面。不损害服务中心业务的通用性，不带太多特定业务属性的需求在这里的实现。而丧失了业务通用性，则会对新的业务需求无法啊提供服务？不断拆分?对服务的稳定性带来风险。
						>服务的稳定性：
						>服务能力的扩展性：
						>服务需求的快速响应能力：
		>"大中台、小前台"组织机制和业务机制：前台更敏捷、更快速适应瞬息万变的市场(团队小而多)，中台则集合整个集团的运营数据能力、产品技术能力，对各前台业务形成强力支撑(支撑服务多而强)。
			>supercell的启示：游侠开发过程中通用、公共的游戏开发素材、算法做很好的沉淀，鼓励员工创新和试错。2-7人团队。科学的研发方法和研发体系，几周时间研发出一款新游戏。
			>共享业务事业部：从淘宝-天猫沉淀而来。
			>整体业务架构：阿里巴巴集团业务(淘宝/天猫/聚划算/口碑/阿里妈妈/菜鸟物流/1688)<----共享业务事业部(用户中心/商品中心/搜索中心/交易中心/评价中心/店铺中心)<--------阿里云平台(分布式应用服务平台/弹性计算服务/关系型数据库服务/开放存储服务/集群监控/集群部署/安全管理)    ;侧面：监控报警/故障处理/系统升级/应用发布/安全防控/业务监控/运维保障部
			>非此模式的弊端：重复建设、沟通协调代价高、不利于业务的沉淀和业务的发展。
		>IT信息中心部门的地位低：被当作支持服务，是一个成本中心。原因在于：不懂业务。业务部门定一个业务流程，然后信息中心去实现。知其然而不知其所以然。谈不上业务领域专家。更不可能对业务发展有创新的想法和独到见解。只是增加了项目经验而已。
		>改造例子：
			>以前：项目管理；现在：核心业务服务能力的沉淀和打造。
			>核心事务：基于阿里云 而 打造业务中台。面向前端提供服务。物资装备电商平台。
		>改造例子2：
			>以前：企业管理软件架构；现在：业务中台+业务前台 + 触点：
				>业务中台：商品中心、库存中心、会员中心、订单中心、结算中心、渠道中心、...
				>业务前台：电商系统、门店POS、会员社区、互动平台、SCRM、数字营销、渠道工程、分销配送、智能供应链
				>触点：微信、官网、数字广告、在线服务、开放平台、线下门店、小程序..
			>供应链协同问题：
				>传统企业供应链体系：提前半年的订货会--->经销商看样下达订单--->品牌商生产、配送给经销商--->经销商配送到自己的门店
					>问题：预测不准--->产品滞销/库存积压--->打折促销--->门店业绩和利润受影响
				>优化：自动补货和配送，而不是先配送到各个门店。
				
1.电子书网站：https://www.torrent.org.cn/bd			
2.参考资料：架构即未来：DevOps:软件架构师行动指南， 架构真经-互联网技术架构的设计原则、系统架构-复杂系统的产品设计和开发、实用软件架构-从系统环境到软件部署、软件架构、
3.http://jos.org.cn/jos/ch/reader/create_pdf.aspx?file_no=5899&year_id=2020&quarter_id=3&falg=1 基于深度强化学习的 查询优化