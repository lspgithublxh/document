---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。
>学习要有战略和格局：过于零散、孤立、末支、叶子的知识点就没必要化太多时间，懂得放弃。聚焦在根节点、枢纽节点、产生分支的节点、主干知识点、枢纽知识点、关联到很多知识点的知识点上。
 >对于主干知识点：要有全新的认识视角和进行精细的描述，大量的提问和好奇和进行必要的实验以获取支撑观点/导出观点的数据。
 >规范是工程最独特的特征.
 >慢慢读：
>一种新技术的学习：
 >它面对的情况和问题、它的世界观、它的方案、它的方案验证/论证/能处理的解决的所有情况及能成功处理的理由/功能边界
  >所有的软件：都可以看作是向上封装一层接口，根据自己的世界观封装底层而向上/对外提供统一的(统一的更简单的更直观的更业务的更少底层信息的)接口，底层包含一系列的第三方的插件/构件/组件；内部则去做兼容和调用(对底层)(对上层则做逻辑分解和底层实现)。

>阿里巴巴的观念：
	>自己遇到的解决的痛点，也是别人会遇到的需要解决的痛点。则将自己的经验能力提取出来，做成通用的服务，互联网服务，通过网络都可以享用到这个服务。从支付服务，服务自己，到服务别人--服务更多的商人--通过卖服务能力赚钱，且首先这个服务能力先服务自己，服务做好自己体验好之后，再拿出来服务更多的人。这就是阿里巴巴的服务理念。让大家通过网络来享用到自己亲自体验过的强大的丰富的各种服务能力，这就是阿里巴巴的服务理念--梦想和信念。
  
>举动-痛点：企业IT架构转型。业务特性:复杂的商业系统，海量的请求和数据。业务要求：很高的稳定性(不能停止服务和服务不稳定)和可靠性(不能丢失结果)。不能错、不能停、不能单机。
	>主要挑战：高可用、海量、复杂的业务逻辑。大规模数据的线性可扩展问题：存储和访问两个方面。
	>主要挑战2：复杂业务系统的解耦问题。迭代速度、发布速度提高。保持业务相对隔离来让工程师大规模并行工作，同时要满足海量访问和高性能的要求。
	>主要挑战3：全球进行分布式部署、99.999%以上的高可用、容灾等。应用需要在运行时可以全程进行动态感知和管理，要有全部的监控能力，和根据业务流量进行业务的优雅降级，确保系统的高可用。
	>解法/处理思想：解决上述挑战和问题：发展了一系列的中间件来支撑这种新的架构。实用的技术方案。
		>电商架构：烟囱式架构-->分布式架构-->共享式架构。
		>避免不断重复建设：数据可以重复使用，服务也要可以重复使用。来提升生产效率。
		>系统的建设：从生产型模型到运营型模型，从版本模型到迭代模型；来沉淀所有的积累。生产型的模型的逻辑来自：几个人的脑子，而运营型模型的逻辑则来自：无数客户、供应商、工程师的脑子。
		>企业级互联网架构：重构企业IT架构(如短期内重构供应链、SCRM等平台)(打造企业全渠道分销平台)
			>企业业务模式：
			>共享服务理念：
				>共享服务体系:	技术平台支撑。项目落地的过程。服务重用。
					>技术框架的选择：
						>服务化框架：
							>需求背景：单机war功能太多-协调问题大业务响应慢，内容越来越复杂、错误难于隔离--变化大的模块和小的模块都在一起, 数据库连接能力难以扩展---每个应用实例的连接池大小压缩到10个--但实例多而给数据库的压力超过5000/s ---过高的连接数使得数据库处于不稳定状态。应用扩展成本高---增加实例来分担服务负载--有的模块调用量高有的其实很低。
							>服务的切分/剥离/改造：某个模块如果业务逻辑相对独立、简单、复用率也高，如用户中心。接着剥离 交易中心、类目中心、商品中心、店铺中心。
								>切分规则：不同模块间进行清晰、稳定的服务契约定义，保证对外服务的接口定义不发生变化。降低不同模块开发团队间的协同成本，内部迭代开发速度也更快。降低系统之间的耦合度，业务更单一专一--提供更加专业和稳定的服务。避免了个别模块的错误给整体带来的影响。解放了对单数据库连接数的能力依赖--因为有独立专门的数据库。精细粒度的扩容，增大集群。
								>SOA特征：面向服务的分布式计算；服务间松散耦合；支持服务的组装；服务注册和自动发现；以服务契约方式定义服务交互方式；。。可以实现异构系统之间的交互。
									>点对点稳定性的解决：负载均衡，多版本支持。
									>中心化：需要连接ESB，ESB又要转发，网络开销大;网络带宽要求也大;升级麻烦；雪崩效应--一台企业服务总线实例出问题拒绝服务则剩下的实例压力更大。淘宝：立即下单---后端调用了200多个服务(主动调用或者发送消息)：但订单创建成功只用了200-300ms
							>分布式服务框架：HSF:High Speed Framework
								>架构设计：
									>服务提供者：真正服务功能的实现的应用实例。
										>集群部署；保障高可用性。
										>运行在：阿里巴巴优化定制后的Tomcat容器中。Tomcat运行在虚拟机/docker容器中; 一对一对一的关系。
											>定制Tomcat容器：容器层已经集成HSF服务框架对服务提供者或者服务调用者进行配置服务器的发现、服务注册、订阅、失效转移等相关功能；不管进行服务提供者开发 还是  服务调用者开发，只需要进行服务相关的配置操作，应用中无需引入任何HSF相关的jar依赖包。
									>服务调用者：也是war包运行在定制的Tomcat容器中。
										>对于C/C++,PHP,Node.js等语言开发的服务调用者：。
									>地址服务器：给服务提供者和服务调用者提供部署环境中所有配置服务器和diamond服务器的服务器列表信息，通过Nginx来提供这个服务能力。配置服务器集群、diamond服务器集群信息都设置在地址服务器上。
										>部署多台：提供负载均衡和高可用服务。
										>访问方式：通过统一域名的方式访问这些地址服务器，通过DNS轮询，实现地址服务器访问的高可用性。
									>配置服务器：记录环境中所有的服务发布(服务提供者的ip地址+端口信息)和服务订阅(服务调用者的ip地址和端口)，并将服务相关信息推送到服务节点上。
										>推送效率：将服务发布和订阅信息保存在内存中。
										>和服务提供者和调用者之间的连接方式：长连接。心跳监控运行状况。当服务提供者故障时自动推送更新后的服务提供者列表给相关的服务调用者端。。
											>推送特性：保障淘宝平台实现单元化(某个客户访问淘宝时，请求路由到某个淘宝机房后，在淘宝上的所有业务操作都可以在这个机房完成，无需访问其他机房的服务)、异地多活。
										>部署多台：用于服务发布、订阅、推送的负载均衡。多台之间：实时数据同步，保障服务发布和订阅信息尽快同步到各服务节点上。
									>Diamond服务器：类似zk,统一配置管理服务。给应用提供统一的配置设置和推送服务。
										>保存配置规则：服务调用安全管控规则、服务路由权重、服务QPS阈值。持久化到后端Mysql中。
										>生产环境：多台，提供负载均衡服务。
										>场景：
											>白名单设置：设置某些服务或者服务中的方法只能让特定IP地址的服务器调用。
											>控制服务是否能够调用：通过用户认证的方式。
											>服务器权重设置：服务调用者对多个服务提供者服务节点的访问。
											>设置服务的QPS能力上限值：一旦该服务的QPS达到该阈值，则拒绝服务的继续调用。实现服务限流。保障平台稳定性。
										>服务节点定时从Daimond服务器上同步相关配置信息。使得规则立即在服务运行环境中生效。
								>工作原理：规则：
									>服务节点对配置服务器列表的获取：服务调用/提供者 如何获取到 配置服务器 和 Diamond服务器的列表？ 就是通过在Tomcat容器启动后 以 域名 的方式 先获取到可用的地址服务器，再向地址服务器发送请求服务器列表信息，从而在容器启动完成时，就已经在该服务节点上获取了配置服务器和Diamond服务器的ip列表信息。
									>服务的注册发布：服务提供者节点，获取到配置服务器ip列表后，向配置服务器发送 当前应用中包含的服务提供者信息(在应用的配置文件中：服务的接口类全名、服务版本、所属服务组；当前服务器ip:port), 进行注册发布。启动后就注册完成了。
									>服务的订阅：服务调用者节点，获取到配置服务器ip列表后，向配置服务器发送 服务消费者相关信息(服务的接口全名、服务版本、所属服务组)，配置服务器收到后则从内存中匹配对应的服务提供者的ip:port列表，返回给调用者。
									>服务规则的发送：先在Diamond服务器提供的规则设置页面，对指定的服务提供者和调用者设置相关的规则，保存后，就会推送到相关的服务节点上。
									>服务交互：服务调用者从已经在启动时获取而保存在本地的服务提供者列表：随机选择其中一台进行服务请求的发送，调用。
								>特性实现：
									>高效交互：
										>网络通信框架：Netty。多路复用的TCP长连接。一个连接交替传输不同请求的字节快。避免反复建立连接的开销，也避免了连接的等待闲置，从而减少了系统的连接总数，同时避免TCP顺序传输中的线头阻塞head-of-line blocking问题。
										>数据序列化协议：Hession。数据量较小时，性能表现出众。精简、高效，跨语言。充分利用Web容器的成熟功能，处理大量用户访问时有优势。比Java序列化开销缩短20倍，。正确率和准确性。性能和稳定性。TPS>10w性能和效率高。
									>容错机制：
										>
									>高可用：
									>扩展能力：

							
								>运行原理：
							
							
					>技术平台：
						>BCP业务校验保障平台：使用业务规则的方式，对交易进行业务和逻辑上的校验。对业务不一致的订单进行处理。
						>业务指标的关注--大数据平台：基于各个服务中心的数据。从各个维度展示各种业务指数。技术功底+精通业务。
					>各个服务中心：
						>组织：每个服务中心的组织构成：业务架构师(业务负责人)、开发人愿、UED工程师、运维工程师、DBA等。
						>对前端业务的支持和对接：
							>收到前端业务方对服务中心能力增加的需求：需要过滤，避免都引入到服务中心层面。不损害服务中心业务的通用性，不带太多特定业务属性的需求在这里的实现。而丧失了业务通用性，则会对新的业务需求无法啊提供服务？不断拆分?对服务的稳定性带来风险。
						>服务的稳定性：
						>服务能力的扩展性：
						>服务需求的快速响应能力：
		>"大中台、小前台"组织机制和业务机制：前台更敏捷、更快速适应瞬息万变的市场(团队小而多)，中台则集合整个集团的运营数据能力、产品技术能力，对各前台业务形成强力支撑(支撑服务多而强)。
			>supercell的启示：游侠开发过程中通用、公共的游戏开发素材、算法做很好的沉淀，鼓励员工创新和试错。2-7人团队。科学的研发方法和研发体系，几周时间研发出一款新游戏。
			>共享业务事业部：从淘宝-天猫沉淀而来。
			>整体业务架构：阿里巴巴集团业务(淘宝/天猫/聚划算/口碑/阿里妈妈/菜鸟物流/1688)<----共享业务事业部(用户中心/商品中心/搜索中心/交易中心/评价中心/店铺中心)<--------阿里云平台(分布式应用服务平台/弹性计算服务/关系型数据库服务/开放存储服务/集群监控/集群部署/安全管理)    ;侧面：监控报警/故障处理/系统升级/应用发布/安全防控/业务监控/运维保障部
			>非此模式的弊端：重复建设、沟通协调代价高、不利于业务的沉淀和业务的发展。
		>IT信息中心部门的地位低：被当作支持服务，是一个成本中心。原因在于：不懂业务。业务部门定一个业务流程，然后信息中心去实现。知其然而不知其所以然。谈不上业务领域专家。更不可能对业务发展有创新的想法和独到见解。只是增加了项目经验而已。
		