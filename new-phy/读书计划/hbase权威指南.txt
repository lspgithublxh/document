---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。
>学习要有战略和格局：过于零散、孤立、末支、叶子的知识点就没必要化太多时间，懂得放弃。聚焦在根节点、枢纽节点、产生分支的节点、主干知识点、枢纽知识点、关联到很多知识点的知识点上。
 >对于主干知识点：要有全新的认识视角和进行精细的描述，大量的提问和好奇和进行必要的实验以获取支撑观点/导出观点的数据。

>举动-痛点:  存储及处理PB,EB量级的半结构化/非结构化的数据。且低成本和高效。
	>解法/处理思想: 与hadoop高度集成的hbase, hadoop将hbase的可伸缩性变得简单.
	 >客户端：java, rest, avro, thrift
	 >架构细节： 存储格式、预写日志、后台进程。集成map-reduce框架。调节集群，设计模式，拷贝表，导入批量数据，删除节点。
     >周边：Hive/pig: 大规模数据的离线批量处理。Hadoop-存储非结构化，高延迟。
	 >Hbase来源：索引存储，Nutch中构建索引并存储。
	 >应用：雅虎用1000台HBase集群定位Bing的爬虫快照。
	 >特征：列式存储，线性扩展。java实现(hbase, es, kafka)
	 >Hush: 一个访问HBase的web服务器客户端。
	 >列式存储数据库：列数据相似，更容易压缩，有更大的压缩比。
	  >Hbase: 利用了磁盘上的列式存储格式。适合有序数据、键值对数据的存储。
	  >关系型数据库：数据分区使得运维代价高。适合事务型操作，但不适合超大规模的数据分析处理--即不适合大范围的数据记录扫描或全表扫描。分库分表的含义： 分表是按照不同的类型如地域、范围等划分存到不同的表中，表的schema一样；或者按列分--列太多；分库是表都不在一个数据库里，而数据库往往在不同的服务器上，来提高存储能力。
	   >横向拆分：重分区。因为数据重做所以消耗资源。
	   >虚拟分区：仍然需要将数据迁移到新服务器。
	  >NoSQL: 不提供SQL查询数据的手段，但有一些工具提供SQL语言入口。区别在底层上：
		>模式和事务特性ACID的比较区别：不支持事务或者辅助索引。没有固定的模式。
	   >一致性模型：数据库状态在操作下从一个一致状态到另一个一致状态---不矛盾无不守恒无不合理的数据状态。
	    >一致性级别：严格一致性：状态原子改变--中间不可读写。顺序一致性：操作顺序不同看到的数据不同。因果一致性。最终一致性：系统广播使得副本之间数据一致。弱一致性：更新以广播形式传递。CAP观点基于的关键事实就是：网络隔离的副本之间数据一致的实现需要消耗一点可用性(即一小段时间不可用--数据不一致)：或者说在更新 网络隔离的副本 时不可能 立刻同时全部更新成功而达到数据一致--即总是有先更新成功有后更新成功(总需要一个时间过程)。花时间获取更高的一致性，有系统消耗时间-一致性 坐标图。
	     >要么等待全部副本更新成功，要么不等待而手动在应用中解决一致性问题：如等待超过一般成功就认为成功。读个数R > N - W
	 >数据库的度量维度：阻抗匹配：通用的和可用的解决方案。
	   >数据模型：键值对、文档？
	   >存储模型：内存、磁盘？
	   >一致性模型：严格一致性、最终一致性？
	   >物理模型：分布式模式、单机模式？
	   >读写性能：支持读多写少、读少写多？范围扫描、随机访问？
	   >辅助索引：支持按不同的字段和排序方式来访问表？
	   >故障处理：数据迁移方案？服务器故障如何处理？恢复的难度？
	   >压缩：有可有的压缩算法？可用的压缩组件？
	   >负载均衡：随着负载变化自动均衡处理的能力？
	   >原子操作的读-修改-写：乐观锁、悲观锁的分布式实现？
	   >加锁、等待、死锁：采用哪种锁模型？能否避免等待和死锁？
	 >数据库的范式化和反范式化：基本原则：反范式化、复制、智能主键。
	   >Hbase支持：稀疏矩阵、宽表、列式存储的支持。使得存储结构无需范式化，从而无需join操作。智能的主键，控制数据存储的位置，范围检索、组合键设计。性能不会因为数据增长而下降。
	   >GFS: 并不适合存储成千上万个小文件，适合存储少许非常大的文件。因为文件的元数据要存储在主节点的内存中。
	   >BigTable: 系统将小文件聚合到非常大的存储文件中。并提供索引排序，查找最少磁盘就能够获取数据。支持简单的CURD,和范围扫描、全表迭代扫描。
		 >功能：是一个管理结构化数据的分布式存储系统。可以扩展到成千上万台商用服务器上存储PB级别数据。	一个稀疏的、分布式的、持久的多维排序映射。
	 >Hbase基本概念：稀疏的、分布式的、持久化的、多维排序映射。由行键、列键、时间戳索引。
	   >列：一个列簇里的列可以无限多个--数百万都可以，列值可以无限长、任何类型。
	    >Null值：空值无消耗。
	   >列簇：设置列簇的压缩特性、指示存储在内存。一个列簇的所有列存储在同一个存储文件中，即HFile中。列簇在建表时定义，列簇名可打印，列簇数量<几十个。
	   >单元格：cell。值有时间戳。一个单元格的不同版本的值降序排列在一起。最大版本数可以设置，一般时间戳作为版本。谓词删除---LSM树中。
	   >行：每行有行键，多行数据按照字典序存储。行键是字节数组，任意。
	   >数据存取模式：SortedMap<RowKey, List<Family<SortedMap<Column, List<Value, Timestamp>>>>> 。自然的，每行每列的单元格里的版本数目相互无关---也说明一个rowkey下有多个版本的值---但是每次取最新的。
	   >事务级别：单行。可以读取单行上的任意数目的列。跨行跨表事务不支持。原子存取促进数据强一致性。多版本和时间戳也可以帮助应用层解决一致性问题。
	   >扩展和负载均衡的基本单元：region。 是一个连续存储区间，存了按行键排序的若干行。region大小会被监控--从而对它们动态拆分和合并。且region可以分布在不同的服务器上，就像kafka里的分区可以分布在不同的broker，broker分布在不同的服务器上一样。从而负载均衡，也提供较强的扩展性。
		>region拆分规则：拆分瞬间完成，在达到配置的最大值--一般1GB-2GB，系统均分为二region。一个region归属一个region服务器，一个region服务器上加载多个region：一般10-1000。。region拆分相当于自动分区。
		>region服务器故障和上面的region的迁移：系统监控服务器的状态和负载，是否可用。
		>region合并：异步完成。异步地把存储文件合并为一个独立的新文件。
	   >API: 建表、删表、增加删除列簇、修改表和列簇元数据--压缩和设置块大小。给定行键值进行增删查。scan高效范围遍历，并且返回指定列、版本数。单行数据原子性的读-修改-写。单元格的值，从而就可以当作计数器使用--因为支持原子更新：一个操作中完成读和修改CAS ，这是一个全局的、强一致的、连续的计数器。
	    >协处理器：在服务器地址空间执行来自客户端的代码。
	    >包装器：集成了MapReduce框架，将表转换为mapreduce的输入源和数据池。
	   >数据存储：在store file中，称为HFile, 存储的是排序后的键值映射结构。文件里的内容以块为单位，块的索引信息存储在文件尾部。打开加载HFile文件到内存中时，索引信息优先加载，每个块默认64KB。可以用API扫描文件内容中设定起止范围内的值。块索引通过一次磁盘查询就能找到数据：对块索引二分查找，确定包含给定键的块，再读取磁盘块找到实际要找的键。
		>存储文件保存在HDFS中，HDFS可扩展、持久、冗余。
		>更新数据过程：先将更新记录写到WAL预写日志中(一般称提交日志；用于崩溃后未写入磁盘的数据恢复用)，再写到内存中的memstore中，memstore增长到设定值后会被移出内存作为HFile文件刷写到磁盘，然后系统丢弃相关的预写日志/提交日志。memstore中数据已经按照行键排序。移出写入磁盘的过程中无阻塞，因为会滚动产生一个新的空的memstore接收刷写过程中收到的更新数据。
		>删除值：因为存储文件不可改变、移除某个键值对；所以通过加一个删除标记。
		>HFile合并：minor合并：多路归并---有序行的合并，快，受磁盘IO影响。Major合并：扫描一个region里的一个列簇的多个HFile中全部键值对，忽略删除标记的数据、超过版本号限制、生存时间到期的数据(断言删除生效)，重写到新文件中，
		>LSM树：将多页块中的数据存储在磁盘中。
      >Hbase三大组件：客户端库，一台主服务器，多台region服务器。
		>region服务器：可以动态增加和移除。在zk上注册了一个自己的临时节点，这个节点会被主服务器监听(解耦了主服务器和region服务器)。
		 >提供了拆分region的接口：
		 >提供负责的region的读写接口：客户端直接与region服务器通信，处理数据操作。
		>主服务器：利用zk给region服务器分配region，对region服务器增减region以负载均衡。Hbase利用zk确保只有一个主服务器在运行。也对region服务器集群管理。不提供任何数据服务，对内也是。提供元数据的管理操作如建表、创建列簇的操作(支持列簇选择不同的压缩算法)。
		  >zk: 可靠的，高可用的、持久化的分布式协调系统。
		   >提供类似文件系统一样的访问目录和文件的功能。用来协调所有权(分布式锁)，注册服务，监听更新。
		   >每个临时节点：属于一个会话，会话是客户端和zk连接之后自动生成的。客户端会不断发送心跳--带会话id 给服务器，zk收不到则认为会话超时而删除临时节点。
		   >Zab协议：
		 
	  >Hbase架构图非常简洁：自定向下：API --> Master主服务器 + region服务器(write-ahead-log/WAL--->memstore--->HFile)---->HDFS + zk	
	  >HBase存储规模：数十亿行*数百万列*数千个版本 = TB级别PB级别的存储。
	  >HBase存储格式：内容为键有序排列的文件块和块索引，对块IO操作做了优化，最大限度利用了磁盘传输通道(I/O利用率饱和)。
      >HBase列式存储结构：无限大的稀疏的表。每行数据只在一台服务器上，所以HBase具有强一致性。多版本的原因：避免并发解耦过程导致的编辑冲突；也保留历史变化。
	  >HBase性能估计：表扫描线性时间增长，行键查找线性时间关系。	   
	  >Hbase提供的索引：单一索引，主键。也提供服务端钩子，实施灵活的辅助索引解决方案。
		   
>举动-痛点：安装和配置
	>解法/处理思想：
	  >安装：下载.gz包，tar解压，配置conf/hbase-site.xml 中的hbase.rootDir写操作的写路径。启动hbase.sh, 启动脚本子命令系统 shell, 输入status查看状态。
	  >基本命令：查看已有表list, 建表create 表名,列簇, 增加数据put 表名,行键,列,列值,列,列值... 。查看：scan 表名 。打印输出顺序：从左到右，从上到下。get 表名, 行键 。删除：delete 表、行键、列。禁用表并删除：disable 表， drop 表。
	  >机器上部署的java进程：master节点上: hadoop的NameNode, mapreduce用途的JobTracker, hbase的Master。 slave节点上：DataNode, TaskTracker, RegionServer。Master需要冗余来提高自身的可靠性。	
	  >硬件要求：2cpu*4核 。内存上：如果使用CMS收集老区---那么不应该太大：因为全量回收代价高--长时间阻塞可能导致被认为这个regionServer死掉。访问模式也影响配置的设置(交互式和批处理)。
		>用户空间：
		>超线程：
		>内存通道和插槽个数：
		>磁盘挂载到服务器上的方式：slave:JBOD master:RAID....。。带RAID固件
		>磁盘驱动器：SATA。 磁盘1TB容量。配6-12个磁盘。4*1TB: 400MB/s 。IOPS:400。 
		>机架：服务器挂载在19英寸机架上。容纳40个机架单元U. 。使用置顶式交换机将40台机器连接起来。需要80个交换机网络端口。 多个交换机+ 核心汇聚交换机CaS..。。要验证集群中的每个硬件组件。
		>网络：双端口千兆以太网卡。
		>电源供应器：PSU slave1， master2
	  >软件要求：挂载的文件系统和各种服务配置。
	    >启动和管理脚本是shell: 类Unix系统上。CentOS,RHEL改造而来，企业应用为主；Fedora工作站。Ubuntu更新过于频繁，不适合服务器操作系统。推荐REHL, CentOS
		>文件系统：HDFS依赖的本地文件系统：建议ext4,XFS
		  >ext3稳定可靠，配置：挂载文件系统时--设置noatime---禁止记录访问时间--大幅度提高磁盘读写性能。挂载选项配置在/etc/fstab中， 配置：/dev/sdd1 /data ext3 default,noatime 0 0 。对数据节点设置保留块的数量：以获取更多的磁盘空间：从5%-->1% tune2fs -m 1 /dev/sdd1
		   >缺点：格式化磁盘慢；启动慢；
		  >ext4:稳定可靠；性能接近XFS；允许单个文件达到16TB。支持1EB:10^18字节的存储空间。延迟分配：尽管会在内存中保留若干数据块以一起刷写到磁盘-减少了磁盘碎片提高文件读取性能；但是增加了服务器崩溃时数据丢失的概率。
		  >XFS: 高性能文件系统。引导服务器时格式化速度快。
		   >缺点：升级需要转储文件
		  >ZFS:可存储10^21字节数据。支持内置的压缩功能。
	  >HBase调用Hadoop: rpc调用。需要版本匹配。比如Hbase需要运行在追加了安全功能的hadoop:cdh3上。每台机器使用ssh登录，且用公共密钥认证。 	  
	  >域名服务：HBase使用本地域名汇报ip地址。ping -c 1 域名 验证正向DNS的正确性。多个接口，通过hbase.regionserver.dns.interface指出主接口。
	  >同步时间：集群中运行NTP以同步时间。
	  >文件句柄和进程限制：太小会有IOException。文件描述符的个数上限：计算：每个region3个列簇，每个列簇3个存储文件，每个region服务器100个region, 则就已经需要打开3*3*100=900个文件；使用lsof -p REGIONSERVER_ID查看确切打开个数。还需要编辑/etc/sysctl.conf调整fs.file-max的值。
		>xcievers需要正确配置。
	  >交换区： 避免内存溢出时，需要给操作系统进程足够空间；如果空间不足，那么os会使用交换区---机器磁盘中独立的分区；服务器避免使用--因为一旦使用了交换区：用户sshd挂起而无法登陆。所以对regionServer来说要减少交换空间的使用：配置/etc/sysctl.conf 下的vm.swappiness=5或者0。。。sysctl -p重启服务器生效。	
      >分布式文件存储系统：HDFS最好，但是可以用自己实现的分布式文件系统---只要满足hadoop的规定。		
		>HDFS: 稳定可靠；保障机制：冗余、容错、可扩展。HBase并不维护自身存储文件的副本。
	  >本地模式：只需要配置file:///path
	  >hadoop集群：并行流式的处理能力。扩展好、系统可靠、冗余。HBase增加了随机存储层，是HDFS的补充。hdfs://namenode:port/path 
	   >hadoop模式：原生模式。块模式。跟弹性计算云环境有关。
	  >KFS: c++开发。也可以作为hbase的存储系统。
	  >伪分布式模式：hdfs也部署在本机上，在hbase-site.xml中配置hbase.rootDir 为 hdfs://localhost:9000/hbase 数据副本个数：dfs.replication=1
	  >完全分布式模式：hbase-site.xml中配置：hbase.rootDir 为hdfs://namenode.foo.com:9000/hbase 即NameNode所在节点，同时指定hbase.cluster.distributed=true
	   >同时：/conf/regionservers文件要罗列regionserver的Ip
	   >zk: 作为hbase管理则hbase-env.sh中HBASE_MANAGES_ZK=true 等。独立zk集群需要在hbase-site.xml中配置zk连接地址和客户端口号。
	  >配置文件：
	   >conf/hbase-env.xml: 启动关闭所用到的环境变量。如可用文件系统和zk可用地址。复制conf目录到其他节点：rsync工具同步到集群中其他节点上。
	   >conf/hbase-site.xml: 部署模式，zk配置，对HDFS特定配置。hbase中配置高于从而可以覆盖hadoop中的配置。
	   >scr/main/resources/hbase-default.xml: 默认配置；进程启动则读取；可以被hbase-site.xml覆盖相关配置。
	   >hbase-env.sh: jvm启动参数的配置。
	   >regionserver: 罗列了所有region服务器 主机名。HBase运维脚本会读每一行来启动对应的region服务器进程。master启动时注册到zk中。
	   >log4j.properties:
	  >使用脚本同步配置文件：
	  >动态环境中运行HBase集群：如公有云服务平台。
	  >Whirr: 帮助动态环境中快速部署完整的hbase集群。Puppet , Chef也可以: 中央配置服务器+脚本发送到服务器并执行。
	  >Web UI: 可以查看master或者regionserver上的运行信息情况。
	  >Hbase shell :ruby开发的。
		
>举动-痛点：客户端API
	>解法/处理思想： 
	 >主要客户端接口：org.apache.hadoop.hbase.client.HTable类提供。HTablePool。存储和检索数据、删除无效数据。这个类的实例创建比较耗时：因为会做表的存在性、可用性验证。
      >单行数据PUT: 指明行键：byte[]数组即可。添加行的列簇-列数据：一个Put对象可以add多列数据(每列数据也可以不指明时间戳)。
       >已设置的数据GET: PUT对象中的指定列簇、列；返回List<KeyValue> keyvalue就是一个单元格的值---有行键列簇列时间戳限制。HBaseHelper内部类会创建有特定行与列数量的数据测试表。
       >Put中允许提交一个rowlock行锁：修改数据时避免其他客户端访问这些行。
	   >getKey()用不到，是一个坐标；getRow()就是设置的行键。
	   >一次Put是一次RPC操作，网络耗时存在，可以配置失败重试。LAN是1ms一次。所以存在批量Put----- 	启用客户端缓冲区.setAutoFlush(false), 然后手动提交(在put了若干行之后，数据存在本地进程中)flushCommits(), 客户端会自动将Put对象按存到的regionserver分组, 然后分别发送给对应的regionserver。缓冲区大小setWriteBufferSize()一般设置2MB; hbase.client.write.buffer。put()操作会比较缓冲区数据大小可能触发调用。
	   >服务端也有缓冲区：接收到的Put先放到缓冲区中，再一个个处理。服务端还会对列簇存在性进行检查。失败的Put会单独返回给客户端；失败的Put实例也会在客户端缓存存在，需要手动清除---访问本地缓存区。客户端也会基本的检查。
	  >处理配置属性：加载hbase-site.xml, hbase-default.xml的内容， HBaseConfiguration.create(), 自然的加载之后可以覆盖。scan时每个版本数据也占一行，可以输出3个版本{versions =>3}而不是最新的版本--------不指定参数就会只返回最新版本数据。
	  >比较器：KVcomparator等实例已经静态化；直接引用，从而可以用来做Set\Map的数据的有序化的比较器：而其中的数据类型就是KeyValue。同样还有KeyComparator, RowComparator
      >单行数据删除Delete: 也是单元格级别。但是DeleteColumn则删除列， DeleteFamily删除列簇	  
      >检查写：check and put: 在服务端值检查成功才put: 使得Put操作有原子性：调用checkAndPut()方法。  用于不存在则插入，是某个值才插入的情形。
	  >单行取数据GET: get()指明行键row:byte[] 增加列簇、列、时间戳范围/版本数限制。
	   >regionserver有个块缓存：保存最近存取过的数据
	   >Result类：根据get()返回的结果量的不同，可以进一步调用它的方法查询精确的数据。raw()返回原始的即有序的KeyValue数组。
	  >批量请求GET: get(List<Get>)可能发往多个regionserver
	  >查找row行或者上一行：getRowOrBefore():用来指定一个大的不存在的rowkey ,从而返回已经存储的最大的rowkey----即按照字典序排在表末尾的那一行。
	  >单行删除DELETE: delete()指明row:byte[]， 添加列簇、列等限制。
	  >批量删除DELETE: delete(List<Delete>)
	  >检查成功则删除：checkAndDelete()
	  >批量处理跨多行：batch(List<Row>) 增删查操作Put , Delete, Get都可以放到List<Row>里。同步操作，不缓存，直接发送到服务端。
	  >行锁：一般不要使用。服务器端隐式加锁。客户端可以显示加锁---显示的从服务端获取锁：。锁超时时间设置：hbase.regionserver.lease.period
	  >读不加锁：因为服务端有mvcc多版本并发控制，不会读取正在写的数据；变动应用到整个行之后，才能被读出(将最新版本的指针指向新写成功的数据)：否则那个正在写的版本不会被读取出----而是读取的老的版本：这就是mvcc好处：写的过程中还可以读--读老版本。
	  >扫描： Htable.getScanner(); 得到ResultScan对象，补充[startRow, stopRow)限制，可以再加addFamily()列簇、列限制；同样精确到时间戳、版本数也可以；
	   >扫描最后需要close: 查询可以next()一行或者next(int nrows)下n行一起返回得到一个Result[]数组(但实质实现是循环N次调用)；自然的，行键设计时要保证字典序是需求；因为返回的顺序就是字典序。
	   >扫描器缓存：表级--所有scan都生效，scan级--单个scan生效； setScannerCaching() setCaching()。缓存太大，会导致查询文件过多反而更慢；
	   >扫描器租约时间：超过之后scan就会报错； 。要延长必须修改各个regionserver上的hbase-site.xml配置；
	   >批量：缓存面向的是行，批量面向的是列；setBatch(int) 可以返回多列，多个列一个Result对象来包括；
	   >批量-缓存量-result个数的关系：批量的大小，决定返回的列数多少---最多到一行的列数个；一个批量需要一个Result来保存；10行数据，则有10行*每行的批量数 个总批量；即这么多个Result。而缓存个数决定一次rpc取回多少个Result, 比如缓存数为10，总共200个Result, 那么会发送20+1次rpc请求，最后一个是确认扫描完成。
	  >跟踪日志：可以建立一个日志Logger,记录某个包路径下的；再建立一个过滤器AppenderSkeleton，统计这个包路径下的输出日志中的某些信息； 
	  >查看region的位置：getRegionInfo() 
	 >高级特性：
	  >过滤器：filter 。在服务器端生效，叫谓词下推。
	  >CompareFilter: 实例化时需要指定CompareOp比较运算符和WritableByteArrayComparable比较器
	  >RowFilter: 行键过滤器：小于等于大于，某个值，匹配某个正则。
	  >FamilyFilter: 列簇过滤器：在列簇一级过滤掉数据--筛选出所需的数据。
      >QualifierFilter:列名过滤器：
	  >ValueFilter: 值过滤器：
	  >DependentColumnFilter参考列过滤器：基于参考列的时间戳进行筛选：不仅可以设置过滤运算符、比较器，还可以设置比较列。	
      >专用过滤器：只用在scan时：		
		>单列值过滤器：用一列的值决定是否过滤掉一行。SimpleColumnValueFilter
		>前缀过滤器：行键前缀匹配模式：PrefixFilter
		>行键过滤器：只返回KeyValue对应的key, 而不必返回实际的更多的数据。
		>包装过滤器：SkipFilter包装一个filter之后，这个filter返回的一个或多个列值为空的行会被过滤。
		>全匹配过滤器：只要被包装的过滤器开始跳过某一行，则立刻从此时返回数据WhileMatchFilter
	  >过滤器列表：FilterList，多个过滤器的组合，关系包括：全部通过、至少一个通过MUST_PASS_ONE	
	>计数器：
	  >一个列当作计数器使用：CAS保证可以原子性的读写一起操作。而使用锁有客户端崩溃锁未释放而等待超时--导致超时期间不可用的灾难性后果。
	  >一次rpc可以同时更新一行的多个计数器。
	  >终端操作：incr 'table', 'row', 'column', increment-value(可正可负)
	  >多计数器：Increment, 指定行键rowkey:byte[], 增加列簇、列、增加量这些配置，可以实现对一行的多列计数器进行递增操作。
	>协处理器：CoProcessor将计算移动到数据的存放端。数据的处理流程直接放到服务端。像存储过程、触发器，像mapreduce 
	  >observer: 与触发器类似。回调函数在一定的事件发生后执行。
	   >RegionObserver: 处理数据修改事件。
	    >region生命周期的pending open-->pending close阶段起作用：是一种过程干涉的被主框架钩子勾住的回调函数。可以在各种region事件做之前被调用---比如拆分、回写、合并、关闭(被转移到其他regionserver)。
		>客户端各种API调用之前之后也可以加钩子函数。
		>可以访问WAL日志getWAL() HLog
		>ObserverContext: 通知协处理器框架回调函数执行完毕时需要做什么。比如在某一步时停止继续执行---不再执行剩下的协处理器--bypass()。
	   >MasterObserver:管理DDL操作。集群级别事件。
	    >提供各种DDL钩子函数：内部可以利用环境对象而对服务器做一些如文件创建的工作，但是钩子函数没有返回结果---毕竟这是一个类似触发器一样的功能：在事件发生时做一些事情。
		>提供region分配、移动、负载均衡操作前后的钩子函数：
		>提供master关闭前后的钩子函数: 
	   >WALObserver: 控制WAL的钩子函数。	
	  >endpoint: 类别存储过程。服务端计算工作。实现CoprocessorProtocol接口
	   >通过添加一些远程过程调用来动态扩展RPC协议。
	   >程序编写：继承BaseEndpointCoprocessor类，获取环境对象，并从环境对象里获取Region对象，然后对Region对象可以进行遍历，获取所有的行、单元格。
	   >程序分发和回调：HTable.coprocessorExec(.class, , , ,new Batch.Call(){}) 返回的就是每个Region上执行统计类程序后的统计结果。
      >用户可以上传region级别的程序操作：
	  >用户可以监听隐式的事件：
	  >用户扩展rpc调用：
	  >使用场景：使用钩子关联行修改操作来维护一个辅助索引
	  >通过优先级和序号控制协处理器的加载顺序：
	   >静态加载：hbase-site.xml中配置：hbase.coprocessor.region/mater/val.classes 三项配置会被对应的CoproccessorHost加载---比如MasterCoprocessorHost加载...master.classes里配置的Class
	   >标描述符来加载：HTtableDescriptor.setValue() 配置：jar路径(hdfs路径)/完整限定类名/优先级(SYSTEM/USER)
	  >特征：不能对一行数据加锁：
	  >生命周期：UNINSTALLLED --> INSTALLED--->STARTING--->ACTIVING--->STOPPING--->STOPPED。执行顺序，客户端请求--->服务端：系统级协处理器(在一个环境Enviroment里)、用户级协处理器(在一个环境Enviroment里)，两个环境都在一个HRegion的CoproccessorHost里。---->返回给用户。
	  >组件元素：Coprocessor, CoprocessorEnviroment, CoprocessorHost 
	  >HTablePool: 需要自己管理HTable.	
	  >HConnection: 客户端连接共享HConnection, 尤其是到zk的实例。获取-ROOT-和.META.	
	  >多个HTable: 可以共享同一个Configuration	
	>管理功能DDL,DML：使用了Hadoop RPC框架，框架要求远程方法中的参数都实现Writable接口以实现序列化反序列化。
	 >表描述符：HTableDescriptor会作为存储路径的一部分来使用。	
	  >列簇描述符：HColumnDescriptor可见字符。列名不必。甚至空列名也可以。
	   >最大版本数：每个值保留的最大版本数量。
	   >压缩：压缩算法可以选择NONE, GZ,LZO,SNAPPY
	   >存储块大小：HFile文件中的划分的若干个小存储块。默认64KB类似RDBMS中的存储单元页。所以一个HFile里大约有1000个存储块。在get/scan时会被加载到内存中。
	   >缓存块：内存缓存中。
	   >生存期TTL: 版本数据保存时间。
	   >在内存中：设置为true则承诺数据块加载到缓存区并保留较长时间。
	   >布隆过滤器：增加了存储和内存负担。减少特定访问模式下的查询时间。
	   >复制范围：跨集群同步的功能。
	  >region分裂时的大小：默认为256MB。。但不是就这么多，毕竟不会跨一行存储，一行数据如果10MB那么是可能会超过的。
	   >一个region多个列簇：一个region的一个列簇下的内容对应可能有多个压缩文件/未压缩文件。
	  >只读设置：
	  >memstore也就是HFile大小：默认64MB.所以默认情况下一个region最大4个HFile存储文件。
	  >WAL保存到磁盘的方式：延迟刷写
	 >HBaseAdmin: 修改表结构、列簇结构等功能。	
	  >通过表描述符建表：表描述符对象可以加列簇描述符对象。同时可以设置region起止边界和region个数，进行预分区，可以打印各个region分区起止位置来查看。	
	  >禁用表后可以删除表：禁用会等待内存中的数据刷写到磁盘，所以耗时。禁用后可以重用。	
	  >修改表结构：必须先删除表结构。然后重建表。
	  >集群管理：如让已经在region服务器上线的特定region下线。
	   >拆分合并region都可以。
	   >将一个region从一个regionServer移动到另一个regionServer也可以。
	   >关闭master, regionserver, 集群都可以。
	   >集群状态信息：活着的和不可用的regionserver数量。regionserver上的存储文件的数量和总存储量、已用写缓存的大小。RegionLoad甚至可以统计当前region统计周期内的TPS, WPS,RPS读请求量和写请求量等。
	>其他客户端：REST,Thrift可以作为网关服务器。
	  >REST 网关服务：如./hbase-deamon.sh start rest。。REST服务无状态。从而以http方式访问htable比如：http://servername:8080/表名/行键/列簇:列    返回值经过base64binary编码。 curl方式可以指定 -H 'Accept: application/json'	
		>Protocol Buffer: 编码效率较高，最合适的网络传输协议，提供了较高的带宽利用率。可以用curl -H "Accept: application/x-protobuf" ... 来实现调用返回protocol buffer协议的二进制数据。
		>Raw Binary: 同样设置报头'Accept: application/octet-stream' 只有单元格内的值返回。
		>REST风格的JAVA API也可以用。
	  >Thrift: 安装好,编译模式文件，并生成用户指定语言的RPC代码。
	   >启动thrift server:  bin/thrift start 无状态的。
	  >Avro: 安装好，根据选择的编程语言 编译 模式预定义文件。	
	>批量处理：
      >使用Clojure编写mapreduce程序来获取hbase表。	
	  >Hive: 提供了基于hadoop的数据仓库。Facebook开发。
	   >安装hive: 配置$HIVE_HOME/conf/hive-env.sh中添加HADOOP_HOME=xxx HBASE_HOME=xxx
	   >启动并创建一张本地表：/bin/hive   create table name(col type)   。。。可以实现复制hive表中的数据到hbase表中。
	  >Pig: 提供了一个分析海量数据的平台：命令式编程风格，迭代式执行。更适合并行处理。
	   >可以读取不同的数据源：HDFS, HBase都可以。
	   >创建安装：是一种Pig shell  。可以将一个日志文件加载到HBase中。
	  >Cascading： 新的计算模式API : 数据被表示为元组，形成了一个元组流，
	   >汇总数据到HBase: 
	>Shell: 基于Jruby , 后者基于Ruby实现的java虚拟机。5类命令，并代表了它们之间的语义关系。	
	 >引用名、引用值：单引号和双引号。
	 >逗号分隔参数：
	 >命令：
      >普通命令：status/version
      >DDL： 表级操作。创建、禁用、启用、存在性验证。	  
	  >DML: 删除单元格、返回一个单元格get,put, scan 
      >工具：assign分配一个region到另一台regionserver	  
	>基于web的UI:
     >masterUI: 集群状态、服务的表、...zk链接，regionserver链接、本地日志链接。打开中、关闭中、拆分中的region。。。手动合并、拆分.。	
 		
>举动-痛点：与mapreduce的集成 
  >解法/处理思想：
    >mapreduce: 被设计为可扩展的方式解决TB级别数据处理问题。性能随机器数增加而线性提升的系统。
	  >分治原则：数据拆分到分布式文件系统的不同机器上，数据块
	 >功能类：
	  >拆分数据：InputFormat:TableInputFormat(对表划分出合适的块)：TableRecordReader(迭代获取每个数据)	
	   >TableInputFormat按照region来划分边界：拆分块的数目和设置的起止键之间的region数目相等。每个块对应一个TableRecordReader----读取并遍历一个region的所有行；
	  >将原始数据转化为更有用的类型：Mapper:TableMapper(key-->ImmutableBytesWritable, value-->Result):IdentityTableMapper(键值对传递到下一个阶段)
	  >将Mapper的输出shuffle/sort传递给Reducer: 一个键和它所有可能的值排序后 传递给Reducer处理：
	  >将数据持久化到不同的位置：OutputFormat:TableOutputFormat:TableRecordWriter(将数据写到特定的HBase表中)
	 >支撑类：
	  >Hadoop块备份策略：
	>任务的编写：Mapper类继承TableMapper<Text, IntWritable>, Reducer类继承Reducer<Text, IntWritable, Text, IntWritable>
	 >构建Job实例， TableMapReduceUtil.initTableMapperJob()参数中包括Scan查询限制、Mapper类，Reducer类等。
	 >读写任务：Configuration配置的是目标列簇/列(HTable使用)，Scan是查询过滤条件，Mapper里map()入参是每一行，结果是一个Put;。没有reducer需要设置.setNumReducerTasks(0)
    >任务的执行：hadoop -jar target/hbase-xxx.jar 主类\ -t 表名 -c data:json -o analyzel
		
>举动-痛点：架构 .数据的组织结构，增删改查过程。持久化机制。
  >解法/处理思想：
    >B+树好处：IO次数少、范围查找快。特征：叶子节点有序连接，中间节点有序，叶子节点至少半满。少则借，满则二分。
	 >页表：块，即叶子节点。包括的信息：上一页指针，下一页指针，key-rowid, key2-rowid, key3-rowid..。。页表是一个数组，头两个元素分别指向上一个数组和下一个数组。
	 >OPIMIZE TABLE: 按顺序把表重写，使得相邻的叶子节点也是磁盘上相邻的。从而范围查询变为了磁盘的多段连接读取。
	 >适合于读多写少：
	  >任意位置写会导致快速产生大量的碎片化的页：更新和删除以磁盘寻道的速率完成。
    >LSM树： 先把数据有序放到日志文件。日志文件的更改先放在内存，内存满了，则磁盘上创建一个新日志文件，并刷入这些有序后的更新。
	 >数据归并：后台线程执行。将小文件聚合为大文件----因为小文件都是有序的，所以是归并算法，较快。最终只有几个数据存储文件，使得磁盘查找更快。	
	 >磁盘上的树结构文件：	
	 >查询：自然先查内存，再查磁盘文件。
	 >删除：是一种带标记的更新。查询时跳过，重写页/文件时丢弃。	
	 >数据记录带时间：所以在设定的TTL之后，后台合并进程会在重写数据块时丢弃过期的数据记录。
	 >磁盘利用方式：存储的随机查找能力---访问磁盘的次数logN
					存储的连续传输能力---LSM利用了，比随机更新B树快1000倍，也比批量更新B+树更快100倍。因为使用了排序和合并方法。
	  >能较好的扩展以处理大量的数据：使用日志文件和内存存储来将随机写转变为顺序写。(批量合并文件操作)//读和写独立，两种操作之间没有冲突。
	   >强调的是成本透明：5个存储文件，一个访问最多需要5次磁盘寻道。
	>存储：(分布式算法考虑：一定想象一个集群，一个客户端写入成功，其他客户端开始读。paxos算法保证集群中的一致性节点越来越多，而R+W>N来保证读取的数据是最新的且各个客户端从那时开始就是一样的)
     >客户端联系ZK来知道该查哪个region server: 查找行键;并且客户端会缓存这次查询。从zk中获取带有-ROOT-的region服务器名，通过这个查询到含有.META.表中对应的region服务器名, 。
	 >HRegionServer负责打开region, 并创建对应的HRegion实例: 同时，在HRegion打开之后，会为每个表的HColumnFamily创建一个Store实例，一个Store实例多个StoreFile\一个memstore, 一个HRegionServer只有一个HLog
      >数据流动顺序：客户端put--->WAL--->memstore-->HFile。每次memstore满了而创建新的memstore,旧的memstore写入磁盘形成一个HFile，同时会保存最后的写入序号---从而系统实时的知道持久化的文件的数据的起止位置--哪些数据被持久化了。regionserver关闭前会检查memstore，有数据则强心刷到磁盘。这种强刷后的region自然不需要在转移到其他regionserver时重做WAL..。。预刷写、二次刷写。	
    >文件：hdfs中的/hbase		
	 >/hbase/.logs/下每个regionserver有一个子目录，子目录下是一个HLog文件的多个回滚日志。默认一个小时滚动一次。append机制追加写入文件。
      /hbase/.oldlogs/存储的是region上的已经被持久化的修改对应的日志文件，自然的，已经不需要这个log了，所以被转移到这里。并在默认的10min后被master删除。	 
		.splitlog, .corrupt文件。存储日志拆分过程中产生的中间拆分文件和损坏的日志。
	 >表级文件：每张表都有自己的文件，.tableinfo存schema信息。每个列簇有单独的目录--目录名称包含部分列簇名的MD5值。	
		/根目录/表名/regionname/columnfamily/filename
		./tmp是保存的一次合并的重写文件，临时文件。
	 >WAL的拆分和回放：WAL回放时，没有提交的修改都会被写入region下一个单独的文件。当region被打开时，这些单独的文件中的内容就会被回放。	
	 >region的拆分：几秒内。先在splits目录临时存放两个region信息，后转移到表目录。	
		./tmp目录代表进行过压缩。
		./recovered.edits目录代表WAL进行过回放。
	  >拆分时间：存储文件总量到达阈值，列簇层面配置的阈值。拆分完成之后，会告知master,master可能会选择将新region移动到其他regionserver
	  >region在.META.表中有表项。
     >一个HRegion只有一个memstore: 只是随着写入的增加，会产生越来越多的HFile/StoreFile。
      >查找时定位到Region的过程：三层。从zk中找到root region位置信息的节点；再从-ROOT-表中查找对应 meta region的位置，再从.META.表中查找用户表对应的region的位置。 
       >.META.表：存储了各个用户表的各个Region对应的位置(regionserver等)。
	   >-ROOT-表：存储了.META.表的位置。在一个region上，该region在一个regionserver上。
	   >zk: 存储了-ROOT-表的位置。/hbase/root-region-server 
	  >region的状态：发送打开状态--->打开中--->打开完成； 发送关闭状态--->关闭汇总--->关闭完成; 服务器开始切分region--->切分完成。
	 >每个列簇都是用多个存储文件进行存储:(每个列簇都有一个memstore?) 存储文件里的就是有序的KeyValue实例。甚至每列的KeyValue都存在不同的HFile存储文件中？---从一个Region只有一个memstore来看，说明一个region所包括的存储范围就是memstore的存储范围。
	 >存储文件的合并：HFile文件数目达到阈值，会触发合并，合并直到产生一个文件它的大小大于配置的最大存储文件大小，从而开启拆分	
	  >压缩合并：memstore写到磁盘后触发minor合并默认最多10个文件到一个大文件。major	合并为周期的，默认24h, 把所有的文件压缩为一个单独文件---过程中会检查删除标记的记录并丢弃。
	 >HFile格式：多个文件块+末尾索引(记录文件块/meta块的偏移量)。块大小由HColumnDescriptor配置--默认64KB：实际更大。块小写得多，查更快--但也需要存储更多索引。StoreFile是对HFile的轻量级封装。
	  >每个文件块里多个有序存储的“操作-KeyValue”: 每个KeyValue: 是一个字节数组，紧凑地存储row, columnfamily, column, timestamp, type 维度精确到的value，允许零拷贝方式访问数据。可以将信息对应转换为Java类KeyValue。相似的维度键是有序存储的，如果存储采用压缩算法，那么重复的维度信息会被大大减少。
	  >QueryMatcher和ColumnTracker:
	  >Store实例可以载入HFile中的单个文件块/数据块(偏移量索引到之后)：然后扫描读取其中包括的目标数据，也是scan做的事情。
	  >定位到HFile的过程：使用keyvalue里的时间戳和布隆过滤器来跳过绝对不包含所需KeyValue的文件。从剩下的HFile和memstore中扫描匹配的键。
	   >扫描过程：RegionScanner类实现。为每个Store实例创建一个StoreScanner,每个Store实例代表一个列簇。StoreScanner对存储文件和memstore有过滤查找功能。从存储文件中按照时间降序读取KeyValue,直到读到不需要的行键。
	  >即便是一行一列对应的3个版本的KeyValue：都可能分布在不同的存储文件中：自然的，一行数据，不同的列簇，都在不同的HRegion上。
	 >WAL: 	存储了对数据的所有更改。很明显，事务提交成功后(比如成功写入磁盘/memstore/HFile)，会在WAL中插入一条保存点之类的信息。使得如果服务器崩溃后，就会将上一条保存点到到末尾的所有更改命令都执行一遍：即回放一遍。自然的是，如果WAL写入失败，那么整体也被认为是失败的。
	  >一个regionserver上的所有region共享一个WAL: 自然的保存点的刷入只需要增加region编号即可。
	  >回放过程：因为WAL保存在HDFS上，所以一个regionserver崩溃了，其他regionserver可以读取到这个regionserver的WAL，从而恢复其中某个HRegion上没有提交的数据更改；自然的，HFile也是可以被访问到的。	
	  >HLog类实例：注入到了HRegion的构造器中，从而共享。	
	  >使用的是Hadoop的SequenceFile为实现类：键值对形式。值是客户端的修改请求。键是HLogKey
      >HLogKey: 存储了一个region的HFile中meta块里的最大索引号---序列号，也存储了region名和表名。存放了KeyValue的归属。也存储了修改写入日志的时间，集群ID.
	  >有复制因子：LogSyncer类来sync()同步写入日志，涉及一个一对N的服务器管道写，N就是复制因子。高代价操作。
	 >WALEdit类：通过日志级别管理原子性。对一行的n个列的n个更新KeyValue都封装在一个WALEdit类实例里，那么会在一次操作中写入一个WALEdit类实例的所有KeyValue。
	  >LogSyncer类也可以作为一个独立的线程中运行。定期1s调用sync()将缓存的修改写入HDFS.
	  >LogRoller类作为日志滚动功能处理：60min旧日志文件被关闭，然后使用新的日志文件。由于记录一个当前写入序号----HFile写入的末尾的键的序号，所以小于这个序号的日志文件都可以移除到oldlog里了。
		滚动的其他条件包括日志块大小：达到配置的95%
	 >回放：
      >日志拆分：将WAL中的日志按照region划分，单独放到对应的region的日志目录recovered.edits中。	 
	  >日志回放：region打开时，从recovered.edits文件目录下读取数据更改记录，加到memstore中，然后执行一次强制的磁盘刷写写入一个HFile中。然后删除recovered.edits
	 >zk重要目录：
       >/hbase/root-region-server: root region所在的regionserver
	   >/hbase/rs目录下所有节点znode都是临时节点；是所有regionserver的根节点。
	   >/hbase/shutdown集群状态信息。
	   >/hbase/splitlog 
	   >/hbase/replication包含副本信息。
	   >/hbase/table 禁用的表所在的地方。
	   >/hbase/unassigned 包含未打开的region的znode, 
	 >zk队列：当从集群rpc不可用时，主集群上WAL回滚信息发送到这个zk队列。
	 >复制：在集群之间传递数据：可以作为灾难恢复的方法，提供hbase层次的高可用性。	目前是实验性的功能？
	   >架构模式：主推送(给从集群)。类似mysql的主从复制，只使用二进制日志来跟踪修改。这里使用WAL----自然的，主集群可以将数据复制到任意数目的从集群。复制异步进行，主集群上的数据复制到从集群上有延迟，但是数据是最终一致的：落后一定的时间。
	   >复制格式：所有的WALEdits都会被发送，保证原子性。因为一个WALEdits保存的是对一行的各个单元格的Put/Delete多个操作。  
				  HLog也会复制，且要求HLog在HDFS上。
	   >复制对象：WAL文件。发送到从集群随机一个regionserver上。收到之后，按表分到不同的缓冲区，然后使用HBase客户端将缓冲区的数据并行写入表中。
					批量读，随机发，发完一次更新WAL读取的位置到znode上。
	   >zk上发生的事情：每个zk节点都有一个会话，如果丢失,则它的队列被锁定，那么regionserver会竞争创建锁，成功的regionserver将锁定的队列转移到自己的对等znode下。
	 
举动-痛点： 高级用法 
   >解法/处理思想：行键设计
     >数据的逻辑布局和物理布局：物理布局：同一个行键范围的2个列簇数据，一定存放在不同的存储文件中。对一个列簇来说，如果数据量太多，比如单元格版本多，那么也可能会分在不同的存储文件中，但是是连续存储的--且按照时间戳倒序的。
	   >事实上，整个KeyValue的键都是从左到右降序排列存储的。键可以被称为限定符。
	   >高表：行多列少。Hbase按行分片, 即如果有一列簇有几百万列，那么依然存在一个HFile中，且不会拆分---即便超过了HFile的最大限制(单元格内容太多导致)，
	   >宽表：可以使用行级原子性。
	   >行键设计：尽可能将查询的维度信息保存在行键中。因为筛选效率最高。
	    >部分键的扫描机制：左对齐索引。使用scan方式：起始键--->起始键+1。即按照字符的字典序增长方式刚好卡住目标范围。
	    >行键中加日期：排序使用。
		>行键中加序号：分页使用。
	    >避免热点region的写：salting加盐方式：即对于那些时间戳为rowkey的写，会导致某时刻只有某个region热点写，所以要对时间戳hashcode并取reiong count的余数，来加到rowkey前面。来分散存储。读取的时候也可以分散读取加快读取进度。
	   >列簇下的列可以作为辅助索引：单独进行排序。因为HFile中的KeyValue中的Key维度里就包括了列簇、列、版本。 
	    >列值可以设置为嵌套的结构：
		>facebook收件箱检索系统：rowkey:userid, col:关键词, version:消息id , val: 关键字在消息中的位置.。多少个列就多少个关键字，多少个版本则该关键字就对应多少个消息。
	    >列索引：
		>内存中建立索引：
		>辅助索引：比如存储搜索条件--一些前缀之类的映射。
	   >实现索引方案：
	    >基于协处理器：
		>基于contrib.BuildTableIndex类：扫描整个表，而建立Lucene索引。存在hdfs上，然后下载到Lucene服务器上，可以合并这些索引。用Lunece查找获得行键，再到HBase上搜索完整的数据。
	    >直接在HBase中建立倒排索引表：词为rowkey, 列为docid。
      >事务：事务型HBase上region上保持了一个事务列表：开启事务-操作-提交事务	    
	  >布隆过滤器： 适合于少数文件更新的场景。开销：一行需要额外一个字节。选择：行级过滤器，行加列级过滤器。
	   >使用原因：HFile中的文件块索引只是索引了块的开始键。要判断一个键是否在一个块里只能加载这个块。而一个块里有几百个KeyValue， 一个文件里有1w多个文件块。所以要尽可能增强判断一定不在的文件和文件块的能力。
	   >minor合并：只是合并最近的几个小文件为不超过的设定大小的文件
	   >块索引：能确定文件中是否包含某个行键。而使用布隆过滤器则可以直接判断文件中是否有某个行键。每个块都要被依次加载。
	  >版本控制：服务器时间不同，而导致单元格的版本数据顺序可能因为region的迁移而产生先插入而在后面的效果。
	   >major合并之前，如果保留版本为 3个，插入6条数据，然后删除2条数据，那么再次查询，会把更老的版本也查出来---即未保留的版本。因为major合并才会清除这种数据，而查询的时候不过滤这种数据只过滤删除标记的数据。
	   >删除数据：会带上一个当前时间戳，导致再次插入老时间的数据时插入不进去，通过对表的刷写和major合并之后可以删除这个删除标记，再插入老数据就可以了。
	   >自定义版本控制；

举动-痛点： 集群监控
   >解法/处理思想：一系列工具
     >从Hadoop里继承了一些API: hadoop是面向批处理的系统，hbase往往被用来处理在线随机访问请求。
	 >常用工具：原生Ganglia, 导出数据后处理的Nagios
	 >监控框架：MetricsContext:监控数据点的生成、绘图和监控。
	  >将监控指标推送到Ganglia: GangliaContext
	  >将监控指标写入磁盘：FileContext, TimeStampingFileContext
	  >指标的值：有的直接读，有的却需要计算----有的还需要复位。
	 >region服务器监控指标：
	  >块缓存追踪命中率：+缓存失效数目
	 >Ganglia: 准备数据，被拉取。
	 >多层次监控指标收集：合并下游监控指标数据，
	 >Nagios: 获取与集群状态相关的定性数据的支持工具。
	  >定期拉取当前的监控指标：并和阈值比较，超过则开启规避动作：包括发送电子邮件、打电话、短信、
	  >用JMXToolkit将Nagios和HBase集成到一起。即由JMXTookit提供监控指标的值。
	
举动-痛点： 性能优化
  >解法/处理思想： 
    >regionserver中region的内存占用情况：老年代：20%块缓存，40%的memstore
	 >日志配置：有规律：详情+时间戳-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/log.log  
	 >年轻代：不建议配置太大，因为暂停时间大。毕竟+UseParallNewGC...其他参数 -Xmx -Xms -Xmn -Xss
	 >年老代：要注意CMS有备份回收算法。当老年代在CMS后空间还是不够，会采用后备方案：ParallOld算法/SerialOld算法，复制算法方式暂停而整理。
	>MSLAB缓冲区：只允许从堆中分配固定大小的对象。这样不会产生提升错误。因为都是固定大小的块，利用率比较高，空洞很小。回收也是整块的。本质就是使用固定大小的一个个缓冲区来存储KeyValue， 默认这个块缓冲区大小为2MB。。虽然避免了后备方案的启用，但是每次分配可能稍微慢了一点。
    >压缩： cpu耗时更短于读取磁盘。Snappy采用-快且cpu消耗小，仅仅压缩比不大1/5。	
	 >列簇上配置是否采用压缩，采用什么压缩算法。
	>预拆分Region: 
	>负载均衡：master上的一个周期运行的任务，判断各个regionserver上的region负载情况，看是否需要重新均衡。
	>静态合并：HBase集群没有工作时,hdfs集群工作，也可以通过脚本实现region的合并。
	>最佳客户端实践：
	 >禁止自动刷写：而是等客户端缓冲区满了之后才发送到regionserver
	 >使用扫描缓存：但效果有限。不能太大，否则传输开销和内存开销都太大。
	 >限定扫描范围：列的范围尤其需要哪些就只返回哪些。
	 >关闭ResultScanner: get之后结束前需要释放关闭close()
	 >块缓存：频繁访问的行。
	 >行键获取方式：只是获取行键，那么使用特殊的过滤器，操作符为MUST_PASS_ALL
	 >关闭Put的WAL: 是一种冒险的方式，数据直接放到memstore里，但效果不明显。可以批量导入更好。
	 >配置优化：
	  >regionserver-zk超时时间：缩短一些。
	  >增加regionserver响应处理线程数：对于get数据量较小时更建议。
	  >对存储文件启用压缩：Snappy压缩算法。
	  >更大的region: >256mB如1G, 来更稳定。只是合并时停顿时间更长。
	  >增加块缓存比例：从20%提高一些。如果因为有很多的文件块从HFile中换出时更是。
	  >调整memstore内存比例：从40%调整小些--对于主要是读数据时是如此。
	  >增加阻塞时存储文件的数目：
    >负载测试/评估：PE  随机或者顺序的读写测试。YCSB专门对写操作评估
	
>举动-痛点：集群管理
   >解法/处理思想：
      >添加备份master:	可以在backup-masters文件来指定备份服务器。在master和regionserver启动之后才启动。通过zk协调主备。2-3个备份节点足够。
      >添加regionserver: 会启动一个regionserver rpc监听端口和webUI监听端口。启动方式：在master上执行start-hbase.sh. 在自己的节点上也可以启动，启动之后注册到zk, 并添加到集群，然后被分配region, 然后开始接收请求。
	  >导入导出mapreduce作业：导出表到一个指定的hdfs路径上，还可以增加行键正则匹配。也可以增量导出。//直接复制/hbase下的文件值得商榷。
	   >将一张表从主集群复制到从集群：
	   >批量导入：bulkimport
	  >hbase.replication:true是开启主集群复制数据到从集群的机制。
	  >故障处理：hbck工具./bin/hbase hbck 可以查看所有幸存的死亡的region数目，-ROOT- .META.表， 用户表的部署。
	  >系统日志文件不同于WAL: 因为只是记录一些系统信息之类的。
	   >INFO级别的输出有可能也是可以参考的：在出现问题时。aborting信息。
	  >安装检查：文件句柄数：cat /proc/jvm-pid/limits  32000个可以。
	   >DataNode处理线程数：16000也可以。
	   >列簇使用压缩：且引入压缩库。
	  >交换分区使用率检查：可能导致regionserver与zk失去连接
	   >vmstat
	   >交换出或入的情况：free -m  调整内核值/proc/sys/vm/swappiness为5、10
	   >监控慢磁盘：
	  >memstore阻塞更新的条件：更新高峰期，太大size。
	  >HRegion阻塞写请求：存储文件太多，需要进行合并。
	  >HBase的负载均衡：均衡的是region数量在各个regionserver上的值。比如多于平均值的20%就会默认进行重新均衡。
	  >CDH
	
	
	
	
	
	
	
	
参考资料：面试，应该主动自己聊，主动聊Hbase, redis, mysql,es, kafka架构、原理等。5+经验，不讲58项目。内容呈现与语言框架/语句格式的关系。不会就先抽象扼要总结型概括描述，然后说几天能完成，甚至开发一个出来。
https://max.book118.com/html/2017/1029/138158849.shtm(本书地址)