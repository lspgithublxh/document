---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。

>举动-痛点:  存储及处理PB,EB量级的半结构化/非结构化的数据。且低成本和高效。
	>解法/处理思想: 与hadoop高度集成的hbase, hadoop将hbase的可伸缩性变得简单.
	 >客户端：java, rest, avro, thrift
	 >架构细节： 存储格式、预写日志、后台进程。集成map-reduce框架。调节集群，设计模式，拷贝表，导入批量数据，删除节点。
     >周边：Hive/pig: 大规模数据的离线批量处理。Hadoop-存储非结构化，高延迟。
	 >Hbase来源：索引存储，Nutch中构建索引并存储。
	 >应用：雅虎用1000台HBase集群定位Bing的爬虫快照。
	 >特征：列式存储，线性扩展。java实现(hbase, es, kafka)
	 >Hush: 一个访问HBase的web服务器客户端。
	 >列式存储数据库：列数据相似，更容易压缩，有更大的压缩比。
	  >Hbase: 利用了磁盘上的列式存储格式。适合有序数据、键值对数据的存储。
	  >关系型数据库：数据分区使得运维代价高。适合事务型操作，但不适合超大规模的数据分析处理--即不适合大范围的数据记录扫描或全表扫描。分库分表的含义： 分表是按照不同的类型如地域、范围等划分存到不同的表中，表的schema一样；或者按列分--列太多；分库是表都不在一个数据库里，而数据库往往在不同的服务器上，来提高存储能力。
	   >横向拆分：重分区。因为数据重做所以消耗资源。
	   >虚拟分区：仍然需要将数据迁移到新服务器。
	  >NoSQL: 不提供SQL查询数据的手段，但有一些工具提供SQL语言入口。区别在底层上：
		>模式和事务特性ACID的比较区别：不支持事务或者辅助索引。没有固定的模式。
	   >一致性模型：数据库状态在操作下从一个一致状态到另一个一致状态---不矛盾无不守恒无不合理的数据状态。
	    >一致性级别：严格一致性：状态原子改变--中间不可读写。顺序一致性：操作顺序不同看到的数据不同。因果一致性。最终一致性：系统广播使得副本之间数据一致。弱一致性：更新以广播形式传递。CAP观点基于的关键事实就是：网络隔离的副本之间数据一致的实现需要消耗一点可用性(即一小段时间不可用--数据不一致)：或者说在更新 网络隔离的副本 时不可能 立刻同时全部更新成功而达到数据一致--即总是有先更新成功有后更新成功(总需要一个时间过程)。花时间获取更高的一致性，有系统消耗时间-一致性 坐标图。
	     >要么等待全部副本更新成功，要么不等待而手动在应用中解决一致性问题：如等待超过一般成功就认为成功。读个数R > N - W
	 >数据库的度量维度：阻抗匹配：通用的和可用的解决方案。
	   >数据模型：键值对、文档？
	   >存储模型：内存、磁盘？
	   >一致性模型：严格一致性、最终一致性？
	   >物理模型：分布式模式、单机模式？
	   >读写性能：支持读多写少、读少写多？范围扫描、随机访问？
	   >辅助索引：支持按不同的字段和排序方式来访问表？
	   >故障处理：数据迁移方案？服务器故障如何处理？恢复的难度？
	   >压缩：有可有的压缩算法？可用的压缩组件？
	   >负载均衡：随着负载变化自动均衡处理的能力？
	   >原子操作的读-修改-写：乐观锁、悲观锁的分布式实现？
	   >加锁、等待、死锁：采用哪种锁模型？能否避免等待和死锁？
	 >数据库的范式化和反范式化：基本原则：反范式化、复制、智能主键。
	   >Hbase支持：稀疏矩阵、宽表、列式存储的支持。使得存储结构无需范式化，从而无需join操作。智能的主键，控制数据存储的位置，范围检索、组合键设计。性能不会因为数据增长而下降。
	   >GFS: 并不适合存储成千上万个小文件，适合存储少许非常大的文件。因为文件的元数据要存储在主节点的内存中。
	   >BigTable: 系统将小文件聚合到非常大的存储文件中。并提供索引排序，查找最少磁盘就能够获取数据。支持简单的CURD,和范围扫描、全表迭代扫描。
		 >功能：是一个管理结构化数据的分布式存储系统。可以扩展到成千上万台商用服务器上存储PB级别数据。	一个稀疏的、分布式的、持久的多维排序映射。
	 >Hbase基本概念：稀疏的、分布式的、持久化的、多维排序映射。由行键、列键、时间戳索引。
	   >列：一个列簇里的列可以无限多个--数百万都可以，列值可以无限长、任何类型。
	    >Null值：空值无消耗。
	   >列簇：设置列簇的压缩特性、指示存储在内存。一个列簇的所有列存储在同一个存储文件中，即HFile中。列簇在建表时定义，列簇名可打印，列簇数量<几十个。
	   >单元格：cell。值有时间戳。一个单元格的不同版本的值降序排列在一起。最大版本数可以设置，一般时间戳作为版本。谓词删除---LSM树中。
	   >行：每行有行键，多行数据按照字典序存储。行键是字节数组，任意。
	   >数据存取模式：SortedMap<RowKey, List<Family<SortedMap<Column, List<Value, Timestamp>>>>> 。自然的，每行每列的单元格里的版本数目相互无关---也说明一个rowkey下有多个版本的值---但是每次取最新的。
	   >事务级别：单行。可以读取单行上的任意数目的列。跨行跨表事务不支持。原子存取促进数据强一致性。多版本和时间戳也可以帮助应用层解决一致性问题。
	   >扩展和负载均衡的基本单元：region。 是一个连续存储区间，存了按行键排序的若干行。region大小会被监控--从而对它们动态拆分和合并。且region可以分布在不同的服务器上，就像kafka里的分区可以分布在不同的broker，broker分布在不同的服务器上一样。从而负载均衡，也提供较强的扩展性。
		>region拆分规则：拆分瞬间完成，在达到配置的最大值--一般1GB-2GB，系统均分为二region。一个region归属一个region服务器，一个region服务器上加载多个region：一般10-1000。。region拆分相当于自动分区。
		>region服务器故障和上面的region的迁移：系统监控服务器的状态和负载，是否可用。
		>region合并：异步完成。异步地把存储文件合并为一个独立的新文件。
	   >API: 建表、删表、增加删除列簇、修改表和列簇元数据--压缩和设置块大小。给定行键值进行增删查。scan高效范围遍历，并且返回指定列、版本数。单行数据原子性的读-修改-写。单元格的值，从而就可以当作计数器使用--因为支持原子更新：一个操作中完成读和修改CAS ，这是一个全局的、强一致的、连续的计数器。
	    >协处理器：在服务器地址空间执行来自客户端的代码。
	    >包装器：集成了MapReduce框架，将表转换为mapreduce的输入源和数据池。
	   >数据存储：在store file中，称为HFile, 存储的是排序后的键值映射结构。文件里的内容以块为单位，块的索引信息存储在文件尾部。打开加载HFile文件到内存中时，索引信息优先加载，每个块默认64KB。可以用API扫描文件内容中设定起止范围内的值。块索引通过一次磁盘查询就能找到数据：对块索引二分查找，确定包含给定键的块，再读取磁盘块找到实际要找的键。
		>存储文件保存在HDFS中，HDFS可扩展、持久、冗余。
		>更新数据过程：先将更新记录写到WAL预写日志中(一般称提交日志；用于崩溃后未写入磁盘的数据恢复用)，再写到内存中的memstore中，memstore增长到设定值后会被移出内存作为HFile文件刷写到磁盘，然后系统丢弃相关的预写日志/提交日志。memstore中数据已经按照行键排序。移出写入磁盘的过程中无阻塞，因为会滚动产生一个新的空的memstore接收刷写过程中收到的更新数据。
		>删除值：因为存储文件不可改变、移除某个键值对；所以通过加一个删除标记。
		>HFile合并：minor合并：多路归并---有序行的合并，快，受磁盘IO影响。Major合并：扫描一个region里的一个列簇的多个HFile中全部键值对，忽略删除标记的数据、超过版本号限制、生存时间到期的数据(断言删除生效)，重写到新文件中，
		>LSM树：将多页块中的数据存储在磁盘中。
      >Hbase三大组件：客户端库，一台主服务器，多台region服务器。
		>region服务器：可以动态增加和移除。在zk上注册了一个自己的临时节点，这个节点会被主服务器监听(解耦了主服务器和region服务器)。
		 >提供了拆分region的接口：
		 >提供负责的region的读写接口：客户端直接与region服务器通信，处理数据操作。
		>主服务器：利用zk给region服务器分配region，对region服务器增减region以负载均衡。Hbase利用zk确保只有一个主服务器在运行。也对region服务器集群管理。不提供任何数据服务，对内也是。提供元数据的管理操作如建表、创建列簇的操作(支持列簇选择不同的压缩算法)。
		  >zk: 可靠的，高可用的、持久化的分布式协调系统。
		   >提供类似文件系统一样的访问目录和文件的功能。用来协调所有权(分布式锁)，注册服务，监听更新。
		   >每个临时节点：属于一个会话，会话是客户端和zk连接之后自动生成的。客户端会不断发送心跳--带会话id 给服务器，zk收不到则认为会话超时而删除临时节点。
		   >Zab协议：
		 
	  >Hbase架构图非常简洁：自定向下：API --> Master主服务器 + region服务器(write-ahead-log/WAL--->memstore--->HFile)---->HDFS + zk	
	  >HBase存储规模：数十亿行*数百万列*数千个版本 = TB级别PB级别的存储。
	  >HBase存储格式：内容为键有序排列的文件块和块索引，对块IO操作做了优化，最大限度利用了磁盘传输通道(I/O利用率饱和)。
      >HBase列式存储结构：无限大的稀疏的表。每行数据只在一台服务器上，所以HBase具有强一致性。多版本的原因：避免并发解耦过程导致的编辑冲突；也保留历史变化。
	  >HBase性能估计：表扫描线性时间增长，行键查找线性时间关系。	   
	  >Hbase提供的索引：单一索引，主键。也提供服务端钩子，实施灵活的辅助索引解决方案。
		   
>举动-痛点：安装和配置
	>解法/处理思想：
	  >安装：下载.gz包，tar解压，配置conf/hbase-site.xml 中的hbase.rootDir写操作的写路径。启动hbase.sh, 启动脚本子命令系统 shell, 输入status查看状态。
	  >基本命令：查看已有表list, 建表create 表名,列簇, 增加数据put 表名,行键,列,列值,列,列值... 。查看：scan 表名 。打印输出顺序：从左到右，从上到下。get 表名, 行键 。删除：delete 表、行键、列。禁用表并删除：disable 表， drop 表。
	  >机器上部署的java进程：master节点上: hadoop的NameNode, mapreduce用途的JobTracker, hbase的Master。 slave节点上：DataNode, TaskTracker, RegionServer。Master需要冗余来提高自身的可靠性。	
	  >硬件要求：2cpu*4核 。内存上：如果使用CMS收集老区---那么不应该太大：因为全量回收代价高--长时间阻塞可能导致被认为这个regionServer死掉。访问模式也影响配置的设置(交互式和批处理)。
		>用户空间：
		>超线程：
		>内存通道和插槽个数：
		>磁盘挂载到服务器上的方式：slave:JBOD master:RAID....。。带RAID固件
		>磁盘驱动器：SATA。 磁盘1TB容量。配6-12个磁盘。4*1TB: 400MB/s 。IOPS:400。 
		>机架：服务器挂载在19英寸机架上。容纳40个机架单元U. 。使用置顶式交换机将40台机器连接起来。需要80个交换机网络端口。 多个交换机+ 核心汇聚交换机CaS..。。要验证集群中的每个硬件组件。
		>网络：双端口千兆以太网卡。
		>电源供应器：PSU slave1， master2
	  >软件要求：挂载的文件系统和各种服务配置。
	    >启动和管理脚本是shell: 类Unix系统上。CentOS,RHEL改造而来，企业应用为主；Fedora工作站。Ubuntu更新过于频繁，不适合服务器操作系统。推荐REHL, CentOS
		>文件系统：HDFS依赖的本地文件系统：建议ext4,XFS
		  >ext3稳定可靠，配置：挂载文件系统时--设置noatime---禁止记录访问时间--大幅度提高磁盘读写性能。挂载选项配置在/etc/fstab中， 配置：/dev/sdd1 /data ext3 default,noatime 0 0 。对数据节点设置保留块的数量：以获取更多的磁盘空间：从5%-->1% tune2fs -m 1 /dev/sdd1
		   >缺点：格式化磁盘慢；启动慢；
		  >ext4:稳定可靠；性能接近XFS；允许单个文件达到16TB。支持1EB:10^18字节的存储空间。延迟分配：尽管会在内存中保留若干数据块以一起刷写到磁盘-减少了磁盘碎片提高文件读取性能；但是增加了服务器崩溃时数据丢失的概率。
		  >XFS: 高性能文件系统。引导服务器时格式化速度快。
		   >缺点：升级需要转储文件
		  >ZFS:可存储10^21字节数据。支持内置的压缩功能。
	  >HBase调用Hadoop: rpc调用。需要版本匹配。比如Hbase需要运行在追加了安全功能的hadoop:cdh3上。每台机器使用ssh登录，且用公共密钥认证。 	  
	  >域名服务：HBase使用本地域名汇报ip地址。ping -c 1 域名 验证正向DNS的正确性。多个接口，通过hbase.regionserver.dns.interface指出主接口。
	  >同步时间：集群中运行NTP以同步时间。
	  >文件句柄和进程限制：太小会有IOException。文件描述符的个数上限：计算：每个region3个列簇，每个列簇3个存储文件，每个region服务器100个region, 则就已经需要打开3*3*100=900个文件；使用lsof -p REGIONSERVER_ID查看确切打开个数。还需要编辑/etc/sysctl.conf调整fs.file-max的值。
		>xcievers需要正确配置。
	  >交换区： 避免内存溢出时，需要给操作系统进程足够空间；如果空间不足，那么os会使用交换区---机器磁盘中独立的分区；服务器避免使用--因为一旦使用了交换区：用户sshd挂起而无法登陆。所以对regionServer来说要减少交换空间的使用：配置/etc/sysctl.conf 下的vm.swappiness=5或者0。。。sysctl -p重启服务器生效。	
      >分布式文件存储系统：HDFS最好，但是可以用自己实现的分布式文件系统---只要满足hadoop的规定。		
		>HDFS: 稳定可靠；保障机制：冗余、容错、可扩展。HBase并不维护自身存储文件的副本。
	  >本地模式：只需要配置file:///path
	  >hadoop集群：并行流式的处理能力。扩展好、系统可靠、冗余。HBase增加了随机存储层，是HDFS的补充。hdfs://namenode:port/path 
	   >hadoop模式：原生模式。块模式。跟弹性计算云环境有关。
	  >KFS: c++开发。也可以作为hbase的存储系统。
	  >伪分布式模式：hdfs也部署在本机上，在hbase-site.xml中配置hbase.rootDir 为 hdfs://localhost:9000/hbase 数据副本个数：dfs.replication=1
	  >完全分布式模式：hbase-site.xml中配置：hbase.rootDir 为hdfs://namenode.foo.com:9000/hbase 即NameNode所在节点，同时指定hbase.cluster.distributed=true
	   >同时：/conf/regionservers文件要罗列regionserver的Ip
	   >zk: 作为hbase管理则hbase-env.sh中HBASE_MANAGES_ZK=true 等。独立zk集群需要在hbase-site.xml中配置zk连接地址和客户端口号。
	  >配置文件：
	   >conf/hbase-env.xml: 启动关闭所用到的环境变量。如可用文件系统和zk可用地址。复制conf目录到其他节点：rsync工具同步到集群中其他节点上。
	   >conf/hbase-site.xml: 部署模式，zk配置，对HDFS特定配置。hbase中配置高于从而可以覆盖hadoop中的配置。
	   >scr/main/resources/hbase-default.xml: 默认配置；进程启动则读取；可以被hbase-site.xml覆盖相关配置。
	   >hbase-env.sh: jvm启动参数的配置。
	   >regionserver: 罗列了所有region服务器 主机名。HBase运维脚本会读每一行来启动对应的region服务器进程。master启动时注册到zk中。
	   >log4j.properties:
	  >使用脚本同步配置文件：
	  >动态环境中运行HBase集群：如公有云服务平台。
	  >Whirr: 帮助动态环境中快速部署完整的hbase集群。Puppet , Chef也可以: 中央配置服务器+脚本发送到服务器并执行。
	  >Web UI: 可以查看master或者regionserver上的运行信息情况。
	  >Hbase shell :ruby开发的。
		
>举动-痛点：客户端API
	>解法/处理思想： 
	 >主要客户端接口：org.apache.hadoop.hbase.client.HTable类提供。HTablePool。存储和检索数据、删除无效数据。这个类的实例创建比较耗时：因为会做表的存在性、可用性验证。
      >单行数据PUT: 指明行键：byte[]数组即可。添加行的列簇-列数据：一个Put对象可以add多列数据(每列数据也可以不指明时间戳)。
       >已设置的数据GET: PUT对象中的指定列簇、列；返回List<KeyValue> keyvalue就是一个单元格的值---有行键列簇列时间戳限制。HBaseHelper内部类会创建有特定行与列数量的数据测试表。
       >Put中允许提交一个rowlock行锁：修改数据时避免其他客户端访问这些行。
	   >getKey()用不到，是一个坐标；getRow()就是设置的行键。
	   >一次Put是一次RPC操作，网络耗时存在，可以配置失败重试。LAN是1ms一次。所以存在批量Put----- 	启用客户端缓冲区.setAutoFlush(false), 然后手动提交(在put了若干行之后，数据存在本地进程中)flushCommits(), 客户端会自动将Put对象按存到的regionserver分组, 然后分别发送给对应的regionserver。缓冲区大小setWriteBufferSize()一般设置2MB; hbase.client.write.buffer。put()操作会比较缓冲区数据大小可能触发调用。
	   >服务端也有缓冲区：接收到的Put先放到缓冲区中，再一个个处理。服务端还会对列簇存在性进行检查。失败的Put会单独返回给客户端；失败的Put实例也会在客户端缓存存在，需要手动清除---访问本地缓存区。客户端也会基本的检查。
	  >处理配置属性：加载hbase-site.xml, hbase-default.xml的内容， HBaseConfiguration.create(), 自然的加载之后可以覆盖。scan时每个版本数据也占一行，可以输出3个版本{versions =>3}而不是最新的版本--------不指定参数就会只返回最新版本数据。
	  >比较器：KVcomparator等实例已经静态化；直接引用，从而可以用来做Set\Map的数据的有序化的比较器：而其中的数据类型就是KeyValue。同样还有KeyComparator, RowComparator
      >单行数据删除Delete: 也是单元格级别。但是DeleteColumn则删除列， DeleteFamily删除列簇	  
      >检查写：check and put: 在服务端值检查成功才put: 使得Put操作有原子性：调用checkAndPut()方法。  用于不存在则插入，是某个值才插入的情形。
	  >单行取数据GET: get()指明行键row:byte[] 增加列簇、列、时间戳范围/版本数限制。
	   >regionserver有个块缓存：保存最近存取过的数据
	   >Result类：根据get()返回的结果量的不同，可以进一步调用它的方法查询精确的数据。raw()返回原始的即有序的KeyValue数组。
	  >批量请求GET: get(List<Get>)可能发往多个regionserver
	  >查找row行或者上一行：getRowOrBefore():用来指定一个大的不存在的rowkey ,从而返回已经存储的最大的rowkey----即按照字典序排在表末尾的那一行。
	  >单行删除DELETE: delete()指明row:byte[]， 添加列簇、列等限制。
	  >批量删除DELETE: delete(List<Delete>)
	  >检查成功则删除：checkAndDelete()
	  >批量处理跨多行：batch(List<Row>) 增删查操作Put , Delete, Get都可以放到List<Row>里。同步操作，不缓存，直接发送到服务端。
	  >行锁：一般不要使用。服务器端隐式加锁。客户端可以显示加锁---显示的从服务端获取锁：。锁超时时间设置：hbase.regionserver.lease.period
	  >读不加锁：因为服务端有mvcc多版本并发控制，不会读取正在写的数据；变动应用到整个行之后，才能被读出(将最新版本的指针指向新写成功的数据)：否则那个正在写的版本不会被读取出----而是读取的老的版本：这就是mvcc好处：写的过程中还可以读--读老版本。
	  >扫描： Htable.getScanner(); 得到ResultScan对象，补充[startRow, stopRow)限制，可以再加addFamily()列簇、列限制；同样精确到时间戳、版本数也可以；
	   >扫描最后需要close: 查询可以next()一行或者next(int nrows)下n行一起返回得到一个Result[]数组(但实质实现是循环N次调用)；自然的，行键设计时要保证字典序是需求；因为返回的顺序就是字典序。
	   >扫描器缓存：表级--所有scan都生效，scan级--单个scan生效； setScannerCaching() setCaching()。缓存太大，会导致查询文件过多反而更慢；
	   >扫描器租约时间：超过之后scan就会报错； 。要延长必须修改各个regionserver上的hbase-site.xml配置；
	   >批量：缓存面向的是行，批量面向的是列；setBatch(int) 可以返回多列，多个列一个Result对象来包括；
	   >批量-缓存量-result个数的关系：批量的大小，决定返回的列数多少---最多到一行的列数个；一个批量需要一个Result来保存；10行数据，则有10行*每行的批量数 个总批量；即这么多个Result。而缓存个数决定一次rpc取回多少个Result, 比如缓存数为10，总共200个Result, 那么会发送20+1次rpc请求，最后一个是确认扫描完成。
	  >跟踪日志：可以建立一个日志Logger,记录某个包路径下的；再建立一个过滤器AppenderSkeleton，统计这个包路径下的输出日志中的某些信息； 
	  >查看region的位置：getRegionInfo() 
	 >高级特性：
	  >过滤器：filter 。在服务器端生效，叫谓词下推。
	  >CompareFilter: 实例化时需要指定CompareOp比较运算符和WritableByteArrayComparable比较器
	  >RowFilter: 行键过滤器：小于等于大于，某个值，匹配某个正则。
	  >FamilyFilter: 列簇过滤器：在列簇一级过滤掉数据--筛选出所需的数据。
      >QualifierFilter:列名过滤器：
	  >ValueFilter: 值过滤器：
	  >DependentColumnFilter参考列过滤器：基于参考列的时间戳进行筛选：不仅可以设置过滤运算符、比较器，还可以设置比较列。	
      >专用过滤器：只用在scan时：		
		>单列值过滤器：用一列的值决定是否过滤掉一行。SimpleColumnValueFilter
		>前缀过滤器：行键前缀匹配模式：PrefixFilter
		>行键过滤器：只返回KeyValue对应的key, 而不必返回实际的更多的数据。
		>包装过滤器：SkipFilter包装一个filter之后，这个filter返回的一个或多个列值为空的行会被过滤。
		>全匹配过滤器：只要被包装的过滤器开始跳过某一行，则立刻从此时返回数据WhileMatchFilter
	  >过滤器列表：FilterList，多个过滤器的组合，关系包括：全部通过、至少一个通过MUST_PASS_ONE	
	>计数器：
	  >一个列当作计数器使用：CAS保证可以原子性的读写一起操作。而使用锁有客户端崩溃锁未释放而等待超时--导致超时期间不可用的灾难性后果。
	  >一次rpc可以同时更新一行的多个计数器。
	  >终端操作：incr 'table', 'row', 'column', increment-value(可正可负)
	  >多计数器：Increment, 指定行键rowkey:byte[], 增加列簇、列、增加量这些配置，可以实现对一行的多列计数器进行递增操作。
	>协处理器：CoProcessor将计算移动到数据的存放端。数据的处理流程直接放到服务端。像存储过程、触发器，像mapreduce 
	  >observer: 与触发器类似。回调函数在一定的事件发生后执行。
	   >RegionObserver: 处理数据修改事件。
	    >region生命周期的pending open-->pending close阶段起作用：是一种过程干涉的被主框架钩子勾住的回调函数。可以在各种region事件做之前被调用---比如拆分、回写、合并、关闭(被转移到其他regionserver)。
		>客户端各种API调用之前之后也可以加钩子函数。
		>可以访问WAL日志getWAL() HLog
		>ObserverContext: 通知协处理器框架回调函数执行完毕时需要做什么。比如在某一步时停止继续执行---不再执行剩下的协处理器--bypass()。
	   >MasterObserver:管理DDL操作。集群级别事件。
	    >提供各种DDL钩子函数：内部可以利用环境对象而对服务器做一些如文件创建的工作，但是钩子函数没有返回结果---毕竟这是一个类似触发器一样的功能：在事件发生时做一些事情。
		>提供region分配、移动、负载均衡操作前后的钩子函数：
		>提供master关闭前后的钩子函数: 
	   >WALObserver: 控制WAL的钩子函数。	
	  >endpoint: 类别存储过程。服务端计算工作。实现CoprocessorProtocol接口
	   >通过添加一些远程过程调用来动态扩展RPC协议。
	   >程序编写：继承BaseEndpointCoprocessor类，获取环境对象，并从环境对象里获取Region对象，然后对Region对象可以进行遍历，获取所有的行、单元格。
	   >程序分发和回调：HTable.coprocessorExec(.class, , , ,new Batch.Call(){}) 返回的就是每个Region上执行统计类程序后的统计结果。
      >用户可以上传region级别的程序操作：
	  >用户可以监听隐式的事件：
	  >用户扩展rpc调用：
	  >使用场景：使用钩子关联行修改操作来维护一个辅助索引
	  >通过优先级和序号控制协处理器的加载顺序：
	   >静态加载：hbase-site.xml中配置：hbase.coprocessor.region/mater/val.classes 三项配置会被对应的CoproccessorHost加载---比如MasterCoprocessorHost加载...master.classes里配置的Class
	   >标描述符来加载：HTtableDescriptor.setValue() 配置：jar路径(hdfs路径)/完整限定类名/优先级(SYSTEM/USER)
	  >特征：不能对一行数据加锁：
	  >生命周期：UNINSTALLLED --> INSTALLED--->STARTING--->ACTIVING--->STOPPING--->STOPPED。执行顺序，客户端请求--->服务端：系统级协处理器(在一个环境Enviroment里)、用户级协处理器(在一个环境Enviroment里)，两个环境都在一个HRegion的CoproccessorHost里。---->返回给用户。
	  >组件元素：Coprocessor, CoprocessorEnviroment, CoprocessorHost 
	  >HTablePool: 需要自己管理HTable.	
	  >HConnection: 客户端连接共享HConnection, 尤其是到zk的实例。获取-ROOT-和.META.	
	  >多个HTable: 可以共享同一个Configuration	
	>管理功能DDL,DML：使用了Hadoop RPC框架，框架要求远程方法中的参数都实现Writable接口以实现序列化反序列化。
	 >表描述符：HTableDescriptor会作为存储路径的一部分来使用。	
	  >列簇描述符：HColumnDescriptor可见字符。列名不必。甚至空列名也可以。
	   >最大版本数：每个值保留的最大版本数量。
	   >压缩：压缩算法可以选择NONE, GZ,LZO,SNAPPY
	   >存储块大小：HFile文件中的划分的若干个小存储块。默认64KB类似RDBMS中的存储单元页。所以一个HFile里大约有1000个存储块。在get/scan时会被加载到内存中。
	   >缓存块：内存缓存中。
	   >生存期TTL: 版本数据保存时间。
	   >在内存中：设置为true则承诺数据块加载到缓存区并保留较长时间。
	   >布隆过滤器：增加了存储和内存负担。减少特定访问模式下的查询时间。
	   >复制范围：跨集群同步的功能。
	  >region分裂时的大小：默认为256MB。。但不是就这么多，毕竟不会跨一行存储，一行数据如果10MB那么是可能会超过的。
	   >一个region多个列簇：一个region的一个列簇下的内容对应可能有多个压缩文件/未压缩文件。
	  >只读设置：
	  >memstore也就是HFile大小：默认64MB.所以默认情况下一个region最大4个HFile存储文件。
	  >WAL保存到磁盘的方式：延迟刷写
	 >HBaseAdmin: 修改表结构、列簇结构等功能。	
	  >通过表描述符建表：表描述符对象可以加列簇描述符对象。同时可以设置region起止边界和region个数，进行预分区，可以打印各个region分区起止位置来查看。	
	  >禁用表后可以删除表：禁用会等待内存中的数据刷写到磁盘，所以耗时。禁用后可以重用。	
	  >修改表结构：必须先删除表结构。然后重建表。
	  >集群管理：如让已经在region服务器上线的特定region下线。
	   >拆分合并region都可以。
	   >将一个region从一个regionServer移动到另一个regionServer也可以。
	   >关闭master, regionserver, 集群都可以。
	   >集群状态信息：活着的和不可用的regionserver数量。regionserver上的存储文件的数量和总存储量、已用写缓存的大小。RegionLoad甚至可以统计当前region统计周期内的TPS, WPS,RPS读请求量和写请求量等。
	>其他客户端：
	  >REST 网关服务：如./hbase-deamon.sh start rest。。REST服务无状态。从而以http方式访问htable比如：http://servername:8080/表名/行键/列簇:列    返回值经过base64binary编码。 curl方式可以指定 -H 'Accept: application/json'	
		>Protocol Buffer: 编码效率较高，最合适的网络传输协议，提供了较高的带宽利用率。可以用curl -H "Accept: application/x-protobuf" ... 来实现调用返回protocol buffer协议的二进制数据。
		>Raw Binary: 同样设置报头'Accept: application/octet-stream' 只有单元格内的值返回。
		>REST风格的JAVA API也可以用。
	  >Thrift: 安装好,编译模式文件，并生成用户指定语言的RPC代码。
	   >启动thrift server:  bin/thrift start 无状态的。
	  >Avro: 安装好，根据选择的编程语言 编译 模式预定义文件。	
	>批量处理：
      >使用Clojure编写mapreduce程序来获取hbase表。	
	  >Hive: 提供了基于hadoop的数据仓库。Facebook开发。
	   >安装hive: 配置$HIVE_HOME/conf/hive-env.sh中添加HADOOP_HOME=xxx HBASE_HOME=xxx
	   >启动并创建一张本地表：/bin/hive   create table name(col type)   。。。可以实现复制hive表中的数据到hbase表中。
	  >Pig: 提供了一个分析海量数据的平台：命令式编程风格，迭代式执行。更适合并行处理。
	   >可以读取不同的数据源：HDFS, HBase都可以。
	   >创建安装：是一种Pig shell  。可以将一个日志文件加载到HBase中。
	  >Cascading： 新的计算模式API : 数据被表示为元组，形成了一个元组流，
	   >汇总数据到HBase: 
	>Shell: 基于Jruby , 后者基于Ruby实现的java虚拟机。5类命令，并代表了它们之间的语义关系。	
	 >引用名、引用值：单引号和双引号。
	 >逗号分隔参数：
	 >命令：
      >普通命令：status/version
      >DDL： 表级操作。创建、禁用、启用、存在性验证。	  
	  >DML: 删除单元格、返回一个单元格get,put, scan 
      >工具：assign分配一个region到另一台regionserver	  
	>基于web的UI:
	
		
		
		
		
		
		
		
		
		
参考资料：面试，应该主动自己聊，主动聊Hbase, redis, mysql,es, kafka架构、原理等。
https://max.book118.com/html/2017/1029/138158849.shtm(本书地址)