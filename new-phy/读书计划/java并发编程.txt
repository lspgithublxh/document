---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。
>学习要有战略和格局：过于零散、孤立、末支、叶子的知识点就没必要化太多时间，懂得放弃。聚焦在根节点、枢纽节点、产生分支的节点、主干知识点、枢纽知识点、关联到很多知识点的知识点上。
 >对于主干知识点：要有全新的认识视角和进行精细的描述，大量的提问和好奇和进行必要的实验以获取支撑观点/导出观点的数据。
 >规范是工程最独特的特征.
 >慢慢读：
>一种新技术的学习：
 >它面对的情况和问题、它的世界观、它的方案、它的方案验证/论证/能处理的解决的所有情况及能成功处理的理由/功能边界
  >所有的软件：都可以看作是向上封装一层接口，根据自己的世界观封装底层而向上/对外提供统一的(统一的更简单的更直观的更业务的更少底层信息的)接口，底层包含一系列的第三方的插件/构件/组件；内部则去做兼容和调用(对底层)(对上层则做逻辑分解和底层实现)。

>举动-痛点：
	>解法/解决方案：上下文切换问题；死锁问题；受限于软件和硬件的资源限制问题；
		>时间片：几十ms
		>线程创建开销：
		>线程上下文切换开销：
			>开销计算：vmstat 1 查看cs列--即上下文每秒切换次数。 Lmbench3可以分析上下文切换的时长。
		>减少上线文切换：
			>无锁并发编程：人工方式，各个线程处理不同的数据段。
			>CAS算法: 硬件指令CMPXCHG, 先比较再更新。
		>使用最少线程：任务少，无需太多线程。
		>协程：单线程里实现多任务的调度。
		>虚拟机栈分析：
			>等待线程分析：jstack 3113 > file;  grep java.lang.Thread.State file | awk '{print $2$3$4$5}' | sort|uniq -c 
				>如果大量发现 WAITING(on object monitor) 即在等待池中的线程。那么可以减少相应的线程池中核心线程的数量。
			>死锁分析：尤其因为异常而没有释放锁，极容易造成死锁和阻塞。数据库锁也是。	
				>如果发现栈信息里出现： BLOCKED (on object monitor) 即在阻塞池entrylist中，则危险。
				>解法：使用一个锁；使用超时的tryLock锁；
		>资源限制的挑战：并发的个数要考虑是否超过了资源限制。
			>数据库连接数|socket连接数：
			>服务器带宽：
			>cpu和mem利用率、硬盘读写速度：
		>volatile: 保证共享变量的可见性
			>基本概念：
				>内存屏障：一组指令，实现对内存操作的顺序限制
				>缓存行：缓存中可以分配的最小单位。64B
				>缓存命中：下次处理器访问的地址是高速缓存行填充的内存地址，直接从缓存读取操作数
				>写命中：处理器回写时先看高速缓存中是否有缓存行的内存地址是目标地址，是则直接写回到缓存；
				>写缺失：缓存行被写回到不存在的内存。
				--lock前缀指令：处理器收到则会强制将当前缓存行写回内存，同时根据缓存一致性协议，其他处理器嗅探到该缓存行失效了，则置自己的该缓存行为失效状态，则下次读的时候就重新从内存读。
					>缓存锁定：即向总线发送锁定对应内存区域的缓存行的指令，同时自己回写主存;锁定期间其他内核不能访问对应的缓存行。
					>MESI控制协议：修改、独占、共享、无效。嗅探其他内核的缓存来保证内存缓存、系统内存和其他处理器的缓存在总线上保持一致。嗅探到其他处理器打算写--且该地址处于共享状态，则设置自己的对应的缓存行为无效；
			>volatile可以保证：某个内核修改变量时，缓存锁定、写回主存和缓存行失效；因为编译后生成了额外的lock前缀指令。
			>volatile内存语义：相当于读写都加了同步锁synchronized
				>过程：缓存行锁定-->回写主存-->MESI缓存行失效
				--应用：volatile修饰的long/double ,则一定是读写原子性的。
				>语义：
					>直接的读写：具有原子性。
					>读：一定是最仅一次的写。
				>jmm对volatile内存语义的实现：编译器重排序和处理器重排序
					>重排序规则表：
						>1.第二个操作是volatile写时，不管第一个操作是什么，都不能重排序；。即写之前必须在写之前。
						>2.第一个操作时volatile读时，不管第二个操作是什么，都不能重排序；。即读之后必须在读之后。
						>3.第一个操作时volatile写时，第二个操作是读，不能重排序；
					>编译器的实现：加内存屏障的规则：
						>1.volatile写的前、后分别加：StoreStore、StoreLoad屏障指令。//前重叠相同，后重叠相异。//写写不重排，写读不重排//保守策略：先正确，再追求效率。
						>2.volatile读的后、后分别加：LoadLoad、LoadStore屏障指令。//前重叠相同，后重叠相异//读读不重排，读写不重排。//本读与后面的读，本写与后面的写；v读与后面的普读,v读和后面的普写，都不重排。
		>原子性：
			>内存顺序冲突：多个cpu同时修改一个缓存行的不同部分，导致cpu的操作无效，而清空cpu流水线；---(即5-6个电路单元上的数据)
			>处理器保证的原子性：从系统内存读取写入一个字节是原子性的。AtomicReference 保证对象引用在不同cpu之间的可见性。
			>cas三大问题及其解决：ABA问题--引入版本号：即同时还对版本号更新-更多一个维度来度量一个变量。cpu消耗问题--pause命令；一次只能更新一个变量--多个则需要放到原子引用AtomicReference里，来让多个cpu之间可见。
		>java内存模型：
			>关键问题：线程之间如何通信和同步：
				>通讯机制：共享内存和消息传递：jmm采用共享内存方式。
				>jmm决定：一个线程对共享变量的写入何时对另一个线程可见。
					>实现方式：jmm控制主内存和每个线程的本地内存之间的交互。
			>顺序一致性：每个操作原子执行且立即对其他所有线程可见。单线程内顺序，线程之间也顺序先后临界区；。但JMM都不保证。JMM还不保证对long,double的操作具有原子性。
				>指令重排：目的为了提高性能。
					>编译器在不改变单线程语义的情况下：字节码顺序重排。
					>处理器在执行多条指令可以并行执行：指令级并行技术；如果不存在数据依赖性，处理器可以改变语句对应的机器指令的执行顺序。--指令并行和指令乱序。
					--内存可见性问题：
						>解决：java编译器生成指令序列时，插入特定类型的内存屏障。禁止特定类型的编译器重排序和处理器重排序。
							>内存屏障指令：StoreLoad: store1,StoreLoad,load2 全能型，前面store指令完成之后后面load才开始执行。
				>同步原语：
					>内存语义：在处理器中的实现：	
						>happens-before: 前面指令对后面的指令可见
						>as if serial: 编译器、runtime、处理器来遵守和实现。处理器目标：在不改变运行结果的情况下提高并行度。
					>重排序规则：在处理器中的实现：
				>总线仲裁和总线拒绝：读写事务。任意的读操作在jsr-133中都必须具有原子性。
		>锁的内存语义：类似volatile的内存语义。
			>加锁时：从主内存加载共享的变量。
			>释锁时：锁缓存-回写缓存-失效缓存。
		>cas具有锁的内存语义的原因：本质上看：cas实现了线程同步；volatile实现了线程通信；
			>当多核时，会在cmpxchg---这个指令本身读写改原子性；前面加lock#前缀，提供内存屏障的效果；。独占下修改，使得共享变量失效。
				>lock#功能：缓存锁定；禁止之前之后的读写指令越界重排
		>线程同步核线程通信的实现：cas(公平同步)和volatile(单共享变量通信)		
			>向上实现：AQS, 非阻塞数据结构、原子变量类
			>再向上实现：Lock, 同步器, 阻塞队列, 并发容器, Executor线程池
		>final的内存语义：
			>重排序规则： 编译器和处理器遵守两个重排序规则
				>初次写后初次读不能重排序
				>读对象引用后读对象的final域属性 不能重排序：
			>实现：
				>禁止把final域的写重排序到构造方法之外；即构造方法里的对final域赋值之后会加StoreStore指令。
				>禁止把读final域的操作重排序到读对象引用之前：即会在读对象引用的final域之前添加LoadLoad指令。
			>final引用对象的重排序规则：
				>
		>理论模型：规范JSR-133
			>双重检查锁定：即常见的实例化方案。
				>存在的问题：当第一次读取instance == null判断为false时，可能instance还没有初始化完成！！
					>原因：new Instance()其实时3条指令，new#11, invokespecial#12, astore 引用指向;而最后2个指令可能发生指令重排，----重排并没有违反happens-before的所有规范要求；
					>解决方案1：定义为final类型？利用写在读之前？
					>解决方案2：不允许2-3重排序；或者允许但是不允许被看到；
						>volatile修饰：利用volatile的写语义，会禁止2-3的指令重排序；
						>内部类静态Holder类，其中有个属性为static实例直接new Instance()赋值： 根据类加载后访问静态属性非final则要初始化类且会带锁初始化----从而只初始化一次，从而其他线程时看不到初始化过程的--就包括2-3的指令执行---只有第一个才看得到。
		>jvm内存模型总结：语言级内存模型
			>处理器内存模型：硬件级内存模型
				>读写指令顺序的重排：
				>as-if-serial的支持：
			>jmm的屏蔽处理器内存模型的不同而做的措施：
				>适当位置插入内存屏障：
			>jmm内存可见性保证：
				>通过限制编译器和处理器的重排序来保证：
		>线程：
			>deamon线程：main的结束而结束：且是直接的杀死；
			>从线程抛出中断异常看：其实这个是底层jvm向上抛出来的。所以jvm其实知道所有的阻塞的线程。
			>线程之间数据传输：
				>管道：PipedWriter-PipedReader配套使用；先调用两个连接起来；然后一个线程往一个里写，另一个线程往读管道里读出即可实时得到数据。
		>AQS:同步器：作用：同步线程(使得线程排队、等待和被唤醒)，记录锁的重入次数。
			>同步器可重写的方法：
				>独占式尝试获取锁：根据当前读取的状态值、是否已有占有线程 等信息；成功则立即，失败也立即；
				>独占式尝试释放锁：根据当前读取的状态值、是否已有占有线程 等信息；成功则立即，失败也立即；
				>共享式尝试获取锁：根据当前读取的状态值、是否已有占有线程 等信息；成功则立即，失败也立即；
				>共享式尝试释放锁：根据当前读取的状态值、是否已有占有线程 等信息；成功则立即，失败也立即；
			>同步器模板方法：
				>独占式获取锁：先独占式尝试获取锁，失败则入队，停车等待，否则成功返回。注意：停车是可以线程中断的,但是只是唤醒而不抛出异常。
				>独占式可中断获取锁：先独占式尝试获取锁，失败则入队,停车等待，被中断则移除出队列,向上抛出中断异常,否则成功返回。注意：停车是可以线程中断的。sychronized在entrylist中的线程不可被中断.
				>独占式可中断限时获取锁：先独占式尝试获取锁，失败则入队,停车限时等待，被中断则移除出队列,向上抛出中断异常,否则限时等待, 超时则返回false, 成功返回true.注意：停车是可以线程中断的。
				>共享式获取锁: 先共享式尝试获取锁,失败则入队,停车等待,成功则要先释放共享锁-循环式唤醒排队中的下一个signal节点(可唤醒的都唤醒,-1更新为0),再返回.
				>共享式可中断获取锁: 
				>共享式可中断限时获取锁:
				>独占式释放锁: 独占式尝试释放锁,成功则唤醒排队中下一个节点;
				>共享式释放锁: 共享式尝试释放锁,成功则循环式唤醒后面的signal节点(-1更新为0);
			>同步器的节点状态:
				>CANCELLED: 1 线程超时或被中断,状态不会改变.
				>SIGNAL: -1 等待状态 
				>CONDITION: -2 等待队列中,等待条件; signal()方法之后会被转移到同步队列;
				>PROPAGATE: -3 下一次共享式同步状态获取将会无条件被传播下去;
				>INITIAL: 0 初始状态
			>state值的当作:  尝试获取释放锁的方法的入参,是state的调节值/增量值; 表示获取锁--这个动作的记录/次数记录;
				>当作重入次数:
				>当作同时获取到锁的线程数:
		>读写锁:
			>重入性:读获取可以再获取读,写获取后可以再获取写和读;
			>锁降级: 支持.写获取后再获取读,再释放写锁;就模拟了一个写锁降级为读锁的过程;
			--不支持锁升级:避免数据不一致.
			>使用场景:缓存读取....一块数据,既想读又想写;
				>读方法:先获取读锁,操作,再释放读锁;;..当有线程获取了写锁未释放时,获取读锁将阻塞;
				>写方法:先获取写锁,操作,再释放写锁;;..当有线程获取了读锁/写锁未释放时,获取写锁将阻塞;
			>数据字段:c 高16位存共享次数, 低16位存独占次数; exclusiveOwnerThread---独占线程;
				>判断逻辑: 即实现上述逻辑:  有读时都不能加写锁,都可以加读锁; 有写锁时,本线程可以再加写锁,也可以再加读锁,但其他线程不能加写锁和不能加读锁;
			>同步器:同一个Sync;  读锁--实现 共享式尝试获取/释放锁方法tryAcquiredShare(); 写锁--实现 独占式尝试获取/释放锁方法tryAcquired();
		>LockSupport:
			>停车:
			>限时停车:
			>带阻塞对象标识的停车: 在dump栈来查看时,会发现 有 parking to wait for <0x0000007d...> 标记;
		>Condition: 
			>对象的监视器方法: 实现 等待/通知 模式.
			>condition的监视器方法:	await()/signal() 
				>支持进入等待状态不响应中断
			>API:
				>等待: 新建节点入等待队列->释放全部state->停车; 唤醒后-->线程已中断则入同步队列后继续下一步|未中断直接继续等待-->排同步队补充state-->自中断;  可中断; 要唤醒同步队列中的一个节点;
				>限时等待: 新建节点入等待队列->释放全部state->限时停车; 唤醒后-->线程已中断则入同步队列后继续下一步|未中断直接继续等待被通知(会入同步队列)-->排同步队补充state-->自中断
				>不可中断等待: 新建节点入等待队列->释放全部state->停车; 唤醒后(中断可以唤醒)-->继续等待直到被通知(会入同步队列)-->排同步队补充state-->自中断
				>唤醒一个: 当前线程独占的条件下(未独占抛异常),找到等待队列第一个CONDITION节点-->入等待队列-->唤醒节点的线程
				>唤醒全部: 当前线程独占的条件下(未独占抛异常),找到等待队列每一个CONDITION节点-->入等待队列-->唤醒节点的线程
				--等待的线程被唤醒后: 还要入同步队列等待, 直到头而恢复state才会真正从await()返回;
		>Fork-join模型：
			>工作窃取算法: 正常的任务从自己的队列的头部取任务, 完成的任务从别人的队列的尾部取任务; 队列是个双端队列.
			>Task: 本身有 .fork() 开始执行, .join()等待结果 两个方法;  就像线程的 .start(), .join()方法一样;
		>原子类:
			>原子引用数组:AtomicIntegerArray, 可以成功cas更新某个index上的值;
			>原子类引用: AtomicReference
			>原子更新或获取整数属性值: AstomicIntegerFieldUpdater
		>并发工具类:
			>CountDownLatch: 没使用锁,直接使用sync同步器完成;cas乐观锁方式释放state ; 而等待则共享式的可中断的获取锁---重新实现获取逻辑-state==0才成功-否则失败排队阻塞再次尝试
			>CyclicBarrier:  使用了可重入锁；每次await()则减少count-1, 如果到0，则激活所有的等待线程；否则直接Condition等待；。。可以reset重置。
			>Semaphore: 控制同步中的线程数不超过多少，超过则阻塞；。很明显可以用尝试获取、尝试释放来实现；其中尝试释放成功之后，需要唤醒下一个节点---显然默认就会；
				>应用：流量控制如数据库连接---总数有限制--同时连接的只能最多有这么多；
			>Exchanger: 两个线程使用同一个Exchanger来交换数据；可以用两个数据来实现---两个线程的交换两个数据；；	
		>线程池： cpu使用多则线程数设置cpu数，而IO多则更多可以
		>Executor框架: 
			>工作单元：Runnalble/Callable
			>执行机构：Executor
				>固定线程个数：new ThreadPoolExecutor(n,n,0,Time,new LinkedBlockingQueue()) 固定worker大小的线程池，且任务无限，不拒绝；
				>单一线程池：也是无限队列，但只有1个worker,所以都交给这个worker来处理；那么可以保证顺序；
				>缓存线程池：同步队列--添加元素后阻塞，直到有线程获取；所以最多只有一个任务等待执行；而worker数最大不限制，最小为0，所以都是限时60s的worker
				>调度线程池：带优先级(最小堆RunnableScheduledFuture[]数组)的延迟取的队列DelayedWorkQueue(取元素可能会阻塞)， 最大线程无限制，0等待-没有闲逛的线程，
			>异步计算的结果： Future,FutureTask,
				>FutureTask：是任务，加到队列里的任务，run()的结束的时候，一方面cas 设置state属性设置为completing/normal,一方面产生结果，放到outcome属性里，一方面唤醒事先get()而wait中的取数据线程；
		--查询：
			>多少台机器在端口上：netstat -nat | grep 1000 -c 
			>查看网络流量：cat /proc/net/dev 
			>查看系统平均负载：cat /proc/loadavg 
			>系统内存情况：cat /proc/meminfo
			>dump栈最简单的做法： jstack pid > a.txt 