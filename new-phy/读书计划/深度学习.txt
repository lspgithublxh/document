---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。
>学习要有战略和格局：过于零散、孤立、末支、叶子的知识点就没必要化太多时间，懂得放弃。聚焦在根节点、枢纽节点、产生分支的节点、主干知识点、枢纽知识点、关联到很多知识点的知识点上。
 >对于主干知识点：要有全新的认识视角和进行精细的描述，大量的提问和好奇和进行必要的实验以获取支撑观点/导出观点的数据。
 >规范是工程最独特的特征.
 >慢慢读：
>一种新技术的学习：
 >它面对的情况和问题、它的世界观、它的方案、它的方案验证/论证/能处理的解决的所有情况及能成功处理的理由/功能边界
  >所有的软件：都可以看作是向上封装一层接口，根据自己的世界观封装底层而向上/对外提供统一的(统一的更简单的更直观的更业务的更少底层信息的)接口，底层包含一系列的第三方的插件/构件/组件；内部则去做兼容和调用(对底层)(对上层则做逻辑分解和底层实现)。
>知识混乱就是因为没有组织：
	>组织就是关键字树：几个单词就是每层上的每个节点的内容；
	>组织也可以看作逻辑树：有逻辑关系，逻辑顺序，逻辑联系的关键字的层层集合。层层囊括更精细的范围，层层划分范围。
>推进理解的属性发展拓展、问题延展：重要方式；
>什么是架构：架构也是从抽象到具体的考虑和描述；树形延展开来，可以写满非常大的黑板和巨大的脑图！！sharding-jdbc,dubbo,spring都可以这样方式来展现它的架构！！它的抽象到具体的考虑---本身才是架构！！！而不是什么模块、模式之类！！
>抽象设计：则某一层就不管上一层的含义和下一层的含义，即更抽象的含义或者更具体的含义；而是实现本层的含义；完成本层的含义指定的功能；。如网络协议的架构设计；	
>面向设计来理解，面向架构设计来理解，面向架构问题一层一层来理解它：面向设计来理解，所以按照面向对象设计的方式，看其中的对象、行为属性、流程环节逻辑。	
>找不到知识/描述 所对应的问题 ， 那么看书将没有条理纲领，变得零散琐碎没有组织。	
>不是按概念方式组织，而是按架构、问题方式来组织 笔记，书本内容。架构顺序，问题层次顺序。	
>架构不是设计出来的，也不是演进出来的(甚至不是迭代出来的--尽可能避免迭代)：而是问出来的。	
>每个方法方案都从属于一颗树，所以找到一个方法巧妙方法仅仅是第一步--找到从属的层次树 有更大的价值；(无论是谁想到的方法/概念，都要这样更进一步)
>解决问题的办法就是提出问题：类似递归和动态规划；。。权衡就是线性规划；	优势劣势在一定场景下也是劣势优势；
>一个词，到一句话，两句话，一段话，一篇文章；这个就是抽象总结，层次总结；越简洁，站得越高
--在网络、搜索引擎、推荐系统 三方面的专家；作为系统方面的独特优势/拔高优势；(网络-查询-推荐)
	
----有且只有响应，通信端才知道连接是否成功；。浏览器自动扩展。
----维持连接，并发连接，都是软件的实现，物理上看都是一条出口；从响应就是维护连接的角度看，不存在需要维护什么连接，维护就是维护连接数据而已；只要发送响应，连接就活了；在网络端口出口，可以连续发送不同目的地的响应报文，这就是并行；所以完全可以用队列来接收请求数据包；而用队列缓存发送响应数据包；多核使用起来，来并行大批量的发送和接收；不存在要维护和持续占用“端口”网络出口这种概念---完全没必要，用完即走 就可；	
	>或者不存在连接这个概念：所有的事情就是接收数据包和发送数据包(接送/发送缓冲区)。(连接 是 软件臆造出来的概念，不要和物理对应；和物理对应就会束缚思想，就会很多事情理解不了不知道原因)
	>连接的状态转移图；
	>应用的固定端口：实际上是建立新TCP连接的请求的处理的端口，请求到达这个端口--后面建立一个独立的TCP连接---来负责和客户端通信-交互数据；
----UML：为什么类继承关系图---因为这就是具体到一般的概念对象抽象过程。自上而下是能力顺序，能力组合；	
----说话和介绍：语速不要快，快就是掩盖问题，掩盖过程步骤；直接导致别人认为思路不清晰，表达不清楚，东拉西扯；也不利于自己思路的成长和扩展和自己主动发现问题，且必然导致不简洁--废话很多；
	>介绍需要先纲目后具体：抽象到具体；而不是张口就是罗列枚举---内容没有结构--全是线性结构；
	>描述更精简：一个字一个词，一句话，两句话，一段话；
----大事和吏治：大事 就像西天取经；吏治就像管理四人；


--计划：nginx/tomcat-->计算机系统-->架构-->自己的系统架构方案:专题研究、大提问、大总结。大简化/模型图化；
>一个进程看作一个消息，代码计划/任务计划；；都是异步隔离；	
	>程序启动点/执行点：可以多个，看作是并行的；(一个机器上，多个程序文件里)；可以留下执行点/新增执行点；可以删除执行点/减少执行点；
	>函数式编程为什么好：因为每个精细环节清晰描述确定了下来；使得充分配置和指明动作；
	>如何看待对象的方法：所有的对象都是被动的；主动的只有cpu/并行点；
	>程序的执行要想象为人在执行；多线程则是交接执行权给其他人执行；
	>抽象编程与具体填充：假设编程和实现假设。面向对象中一个对象属性就假设已经填充好了，一个接口的实例就假设已经在容器中有了。代码编码层面和虚拟机执行层面，都规定/定义/设计为 将 接口和实例分开，系统启动时/甚至具体接口调用时才去容器里找接口的实例(启动时 一方面建立实例容器，另一方面 对接口寻找匹配的实例和链接到实例---进行连接和关联)。资源填充 和 接口调用(抽象调用)，资源--接口的映射对。领域抽象，资源抽象，功能抽象。
----未知和迷惑的地方：痛点	；
----关键和核心的地方：要点

--混乱的答案，宁可不说；只回答真正掌握的；。没有逻辑，因为没有进行抽象；没有找到所在的流程环节、模型中的位置
--系统、中间件的介绍，不是一来就是组成结构；这层次已经太细了太具体了太里面了，必须要从最简洁最抽象最上层开始；最表面最近开始；务实，不僵硬，不突然，要自然，不要忽视和没注意没意识和跳过很多步；而是从问题出发、从困难出发、从疑惑出发
	>从问题出发：先明确问题；先明确表达出问题、疑惑点、黑箱、痛点、矛盾点、难点，表述的范围可以很大(完全不知道是什么怎么办)后面逐渐具体问；无论多少问题，先明确下来；尤其要全部且完整的描述下来；
		>问题的提出：先明确背景，自然衍生、过渡、转折、演化，逻辑关系上，什么时候什么事情什么人，事情什么阶段遇到的什么问题、阻碍、阻挠、缺陷、瓶颈、不够简洁、不够简便、不够方便、不够优雅、离目标远、离理想情况远、离期望/极限效果远；不够抽象的地方；把它们充分描述完整叙述结构式组织起来。
			>问题抽象：归结为一类问题；去除具体和细节而明确问题模型；
		>问题产生原因：过程；条件；	
		>问题导致的恶果：阻碍、损失；
		>理想的方案特征/效果/必做必不做的动作&事情/应当改变的环节: 
			>这种特征/必做必不做的前提、必要条件、必然要求、必然说明、必然指示、必然可以确定的更多的事情/结论：
				>一系列结论、约束得到之后(结合条件/问题/情况本身)逐渐可以清晰看到/归结出该具体问题符合的/满足的通用/一般的/一类的问题模型/函数模型/服务模型/IO模型/请求响应模型的轮廓：若干个具体模型
					>方案的装饰/补充: 补充可靠性/稳定性/高性能(从而高可用/高并发)：因为暂时只是一个裸机、容易受到伤害、有功能但没有抵抗力(仅为打火机的火而不是熊熊大火)
							>方案的用法规则：在请求缓存前使用
		>能将具体方案进行分类的维度/情况/模型/环节/流程/抽象表述 的确定：然后使用 抽象-具体 的方法来得到新方案；					
		>普通的方案：已有的方案；方案的抽象，方案的取值选择评价；方案的表象缺陷、劣势；。。模型、数据结构和算法、协议约定分担 维度 上考虑；
	>任何事情/事物都有顺序/逻辑：且几乎都是几种常见逻辑中的一种: 时间先后、空间远近、因果环扣、程度递增
--大总结：含义包括：重新 深刻理解：
--通用的建模架构能力+Flink深度强化学习的推荐系统。。。。而不是做简单的业务逻辑开发；用深度强化学习来做应用/解决实际问题(用户的识别问题和抉择问题)；用抽象建模架构出逻辑完整的方案(工具方案/服务方案)；
--一体两翼的发展模型：底座：增强操作系统、网络、搜索、推荐能力； 两翼：普通项目：则架构建模；特殊项目：则深度强化学习；。。四大基础+两大实践(应用/使用)。。基础：是为了解决自己的问题；实践：为了解决别人的问题(用户的问题/大众)；
--一次彻底弄懂，而不是 反复低温加热。看架构书和源码书，不看使用书。
--对话中胜利：一个是提问，二个是不断的输出-高能输出。
--牢固的观念: 系统都是被使用的。
--任何一个系统、产品、服务、方案、东西、事情，它要解决的核心问题是什么？理想的形态效果影响应该是什么？市面上对应的哪些产品达到了或者没达到或者很难达到？没达到是为什么是否有我们的机会？是否还有我们可以满足的缺口。找市场缺口。
--从缝纫机原理看方案所属的分类和方案内的环节。抽象出分类和环节。
--把自己当做cpu,调/使用各个服务/接口/类/系统。
--全新学习掌握方法：先定问题体系，先提问，定逻辑路径，后开始找答案--推导和思考和查阅资料。
	>旧方法：还是盲目阅读，还是从头到尾的阅读一遍的阅读。
--书写和思考：把手写的东西/写下的东西要能成为艺术品一样具有可观赏性 的 心情和目标 来 书写。	
--所有系统和方案的理解/研究/制定的出发点：都是 出发顶点/问题顶点/概念顶点，找到了顶点才能出发，出发要从顶点出发，才顺畅，没有后顾之忧
--计算机解决问题的方法：采集，记录，计算，展示，通知，跟踪 数据/信息。

--商业经营才能：(全新的解决问题的方法：提出问题)
--什么问题 是可以用互联网来解决的？ 只要满足什么特征，该问题就可以用互联网方式来解决? 该问题的本质是什么问题(如是信息传递问题？)该问题抽象一般化后是什么问题(资管、记录、通知、计算、采集识别调动、代理/中介/数据中心/信息中心/信息模拟线下过程中心、信息传递)(信息展示/发布/搜索/推荐 平台；信息池；信息可以是映射描述的现实事物的信息(如商品/价值物/人)，也可以是纯虚拟信息(游戏/知识/新闻/视频))
	>能用信息发布展示/活动追踪-信息记录/通知/协作/推荐/搜索 解决的问题：信息可以是 编辑的信息/映射的信息/等价现实的信息/等价现实任何事任何物的信息。内涵BAT的老业务(电商/聊天/游戏/信息流/搜索)。
	>能用数据驱动决策抉择与调度分配/识别与推理/观测采集与调动机电-生物 而解决的问题：如alphazero/图像识别/语音识别/视频识别/云计算/智慧城市/自动驾驶。内涵BAT新业务(云计算/视频语音识别-下棋打游戏智慧体/智能音箱/自动驾驶)
--码农之家和脚本之家、异步社区。从 面向对象-->模式-->架构：由小到大。
--给其他非软件行业的工程师/人员开发软件。三高三可(高可用高并发高性能,可扩展性可维护性可重用性)。
--最佳：一边看书，一边独立思考；(比起纯看书和纯思考都好)。因为 看书 要有沉淀，积累，独立思考 结论，总结。而不是流经书，翻翻而已，左进右出。
--说话：社交场合，最重要的是：要有自己的认知、决策和行动；对形势的判断和分析和预测。而不是被别人牵着走，或者置身事外。
--什么是最重要的? 就是你想做出一个什么东西...  这是最有价值的事情,是最重要的目标(比起学位和金钱和看多少书都重要);
	>不讲在公司的项目: 只讲自己 业余时间 做出的东西; ..  和 理解的东西... 总结起来 就是 四个方面: 架构 + 源码 + 算法 + 产品/作品
--不必指责和愤怒: 只需 战胜 和提防.
--莫忘理性决策：详细的明确和计算，一点也不能含糊。看看这次搬家时机的决策失误和搬家房间选择的失误，造成了金钱和时间上的巨大浪费。又比如以前自大没有投资股市---导致损失很大！！	拒绝感情用事和感动别人。
	>不能武断：不能一厢情愿。不能只见树木不见森林。
	>和人沟通打交道：都要随时监控预测对方的下一步会做的行动，如果不利于自己则应该怎么立刻做什么应对接下来做什么制定什么抗衡措施策略事情-对抗 来避免自己实质的损失，必须早早想,不能坐以待毙，否则一旦对方做成了则危险了--这个竞争是必须的。
--兴趣不仅决定能力，而且决定性格，比经历更加影响性格；总结和思考 是兴趣带来的根本也是兴趣导致的根本活动；而性格能很大程度上影响命运---作。韩信(仰仗自己聪明而希望别人只记得我的好不记得我的坏)、赵括(自己负责精彩，麻烦交给别人--希望别人帮自己解决；只知顶层逻辑不知道低层逻辑)、李广(总觉得别人对自己不好，发现不了自己的错，不按程序办事，爱自己来，当别人按程序来--总觉得别人是让自己受气，受不了别人的指责)	
--在大部分有钱人和机构都将资本+利润存入股市的时候：说明股市是会升值的，股市就是最大的公司。多余的所有钱+利润 只有在股市才能分享。
	>既要有信心，也要有担忧： 激情和忧虑 都有 ，才是 理性的人。而不是两者都没有的麻木不仁、冷血和装。
	>做人不要尖酸刻薄，说话要说别人的好，坏处知道 但是不要说出来。有的人不喜欢被戴高帽，或者认为是反讽，这只是 好 没有说到点上而已。
	>面对别人指出的问题: 改就完了,且深刻反思和各种改进, 大量的收获和扩展.更强!!!错误的反思总结得到的东西让人更强!!!
	>已经是青年人了：要敢于争取
	>控制欲望、总结反思验证、勤于积累和使用：控制无收益的欲望-事情白白浪费时间；总结反思-站在第三方检查和痛批评价自己的想法和行为-从细节到方向目标-做得不行的要改变；勤于积累和使用-阅读和实践都要彻底掌握弄通；
		>读书不破、万卷何益：不会有长板；
		>不痛苦痛批、不是反思：
	>择人：最忌讳的是好吃懒做的人。喜欢比较的人--一上来就卡脖子；眼睛没必要跟鼻子比。
	>不要轻易拒绝做 看不起的事情、还早的事情、不重视的事情、觉得自己不需要的事情-觉得世俗/自大的事情(世俗也要同流合污)：买车摇号、社保缴纳、公积金缴纳、找女朋友、找好的房子、考在职硕、炒股-投资。多少件事情--明明顺手就可以做，却没有做，一直拖，导致损失惨重！！
		>每一件瞧不起、不重视、还早的事情：相反，都要早点做！！
		>机会错过 难再有：往往影响是决定性的。没把握机会 就是 决策性失误！
		>其他事情：私活-阿里云-本地开发-换电脑。
	>想到就要做，错过不再有：这是最重要的名言！！只有这样才能发挥才智 并且 赢得 胜利！！！	最重要的就是时机、机会，把握机会把握时机。
		>时机、战机 转瞬即逝：稍纵即逝。错过就是一辈子，错过就是奉上身家性命。犹豫不决和瞧不上和不需要而暂时不做延后做 都是错失良机、贻误军机。
			>说小了是 酸，说理性点 就是 后悔遗憾 承受损失。
	>商业机会：大部分来自于持续观察大家的生活---什么社会事件政策会怎样的影响人们的生活/会让人们的生活发生哪些改变； 人们生活中遇到了什么瓶颈/矛盾/困难/问题/需求/不足/遗憾/不开心的事情。		
--理论观点/方法缺乏层次系统的组织，还是在等别人分配赚钱任务。概念、观点 还是片段、不连续的。
	>基本认知方式：提炼、总结、简化、抽象、封装 。
	>问题为 元素 进行组织：这次不以概念为元素，来更方便于实际使用、进行设计训练---工作也当作是设计训练。
	>填充所有的逻辑漏洞：
	>分层问题森林\分层问题 群: 一个n层的问题,其顶元素 只是 另一个n层问题的一个元素. 因此 k个 分层问题  构成 了 一个最顶层分层问题 , 整体 是一个 问题森林.
	>每一种策略/设计/具体化： 都必须明确它到底是为了解决什么问题。
	>范畴不明确：就会 陷入 “还有什么” 的困惑和迷途。范畴的向下具体划分和向上抽象统一更大范围。
	>讲一个东西的内容时：可以看出是否结构化思考，是否很明确范畴，范畴化思考，不断的抽象上升又具体下降--树上遍历。因为问题，产生目标，想到思路，立刻执行。
	>输入不必多：关键是转化 为 总结、实践  有 多少。	
	>可视化：为最佳的理解方法。
	>深入一个事物：就是问它的什么的什么的什么的的的的的的的的的的的的的的。
--领域、问题、逻辑。(路径/方案/思路)(条件/目标/路径)	范畴。顶级思路/顶层思路。顶层领域，顶层问题。范畴与逻辑。范畴关系(同一范畴内、不同范畴内)、比较关系、规则关系、联动关系。
	>不能只以概念入手，也要从问题、思路入手。
	>谈话永远不能泛泛而谈：回答而已，别总想着怎么回答会好-显得聪明有创意有见解之类。必须要有目标，要预见和引导 话题走势走向，进而从当前条件 寻找 路径/思路。
	>开发时：反思总结 而 扭转观念，不要一来就想着实现代码，而应该想着本层逻辑、本层事务、具体交给下层实现-下层再进行拆解-完成它要做的那部分工作-塔式调用/直到具体的专门的一个个的实现//这个也是逆封装过程，类似塑造过程。
		>链式调用：默认实现Filter 放在 最后，用户提供的放在前面；从而用户提供了实现如果想提前返回则可以直接先返回不走默认。
		>插件的发现/用户回调类的注入：往往都是自动配置类bean  注入了外部的 beanFactory，而自动配置bean里就有本框架的核心类，而获取了bf,则各种规范的实现bean就都可以获取了，从而用在框架各个位置。
	>一切技术都是简便方法：更快速的方法、更省的方法、更安全的方法。更可靠、更通用的方法。
	>调用 就是 询问：询问就是调用。
--顶级关注点：任何事物 寻找到它 最有价值的一面  对我有用有好处，如果有 则 认识到通和使用到精。没有看到组织结构，必然混乱和觉得复杂、含糊不清、仍然不懂、没有消化--分解/拆解出有营养可以被利用来构造系统的基本元素出来。
	>于社会：形势和机会：最理性的分析 -->最准确的预测 --> 最周全的方案 --> 最简便的验证。很多人不相信完美的十全十美的方案的确存在，也就不愿思考和制作和逼近，而是找了一个草率的方案，执行后失败而亏损。
		>人际关系：在不能够失败的事情上取得胜利 才叫胆子大胆略胆识,其他不能产生直接或间接作用用处的事情 失败了也无所谓--就当作给对方一个面子--做个厚道人-而不是尖酸刻薄没人接近没朋友-朋友就是大量小事上免费的互相帮助。办事：对方道德好-好心人，道德一般讲利益-合情合理，道德差坑蒙拐骗一把-就损失了。
	>于痛点：目标和思路：最理想的样子 -->最真实的现状 --> 最顶层的思路 --> 最简便的做法。
	>于业务：领域和关系：帮助用户解决的问题是什么(用户只需要有什么条件、只需要做什么)(概括) --> 最理想的方案的最顶级的思路是什么 --> 要展示什么信息 、后端计算要什么信息 --> 领域的属性结构、领域在某活动中关联的领域(如人和商品在购买活动中关联了起来)--活动的间接结果/结果描述：就是将领域和领域绑定在一起--划归到一个活动领域中而成为一个活动领域的两个属性--毕竟一个活动本身也有领域/流程/规则--单独的领域是独立无关的,只有在活动中才和无关的领域关联起来;反过来发现两个无关领域关联起来了则一定在某个活动中(如人与商品-在购物活动中；商品和地点-在物流活动中)；领域是基本不变的有限的，真正大量的不断产生的同样需要记录的是活动数据--一是活动参与者多二是活动步骤多三是活动多，此外，展示独立的领域信息 其实价值有限且固定难以增长，而不断新增的活动让领域参与者有新的参与感、活动服务了参与者、给参与者带来了新的服务价值、解决了新的问题 满足了新的需要--这些都是在活动中完成的；所以创造活动并记录活动 数据 才产生巨大价值。纯线上活动、活动的线上部分、线下活动的线上模拟活动 ，寻找、熟悉和创造 新的 活动，是对一门业务能力的三个阶段。
						 --->明确 所有的领域(属性结构)、活动和活动领域(其属性为若干个独立领域+活动本身信息) --> 各个活动领域 将各个独立领域 直接或者间接 关联了起来，则可以建立起一张关联起所有独立领域的大表；也可以判断是否存在和 存在则找出 两个独立领域的关联路径；帮助数据分析和数据挖掘。---> 使用 活动领域 记录和重新梳理 活动的过程。记录了一个活动过程，就是记录了一个服务过程(展示-交易-消费-评论)  。例子：一个购物：独立领域的关联过程： 人和商品(下单) --> 人和商家(支付) --> 商品和地点 (物流) --> 人和地点(固定信息) --> 商品和人(物流终点) --> 商品和评论(消费者对服务打分)
		>澄清 业务流程是什么-->每一步需要展示什么信息、要上传什么信息
			>信息分成哪几部分: 本质上可以归结为 哪几个独立的 领域(有明显的自己的边界)，这些领域的属性结构树是怎样的。
		>有 存储、筛选、推荐和展示海量数据 功能的 系统：供看、消费、互动。
			>展示的目的：>展示的东西：展示有价值的信息、宣传(广告)、引导用户购买、引导用户提供信息(生产/上传)、引导用户交互信息。
	>于系统：：拆解和组建。本身的规律和统一的路径。范畴式创新(补集/包集/子集)
		>认识：
			>系统一定不是一个单调的整体，而一定是组装起来的。单调的整体 只是一个部件，甚至不能用来构造复杂稳定的系统，没有这个扩展性、接口。
			>系统的运转：组装起来之后，启动系统，开始触发系统，向多米诺骨牌一样 传递下去。
			>系统的逻辑结构：假设...则有。如果...并且...,那么对...情况,则有...。假设有几个关键的暴露关联关系并好度量的事实、实验、条件、不变性，则可以得出xx间接的关联关系。具体关系直接向上抽象，将具体量消除,推导更一般的关系；。或者将关系和关系封装在一起，得出更间接更远距离的关联关系，得出对一个事物的完备的关系集合。或者规律本身特征的概括，来得出其他未知规律也一定满足的特征-抽象特征(如不变性)
			>系统的元素：系统的元素可以用来构成其他系统。因为它职责单一，可以替换别人也能被别人替换，可以共享(越单一越能被共享,越复合越不能被共享)(共享就是拿来使用,避免重复劳动)，可以互换，各个系统的若干个位置上都需要，从而用这些元素构成的若干个这类系统，一个系统报废了拆分出的组件元素 还可以放在其他系统中，从而元素利用 最节省 最经济、利用率高-浪费少。单一职责 是 有限的 ，构成一个 有限的集合。承载单一职责的元素 种类 就是 有限的，构成有限的类别的集合。从而每类元素可以批量生产，缓存起来，按需提取。
				>元素的类别：职责类别：特殊的元素--骨架型元素(支撑连接其他元素)。业务型元素(完成自己的输入输出模型)。
				>元素的接口：元素 的 可以和其他元素连接咬合起来 而传递/接收 刺激/物质/信号 从而发挥自己的作用 的部分。
				>元素的连接：一个元素的接口和另一个元素的接口咬合在了一起，形成 信号/刺激/物质 可以从一个元素传递到另一个元素的状态、复合形态、复合物形态。新的复合物有新的接口。不同于两个元素的输出输出模型 的 新的输入输出模型 产生了，这是连接起来 最大的意义和作用和目的，还具有新的状态转移图---也有使用用途。
				>元素的组建：在一个框架型/架子型/骨架型元素上(丰富的接口)(本身甚至就简单的仅仅是一个接口集合) 连接 接入 若干 功能型元素 和 骨架型元素。
					>组建的目标-条件-路径：最顶层的目标--产生一种更间接更长路径关联起输入输出的高度更高长度更长远度更远的输入输出模型的子系统/子模块(以进一步组装出更强的系统/让本系统更强)
				>元素的封装：用边界将 若干个临近的元素(无论是否连接起来) 包装起来，隔离其他 集群元素，而有自己的独有的基础资源、基本元素资源池。形成独立的环境、独立的上下文、独立的数据中心(信息中心)，统一的对外接口、复合的多模式的输入输出模型。
			>系统的状态转换：可以从外部施加给系统的动作，系统接收该动作后向内传递 而改变内部形态 最终传递反馈到外部(表面) 输出 响应，施加的每个动作每种动作都会引起系统状态的往不同路径上的迁移转化。这个状态迁移图 往往 固定的，即从某状态到另一状态可复现/永远不变的，从而沿着这些路径找到最短路径达到我们希望的具有某特征的状态---往往也是我们的目标 和有用处的事情。
			>系统的拆解分解：(自然界的系统没有飞地--即没有可以控制感知的物理隔离的另一部分; 生的属于后代属于另一个系统;甚至不能产生和发送控制用途/通讯用途的电磁信号;声音等是否算)
				>系统要解决的问题本身的拆解：问题的树形结构。问题模型。
				>系统的目标的拆解：功能的拆解，职责的拆解。从整体 最顶层 的 一句话概括  到 最底层的 若干个 元素的单一职责的表述/表示。(最高目标就是最高层的目标,而非平等的目标中最重要的目标;区分两种概念表述)
					>顶层目标的确定过程-就是需求明确的过程:
						>最简理想模型功能的明确过程：最简单目标、简单模型功能、理想模型功能、最理想条件下的模型功能、各种因素都不考虑进来的模型(正相关影响因素/负相关影响因素/微扰因素)、理想实验模型、实验功能、脆弱功能、简化功能、本质功能、核心功能的明确过程、澄清过程、细化过程、精准描述的过程。还只是一个实验理想模型。
							>理想条件的特征：其他阶段不考虑-只考虑核心阶段、其他影响变量不考虑-只考虑主要影响变量、具体细节情况不考虑-只考虑最抽象上层的情况。其他功能不考虑-只考虑单一功能。
						>其他一个个平级约束条件逐个考虑进来后 模型逐次扩展后的新而又新的模型的明确过程：新目标、新的主体架构、演化后的主体架构。扩展对基本功能的表述，将其中的具体的东西向上抽象化一般化。最苛刻条件下的模型功能。
							>约束条件的特征：限制条件、故障、条件变量值变为极值特殊值、险恶的环境、多变复杂的环境。必然会自然会发生的遇到的新需求，第一版现在就会遇到的其他需求。把这些需求明确下来--为了需求的完整和深刻 先只讨论需求有什么是什么 此环节不考虑实现。
							>考虑的目的/必要性：让系统健康成长、稳定成长、能适应险恶的环境、自愈自修复。生命力更顽强。
						>目标模型的向上抽象过程：功能增强过程。封装进其他目标模型，和其他目标模型封装到一个新的目标模型。
							>增强这个目标模型：依赖这个模型、监控这个模型、统计分析监控量并得出结论发出控制信号调节/指挥/引导/服务/治理/保护/优化/自动化/智能化 这个目标模型/系统。
							>考虑的目的/必要性：考虑到各方面各线可能增加的需求和功能-进行预留和预备、让系统扩展性更好、性能更好、功能更多还不乱、更稳定可靠、可扩展也可卸载替换功能模块、更好的可控性-更细更广可控。
						>目标模型的向下细化过程：向下一层层具体化展开明确确定。
				>系统的解决思路的拆解：方案思路的拆解。思路模型。
					>思路中的有限类型元素集合：
					>思路中的当前可以使用的条件的集合(组织成结构)：
				>系统的组件的拆解：组件模型。分解出下一层的构件，下一层的构件再到下一层的构件。一层层分解下去(如肉分解出蛋白质-蛋白质到氨基酸,来到可以被吸收的层次)(如汽车拆解出底盘-底盘拆解出发动机系统)，拆解到元素层级，最广泛被使用/重复利用的 元素级别。
		>使用：	利用和扩展。训练(它)和增强(它)。整合(到一个方案中)与改造(更适配一个方案)。
			>该系统如何应用在方案中：哪些类 哪些问题 的 方案  会使用 到 这种系统。就像用人 一样 用系统。
			>这些方案如何应用在业务中：这些方案又可以用在哪些业务中。
				>方案的评估和论证：
			>这些业务如何解决人们遇到过和将遇到的系统型的个人又难以独立解决的希望有组织-公司提供专业的服务来解决服务的问题/痛点： 
				>动作的条件、目的和规范：如伸缩 这个动作。
	>于创业：人们当前遇到了什么问题 --> 人们会遇到什么问题 --> 人们想要解决而无法解决想别人提供可以解决的哪怕是有偿的也要解决的问题是什么 --> 先 概括出用户想要而缺乏且看到则感到惊喜的服务是什么
>使人不惑：
	>结构：将系统呈现为结构形式来表达，可以帮助分清主次，找到本质入手，立刻看到关键、看到本质的轮廓、要义的切入点、概念。
		>一个变量 可能 也是 一个结构：
	>大量的困惑和不理解：都是 理解推进过程中 没有建立中间概念 ，即 没有找到可以 对某 事物的 理解作用起到增强作用的 中间概念、桥梁概念，通过桥梁概念来找到前后的联系，上文和下文的联系，导致结果突兀难以理解不明所以 。所以 理解的推进过程 中 中间概念的 找到 至关重要。	
		>困惑的根本原因：联系是断开的。上文和下文的联系 是断开的。或者说是 强硬 联系起来的。联系的一点也不自然。
		>没有归结到根本：没有将上层抽象的概念和 底层 基本基础的 牢固的概念 建立 联系，建立 坚强 牢固的 联系。上下之间缺少 中间概念 也会悬空 而 难以理解 抽象概念。
		>强硬理解：就是 不知道 条件到目标的路径，而强行让自己认为条件到目标就是必然的/直接的/一步的/显然的/不关心中间过程的。完全就是记。只知结论。
		>明确定义构成整件事情的所有 没有 清晰准确定义的 基本概念：都要 明确 提出 和定义 一遍。并构成 概念树。
			>要基于实验模型 进行思考：上面的概念页来源这个实验模型；没有实验模型 那么 就没有任何图像，只剩下逻辑了。
		>任何的总结都必须总结到最顶层：形成最简洁扼要的一句话的总结。由它可以层层推出到最底层的 整个 树形细节过程。	
		>使人困惑和书上东西看不懂的原因：一个动作、操作、名词、式子的定义/含义 理解错了，或者根本不理解。第二，才是 跳跃，中间过程 和 联系 省略了/显然了。 如 min max L的含义。
		>没有做铺垫没有说概念是从什么实例实际具体中抽取抽象而来的：没有关联结构图，就会很抽象。比如 条件随机场。
			>没有梯子的介绍/书：除非作者能腾云驾雾飞上去。
	>以前总是失败的原因：
		>大量的基本概念的含义都理解错了：错误 或者不够本质，拆解到不可拆分的 毫无封装的东西。
	>要在实际问题中使用概念：不能只是喜欢理论推导。	
		>概念图像化、朴素文本化：也要在实际问题的描述时， 主动 使用 上 理论 中的  概念， 来 和理论发生联系。因为 理论中的 概念 朴素文本化了，所以 在描述 实际问题时，尽管 最开始也是朴素的文本，但是 也就能 直接 联想到 理论 中 某概念的 朴素的文本，理解后发现 含义  是 一样的，这样 就会对实际问题 能够用高明的手法来解决， 而不是 还是 常规的普通的 效率不高的方法。不简便的方法。
>目标：对一种系统 进行 领域建模，系统架构。总结出新的 一类架构，新的一类问题/共同问题。		
	>写研究报告、策划方案、完整讲清楚-八面受敌：
	>验证方法的进步和改进：
	>论文阅读：先浏览一遍，局部看懂，局部能看懂的看懂，主线清楚。然后才开始细看，把中间过程补充上。所以至少两遍。
	>明确目标和要做的事情的推进顺序：要做的事情的层次结构、推进顺序，就是路线。目标和路线明确，事情才能有条不紊 井井有条的 一定能成功的 落实实现。认识路线，推进路线。
>公式推导的问题/痛点、想法、目标、起点、条件和路线：每次公式推导必须明确，从而清晰思路。(想法和条件,想法和认知,想法和情报,想法和信息)(想法好信息足判断对目标定)
	>问题的提出：推荐算法中 通过 相似用户、相似物品计算 用户对 物品的 喜好值 并不是彻底的一般的方法，更像是一种临时的方法。所以想寻找一种通用的统一的一般方法。
	>想法的得出：评分矩阵里的每个评分 为什么是这个 值，思考这种必然性、确定性，不那么显然的显然性，任何一个确定的一定的取值则要么是公理要么就可以继续分解用更基础的概念集合更本质的元素集合 来解释来计算来表达出计算过程来表达相互作用得出该结果的过程，任何一个值 要么 是 公理值 要么就是 计算结果。
		>把评分值当作计算结果 反推计算过程：假设是一个简单的线性组合得到的计算结果，而线性组合可以分解为两个向量的乘积，而这个评分值显然是由用户的特性和物品的特性决定的，而特性相乘，必然一个是数量-一个是含量，显然用户要求的是数量，而物品有的是含量；所以两个特性向量，用户的是用户的特性数量/偏好数量/需求数量，物品的是物品的特性含量。
	>转换后的目标：计算最优的用户特性数量矩阵和物品的特性含量矩阵。最优的标准：明确为 方差最小。在这个约束/要求下 计算PQ矩阵。
	>起点条件和顶级想法层层细化：在这个约束/要求下 计算PQ矩阵的数学方法：梯度下降法/最小二乘法。用差量的方式 甚至可以用 迭代法：Puf(t+1)=Puf(t) + α * зLossui/зPuf(t)	, 而 ^rui = ∑Puk*Qki, Loss = ∑(rui - ^rui)^2, 为防止overfitting,添加正则项控制过拟合：Loss = ∑(rui - ^rui)^2 + λ(∑Puf^2 + ∑Qfi^2) = f(P,Q)
	>方案的证明和优化：
		>对于 ^rui = ∑Puk*Qki ： 此表达式计算^rui 显然只考虑了 Pu和Qi相互作用的部分，即用户和物品共同决定的部分--通过需要数量*具有含量 来 共同确定的部分，实际情况中 可能还有 只跟 用户、只跟物品有关的因素 bu, bi, 甚至跟用户和物品都没关系的因素μ ，所以 修正后的 ^rui = ∑Puf*Qfi + μ + bu + bi ; bu往往可以用用户的性格打分习惯严格、温和来决定；bi往往和物品质量有关；
		>对Puk: 是用户对各个特性的需求量，而用户实际 对这个特性 的 需求量 可能 会 因为 用户 购买了 越来越多的物品后 而 有所改变--比如历史买了富含维生素C这个特性的西红柿 而 下次对 黄瓜这个同样富含维生素C这个特性的物品 需求量 就更低了，所以需求量 需要和用户的购买历史物品 关联起来，即 Puf 替换为  Puf + ∑Yjf/sqrt(N(u)), ∑Yjf就是已购买的物品对此特性的需求量的影响力之和。SVD++
		>再次优化：则是 考虑时间因素了。rui的各个因子都是时变的了。
	>符号的拆解和封装：符号也可以拆解开 用 更基本的 符号来表示，直到 +- 等。其他符号都是简化某个使用基本符号构造的复杂运算过程 而 定义的；就像 计算机里封装出一个个函数一样 封装出一个个符号。
		>积分符号 也是 一种简化符号，是对 某个区间 分割 出的 无数个 连续的无限小量 求和 这个过程 的简化 表示。Σx*f(x)dx = ∫x*f(x)dx 
		>乘法符号 是一种简化符号：是对 k个 相同值m的 求和 这个 计算过程 的 简化 表示，等值表示。C = m+m+..+m = k*m。。。数量和单位数量值--方便总量值的计算。(计算总量 和 可以摆出多少种组合 都是最原始的需求，野人时代就有)
			>乘法第二种含义：同时性/两个集合的元素能够组合出的所有结果构成的集合-方便组合数的计算。比如两个独立事件同时发生的概率。一个事件发生的概率 等于 这个事件代表的情况数 占 全部情况数 的比例，(一个事件 是若干个情况的集合)(一件事情 全部可能出现的情况)(一个实验 全部可能出现的结果)(一件事情 出现了某种情况(即所有可能情况中的某种) 是 一个 概率事件)。
				>当两件独立的事情联合构成/视作 一件新的事情时：则这个新的事情 全部可能出现的情况数 就是 两个子事情的情况数之积(组合出的结果)。
				>同时性：代表了组合空间的增大。
			>乘法的第三种含义：映射。如 [0,c]*[0,b], 如果 每个当作轴上的点区间，则两个集合相乘 就是映射到 相交的一片平面区域，这也可以看作两个集合元素完全组合的结果。	
		>触发符号的来源：计算比例、占比。计算倍数则是另一种 源头。概率 是 表达 情况的占比。		
	>推导这件事情也需要一步步推进：每一步都走的很小，很连贯，来保持思路的连贯。
		>合理的充分的代数表示：非常重要的一步，表示事情、过程 应该完全代数化，没有具体变量的痕迹。
		>先明确概念再明确概念之间的关系：
	>数学：就是在基本概念和定义(元素、目标、条件、路径) 下 向上 封装、抽象 、推导推论 而 建立起来的 关联系统结构。如线性代数、概率论。
		>抽象：就是省略具体特征。
		>数学的结构：所有的概念的定义和关系和层次结构(引用/封装/抽象)。
		>清晰的定义：精美的书写。关联关系明确。反复描述/仔细描述/重头开始描述/重头梳理整个过程(什么条件什么结果已有什么关系什么约束,不多不少,梳理清楚后,完备逻辑推理之后--已有条件下所有能得到的结论已经的出来-再得不出新的关系, 所有能做的等效处理动作已经做了--也得不出新的关系, 下一步则必然是 引进新的 约束条件，新的具体条件,新的关联条件，无论是客观存在的还是认为假设的, 只有引进了新的关系/新的约束条件，才能得到新的结论，才能在这个基础上 进一步 做 出 不变性处理/可以进行的操作/不妨进行的操作/进一步可以做的动作/可行的动作而得出新的结论(如具体化/特殊化等)；就像堆积木一样--知道很高层之后哪里可以再塞一个积木)。这就是逻辑推导的过程，数学推导的过程，物理推导的过程的 一般整体 方法/思路。
			>数学推导的总体思路：总体步骤。先在已有条件下 推出所有能推出的结论，(接着 假设知道了什么则可以破局;知道了哪些可以破局;然后猜测/分析引入什么新的约束条件则可以达到假设设定的子目标)然后 再引进新的约束条件，进一步进行逻辑推理 推出所有能推出的结论。 如此永远循环下去(这也说明了要有目标和思路;思路就是 条件关联路径,变量关联路径,事实确定路径,信息确定路径网)。比如 高斯推导 随机误差满足的分布时 最后一步采用的方法---就是引进了新的约束条件(具体化xi)来让推导继续下去并找到了结论。否则就会陷入条件不足 而无法推进 不知道干什么的情景 却 还不知道 什么原因导致的 和 怎么破解这种局面。(新的约束条件如：具体化/简化/特殊化具体例子处理)
	>物理：也是基本定义和概念、不变性等 上 进行抽象、封装、推导推论 而建立的 关联系统结构。
	>分析分解：从某个量的取值分解开始，到第二个量的取值分解，直到最底层，分解出最基本情况下 / 最小情况下 的 原问题的解，然后再 按照分解 关系 综合 各个最底层的解 上来，选择或者 合并 出 /封装 出 原问题的解。
		>上述就是分析和综合 的思想：先分析 后综合。证明 广义拉格朗日函数的极大极小值 就是 原函数的 极小值 和起点思想 就是 分析综合思想。
	>推导-计算-解释/理解-使用/验证： 这是 对 一个问题 思考 经历的 三个 步骤。都重要。推导 则是 根据已知条件  得出 关系式， 计算 则是明确指出 计算表达式的构成/计算表达式的性质-特征/结构特征/关键/组成成分，理解和解释 则是 表达式的 各部分 怎么和已知相联系-即为什么会是这个关系-关系为什么会是这个样子 ，并给出合理的解释和认为和理解和当作。 使用就是用来计算解决实际问题/证明验证这套方法的效果/精确度/可信度/实际价值量。
	>数学运算的深刻理解：多种理解；如矩阵乘法、矩阵、微分；
	>数学的封装大厦：如线性代数，最底层的概念开始，层层向上封装，逻辑严密，关系牢固。内积空间--线性空间--规范正交基-共轭转置/伴随矩阵--自伴算子/正规算子--等距同构
>举动-痛点：要解决的问题、目标、条件/起点/底层/原子、思路。最顶点的思路，最上层的确定性/认识。难以形式化描述的任务 如何 用 软件来解决。	人类靠直觉解决的问题 如何通过计算机来解决。
	>解法/解决方案：根据层次化的概念体系来理解世界。某个概念通过 与某些相对简单的概念之间的关系来定义。从而让计算机构建较简单的概念来学习复杂的概念。绘制出一张 这些概念如何建立在彼此之上的图，来得到层次很多的 深图。这种方法就叫 深度学习。
		>特点： 规则简单：策略困难。规则可以 完全形式化的非常简单的 规则列表来描述。抽象和形式化的任务 对计算机 来说 是容易的。
		>特点2：非形式化的内容 表示出来。
		>机器学习：从原始数据中 提取 模式 的能力。从数据中获取知识(估计的参数)，然后用来决策(用估计的参数计算决策条件值得出判定结果/选择选项)。 
			>数据-->特征集-->机器学习算法-->得出判定选择选项
			>数据-->机器学习算法发掘出特征集-->机器学习算法-->得出判定选择选项: 称为 表示学习。
				>表示学习算法的例子：自编码器。它由 编码器函数 + 解码器函数 组成。
					>编码器：输入数据-->该数据的不同的表示/新的表示。
				>变差因素：不可观测 但 影响 可观测的量。且它们同时影响着 可观测的量。
				>目标：从原始数据 中 提取 高层次、抽象的特征。如各种角度拍摄的汽车图片-->汽车。如带口音的说话-->内容。		
		>深度学习：让计算机 通过较简单的概念 来 构建 复杂的概念。因为 计算机是不能直接理解 感官输入的数据的。将事情表示为 嵌套的层次概念体系。
			>从原始数据开始，一层层向上提取 本层 能够直接 提取的 更抽象一点的/一般一点的/一类一点的 特征/基本构件 的集合。 如  原始图片的像素集合-->通过相邻像素的亮度 而 抽象出边缘实例 若干---> 通过边缘 而 组合/抽象出 角/轮廓 实例 若干。--> 通过轮廓和角的组合/抽象 出 物体的部件/部分 实例 若干。---> 通过 部件/部分 而 组合/抽象 出 完整的对象 实例 若干。
				>表示方法：深度概率模型
					>前馈深度网络：
					>多层感知机：
			>整流线性单元：
			>反向传播：
			>长短期记忆网络：LSTM: 
		>线性代数：
			>标量：坐标轴上的一点
			>向量：空间上的一点。多条轴 的联合值域/组合出的值域/构成的值域 就 张成了一个空间。 
			>矩阵：1.看作初始坐标系下的n个列向量构成的新的坐标系。(这是 对向量的坐标系变换角度 看)
			>张量：1.在各种坐标系下值/坐标值 都相同的量(坐标系也考虑进来？)。
				>黎曼流形：曲面上的各种对象在不同 坐标系下的 表示  之间的 联系。
				>一个具体取值的向量：显然要考虑 这个向量在哪个 坐标系下，如在直角坐标系下，球坐标系下，还是一个 自定义的坐标系下。
					>坐标系的表示：x = [x1,x2,...xn]^T  上标就表示坐标轴索引。
					>坐标值的表示：就是[...]表示。 空间中同一个点，在选择不同坐标系 来 表示它时，坐标值 是 不同的。
				>本质定义：x是直角坐标系下的点, f(x)表示 这个点处的一个物理量；在坐标变换后，x变换到了球坐标系下的x', 对应的物理量为f'(x')； 如果 f(x)=f'(x') 则f(x)是0阶张量，	f(x)=f'(x')是变换法则。
					>坐标变换：x-->x' : 可以写作： x = g(x')   dxi = Σзxi/зx'j * dx'j  矩阵形式：dx = S dx' 这里 S就是一个偏导数矩阵，第i行j列就是S(i,j)= зxi/зx'j;  S就是 x用x'来表示 即x(x')这个函数 的 雅可比矩阵。x和x'之间的变换是非线性的，但是dx,dx‘之间的变换则是线性的。物理量dx具有线性的变换关系，所以定义为 1阶张量。 同理 ，dx/dt = S * dx'/dt 则 dx/dt也是一阶张量。
						>爱因斯坦求和约定：dxi = S(i,j)*dx'j  某个字母同时出现在上标和下标，表示要对它求和，这里就是对j。或者说要把哪个多余的标 消除--就对它求和。
						--读音：西格玛：∑Σσ ， 拍：Π， theta: θ  delta:δΔ， lamuda：λ gama:γΓ 倒三角：▽▼  rou：ρ,  uu'sx： 所有数学符号。uu'jh：几何符号； 
						>梯度变换：▽f = [зf/зx1,зf/зx2,..зf/зxn]^T,  而 ▽'f' = [зf'/зx'1...]^T  因为已经假定 f=f', 所以 зf'/зx'j = зf/зx'j = Σзf/зxi * зxi/зx'j = Σзxi/зx'j* зf/зxi 对i求和---因为这样才能把i消除。  纵向拉开后，形成方程组 而用矩阵表示：▽f' = S^T*▽'f
							>进一步：假设令 ▽ = [з/зx1,...]^T = [з1,з2...]^T ,  ▽' = [з/зx'1,...]^T = [з'1,з'2...]^T 则 上式可以简化为  ▽' = S^T*▽,  因为 f'=f 所以消除了。或者简写为 分量 形式 ： з’j = S(i,j)зj  从而说明 ▽ 和 зj 的变换也是线性的，是 一阶张量。‘在S侧则这种张量的变换规则 是 协变的，'不在S侧则这种张量的变换规则 是 逆变的。矩阵运算的变换：将S移动到另一侧：S^(-1)(j,i)*з'j = зi从而逆变形式 变协变形式。
							>S: 是一个i*j的偏导数矩阵，融合了张量的变换规则，就是张量的变换规则：(S(i,j)) = зxi/зx'j  即 上标一定在分子上，下边在分母上。
								>协变的：协变的变换规则，这个变换规则 在 新坐标系侧，用在新坐标系上。协变的 求和 对 上标求和。
								>逆变的：逆变的变换规则，这个变换规则 在 原坐标系侧，施加在原坐标系上(进行逆变,变为逆)。逆变的求和 对 下标 求和, 但是微分的逆变 是对上标求和----所以一般看 这个一阶张量的指标写在上面还是下面--写在上面则对下标求和-写在下面则对上标求和。显然，逆变的变换规则 矩阵 可以转换为协变的变换规则矩阵，同理反过来也可以。
							>▽：(对坐标的)偏微分算子向量：分量即为 зj = з/xj。这个算子的变换是线性的，所以是张量，因为是一维的，所以是一阶张量。即梯度 是一阶张量。
				>度规：长度和角度的度量。			
					>直角坐标系下相邻两点的距离：用勾股定理得出：ds^2 = Σ(dxi)^2 = dx^T*I(n,n)*dx = ΣΣδij * dxi * dxj =δij * dxi * dxj  这里用了爱因斯坦求和约定；为什么要用i,j因为避免两个向量的约束关系直接在i,j上体现出来，而应该无关的，所以应该提出这个关系而在δij这个矩阵里表达出来。
					>任意坐标系下相邻两点的距离：借鉴直角坐标系中的表示，定义为：ds^2 = gij * dxi* dxj 矩阵gij 就刻画了 该坐标系中 某两个坐标分量积 对 长度平方的影响权重 ，矩阵中某个元素表达了 某两个坐标轴分量积 对 度规结果 的 影响程度。 在另一坐标系下 来表示同一个 长度 则： ds'^2 = g'(p,q)* dx'p * dx'q , 都表示的同一个东西的长度，所以取值应该跟坐标系无关，所以长度应该相等，即 ds^2 = ds'^2, 所以 gij dxi dxj = g'pq dx'p dx'q = gij * (S(i,p)*dx'p)*(S(j,q)*dx'q) = gij*S(i,p*S(j,q)*dx'p*dx'q  这个说明了 g'pq=gij*S(i,p)*S(j,q) 说明 gij 是二阶张量。变换规则是线性的。显然右边 是隐含了对i,j求和的。写擦矩阵形式：G' = S^T*G*S。。。
						>原：ds^2 = gij * dxi* dxj = dx^T * G * dx = (Sdx')^T * (S^(-1)^T G'*S^(-1)) * (Sdx') = G'dx'^Tdx' = ds'^2 证明了 长度 在两个坐标系下是等值的。
							>注意：1.分量相乘是可以交换的(求和符号也可以在交换后才起作用/且按交换后的指标顺序来对指标求和)，所以gij * dxi* dxj = gij*(Spi*dx'p)(Sqj*dx'q) = Spi*gij*Sqj*dx'p*dx'q 。2.同一个元素在原矩阵中的上下标和在转置矩阵中的上下标是相反的，或者说 原矩阵中的一个元素可以用转置矩阵中的一个元素来表示：如Spi 就是S^T(ip) . 3.几个不同矩阵的各自的元素相乘，在进一步表示矩阵相乘的时候，需要加求和符号，此时应当对矩阵元素的表示和相乘顺序作调整以能 正确表示 矩阵先后相乘计算的求和过程。或者说 (调整后的)元素相乘的 求和 过程能够反应/等效 出的矩阵相乘过程。对所有指标都要求和： Spi*gij*Sqj*dx'p*dx'q = Spi*gij*S^T(j,q)*dx'p*dx'q = dx'^T* S*G*S^T * dx' = dx'^T*G'*dx' , 即 得出 G' = S*G*S^T 这就是长度 的 度规张量G 的逆变的变换规则。从形式上看，这个度规张量 符合一般的张量形式。长度的度量是度规，这个度规是2阶张量。
								>长度的度规张量 的变换的 带指标形式：G' = S*G*S^T 的带指标 形式： G'(p,q) = S(p,i)G(i,j)*S^T(j,q) 注意这个是逆变形式-是对下标求和 。。长度的 度规 是2阶张量，而长度本身 是 0阶张量(即标量)
								>张量变换：一阶张量的变换，用线性变换表示；二阶张量的变换，用矩阵合同变换 来表示。更高阶张量的变换 表示 如下--协变形式(注意是对上标求和)。
						>G: G=(gij) 称为 metric 张量 或者 度规。直角坐标系下 这个 度规G 是 单位矩阵I = [1 0 0; 0 1 0; 0 0 1] ，球坐标系下 这个度规 G' = S^T*G * S = S^T*S = [1 0 0; 0 r^2 0; 0 0 r^2*sin^Φ] 分别代表了 在径向、纬度、经度上的影响。
							>度规 gij: 表示 基向量的变化量 引起的 向量的 变化，两个变化的比例 的平方。比值的 平方。如直角坐标系中，三个基向量上的这个比值都是1，圆柱坐标系中 三个基向量上的这个比值 则是 1 r 1。圆柱坐标系中，认为dr,rdθ,dz才是基向量，即基向量的长度不一定是1：而是sqrt(gij)
						>T: 如果T是一个张量，即原坐标系下的张量，T=T(i...k)(p...q),  则这个形式 说明它满足 线性变换法则：1.如果坐标系变换表示为：dx = Sdx', S是坐标系上点的变换度规， 2. 则和新坐标系下的张量T'(u...v)(m...n) 的变换关系(即协变形式的变换关系)为：T(i...k)(p...q)= (S^(-1)(m,p)...S^(-1)(n,q)) * T'(u...v)(m...n)*(S(i,u)...S(k...v)) 新张量左边为协变而对上标求和，右边为逆变而对下标求和。显然要比微分/梯度/长度的度规 这些张量 的变换要复杂，但它 又必然是 微分的变换规则矩阵和逆变换规则矩阵的若干次组合。比如长度这个标量的度规G/张量G 就是一个点度规和一个逆点度规 的组合。
							>注意：m...n,u...v都是求和指标。为什么要定义这么多指标？因为发现了通项的存在，然后 为了描述准确通项 是 什么 表达式的 通项/通项如何延展为完整的表达式，所以要定义/加上指标来指示说明(就像代码里的指标/标记一样)。但大多数情况下 带有大量指标的通项 本身并没有给出可以计算的信息，这个通项 是 自底向上 从 单一指标的量/可观测的量/可接触的量 层层 向上 封装 出来的(因为每个不同的指标 之间没有依赖关系，所以它们凑到一个表达式中 就是 组合的关系,形成的量 就有多个指标)
							>上标：新坐标系 的 分量索引。如球坐标系。第二个坐标系的轴号/基向量号/微分向量号/偏微分分量号。或者 一个坐标系下的第二套指标。上标逆变。可以单个指标，组合指标，一个量有很多指标，说明它是构成一个巨大巨多层次求和的表达式中的单个元素的一般形式；
							>下标：原坐标系 的 分量索引。如直角坐标系。第一个坐标系的轴号/基向量号/微分向量号/偏微分分量号。或者 一个坐标系下的第一套指标。下标协变。一个带指标的量 是 一个求和表达式的 单个元素形式/单项统一形式。这个单项可以是一个 四则运算表达式，也可以是微分偏微分表达式/求导/求偏导表达式。为什么需要求和？为什么会出现求和形式？：线性组合 出现、向量点乘出现、点用坐标系表达时出现、描述梯度时出现、描述点的运动速度时出现、描述坐标系变换时出现(两个在空间位置确定的坐标系,求空间上一点在两个坐标系中的各轴分量取值)。
								>下标有两个分量：表示两套分量指标。同一坐标系下/同一基矢量系的两套指标。
							>单指标的量：向量元素。两个指标不同的 单指标量相乘，结果显然应该被定义为一个双指标量--且指标就分别是这两个量的指标。
							>双指标的量：矩阵元素。
							>创新之处：将多个带不同指标的量的相乘  表达 为 一个 带所有这些指标的量。即向上 概括了以下。封装了一下，简化了一下。
					>体积元的度规：点x到x+dx之间确定的小面积或者体积或者超体积 A ，在直角坐标系下 A = dx1 dx2 dx3 ...dxn , 在其他坐标系下，根据坐标系变换规则 dx = Sdx' , 则 dx1 dx2 dx3...dxn = (ΣS1j*dxj')(ΣS2j*dxj')... = 和 S的行列式有关 = |S| dx'1 dx'2 dx'3...dx'n，另一种解释方法 ，假设系数是λ,则 dx1/dx'1 * dx2/dx'2 * ... = λ, 则发现每个元素是S的对角线元素： = S11*S22*S33*S44... = 假设S是对角矩阵--比如球坐标系中就是，很显然乘积 就是 S的行列式 |S|, 因为 G'=S^T*S, 因此 sqrt(|G'|)=|S|	, 因此 A' = sqrt(|G'|)*dx'1...=A 
					>协变导数：在计算 度规 随 坐标位置的变化 的变化程度，如 加速度的协变导数。
						>一阶张量的求导：设有一阶张量的变换关系：W'μ = S^(-1)(μ,v)*Wv  ,  则继续求导 ，即左乘 з‘λ ，从而 з‘λ*W'μ = з‘λ*(S^(-1)(μ,v)*Wv) = S(i,λ)*зi * [S^(-1)(μ,v)*Wv] = S(i,λ)*S^(-1)(μ,v)зiWv + S(i,λ)*зi)*S^(-1)(μ,v)Wv  这里因为 中括号里两个单项相乘，按照微分计算规则，需要对两个单项分别进行微分，因此展开成了两项。这样，看到 直接进行这样求导 形成的不是张量，因此 寻找另一种求导方式，使得 结果 是 符合张量定义的(形式有意义)；定义这种 求导为 Dλ， 它对 某个张量求导 的结果 满足形式：Dλ * Wμ = зλWμ + Γ(μ)(λv)*Wv 这种求导 称为 协变导数。之所以是这个名称，因为这种求导的协变形式是 D'λ*Wμ = S(i,λ)*S(-1)(μ,v)*Di*Wv  满足二阶张量的 变换关系。Γ(μ)(λv) 称为 Christoffel symbol
							>上式的说明：是基于这样的逻辑过程：假设 存在一种求导Dλ (一阶张量), 它能既能满足 张量变换关系：D'λ = S(i,j)*Dλ, 还能满足 具体作用于 另一个一阶张量 即对它进行求导时具有 D'λ W'μ = S(i,λ)*S^(-1)(μ,v)*DλWv
							>一个张量乘以另一个张量 而 形成的量：可能不是一个张量。
							>两个量相加 后乘以另一个量 而 形成的量：可能是一个张量。
							--要用坐标分量来表示的量：如速度，加速度，力，则坐标分量上的值 随着 坐标系的不同而不同。但不用坐标分量表示的量如质量、温度则跟坐标系无关。
						>Christoffel 符号：Γ(l)(i,j) = g^l * зgi/зxi  或者： зgi/зxi = gi * Γ(l)(i,j) 基矢量gi对 xi轴的偏导数 在gi上的分量 就是 Christoffel 符号。Christoffel 符号 是一个 三阶量。
							>推导过程：
								>直角坐标系：下的一个位置矢量x
								>空间曲线坐标系：空间上一点x在 这个曲线坐标系下的分量(x^1,x^2,x^3)
								>位矢：x 为一个矢量。位矢的增量/微小变化 dx = Σ зx/зxi * dxi = зx/зxi * dxi 注意,x的i在上标。
								>位矢的增量比率：gi = зx/зxi  是一个向量，沿着xi分量轴方向。称为 协变基向量。g的i在下标。则 dx = Σ gi*dxi =gi*dxi 
									>gi:曲线在x^i轴方向的弯曲程度。是向量，方向同x^i.
									>g^i: 曲线在x^i轴方向弯曲程度的倒数。是向量，方向同x^i.
									>gij: 曲线在x^i轴方向的弯曲程度向量与x^j轴方向的弯曲程度向量之点积。点积有协同的意思，所以同向最大，垂直为0.
									>gj：可以被g^i线性组合表示。gj = Σ αjr*g^r = gjr*g^r  是行的线性组合。
									>g^i: 可以被gj线性组合表示：g^i = g^(ij)*gj 也是行的线性组合。
								>距离的 表示： (ds)^2 = dx * dx =  gi dxi * gj  dxj (可以这样写因为 和的乘法 展开也是 和的每一项分别相乘对方的每一项, 所以这里也引进了新的指标来表示最终结果 的 单个结果项的一般表达式) = gi * gj dxi dxj = gij  dxi dxj  这里 gi * gj就是两个向量的点积，显然这个点积 有 i*j个,每个结果表示在一个矩阵里的一个元素，则这个矩阵定义为/称为gij。这样整个表达式的 含义 变为 对 整个矩阵 的每个元素进行求和(同时每个元素乘以dxi dxj)。又因为 gi * gj  当 i != j 时，根据垂直曲线坐标系的定义 中 的 垂直要求，则 结果=0，即 i!=j 则 gi*gj = 0 因此这个矩阵gij 是 一个对角阵。
									>如果gi 用欧式空间的直角坐标系ej 来表示： 则 gi = Σ eik* ek = eik*ek  即是直角正交基的线性组合。eik是一个矩阵的元素。用来表示 gij这个矩阵的行列式 ： |gij| = det(gij) = det(gi*gj) = det (eik*ek * ejl*el) = det(eik*ejk) 可见新矩阵的pq位置元素 是 eij矩阵的p行和q行元素的线性组合。计算它的行列式就比较麻烦。它的行列式 = (det(eij))^2 = [g1 *(g2 x g3)]^2  即为gi为棱的平行六面体的体积的平方。定义混合积 g1 *(g2 x g3) = [g1,g2,g3] 这是一种 科研 手法，就是假设引进一个 成立的 等式，尽管未证明。
									>定义逆变基向量 g^k = gi x gj / [gi,gj,gk] : 直接可以得出 g^i 和 gi 是平行的。而 g^i 和 gk 其中i!=k 则是垂直的/正交的，那么 可以得出 g^i * gj = δ(i)(j) 即 对角矩阵。δ^i,j 称为 Kronecker 符号。所以 [g^1,g^2,g^3] = (g^1 x g^2) * g^3 = 1/ [g1,g2,g3]
									>定义二阶逆变度规张量 g^(ij) = g^i * g^j  目标：计算混合积，最后会发现=sqrt(det(gij))
								>最终推导过程：
									>对gij求x^k的偏导数：зgij/зx^k = зgi/зx^k * gj + зgj/зx^k * gi , 依次轮换指标得到另外2个方程，然后 利用  зgi/зx^k = зx/(зx^i*зx^k) = зgk/зx^i 即上下标 交换后不变，可以 用 后两个方程 相加 减去 第一个方程，因为 有3对相同项，并去了2对 而合并了一对，得出 1/2 * (зgik/зx^j + зgkj/зx^i - зgij/зx^k) = gk * зgj/зx^i  然后令 зgj/зx^i = Γ(l)(i,j) * gl , 相乘得出  зgj/зx^i * gk = Γ(l)(i,j)*gkl ,  然后联系两个关系，第二个关系左侧就是 第一个关系的右侧的指标轮换形式，所以  Γ(l)(i,j) = 1/2 * (зgik/зx^j + зgkj/зx^i - зgij/зx^k) * g^(kl)
						>协变基矢量：з/зx^λ ， 满足  з/зx^λ’ = зx^λ/зx^λ' * з/зx^λ  
						>协变基矢量对坐标的导数：з/зx^μ (з/зx^λ)   用上式 四项展开后发现它不是张量。з/зx^μ‘ (з/зx^λ’) = зx^μ/зx^μ' * зx^λ/зx^λ'  з/зx^μ (з/зx^λ) + (з^2x^α/зx^λ'зx^μ')(з/зx^α) 
							>Christoffel 符号 的假定 后推导 协变导数：想 将 上式右边第二项 分解为2项，然后一项移动到左边，一项还在右边 ，然后右边两项合并为一项，然后整体是一个张量变换式，从而得出两项之和是一个张量。很自然的，为了合并，则分解出的两项中的一项必然有 зx^μ/зx^μ' * зx^λ/зx^λ' 系数，假设 (з^2x^α/зx^λ'зx^μ')(з/зx^α)  = K(α')(μ',λ') + зx^μ/зx^μ' * зx^λ/зx^λ' * K(α)(μ,λ)  则带入上式后 发现出现了张量变换式，且 з/зx^μ (з/зx^λ) - K(α)(μ,λ)  就是一个二阶协变的张量，称为 协变导数。略微变换后发现，K(α)(μ,λ) = Γ(α)(μ,λ)*з/зx^α   所以 协变导数 = з/зx^μ (з/зx^λ) - Γ(α)(μ,λ)*з/зx^α  
							>协变导数对速度： ( з/зx^μ (з/зx^λ) - Γ(α)(μ,λ)*з/зx^α  ) V =  з^2V/(зx^μ*з/зx^λ) - Γ(α)(μ,λ)*зV/зx^α  令 = 0  з^2V/(зx^μ*з/зx^λ) - Γ(α)(μ,λ)*зV/зx^α=0
							>协变导数对曲线：( з/зx^μ (з/зx^λ) - Γ(α)(μ,λ)*з/зx^α  ) s^λ = з/зx^μ (v^λ) - Γ(α)(μ,λ)*зs^λ/зx^α = dv^λ/ds * ds / зx^λ - Γ(α)(μ,λ)*зs^λ/зx^α  两边乘以зx^λ /ds后，  d^2x^λ/ds^2 - Γ(α)(μ,λ) dx^k/ds dx^λ /ds = 0 认为就是测地线方程。
						>空间的基底：每个空间都有一组基底。
						>f(x)和x(f)的等价性：因为 f和 x共同决定了一个结果：这个结果用 f(x) 来表达 还是用 x(f)来表达：结果都是确定的 且 一致的。x来自矢量空间，f来自函数空间。这两个元素 形成一个映射。矢量空间中的所有x 和 函数空间中的一个f 进行组合确定；或者函数空间中的所有f 和 矢量空间中的一个x 相组合 而确定形成 一个新的元素/新的空间中的一个元素；所有双向的组合映射，形成一个新的 矢量空间 乘以 线性空间 的 新空间。函数空间为V*,矢量空间为V,称V*是 V的对偶空间。
							>有限维矢量空间V是有基底的：标准基底 = e1,e2,e3,...;  函数空间V*也有基底：e^1,e^2,e^3...
							>利用函数空间和矢量空间的组合 来定义张量： L: V^* V^* V^* V^* ... V^* V^* V V V V ...V --> R   用计算机程序 函数定义 来表述 就是： 一个函数，入参有 r 个 参数 是 函数类型，s个参数 是 向量类型， 返回值 是 一个整数。整个函数 被称为 张量。(r,s)型张量。 其空间记作 T(r)(s)(V)
								>上式读作：L 将 V^* ...V 空间中的元素 映射 到  实数空间中的一个元素。
									>因此： x(f) 读作 ： x 将 函数空间中的f  映射 到 实数空间中的一个元素。 显然，当x是矢量空间中的一点，是满足要求的---有这种映射能力的，所以x是张量，因为 是将函数空间中的f 进行映射到R,所以 x是(1,0)张量；或者说 矢量空间 是 (1,0)张量空间T(1)(0)(V)。
										>多个函数空间：每个函数空间中的元素可以是不一样的--如函数空间中的某些元素甚至不能作用在向量上--所以这个函数空间一定是能作用在该量上的所有函数构成的空间。
									>因此： f(x) 读作： f 将 矢量空间中的x 映射 到 实数空间中的一个元素。 因为 是对 是对矢量空间进行映射，所以f 是 (0,1)张量。 f是函数空间中的一点，所以 函数空间 是 (0,1)型张量空间。即：L:V->R
										>注意：f是线性映射，不是任意函数。或者叫 任意某个 线性函数/线性泛函。
									>因此：对一个矩阵A, (采用分量分析的方式), 它是m*n阶的，对于任意一个m阶的向量v, 则 v *A = u, u是n阶的。所以一个矩阵可以实现全部m阶向量 到 n阶向量的映射： A: V->V 。。另外，对于任意一个m阶向量v和任意n阶向量u,矩阵可以实现它们到 实数R的映射：(Av)^T*u = 实数，把元素(Av)^T 当作 一个整体，它只能被认为是一个线性变换/函数(只有原始量才能被认为是矢量)，u是一个矢量，中间是点积运算，所以实现了 V^* V ->R 映射。
										>一般的：对于一个量K，都要 和 函数空间/矢量空间中的任意元素运算 一下，来看 它实现了 怎样的映射：(还是觉得矩阵实现了V V ->R的映射：A(u,v)=实数；A:V V->R；；；但是，如果认为 u是一个线性映射，而v是一个向量 则也是合理的， Au 则是将一个线性线性映射转换为另一个线性映射：V^*->V^*;所以 Au *矢量v = 实数，是 V^* v ->R的映射)
								>(r,s)张量的基底元素的个数：如果是V是n维的，则一个V空间有n个基底元素，因此(r,s)张量有 n^(r+s)个基底元素。即每个空间中的基底和其他空间中的基底构成新的基底是组合的关系。
									>张量可看作：多线性映射。即V^* V^* V^* V^*... ->V V V V...
			>转置：用矩阵的元素来描述：(Aij)^T = Aji   用张量分析的标记来做 (AB)^T = (AikBkj)^T = (Aik)^T*B(kj)^T = Bjk*Akj =B^T*A^T
			>Hadamard乘积：A ⊙ B  计算过程 为 元素 对应乘积。
			>Ax分解等值为多个向量的和/线性组合的形式：Ax = Σxj*A;j 这样， 这明显是一个线性组合的形式， 把{A;j}看作是一个基底集合，则xj就是对每个基底的放缩比例，而由放缩后的向量进行组合相加后 如果得到b,则 这个放缩就是解。一个矩阵的列向量 确定一组基底，确定 了一个 生成子空间。所以 确定Ax=b是否有解 就是 确定 b是否在 A的生成子空间中。A的生成子空间 也叫 A的 列空间 或者 A的值域。如果对任意的b都有解，则A的列空间必须可以构成整个R^m， 则A的列必须有至少m个线性无关的列。			
				>列向量 线性相关的矩阵：奇异的。				
			>范数：衡量一个向量的大小的 函数 。 L^P范数 即 向量 元素的p次方的和的开p次方。根本定义，范数 是满足这样条件的这样的函数：1.函数值=0，则向量x=0; 2.满足三角不等式：f(x+y)<=f(x) + f(y); 3.对任意α, f(αx) = α*f(x) . 
				>L^2范数的等价形式：x^T*x  也是  x的模长。
				>L^∞：最大范数。 ||x||∞ = max|xi| 
				>L^1: ||x||1 = Σ|xi| 因为 当无穷次方时，其中最大的元素 对应的值已经远远超过了其他值，其他值就可以忽略了，再开方 就是 这个最大元素；如果有多个最大元素比如k个，那么开无穷次方是 就会无限接近于1.
				>Frobenius范数：||A||F = sqrt(ΣAij^2)	
			>对角矩阵：仅主对角线上有非0元素的 矩阵。i!=j,Dij=0  。。如果对角元素构成一个向量v, 则这个对角矩阵 表示为：diag(v) 优点：逆矩阵方便计算。存在逆矩阵：对角线没有0元素，逆矩阵就是 对角线元素的导数。	
			>对称矩阵：参数顺序无关的双参数函数生成元素时，元素构成对称矩阵。
				>单位向量：模长=1； 
				>两个向量正交：x^T*y = 0 
					>标准正交：R^n中，n个向量互相正交，且范数是1。
						>正交矩阵：行向量和列向量 都 标准正交。Ai;*Aj; = δij = A*A^T = I , A;i*A;j = δij = A^T*A = I ， 所以 A^T = A^(-1)   优点：逆矩阵方便计算。 
			>特征分解：矩阵分解方法。用来发现 不明显的函数性质。矩阵分解为 一组特征向量和 特征值。			
				>如果v满足：Av = λv	， 则v称为矩阵A的特征向量，λ称为 v的特征值，因为sv对应的特征值也是λ，但是v和sv线性相关，所以 只考虑 单位特征向量。
					>假设A有n个线性无关的特征向量：{v1,v2...}, 对应的特征值{λ1, λ2...} 利用特征向量的基本性质：则 AV=A*[v1,v2...] = [Av1,Av2...]=[λ1v1, λ2v2...]= V diag(λ) 其中λ是n个特征值依次构成的向量。所以A=V diag(λ) * V^(-1) 从而 这就是 矩阵A的特征分解。此时V一定三角阵 甚至对角阵。则|V|=1
						>如果A是 实对称矩阵：则 A = QΛQ^T 其中 Q 是特征向量构成的矩阵，且 是 正交矩阵。和上式融洽的。
							>用于优化二次方程：f(x) = x^TAx  其中限制x的模长=1， 将 实对称矩阵的分解带入，则 当x 是A的特征向量时，x^TAx  = λx^Tx = λ 函数f的最大值：最大的特征值；函数f的最小值：最小的特征值。证明方法：将A分解为 QΛQ^T ,那么  x^T*QΛQ^T*x = Σλi*xi^2 ， 其中满足约束 Σxi^2=1 构造拉格朗日函数 则可以 得出 L=λi, 然后argmax L = max{λi} 得出最大值，同理得出最小值min{λi}。。。第二种证明方法：即函数的某条等值线 是一个超椭圆，而约束关系是一个超求面，容易得出极值在轴上，轴上的值就是xi=1,而其他xj=0,所以得出极值=λi 中的max/min
					>矩阵A是奇异的：则A有0特征值。	
					>矩阵A的特征值都是正数：正定的。 优点：x^T*A*x >=0 恒成立，且当且仅当x=0时 =0.
					>A的特征值非负：半正定的。
			>奇异值分解：SVD。将矩阵分解为 奇异向量和奇异值。应用更广泛。
				>实数矩阵：都有一个奇异值分解，但不一定有特征值分解--例如非方阵矩阵就没有。
				>为何可以进行奇异值分解：因为发现假定 矩阵A可以分解为 UDV^T, 其中 U是m*m正交方阵，V是n*n正交方阵，D是m*n对角阵，发现可以反过来得出矩阵A。		
					>奇异值：对角阵D对角线上的元素。
					>左奇异向量：U的列向量。是AA^T的特征向量。显然AA^T是对称矩阵。
					>右奇异向量：V的列向量。是A^T*A的特征向量。显然A^T*A是对称矩阵。
						>新问题：A^T*A的特征值和 AA^T的特征值的关系。奇异值和 这两个特征值的关系。
			>Moore-Penrose 伪逆：非方阵A: 有  Ax=y，求解x。			
				>对角矩阵的伪逆：为对 该对角矩阵 进行 对角元素 取 倒数 后 再 转置 得到的。		
				>A的伪逆：A^+ = VD^(+)U^T 		
					>用途：A的列数多于行数时，A^(+)y 是 Ax=y的所有解中欧几里得范数||x||2最小的一个。当A的行数多于列数时，可能无解。此时 A^(+)y 是 使得 Ax和y的欧几里得距离最小的x: ||Ax-y||2
			>迹运算：Tr(A)=ΣAi,i
				>用途1：矩阵的Frobenius范数(每个元素的平方的和)的表示：||A||F = sqrt(Tr(AA^T)) 
				>用途2：Tr(AB) 其中，A是m*n的，B是n*m的，则 Tr(AB) = ΣΣ Aij*Bji 先对j再对i, 而很明显这个满足 交换规律：ΣΣ Aij*Bji  = ΣΣ Bji*Aij 先对i再对j。而 ΣΣ Bji*Aij = Tr(BA) 所以： Tr(AB) = Tr(BA)
			>行列式：det(A)。det(A) = Πλi	
				>用途：行列式=0 则 有维度完全收缩了。行列式=1 则 转换保持空间体积不变。
			>主成分分析：将 R^n空间中的点x和 R^l空间中的点c进行互相映射。显然，定义D为n*l矩阵，则 Dc=x 其中 n>l, 则 D就是 解码矩阵。
				>限制条件和目标：条件：D的列向量都有单位范数；目标：计算编码器的最优编码---不是l最小可以达到多少,而是 原始向量x 和 重构向量g(c*) 的距离最小--欧几里得距离最小：即argmin ||x - g(c*)||2 其中 g(c)=Dc。显然 计算D的伪逆 而再计算 D^(+)x 是使得 Dc和x距离最小的c: 
					>增加约束条件：D中所有列向量都是正交的。则 D^T*D=I
					>对目标函数argmin ||x - g(c*)||2： 展开计算：则 argmin ||x - g(c*)||2 = argmin (x - Dc)^2 = argmin (x^Tx - 2x^T*Dc + c^TD^T*Dc) = argmin(-2x^T*Dc + c^T*c) 再使用向量微积分：
						>向量微积分：
						>用向量微积分计算上式：▽(c)(-2x^T*Dc + c^T*c) = 0  得出 -2D^Tx + 2c = 0 得出 c=D^T*x  这个 和 用 伪逆来计算稍微有差异。 从而得出了 f(c)=D^T*x, 而 g(c) = DD^T*x
					>挑选编码矩阵D: 这个矩阵应该使得 还原之后的误差最小：即 argmin sqrt(((x^i)j - (DD^T*x^i)j)^2) 待定参数为D。 即使得n个误差向量构成的误差矩阵 的Frobenius范数 最小时的D。D满足的约束就是DD^T=I
						>目标函数矩阵化： 则定义出X: xij构成的矩阵，X;i是一个样本点； 当D的维度是1时，可以 等价为：argmin ||X - Xdd^T||^2,F , subuject to d^T*d=1 ;进一步转换为迹运算，argmin Tr((X - Xdd^T)^T*(X - Xdd^T)) 展开并利用迹运算的矩阵顺序前移规律，可得 argmin -2Tr(X^T*Xdd^T) + Tr(dd^TX^TX*dd^T) 再利用 约束条件 得：argmin -Tr(X^T*Xdd^T) = argmax Tr(X^T*Xdd^T) = argmax(d^T*X^T*Xd) 这个形式就是 二次型的形式，所以 它的极大值就是 X^T*X矩阵的最大特征值，d取值就是最大特征值对应的特征向量。得到的d就是第一个主成分。其他的l-1个主成分 就是 特征值排序后更小的l-1个特征值对应的特征向量，这l个特征向量就构成了D。

		>概率和信息论：比例和组合 都是 最基础的概念。比例值 和 组合集。
			>概率论：提供量化不确定性的方法，提供导出新的不确定性声明的公理。提出不确定的声明 并在此情况下 进行推理：
			>信息论：量化 概率分布中的 不确定性 的 总量。
			>一种对 不确定性 进行 表示和推理的方法： 
				>概率：频率、信任度。频率派概率，后者涉及确定性水平--贝叶斯概率。
				>逻辑：逻辑是这样的一种思维活动： 给定若干个 真或者假 的 命题 下，使用一套形式化的规则 来 判断 另外 一些 命题是否 是真或者假。
				>概率论： 是这样的计算活动：给定若干个 命题的 似然后，使用 一套形式化的规则 来 计算 其他命题 为 真 的似然。
			>概率分布：天然的向量。
			>概率密度函数：不要求p(x)<=1 ，只要求定义域内的积分为1
			>条件概率：在A发生的条件下，B发生的概率p。即A发生的条件下，B不发生的概率为(1-p)	。
			>边缘概率：是在离散联合概率分布的场景下 计算 X=x 发生的概率。结果写在边缘上。	
			>概率的链式法则：用一个棵树 来表示 条件的发生先后关系 ：最先发生则为根节点，概率为P(A); 然后发生的B则是在根节点下延申出左节点，B在A发生的条件下发生的概率P(B|A)就写在左分支上,则A和B同时发生概率 就是先走A再走B分支 这件事 的概率=P(A,B)=P(A)*P(B|A)。同理可以从B后可以延展出更多的分支下去。这样：从A1走到最底层的An 这件事 发生的概率 = A1...An同时发生的概率 = P(A1,...An) = P(A1) * ΠP(Ai|A1,A2,...Ai-1) i 从2到n
			>独立性和条件独立性：x⊥y 表示x和y相互独立，x⊥y|z 表示x和y在给定z时条件独立。	
			>期望：函数关于某个分布的期望：Ex[f(x)] = Σf(x)P(x)		。E[.] 则是对所有随机变量的求平均。
			>方差：按照x的分布来对x采样时，取值的差异程度 有 多大的 度量。Var(f(x)) = E[(f(x) - Ex[f(x)])^2]  方差的平方根 称为 标准差。
			>协方差：两个变量 线性相关性 的 强度 。Cov(f(x),g(x)) = E[(f(x) - E[f(x)])(g(y) - E[g(y)])] 如果 f(x) 和g(x) 有线性关系，即 f(x) = ag(x) 则 带入可知 协方差 Cov(f(x), g(x)) = a*Var(g(x)) 显然方差一般不为0，所以协方差不为0；
				>函数当作向量：则x是索引，f(x) 是向量的这个索引维度的分量取值；函数向量化  则 利用向量的概念--线性相关/线性无关。
				>独立性：既无线性关系，也无非线性关系。 非线性相关：f(x) = l(g(x))
				>协方差矩阵：x ∈R^n 是随机向量，则 x的协方差矩阵：Cov(x)i,j = Cov(xi,xj)  则对角线 元 是 方差。
			>相关系数：
			>常用概率分布：
				>Bernoulli 分布：二值分布： P(x=1) = Φ,  P(x=0)=1-Φ,  P(X=x) = Φ^x(1-Φ)^(1-x) , 则 E(x) = Φ, Var(x) = Φ(1-Φ)
				>Multinouli 分布： 具有k种取值状态的变量的概率分布：
			>正态分布：N(x;μ,σ) = 1/sqrt(2πσ^2) exp(1/(2σ^2 * (x-μ)^2))  可以令 1/σ^2 = β 来 重新 表达这个式子。	
			>中心极限定理：很多 独立的 随机变量 的  和 的 分布 ：近似服从 正态分布。  
			>同一方差下 在实数上 不确定性最大的 分布： 是 正态分布。 函数的不确定性：信息熵 -P(x)*log(P(x)) 其中 Var(P(x))=σ^2， 则 argmax(-P(x)*log(P(x)))  对 P待定，即 P(X=x)=? 满足 ∫P(X=x)dx=1 且 P(X)>=0 
				>多维正态分布：其中x,μ都是向量，Σ是一个正定对称矩阵(x^T*Σ*x >=0 恒成立;对称意味着Σ^T=Σ^(-1))--也是分布的协方差矩阵:N(x;μ;Σ) = 1/sqrt((2π)^n*det(Σ)) * exp(-1/2*(x-μ)^T*Σ^(-1)*(x-μ))
					>精度矩阵： 
					>协方差矩阵固定为 对角阵： 
					>各向同性高斯分布：它的协方差矩阵 是 一个 标量 * 单位矩阵。
			>指数分布：
			>Laplace分布：1/2γ * exp(-|x-μ|/γ)
			>Dirac分布：概率集中在一点：p(x) = δ(x-μ)  是广义函数。
				>广义函数：根据 积分 性质 定义 的 数学对象。
				>经验分布：是 训练数据 的似然 最大的 概率密度函数。
			>混合模型：
				>高斯混合模型：任何平滑的概率密度函数 都可以 用 具有 足够多组件的高斯混合模型 以任意精度 来逼近。
				>先验概率：P(c=i) 观测到x之前 传递 给模型 关于 c 的信念。
				>后验概率：P(c|x) 观测到x之后 计算的。
			>有用的函数：
				>logistic sigmoid 函数： σ(x) = 1/(1 + exp(-x))  用来产生 Φ
				>softplus 函数： ζ(x) = log(1+exp(x)) 用来产生 正态分布的 β 和 σ参数。
				---二者联系：ζ(x) = ∫σ(y)dy 从 负无穷 到 x 进行积分。 所以 dζ(x)/dx = σ(x)   对数关系：log σ(x) = - ζ(-x) 
			>贝叶斯规则：需求：已知P(x|y), P(x) 时计算 P(y|x) 。 利用 联合概率关系 可得： P(x|y) = P(y|x)*P(x)/P(y) = P(y|x)*P(x)/ΣP(y|x)*P(x)
			>测度论：measure theory	
			>信息论：研究 信号 包含 多少 信息 的 度量。	对 消息 设计 最优编码 和 计算 消息 的 期望长度；
				>基本假定/认为：一个不太可能发生的事件 发生了 则 能 提供 的 信息 多于  一个 非常可能发生的 事件 发生了 而 提供的信息。
					>确保能发生的事件：则没有信息量。
					>较不可能发生的事件具有更高的信息量；
					>独立的事件有增量的信息：
				>事件的自信息：X=x 的自信息 = -logP(x)
				>香农熵：每一点处的自信息的和的平均值：H(x) = -E[logP(x)] = H[P]  熵大 则 按P生成的符号进行  编码 所需的 比特数 在平均意义上的下界。
				>KL散度：衡量X的两个分布的差异：分布之间的某种距离。
					>定义：概率密度 比值 的对数的 期望： Dkl(P||Q) = E[logP(x)/Q(x)]
				>交叉熵：H(P,Q) = H(P) + Dkl(P||Q)
		>数值计算：通过迭代过程 来 更新 解的 估计值 的 算法。 如 找 函数 的 极大值 ， 线性方程组的求解。
			>上溢和下溢：
			>softmax函数：指数值的比例：softmax(x) = exp(xi)/Σexp(xi)
			>病态条件：如 A^(-1)的  max |λi/λj| 如果很大，则 矩阵求逆很敏感。
			>基于梯度的优化方法：
				>目标函数：损失函数/误差函数。
					>最小化 目标函数： x* = argmin f(x)
				>梯度：▽x f(x) = [зf/зxi]
				>方向导数：u向量方向 的导数： зf(X+αU)/зα = зg(X1+αU1,X2+αU2,X3+αU3)/зα = Σзg/з(Xi+αUi)*Ui = 当α=0时 = Σзg/з(Xi)*Ui  = U^T * ▽x g() = U^T *  ▽x f() 其中 U^T * U = 1 和 ||▽x f()||2 = 1  所以  min U^T *  ▽x f() = min ||U^T||*||▽x f()||2*cosθ = min cosθ, 说明 当 梯度方向和 方向U相反时候 方向导数取得最小值，梯度方向和U相同的时 方向导数取得最大值；所以在X处取得一个函数值，则 在 X + (εU) = X - ε * (▽x f) 取得 临近最小的函数值， 其中 ε称为 学习率。这种在负梯度方向移动 来 寻找 函数的 极小值 的 方法 被称为 梯度下降法。
					>差积：a = (ai,aj,ak) = Σai*ei, b = (bi,bj,bk) = Σbi*ei, 则a x b = (Σai*ei)x(Σbi*ei) = aiei x bjej =ai*bj ei x ej = 因为 ei x ej = ek = - ej x ei, 且 ei x ei = 0, 所以ai*bj ei x ej = (aibj - ajbi)ek = |[i j k ;ai aj ak;bi bj bk;]| 即 是 这个三阶行列式。
					>混合积：[a,b,c] = (a x b)*c = (aibj - ajbi)ck 其中 i!=j!=k 。ijk指标取三组1,2,3;2,3,1;3,1,2; 对应的第二项2,1,3;3,2,1;1,3,2;很明显 aibjck - ajbick = ak(bicj - cjbi) = (b x c)*a = bk(aicj - cjai) = (cxa)*b  三组都是按照 1,2,3顺序轮换而来的。
					>矢量微积分：
						>定义：X(t) = (x(t), y(t), z(t)) , 则  X'(t) = (x'(t), y'(t), z'(t)) ; X(t)可以表示 空间里一点的运动轨迹，即任意时刻的位置是可以确定的，则轨迹的长度 L(t) = Σsqrt(Δx(t)^2 + Δy(t)^2 + Δz(t)^2) = Σsqrt(x'(t)^2*dt + y'(t)^2*dt + z'(t)^2*dt) = ∫sqrt(x'(t)^2 + y(t)^2 + z(t)^2)dt = ∫||X'(t)||dt
							>第一类曲线积分：已知曲线的每一点的空间位置X(t), 曲线每一点上的质量密度/线密度 f(X(t)), 则曲线的 总质量,曲线起点a,终点b，中间无数个时刻的点ti , 则 中间 t(i-1)到ti 对应的两个点X(ti-1),X(ti)之间的这段弧长dsi 用拉格朗日中值定理得：dsi = |t(ti-1) - ti|*||X'(t)|| = ||X'(t)||dt , 而这段弧长的平均线密度 = f(X(t*)) 即中间某个点的线密度，所以 这段弧长的总质量 =  ∫f(X(t))*||X'(t)||dt 
							>第二类曲线积分：已知质点的每一点的空间位置X(t), 质点在每一点上受到的力F(X(t)), 即 这是一个矢量，则 从起始点a到 终点b, F对质点做的总功W 为 计算目标。对中间 t(i-1)到ti 对应的两个点X(ti-1),X(ti)之间的这段弧向量dS = X(ti) - X(ti-1)=ΔX(t)/Δt*dt = X'(t)dt ,这段距离上的功 =F(X(t))*X'(t)dt   所以 总功 W = ∫F(X(t))*X'(t)dt   如果 t处的曲线切向量 T(t) = X'(t)/||X(t')|| 则 W = ∫F(X(t))*T(t)*||X'(t)||dt  和 上式比较，发现只有 f(X(t)) = F(X(t))*T(t) 的区别。
					>爬山算法：对应于离散空间的 梯度下降算法。
				>Jacobian 矩阵： 由 f: R^m ->R^n ， 则 f的 jacobian 矩阵 J ∈ R^(n*m)元素 Jij = зf(X)i/зxj 即 X ∈R^m, 即 输出向量的每个输出分量 对 每个输入分量的偏导数。 
					>二阶导数： 由 f: R^m ->R , 则 f的二阶导数 f'' = з/зxj * (зf/зxi) = з^2f/(зxi*зxj) 可以表明 梯度 下降 的 改善情况/力度。有这个元素 构成的矩阵 称为 Hessian 矩阵。显然是 对称矩阵。且 是 实对称矩阵，因此可以分解为 正交矩阵 作为 特征向量 和 特征值对角阵 的构成。 
						>方向导数的导数：即u向量方向的二阶导数：则 对 U^T * ▽x g() =Σзg/з(Xi)*Ui 的每一项 都需要继续 对 α求 偏导：所以 三项变为 9项， u方向的 二阶导数 = Ui * з^2g/(з(Xi)з(Xj)) * Uj = Ui * Hij * Uj = U^T * H * U  很明显 这个 是 二次型 的 表达式形式，当u是一个H的特征向量时，则 结果 就是 H的这个特征向量对应的 特征值。当U是 其他方向向量时，假设可以分解为 这些特征向量的 线性组合，实际也必然可以分解为，U = Σαi*vi ,其中 0<=αi<=1因此 这个方向上的二阶导数 就是  Σαi*λi  。显然 该方向与某个特征向量的夹角越大，则权重越大，很明显，当最大特征值的权重为1时，这个二阶导数取得最大值；而当最小特征值的权重为1时，这个二阶导数取得最小值；即 最大特征值对应的特征向量的方向 就是 二阶导数最大的方向，最小特征值对应的特征向量的方向就是二阶导数最小的方向。
					>二阶泰勒展开： f(X) ≈ f(X(0)) + (X - X(0))*^T* g  + 1/2*(X - X(0))*^T * H * (X - X(0))  
						>考虑采用新的点对函数值的影响：f(X(0)-εg) ≈ f(X(0)) - εg^T*g + 1/2*ε^2 * g^T*H*g  从而看出 增加的部分：斜率的补偿和曲率的补偿、改善、矫正。 改善的部分 的 最小值 argmin (- εg^T*g + 1/2*ε^2 * g^T*H*g) 对学习率 ε 待定，则 使得最小的ε* =  g^T*g / g^T*H*g  = 1 / g^T*H*g 这就是 最优 步长。当梯度g方向和H的最大特征值方向相同时：则此时最优步长 = 1/λmax
					>牛顿法： 基于 矢量函数 的 二阶泰勒展开， 得到 二阶最优化算法。
						>向量函数对向量求导数：	df(X)/dX 其实就是上面讨论过的。因为对于一个 由向量X决定的标量函数f(X)，它的极值 跟X有关，则X处的附近的向量为 X + αU, 因此 求附近向量的函数值 / 附近程度 就是 函数 关于这个向量的导数：df(X)/dX = зf(X+αU)/зα = U^T * ▽x f()
						>矢量函数的二阶泰勒展开 ：f(x) = f(x0) + (x - x0) * ▽x f(x0) - 1/2 * (x - x0)^T * H(f,x0) * (x - x0)  ， 所以 它的方向导数 df/dx  由上面的分析可知，为 U^T*▽x f(x)=0， 方向导数恒为0, 则函数在这里的梯度为0，即 ▽x f(x)=0， 即 每个分量=0= ▽x f(x)k = 0, 则对 式子 用 xk来表示，然后对xk求导，可得 df/dxk = ▽x f(x0)k + 1/2 * ( Σ2Hk*xj - Σ2Hki*x0i) = ▽x f(x0)k + Hk*(x - x0) = 0  整理 为 矩阵表达式形式则为：▽x f(x0) + H(f,x0)*(x - x0) = 0, 得出 x = x0 - H(f,x0)^(-1)*▽x f(x0) 得到 x就是 极大值 所在的点。
					>Lipschitz 连续： |f(x) - f(y)| < L * ||x - y||2		
						>凸优化：其分析思路对证明 深度学习的 收敛性 有用。
			>约束优化：不再简单的 在 x的完整值域 上 寻找极值，而是部分值域上寻找极值。	
				>可行点：部分值域上的点。		
				>KKT方法：
					>广义拉格朗日函数：S由m个等式约束 g^(i) = 0 和 n个不等式约束 h^(j) <= 0 来描述。即： L(x,λ,α) = f(x) + Σλi*g^(i)(x) + Σαj*h^(j)(x)  当 αj足够大的时候，即便αj很小的时候L的最小值 不在 h^(j)(x) <=0 的x范围内，则随着αj的增大到某个值以后，也会导致最小值移动到h^(j)(x) <=0 的x范围内。 所以 只需要 计算 argmin argmax argmax L(x, λ,α) 依次待定 x, λ,α 就可以 得出 符合约束条件 的 L的最小值。
						>另一种语义上看：当 x满足 约束条件：g^(i) = 0 时 则第二项=0，h^(j) <= 0 时 则 argmax Σαj*h^(j)(x) = 0 从而 min L = min f ；  当 x不满足 约束条件：则h^(j) > 0 那么 argmax Σαj*h^(j)(x) = ∞  所以  只要 argmin argmax argmax L 有解， 则就是 原函数f的符合约束条件下的最优解。
							>说明：就是 对x分区域(分h^(j) <= 0和h^(j) > 0两个区域) 先计算 待定αj,λi时的极值 然后再计算 待定x 时的极值；显然 在第二个区域计算待定下的极值 第一步就是∞则第二步计算最小自然也是无穷,而在 第一个区域计算 待定下的极值 第一步则是0 +f(x) 从而第二部计算最小 就是 min f , 综合两个区域，则在全部区域上的最小值 就是 min f 即第二个区域上 来 计算出的极大极小值。这就是 求L的极值的完整分析逻辑/分解逻辑(从某个量开始分析分解)。
								>进一步讨论说明：当 f(x)的极小值 在 h^(j) > 0 这一侧，那么 按照上述的讨论，则在这个区域上求L的极小值是∞,而在 h^(j) <= 0 侧区域则 先极大则α=0从而L的极大值在 f(x) 满足  h^(j) <= 0 时的取得，即一般实际就在 h^(j) = 0 上的某个点上取得；而当 f(x)的极小值 在 h^(j) <= 0 这一侧时，同理 在h^(j) > 0这个区域上L的极大极小是∞，在 h^(j) <= 0 上时的先极大 则α=0 再极小 则 就是 minf(x) 就是 f(x)极小值应该在的位置上。
									>补充说明：第一种情况下，h^(j) = 0 ，称 这个约束 是 活跃的。
							>用上述语义来 寻找f的最大值：则计算  argmax argmin argmin ( f(x) + Σλi*g^(i)(x) - Σαj*h^(j)(x))  依次待定 x,λi,αj即可。
				>KKT条件：是 确定一个点 是 最优点的 必要条件：
					>充分条件：充分 就是 充足/完全/全部 的意思。A1|A2|A3|A4... -> B  则 A1是B的充分条件。即 若(当且仅当) A1|A2|A3|A4 为真 则 B 为真。
					>必要条件：必要 就是 必需/需要 的 意思。 A1&A2&A3&A4... -> B 则 A1 是 B的 必要条件。即 若(当且仅当) A1&A2&A3&A4 为真 则 B 为真。
					>充要条件：充要 就是 充足 且 必需 的意思。 A1 -> B 。 即 若(当且仅当) A1 为真 则 B 为真。  用 计算机程序语言来 表达 则非常准确。
			>线性最小二乘： min f(x), f(x) = 1/2 * ||Ax-b||^(2)2  	计算 梯度 ：用偏导数的方式 分解计算 最后可知 ▽x f(x) = A^T * (Ax-b) 可见形式非常巧妙。和把A当作系数类似。
				>梯度下降算法： x -> x - ε▽x f(x)  来 迭代计算 新的位置。
				>牛顿法： 采用函数的二阶泰勒展开计算极值的一步的方式 计算出 最小点。x = x0 - H(f,x0)^(-1)*▽x f(x0)
				>广义拉格朗日函数方法：L(x,λ) = f(x) + λ(x^Tx - 1)  即 满足约束 x^Tx <= 1 ,λ>= 0然后 计算 argmin argmax L 依次待定 x和λ。。此时 对 L 求X的微分：则 ▽x f(x) + 2λx = A^T * (Ax-b) + 2λx  令 = 0 ，则 x = (A^TA + 2λ)^(-1)*A^Tb  带入 λ=0得到 L的值，这是第一类值，第二类则是 对于x = (A^TA + 2λ)^(-1)*A^Tb  进行迭代计算x: 从0开始增加λ的值 直到 整个函数的极小值 落到无限靠近 x^Tx-1=0这条线 可以停止增加λ(实际操作时可以继续增大,但无限接近的点不会再改变了)，而无限靠近的 线 上的点 自然满足 dL/dλ=x^Tx - 1 = 0 ，那么这个点就是得出的最小值。
					>第二种方式：用Moore-Penrose 伪逆 ：A^(+)b 是 Ax=b的解中||Ax-b||2最小的那个，自然也是 min f(x)的解。
		>机器学习基础：			
			>学习：通过经验E, 程序 能对 任务T 在性能度量P下 的度量取值 有 提升。		
			>分类、缺失输入分类和转录任务：准确率 来度量。
			>密度估计：测试集数据 来 度量 系统性能。
			>无监督学习算法：学习生成数据集的 整个概率分布。观测 随机向量x的若干个样本，然后 给出 p(x)
			>监督学习算法：数据集 中 每个 样本 都有一个标签。观测 随机向量x和对应的y, 然后给出 p(y|x)
			>数据集是样本的集合：样本是 特征的集合。		
				>均方误差来度量： MEStest = 1/m * ||Xw - y_||^2,2  最小化 它 来 计算 w, X的行为样本，则 计算它的 对w的导数=0，因为二范数的平方 就是可以写成 A^TA形式， 即 ▽w MEStest = ▽w(Xw - y_)^T*(Xw - y_) =  ▽w(w^TX^T - y_^T)(Xw-y_) =▽w( w^TX^T*Xw - w^TX^T*y_ - y_^T*Xw + y_^Ty) = 0  则 等价于 2X^TXw - X^T*y_ - (y_^T*X)^T = 0 即 X^TXw - 2X^Ty = 0 即 w = (X^TX)^(-1)X^T*y  这个方程被称为 正则方程/正规方程。是一个最基本的最简单的学习算法。
			>泛化误差：训练集中分出来的测试集样本上的性能。	
			>测试集和训练集数据的生成过程：通过 一个 数据生成过程 的 概率分布 生成。样本独立，而同分布。根据这个假设 来 研究  训练误差 和  测试误差 之间的关系。	
				>欠拟合：模型不能在训练集上 获得足够低的误差。
				>过拟合：训练误差和测试误差之间的差距太大；
				>模型的容量：指 模型 拟合 各种函数的能力。
					>选择假设空间：模型的函数空间/函数集扩展。如 线性回归模型 y = wx + b 修改为 y = b + w1x + w2x^2 增加输入的二次项，但输出对参数还是线性；所以依然可以用正则方程来求解。容量等于 任务的 复杂度 最佳。比如数据是二次复杂度的，则用二次模型，而用9次模型就过拟合了，而用1次模型则欠拟合。
						>上述说明：模型的选择 可以 改变 模型的容量。
					>提高机器学习模型的泛化的现代思想：奥卡姆剃刀 原则：同样能够解释已知观测现象的假设中，我们应该挑选 最简单的那一个。		
						>VC维：统计学习理论中最重要的结论阐述了 训练误差和 泛化误差 之间的差异的上界  随着 模型容量的增长而增长，随着训练样本的增多而下降。
					--一般：泛化误差随着模型容量的增加而先下降后上升。训练误差和泛化误差的间距 一直在增加。
					>修改线性回归的训练标准：均方误差 + 正则项。所以 J(w) = MSEtrain + λw^Tw  即 偏好 于 平方L2范数较小的权重。这里的正则化项 就是 λw^Tw
						>添加正则项的方法：称为 正则化。正则化 可以 降低 泛化 误差 而非 训练误差。
			>超参数和验证集：不适合在训练集上学习的模型的 参数，如控制模型容量的参数。	
				>验证集：用于估计训练后 的 泛化误差，更新超参数。
			>参数估计、偏差、方差： 
				>点估计：参数θ的点估计θ^。定义 独立同分布 的  数据点 m 个： {x1,x2,x3...xm};  点 估计 /统计量 是 这个m个点的任意函数：θ^.m = g(x1,...xm), 良好的估计量 的 输出 接近 生成 训练数据 的 真实参数θ。由于xi 是 随机变量 ，所以θ^.m也是随机变量。
					>偏差：bias(θ^.m) = E(θ^.m) - θ  如果  E(θ^.m) = θ 则 估计量θ^.m被称为 无偏估计。如果 limit m->+∞ (θ^.m) = θ，则 θ^.m 称为 是 渐进无偏估计。
						>实际计算：条件：1.样本独立同分布，服从的分布为P(xi|θ)已知。  目标：计算 最佳估计量/无偏估计量/极大似然估计量，再算估计量的偏差。
							>如果使用极大似然估计：如对于 伯努利分布，则 得出 θ^.m = 1/m * Σxi   ,   这个估计量的 偏差 = E(θ^.m) - θ = 1/m * ΣE(xi) - θ = 1/m *Σθ - θ = 0 所以 这里的θ^.m是无偏估计。
								>如对于 高斯分布，则 得出 μ^.m = 1/m*Σxi, 同理得出也是无偏估计； σ^2.m = 1/m*(xi - μ^.m)^2  计算 E(σ^2.m) - σ^2 = (m-1)/m * σ^2 - σ^2 即 是 有偏估计；显然 当 	1/(m-1)*(xi - μ^.m)^2 则是 无偏估计。
									>尽管有偏估计：但是可能其他性质更重要更需要。
					>方差和标准差：估计量的方差 Var(θ^), 方差的平方根 为 标准差：SE(θ^)
					>均值的标准差：SE(μ^m) = sqrt(Var[1/m*Σxi]) = σ/sqrt(m) 	σ^2 是 样本xi的真实方差；。根据中心极限定理，均值会接近服从 高斯分布，从而可以用标准差 计算出 真实期望落在 选定区间的概率。
					>样本：本质。概率的本质 是 比例，该类情况的占比，分布 的本质 就是 各种情况的占比，各类情况所属的情况数对总情况的占比。
						>随机变量X取某个值有一个概率：
						>一个可重复实验的随机变量X每次取某个值都有一个概率：而且都是相同的概率。可重复实验，就是可重复取值，且每次取值都服从相同的分布，可以把每次取值当作一个新的随机变量，这就是样本，或者样本变量。这是样本的真正定义：可重复取值的某随机变量的某次取值(实验/)。
							>样本是变量：而且是随机变量。变量 则 是 指代的意思，假设的意思，假设其值为的意思。随机变量 则 是 给 这个变量 加了一个约束关系 一样，即约束了取值范围 和取 每个值的概率，即 取值 服从一个分布。是个样本，就可以指代取值/假设取值为 10个 变量/随机变量。
								>样本变量的具体取值的确定：不像普通变量一样 是 用 变量关系 ，从 某个/某些相关的/满足运算关系的变量的取值 已知 时 通过这个运算关系 直接 运算 确定 而得到这个变量的值；
									>对随机性和同等性的进一步认识：输入都一样，而输出却 不是固定的，而是   多个 可能的结果中的某个，每个可能的结果 都有一定的出现的概率---这个概率是统计的/计算的比例，那么就认为 这种输出的结果 是 随机的。
									>对确定性和可能性的进一步认识：做一个实验/一个简单明确的模型，所有的影响因素/输入因素都知道取值 且知道 它们如何决定输出结果的关系，则 输出结果 就是确定的。比如勾股定理，知道2直角边 就可以确定斜边长度。但做一个充满各种微扰型因素的实验/尚未知道各个因素具体影响过程的模型，每个微扰因素的值几乎无法精准测量，且对输出结果的影响过程也几乎无法得知，而且某个微扰因素的即便 发生了 在肉眼甚至机器的观测下几乎一样几乎没变或者无法测量到的极其微小的变化--如单位是米而发生了小数点后20位上减1的变化，而输出结果居然就改变了/跳变了，那么至少在肉眼范围和当今科技可观测的能力范围内 某个实验下的输出结果到底会是什么，只能说值域范围内都有可能，而且按照 对 扰动/微小变化 不改变输出结果的因素/且易测量的因素 作为 区分输出结果 的 观测维度， 则 按该维度的不同取值而统计输出结果的比例，就是 在该条件/维度下的分布概率,即条件概率。如果认为输入条件没有一个是可作为区分输出结果的维度的，即认为输入条件都是 微扰因素，比如掷骰子实验的输入因素(手力/高度/桌面摩擦力/骰子的重量-表面光滑度...)，那么就认为所有可能的结果都是同等的，同等的可能性出现的。概率就是对可能性的度量。则每种可能结果的出现概率就是: 1/可能结果数。
										>反过来：所有的不能确定性的推断结果 而 只知道 结果出现的值域 和 统计 每个结果出现的频率 的  实验/现象， 就一定存在 微扰因素。导致某结果值的出现就是随机的/伪随机的。
										>把这种输出结果当作变量：取值不能由 可以确定值的 因素/变量 确定，只能做到 /认知到  取值 在一个值域上， 值域上每个值都有出现的可能，每个可能性可以量化为概率。即便增加了信息而调整认知改进了认知，也仅仅是 让 值域上的各个值的取值概率 向某些值更集中，整体上这个分布的熵就更小了/均方差更大。当完全不知道任何关于该实验的输入信息时，只知道值域，那么此时的认知就是 值域上的某个值出现的可能性都是相等的，就是等概率的，是值域上的均匀分布。
											>实验容易做-实验结果容易产生和测量：但是实验不容易度量：输入输出关系不容易度量/确定。
										>我们想做的事情-达到的目标-认识的目标-计算的目标：变量的值不能确定，但是它的分布要能确定；从而它衍生的分布也要因为 确定的/固定的容易知道的测量的衍生关系 而 能确定。
											>目标演进：认识路线：最基本的要知道 变量的值域 ---> 值域上每个值的取值概率 --> 变量在每个值上的取值概率的统一的计算表达式：即表达式的入参是变量的值，输出是概率，即值-概率的映射关系式。--->如果 值-概率映射关系上有待定参数，则需要计算知道这些待定参数。
												>例子：0-1分布对应的变量：值域：0和1；--> 每个值的取值概率：1取值概率θ, 0取值概率1-θ --> 取值-取值概率 关系式：P(x) = θ^x * (1-θ)^(1-x)
											>实现目标的方法列举/集合：(可以收集到的数据)	做实验 、做n次实验记录每次的结果、统计结果出现频率、计算均值和方差、实验结果带入更复杂的理论模型。。可在理论上做的事情：设第k次实验的结果是变量xk 来尝试用n个变量来建立一个估计/计算 随机变量的每个取值的概率的模型/计算模型/理论模型--或者建立一个 估计/计算 “取值-取值概率”关系式中某些参数的具体值的理论模型/计算模型。	
												>值域的确定方法：
												>某个值的取值概率的确定方法：如果只能用实验结果来估计，或者不妨考虑 用 实验结果 来估计/计算/确定。
													>用 极大似然法 得出 用n次实验的结果 来 计算/导出 取值概率的计算表达式/估计量表达式：假设 做n次实验 得到的 服从0-1分布/伯努利分布的 n个结果为x1,x2...xn，那么通过极大似然法得出的用x1...xn来对θ进行估计的 估计关系式 就是：θ = 1/m * Σxi . 具体得出过程中关键的就是认为 概率越大的事情越可能出现，而反过来 实际出现的情况 就是 所有可能出现的情况中概率最大的，而 所有可能出现的情况的任意一种的出现概率的计算表达式 是统一的--仅仅输入变量的值不同，且这个概率表达式容易表达出来，从而求它的极值所在的参数，由参数而表示出取值概率。
														>由于估计量是用n个实验结果即n个随机变量来表示的：所以 估计量 也是变量/随机变量。
													>评价这个估计量表达式的估计准确度：(估计量的期望)无偏估计 还是 渐进无偏估计。
														>无偏估计： E(θ^.m) - θ = 0 对 0-1分布，则 偏差为0.
															>例子：
																>高斯分布的μ的估计：使用 均值表达式 来估计：μ^.m = μ^ = 1/m * Σxi 。则 E(μ^.m) - μ = 1/m*ΣE(xi) - μ = μ - μ = 0 这里 μ就是 xi服从的高斯分布函数中的μ参数。
																>高斯分布的σ的估计：
																	>使用 样本方差：σ^2.m = 1/m * Σ(xi - μ^.m)^2 。 可见表达式中的所有变量也都是实验结果确定的。则 E(σ^2.m) - σ^2 = 1/m*E[Σ(xi - μ^.m)^2] - σ^2 = 1/m *( m(σ^2 + μ^2) - 2m*(σ^2/m + μ^2) + m*((σ^2 + mμ^2)/m) ) - σ^2 = (m-1)/m*σ^2 - σ^2 = -1/m*σ^2   这里利用了 方差 var(x) = E(x - μ)^2 = E(x^2-2μx+μ^2) = σ^2 从而得出E(x^2)=σ^2+μ^2 第二，是利用E(xi*xj) 其中i!=j此时因为xi和xj独立，所以E(xixj)=E(xi)E(xj) 。。。。总之，这是一个有偏估计，但是当m->∞时是无偏的，所以渐进无偏。
																	>使用 无偏样本方差：意识上上述公式如果稍作处理，则结果中就可以消除多余项-σ^2/m, 显然，只是分母引起的，所以 只需要 σ^2.m = 1/(m-1)* Σ(xi - μ^.m)^2   那么 E(σ^2.m)=σ^2 就是无偏估计了。
														>有偏估计： 
															>渐进无偏估计：
													>评价这个估计量表达式的估计准确度的变化程度/集中程度/聚集程度：估计量的方差是多少：Var(θ^),  以及 方差的平方根--标准差 是多少：SE(θ^) 注意此时估计量θ^是一个随机变量。所以以计算随机变量方差的方式计算 估计量的方差：Var(θ^) = E((θ^-E(θ^))^2)
														>均值的标准差：SE(μ^m) = sqrt(Var[1/m*Σxi]) = aqrt((E(x^2) - E^2(x))/m)=sqrt(Var(x)/m)=σ/sqrt(m)  这里利用了 下述的 两个 方差 关系。
															>均值的分布：当m足够大后 根据中心极限定理 ，均值会接近服从 正态分布(均值的期望/均值 和方差作为参数)。使用这个正态分布，可以计算95%置信区间的上下界：即(μ^m - 1.96SE(μ^m, μ^m + 1.96SE(μ^m))
														>标准差：方差的平方根。各种方差都有平方根(样本方差和真实方差)，所以各种方差都有标准差。方差 可以判断 有偏无偏性，标准差也可以判断有偏无偏性。
														>例子： 
															>伯努利分布的θ的估计量的方差：Var(θ^) = E(θ^^2) - 2θ^2 + θ^2 = 1/m^2 * (mθ + m(m-1)θ^2) - θ^2 = θ(1-θ)/m = Var(θ)/m 这里可以利用Var(x) = E(x^2) - E^2(x) 以及 Var(θ^) = E(θ^^2) - E^2(θ^)
															>高斯分布的μ的估计量的方差： 
													>多个估计量表达式选择哪个：方差大的还是偏差大的
														>交叉验证：
														>比较这些估计量的均方误差：即需要将偏差和均方误差都融合进来；MSE(θ^) = E((θ^ - θ)^2) = E(θ^2) - 2E(θ)θ + θ^2 = Var(θ^) + E^2(θ^)- 2E(θ)θ + θ^2 = Var(θ^) + Bias^2(θ) 即 偏差的平方+方差。 
															>用MSE度量 泛化误差：增加机器学习容量 则 增加方差 而 降低 偏差。偏差 随容量的增加 而减小 ， 方差 随 容量的增大而增大。得到泛化误差的U型曲线(看作是反差和偏差的和类似)，可以找到泛化误差最小的容量--最佳容量，当实际容量小于这个容量 则 系统欠拟合，大于这个容量则 过拟合。
													>评价估计量的一致性：即数据量增多后估计量的效果。 目标/希望是： plimit m->∞ θ^.m = θ  即  当m->∞时， 对任意c>0, 都有 P(|θ^-θ|>c) → 0 , 这就是 一致性条件。
														>渐进无偏 不意味着 一致性：
													>寻找估计量-好的估计量：极大似然估计。
														>预备：值-概率关系：Pdata(x)
														>极大似然估计：定义为 θml = argmax ΠPmodel(xi;θ) 对数 化 后  不影响结果：argmax ΠPmodel(xi;θ) = argmax Σlog(Pmodel(xi;θ)), 除以m也不影响：argmax Σlog(Pmodel(xi;θ)) = argmax 1/m* Σlog(Pmodel(xi;θ)) 如果不是除以m而是乘以Pdata(xi) 其实也是可以的，这样可以更加向经验分布靠拢:(但这需要已知经验分布) = argmax Ex~pdata[log(Pmodel(x;θ)] 此时 
															>最大似然估计的第一种解释：最小化 训练集上的 经验分布 Pdata 和  模型分布 Pmodel 之间的差异： 通过KL散度度量 这种 差异：Dkl(Pdata||Pmodel) = Ex~pdata [logPdata(x) - logPmodel(x;θ)]   即 需要 argmin Dkl(Pdata||Pmodel)  = argmin  -Ex~pdata[logPmodel(x;θ)] = argmax Ex~pdata[logPmodel(x;θ)] 从形式上看 和 上述是一样的。
																>负对数似然组成的损失  是  定义 在 训练集 上的 经验分布 和 定义在 模型上的 概率分布 之间的 交叉熵：均方误差 是 经验分布 和 高斯模型  之间的  交叉熵。
																>等价说法：最大化似然 / 最小化 KL散度 和 最小化交叉熵。
															>最大似然估计可以看作 使模型分布尽可能和 经验分布pdata 相匹配的尝试：
															>估计条件概率P(y|x;θ)：X是输入，Y是观测到的目标，条件最大似然估计：θml = argmax P(Y|X;θ) 即 输入x的条件下 输出y的条件概率 , 当 样本独立时，θml = argmax Σlog P(yi|xi;θ)
																>例子： 
																	>线性回归作为最大似然：先前： 最小化 均方误差 得到 x到 y^的 映射。
																		>目标：得到条件概率P(y|x) 相当于  x有n个取值，则应该有n个分布；所以样本个数应该>>n, 如 mn。。
																		>已知：样本(x1,y1)...  
																		>思路：假设 这n个分布 都是正态分布，而且方差都一样，只有均值和x有关：即 p(y|x) = N(y;y^(x;w),σ^2) 其中y^(x;w)即预测高斯的均值。y就是真实输出：y^是模型均值。运用 最大似然估计：即 argmax Σlog p(yi|xi;θ) = - argmax (-log2π - log(σ) - Σ||yi^ - yi||^2/2σ^2  形式和 MSEtrain  = 1/m * Σ||yi^ - yi||^2 比较，则就是 最大化似然估计的参数值就是 最小化 MSE均方误差的参数值。
															>性质：m->∞ 时 是 最好的渐进估计；且估计值会收敛到参数的真实值。
																>条件是：真实分布pdata 必须 在 模型族 Pmodel(.,θ)中。且pdata刚好对应一个θ。
																>价值：机器学习中的首选估计。当样本数太小 而 发生过拟合 则 增加 正则化策略 如 权重衰减。
												>取值-取值概率的关系式的确定方法：构造法。构造一个满足输入输出特征的函数/表达式。
												>假设的随机变量的实际取值的确定方法：只有实验。理论上则可以计算它的概率分布和期望和方差等。
									>强大的数学家：就是 把 普通人 只能得出的 可能性的结果 而 补充条件和引入完备的隐变量到计算关系当中 而从 只要知道 这个计算关系要求的 大量的所有的信息条件  就 可以 得出 确定性的 结果；例如精确计算抛掷一骰子最终哪面朝上。另一种能力：就是简便方法。
										>可能几十个输入变量，而只有一个输出变量：且输出变量只有几个取值。且结果对 几十个 输入变量 扰动 很敏感，稍微其中某个变量取值变了微微一点点，则输出结果都不一样。而输入变量的取值很难确定。这导致 在 不知这些微小变化/不同就能改变/不同结果的 变量的作用下，看每次的输入几乎都一样-相差都极小，但看输出结果，则各种结果都有可能，完全不能事先确定，导致对于结果---只能说都有可能。这就是可能性产生的原因。
											>在无数组 几乎相同的输入下 看 输出 的各种可能的结果 分别 占 总结果的比例：如果认为输入就是一样，则认为输出结果就是随机的。而统计输出的各种可能的结果分别出现的次数 占 总结果总次数的比例，就是各种可能的结果出现的概率。
							>用样本随机变量来估计随机变量取值概率的参数：		
			>贝叶斯统计：真实值θ也是随机变量。θ的分布 称为 先验概率分布：p(θ) 一般高熵，表示 在 观测到任何数据前 对参数θ 的 高度不确定性。比如最大的不确定----均匀分布(一无所知:只能同等对待)。
				>不像 极大似然法 得出的θ的点估计，这里得出的是θ的全分布：
					>条件：一组数据样本 {x1,x2...} , 先验概率分布p(θ), 数据似然p(x1,x2...xn|θ) 
					>目标：计算得出对θ新的认知/改进的认识/更精准的认识/更偏向的断定/信念/低熵认识/有偏向的认识：p(θ|x1,...xn)
						>进一步目标：计算下一个样本x(m+1)的分布概率：p(x(m+1)) = ∫p(x(m+1)|θ)*P(θ|x1...xm)dθ  每个条件概率都是一个 分布 ，积分项 第二项 是 得出的一个分布，第一项是 在第二项的θ的各个值上的条件概率分布。
					>思路： 贝叶斯公式：p(x|y) = p(x)*p(y|x)/p(y) , 所以 p(θ|x1,...xn) = p(θ)*p(x1...xn|θ)/p(x1,...xn)	
						>对p(θ)的假设：一般为 均匀分布或者 高熵的 高斯分布。
						>例子： 
							>贝叶斯线性回归：
								>输入输出模型 y = w^t*x , 当给定m个样本 构成 (X,Y), 则 模型表示为： Y = Xw 。
								>目标： p(w|Y,X)
								>思路：假设 y满足高斯分布：p(y|X,w) = N(y;Xw;I) ∝ exp(-1/2 * (y -Xw)^T * (y - Xw)) 这里假设了 y上的 高斯方差为1
									>先验分布的选择：p(w) = N(w;μ0;Λ0) ∝ exp(-1/2 * (w - μ0)^T*Λ0^(-1)*(w - μ0) 其中μ0就是先验分布的均值向量，Λ0就是先验分布的协方差矩阵。
									>将后验改写为高斯分布：p(w|Y,X) = p(Y,X|w)*p(w)/P(Y,X) = p(Y|X,w)*P(X)*p(w)/P(Y,X)  去除w无关部分则： ∝ p(Y|X,w)*P(w) 正好是 可以利用 上述的两个概率：exp(-1/2 * (y -Xw)^T * (y - Xw)) * exp(-1/2 * (w - μ0)^T*Λ0^(-1)*(w - μ0)) ∝ exp(-1/2*((y -Xw)^T * (y - Xw) + )+(w - μ0)^T*Λ0^(-1)*(w - μ0)) =  exp(-1/2*(-2y^TXw + w^TX^TXw + w^TΛ0^(-1)w - 2μ0^TΛ0^(-1)w)) 然后对 其中进行合并：中间2项合并：w^T(X^TX + Λ0^(-1))w,  两边两项：-2(y^TX + μ0^TΛ0^(-1))w  令 Λm = (X^TX + Λ0^(-1))^(-1) ,  Λm^(-1)μm = (y^TX + μ0^TΛ0^(-1))^T  则 原式 = exp(-1/2*(w^T*Λm^(-1)w -2 μm^TΛm^(-1)w))	  Λm和 μm都和w没有关系。如果再补充一项和w无关的：μm^TΛm^(-1)μm 那么 里面 可以合并为：exp(-1/2((w - μm)^TΛm^(-1)*(w - μm)))  这就是一个高斯分布，这就找到了w取值-取值概率的映射关系，目标达成。
						>最大后验估计：上述已经计算出 P(θ|x) 则 认为 θ map = argmax P(θ|x) 对θ待定  ，就是 MAP估计选择的θ的取值点。 argmax P(θ|x)  = argmax logP(x|θ) + logP(θ) 可见右边第一项 就是 对数似然项，第二项为先验分布。如果先验服从N(w;0;1/λI^2)则 logp(θ) 正比于 λw^Tw ,对应着权重衰减。先验也可以设计为服从 混合高斯分布。
							>减小了点估计的方差而增加了偏差：
					>价值：训练数据有限时 泛化的好，但是量大时 计算代价偏高。
			>监督学习算法：给定x,y,寻找它们之间的关联。		
				>分布族：p(y|x;θ) = N(y;θ^Tx,I)	
				>支持向量机：目标：计算w^Tx + b 模型中的w 
					>核技巧：认为 w可以由样本进行线性组合出来：即 w = Σαi*xi , 从而  w^T*x + b = Σαix^T*xi  再定义 核函数 k(x1,x2) = φ(x1) . φ(x2) 从而 w^T*x + b = b + Σαi*k(x,xi)
						>高斯核：k(u,v) = N(u-v, 0, σ^2I) 也被称为 径向基函数。即 当v为0向量时，则 u向量-v = u，带入N函数 得出的值 就是 u的模长 作为长度/x的值而在 高斯分布N(x,0,σ^2)上取的值； 或者 说把v点作为原点，而u-v这个向量作为x轴 所建立的 高斯分布N(0,x,σ^2) 取值是一样的。由于向量模长 就是这个向量和自己的点积，所以认为 高斯核对应的是无限维空间上的点积 (作为模长 作为x轴上的值而映射到的高斯分布概率值)
							>空间高斯分布：就是从原点出发的每条射线都当作x轴 而在x轴上建立高斯分布 得出的 点对应的概率值。  且满足 1= ∫N(x;0;σ^2I)dxdydz
							>分类函数 用高斯核函数来解释：已有固定的：样本点xi在空间分布，新的待分类的点x 在空间某个位置，然后以x为原点而建立N(x;0;σ^2I)的空间高斯分布，则此分布在各个样本点上的取值就确定了，乘以各个样本点的权重αi, 再相加的得到的值就是 分类函数的值。显然，样本离得越近则值的绝对值越大。第二，每个样本点 都有一个训练标签y, 如果新的点x和某个样本点很近，则 这时可以把高斯核的结果exp()这个在(0,]的值当作一个权重，而把αi当作i类型标记值, 如此表示离A样本越近则作为i类型的可能性就越大--即i类型标记值的权重就越大。总的分类类型值 就是 这些 类型标记值的权重处理后的值的综合---相当于平均来看 是 个 什么类型 。相当于类型标签的加权组合。即 高斯核 在执行一种 模板匹配/类别就是模板。
								>通过学习后 α主要包含0：即元素主体基本都是0，不是0的分量 对应的样本 被称为 支持向量。
									>α主要包含0的目的：因为每个分量样本都有αiK()的贡献，所以样本多了 计算成本越来越大。因此让大部分为0就减少这个负担。
				>输入空间分成不同的区域：
					>K临近方法：
					>决策树及其变种：每个叶节点 对应一个划分的 不重叠 而组合起来完备的小区域， 小区域上的每个点 都 映射到相同的 输出--叶节点对应的类型输出。
				>无监督学习：找到数据的三种简单 的表示： 低维表示、稀疏表示、独立表示
					>主成分分析：PCA 学习 是 一种 比 原始输入维度 更低 的 表示。作为保留 数据 尽可能多 信息的 降维方法。让原始数据表示 X 去 相关。
						>例子：学习 数据的 正交线性变换：将输入x投影表示为 z: 使得具有 最大方差的 射线方向 从 原始时 不和 x轴重合 到 转换为 和 x轴重合。：只需要让 每个样本点 都进行 x^T *W 这样的一个 W矩阵的变换，就可以将样本点 方差最大的方向 调整为 和x轴方向一样。变换后使得  第一大方差的方向和第二大方差的方向分别为x轴和y轴。
							>数据平移：很容易做大。比如 样本点 群 的均值在(m,n) 只需要 每个样本点 都 减去(m,n) 则 一一映射 出的 新的同数量的点集 就 以(0,0)为 均值。
							>协方差矩阵： 因为无偏样本均方方差 = 1/(m-1)*Σ(xi - E(xi))^2  如果均值为0，则 Σxi^2 = Σxi^T*xi 从而 产生了协方差矩阵的想法，因为 左边的形式 是 i,i, 那么 只要i,j就产生了n*n个方差值了，就定义为无偏样本协方差矩阵：Var[x]ij = 1/(m-1)xi^T*xj  矩阵即： 1/(m-1)*X^T*X   因为 X^T*X是对称阵，实数矩阵，所以特征分解为 正交矩阵类型的特征向量矩阵：X^TX = WΛW^T 而 XX^T = U∑U^T, 且 上述分析有 X = UDW^T   带入 1/(m-1)X^TX = 1/(m-1) * WD^TU^T*UDW^T = 1/(m-1)*WD^2W^T 这里 因为正交矩阵 U^TU=I ,  而 上述 说明 Z=XW,所以 var(z) =1/(m-1) Z^TZ =1/(m-1)W^TX^T*XW =1/(m-1)W^TWD^2W^TW = 1/(m-1)*D^2 也用 W^TW=I  很明显 D^2 是一个对角阵，那么反推Z的元素是彼此无关的。
					>k均值聚类：初始化k个不同的中心点：{μ1,μ2,μ3...},   然后 将 每个样本点归属于 最相近的中心点，然后把归属于同一个中心点的 所有的样本点 计算样本均值 来作为新的中心点，这样 得到了 新一轮的 中心点：{μ1,μ2,μ3...}	
					>随机梯度下降：因为 代价函数 = J(θ) = Ex,y~pdata L(x,y,θ) = 1/m*ΣL(xi,yi,θ)  而 L(xi,yi,θ) = -log p(y|x,θ)	 所以 最小化 代价函数 = 沿着梯度方向变化，所以计算梯度 = ▽J(θ) = 1/m*Σ▽L(xi,yi,θ) 说明 样本数越多，则 代价函数的项 就越多， 因为本质是估计期望，所以只采取 抽取 一小批量m' 的 数据 来 更新:g = 1/m'*Σ▽L(xi,yi,θ)。θ <- θ - εg 但梯度下降很慢 或者 认为 没有原则。 
					>线性回归的另一种看待：数据集、代价函数、优化过程、模型。  线性回归 的代价函数 看作： 模型输出 和 真实 输出 之间的 欧式距离 服从 均值为0 方差为1的正态分布(即越接近的概率越大) 后 计算这些 样本的取值概率的平均概率的最大值：即满足分布 N(y-(w^Tx+b),0,1) 或者 N(y,x^Tw+b,1)， 代价函数 即 计算 max J(w,b) = -Ex,y~pdata log Pmodel(y|x)  然后计算 使得 代价函数的 梯度函数=0的正则方程。
						>基本思路：明确数据集 --> 建立模型 --> 构造 代价函数 (代价函数可以增加正则项/权重衰减项)--> 最小化代价函数(得到模型的参数/来将模型明确)
							>例子：PCA中，数据集就是X, 模型就是 w^TXw --即重构X, 代价函数 J(w) = Ex~pdata ||x - w^TXw||2,2  且增加限制 w^Tw=1的限制。
						>基本思路2： 极大似然法 也可以看作 xx 代价函数。最大后验估计 也可以 看作 xx 代价 函数。
					>流形：道路 是 嵌入 在三维空间中的 一维流形。		
					>极大似然和最大后验估计的思想区别：
						>极大似然估计 的 思想： 一连串独立的 事件 发生了，那么 相比于若个个 未发生的 另一连串的  独立 事件，概率要更大。
						>最大后验估计 的 思想： 实际发生的事情，是后验概率最大的事情(占比最大的事情)(实际发生的事情是概率最大的路径)(而不单是先验概率最大的事情或者单是转化概率最大的事情)。一件事情A发生的概率几乎为0，但它发生之后 B发生的概率 几乎 为1，即 P(A)->0, P(B|A)->1； 而B发生的概率比较大，那么B发生了 有 多大的概率 是由 A引起的？
							>条件分支概率树 理解 分母的含义：B发生了的概率。则等于 所有的导致B发生的分支/路径 发生的概率 的 和：P(A)*P(B|A) + P(~A)*P(B|~A)。即P(B)=ΣP(Ai)*P(B|Ai)
							>条件分支概率树 理解 分子的含义：某个分支路径发生的概率。
							>条件分支概率树 理解 分数的含义：某个分支路径发生的概率 占 所有导致B发生的路径的总概率 的比例。也可以说是 B的发生 是因为 某个分支发生 的可能性 	的大小。P(A|B)
							---因果顺序没变。一个条件概率 表示的是A作为原因 会 引起/导致/触发 的所有事件中B的占比 (A->B|C|D|E)。一个条件概率 表示的是 B作为结果 由 可以独立产生它的 所有事件中 A的占比 (A|G|D|E|R->B)。P(A)*P(B|A)和 P(B)*P(A|B)都只说了两件事发生的概率--A和B同时都发生的概率，所以值相等。
						>样本的作用：样本是变量，P是 映射。样本就是已经发生的事件。样本发生的概率 即 这些 事件 同时 发生的概率。事件可以制造， 制造一些事件 来 估计 某 事情的概率。	
		--深度网络：现代实践
		>深度前馈网络：前馈神经网络、多层感知机。
			>目标：近似某个函数f*, 如计算 x的类别值：f*(x) = y
			>结构： 前向的。模型的输出 和 模型本身 之间 没有 反馈 连接。f(x;θ)
				>链式结构函数：f(x) = f3(f2(f1(x)))  f1 为网络第一层，f2为网络第二层；链的全长 称为 模型的 深度。 最后一层 被称为 输出层。中间的称为 隐藏层。
					>训练目标：让f(x) 近似 f*(x)
			>神经网络实现：卷积神经网络。
			>学习例子： 
				>学习XOR: 
					>目标函数：XOR函数：y = f*(x); x = [x1 x2]  ，  即 [0 0]则0，[0 1]则1,[1 1]则0,[1 0]则1
					>方案1：	
						>模型函数：y = f(x;θ) = w^Tx + b 
						>损失函数：均方误差。J(θ) = 1/m*Σ(f(xi) - f*(xi))^2 注意 均方误差 不是 均方差----这就是 方差和误差的区别。误差的平方的均值---均方误差。
						>最小化损失函数计算的结果：w=0,b=1/2   从 图上看 ，异或 的4个点 和对应的 类 标签 是 交叉的，所以线性不能分开。
					>方案2：
						>模型函数： y = f2(f1(x,W1,a1),w2,a2) 其中 f1的输出 是一个向量h。此时 忽略 a则 y = w^T*Wx  相当于先 对 x做一个变换。最后 y = x^T*w' 还是一个 线性变换。
						>损失函数： 
						>最小化损失函数 计算模型待定参数：
					>方案3：
						>模型函数： y = f2(f1(x,W1,a1),w2,a2) , 其中f1 是非线性函数，h = g(Wx + c) , hi = g(x^T*W;i + ci), g通常取 整流线性单元，ReLu = g(z) =max{0,z}
			>变分法：
				>问题1：求解 f* = argmax Ex,y~pdata||y-f(x)||^2 其中 待定f , 这就需要用变分法 来计算。
					>这个问题的意义：是 将代价函数 定为 交叉熵 而不是 均方误差  或者  平均绝对误差的 原因。
			>神经网络： 
				>没有隐藏层的神经网络：实现的就是线性回归：y = g(x0a0 + x1a1 + x2a2) , g(x) = 1/(1+e^(-x)) 。其中 x0=1
					>调整a0/a1/a2 则模型实现 “或” 特征的 输入输出：如 a0= -10, a1=a2=20  。。模型 描述即： y = x1|x2
					>调整a0/a1/a2 则模型实现 "与" 特征的 输入输出：如 a0 = -20, a1=a2=15 。。 模型 描述即： y = x1&x2
					>调整a0/a1/a2 则模型实现 "或非" 特征的 输入输出： 即或的非： 如 a0=10, a1=a2=-15 。。 模型 描述 即： y = ~(x1|x2)
					>调整a0/a1/a2 则模型实现 “与非” 特征的 输入输出： 即与的非： 如 a0=20, a1=a2=-15 。。模型 描述 即： y = ~(x1&x2)
					>调整a0/a1/a2 则模型实现 “非” 特征的 输入输出： 如 a0 = 10, a1=-15，a2=0 。。模型描述 即 y = ~x1 
					---上述是 基本功能：则 接下来 利用 计算机最基本的功能：组合 基本功能/封装 基本功能 为 更加复杂的 上层的功能。
					>列出上述4中基本模型的输入输出表： 则 发现 将 “或” 的结果 和 “与非” 的结果相 与 后 得出 的 就是 x1⊕x2 的效果： 即  (x1|x2)&(~(x1&x2))
					>列出上述4中基本模型的输入输出表： 则 发现 将 "与" 的结果 和 “或非” 的结果相 或 之后 得出的就是 x1⊙x2 的效果： 即  (x1&x2)|(~(x1|x2))
					---上述组合的效果的图像化表述(输入范围推广/更一般)：把(x1,x2)看作是平面上的点(不仅仅是01,10,11),且假定模型参数就是上述的基本的模型的假定的参数；
					>总结：
						>输入向量和权重向量 线性组合后 经逻辑函数处理后 再二分输出：可以看作是对 输入空间 的 一次 二分分割/线性分割,分割的两个区域 同一个区域上的点 对 模型的 输出 都是 一种类型的值/一个值，不同区域上的点则不同 。
							>多次 不同的 线性组合 而 产生 的 多个 输出 ， 可以  构成 一个 新的  空间， 当作是 新的 输入空间，则可以再次 进行二分。
								>如果直接对 多个 输出 进行 逻辑运算 ，得出的结果 为最终的输出：那么 相当于 在 原空间 进行 多次 不同的 线性分割，则 通过 "与"的 线性组合 运算 就 只有 在 某分割的A区域 且在 某分割 的B区域 且在 某分割的C区域 上  的点 才 会输出1，其他区域上都是0，从而分割出一个任何形状的凸区域 ，它上面的点 是专门一类，凸区域外的点是另外一类。 
									>再并联这么多个凸型分割，那么可以精准的划定 每一类 所在 的 输入空间的范围(前提假设 是 每一类 的输入空间 都是 有固定边界的，且和其他类是分开的，没有重合的)。
						>神经网络的 两大类基本功能：1.实现 逻辑 与或非 功能。 2.对输入空间线性分割 划分为两类(两个区域上的点各自分别属于一类)(线性组合运算 已经实现对输入空间的正负分割--即直线两侧的点分别映射输出正值和负值-而直线上则是0值)(增加的额外的函数 只是对正值和负值和0的进一步映射而已-如映射为概率/属于该类的概率)。	
						>神经网络 实现的 复合功能/整合功能/组合功能/封装功能： 对 多个 二分类(对输入空间)(最底层是 线性二分类(线性边界), 上层一点区域可以折线二分类,再上层 则可以是 封闭凸多边形二分类) 进行  逻辑 组合 产生 全新的 二分类(形状更凸/短边更多)
				>如何通过样本 确定 分类边界：即确定 各层神经网络的参数。(选定几层/每层的权重)	
					>代价函数：损失函数：目标函数：
						>中间层节点的输入输出关系：第k层的输出 a^(k) = g(a^(k-1))  第一层的隐藏层的输出值/激活值： a^(2) = g(θ^(1)*x), 第二层隐藏层的输出值=a^(3) = g(θ^(2)*a^(2)) 注意此时g的输入是向量，输出也是向量。
						>输出层的模型激活值-真实值之间的误差和上一层模型激活值-真实值之间的误差 ，两个误差 之间的关系： 输出层的误差：δ^(i) = a^(i)-y ;  
						>以误差均值为代价函数：已知 第i个样本的 模型输出 为  hθ(xi), 实际输出yi,  则 (hθ(xi))^yi* (1-hθ(xi))^(1-yi) 可以统一 表示 出 输出和模型的误差：即统一的误差表达式,=0就是有误差，=1表示无误差；(无论真实输出是0还是1)， 进一步函数映射 来 规范误差：使得映射 结果=0表示无误差,映射结果>0表示有误差，而 不妨 取负对数： -log((hθ(xi))^yi* (1-hθ(xi))^(1-yi)) , 则 平均误差 = J(θ) = -1/m*Σlog(hθ(xi))^yi* (1-hθ(xi))^(1-yi) = -1/m*Σ(y*log(hθ(xi)) + (1-yi)*log(1-hθ(xi))), 而 hθ(xi) = g(a^(i)) = g(θ^(i-1)*a^(i-1))
							>则 待定θ^(i-1)， 而 зJ(θ^(i-1))/θ^(i-1) = 1/m* Σ(g'(θ^(i-1)a^(i-1))*a^(i-1)*(g(θ^(i-1)a^(i-1))-yi)/(g*(1-g)))  = 1/m* Σ(a^(i-1)*(g(θ^(i-1)a^(i-1))-yi)) =  1/m* Σ(a^(i-1)*(a^i - yi)) 化简是因为 g(x) = 1/(1+e^(-x)), 所以 得出g' = g*(1-g)
								>再往上一层，则 зJ(θ^(i-2))/θ^(i-2) 利用 a^(i-1) = g(θ^(i-2)*a^(i-2)) 可以 得出   зJ(θ^(i-2))/θ^(i-2) = 1/m*Σ(g'(θ^(i-1)*g(θ^(i-2)*a^(i-2)))* θ^(i-1)*g'(θ^(i-2)*a^(i-2))*a^(i-2) * (g-yi)/(g*(1-g))) 同理 利用 g' = g(1-g) 进一步得出 ，=1/m*Σ(θ^(i-1)*g'(θ^(i-2)*a^(i-2))*a^(i-2) * (g(θ^(i-1)*a^(i-1))-yi)) = 1/m*Σθ^(i-1)*g'(θ^(i-2)*a^(i-2))*a^(i-2) * (a^(i)-yi) = 1/m*Σδ^(i-1)*a^(i-2), 即 这里 令 δ^(i-1)=θ^(i-1)*g'(θ^(i-2)*a^(i-2))*δ^(i)  这就是反向传播算法 的 推导过程。
					>训练方法：学习方法：阶段式减少代价函数的方法
						>前向传播：随机初始化所有层的参数矩阵-->从输入一直计算每一层的激活值到输出层的激活值
							>顶层思路：
						>反向传播：根据前向传播计算出的每层的激活值，从最后的输出层的激活值 的计算表达关系开始处理：将参数矩阵当作未知变量矩阵，将上一层的激活值当作已知，再将 输出值的计算表达式 和 样本的标签/目标/应该输出值 的误差的 函数 当作 代价函数，待定参数矩阵 而 最小化 代价函数，这样得出 新的 参数矩阵。显然这样的参数矩阵就更准确--而不是初始/上一步时的 随机值。再通过实际输出和参数矩阵 来反算 输入向量--作为上一层的真实/标签激活值...如此 来从后往前 更新 参数矩阵。对每个样本都这样更新。最好：显然代价函数 不是 单个样本的误差函数，而应该是全部样本的 误差函数/如均方误差，并在计算出参数矩阵后 计算每个样本的 上一层的激活值。 
				>混合密度网络：
				>高斯混合模型： P(x|θ)是 一种 指定分布，即指定θ=θ时的x的概率分布。
					>单高斯模型：x是一维数据，则建立在一维数据上的高斯模型就是普通的高斯模型。x是 多维数据，如n维向量，则建立在多维数据上的高斯模型 = 1/((2π)^(D/2)*|Σ|^1/2) * exp(-1/2*(x-μ)^T*Σ^(-1)*(x-μ)) 其中 Σ 是 协方差矩阵。μ是各个维度分别的均值构成的向量。
					>混合高斯模型：P(x|θ) = Σαk*Φ(x|θk)  其中 显然有，∫P(x|θ)dx = Σαk = 1 所以 是 一个正常的概率分布。
						>估计α,θ: 如果用对数似然法，则每个样本的概率 的表达式中 始终 有 求和 项在对数里，所以难以 计算。
							>EM算法 来估计： 思路：正向推理：已知有k个子分布/高斯分布， n个样本点， 则假设 第j个样本xj是 第k个子分布Nk产生的 概率 为 γjk,  很明显，很直接可以计算γjk, 因为根据子分布模型，取值xj时对应在 各个模型中的概率 Nk(xj)可算，那么它们的比例自然也可算，则来自第k个模型的概率就是第k个模型的概率在整体概率中的比例：γjk = Nk(xj)/ΣNk(xj)即对 k求和。 这样，可以计算出k*n个概率，得到一个概率矩阵。观察这个概率矩阵，每一行 表示 第j列个样本 来自第k行表示的k类子分布的概率，则 n个样本 分别 来自第k类的概率已知，则这n个概率的比值 而建立的新的概率 表示的含义 则是 在这第k个子分布下 取值概率---即 为什么第j个样本来自k子分布的概率就比第j-1个样本来自k子分布的概率要大?就是因为取值概率更大--合理解释--认为它就是取值概率就是对的--就可以解释为什么更大，在此认为之下，有取值概率，有取值，则均值可以计算，即均值=Σ取值概率*取值; 即 μk = Σ(γjk/Σγjk)*xj  内部求和就是对j,外部求和也是对j。。。自然的，方差可以估计  σ^2 = Σ(γjk/Σγjk)*(xj - μk)^2,  而 αk = Σγjk/ΣΣγjk = Σγjk/N 对 j求和。这样αk更新了，则γjk也可以更新了，从而 μ,σ^2也更新了，一直循环下去，直到 ||θi-θ(i-1)||<ε 那么就可以停止，而得到最接近的混合高斯模型了。
			>感知机：
				>sigmoid函数：1/(1+e^(-z)) 以概率的方式展示 2分类的分类可能性。即归类为A的概率。(饱和问题--z很大时)
				>softmax函数：归一化指数函数。将多分类的结果 以 概率 的方式 展现出来。：：得出过程解释：1.结果x非负化：exp(x) 2.取值归一化：分子=取值，分母=所有的样本的取值之和 即 exp(fy) / Σexp(fc)  即 取第i类的概率P(i) = exp(θi^T*x)/Σexp(θk^T*x)  或者 表达为 yi = exp(zi)/Σexp(zk)  那么зyi/зzj 可以计算出来。可以用在 计算交叉熵 L=-Σyc*log(pc) 的最小化的 зL/зzj 中 。
					>逻辑回归模型：1/(1+e^(-x)) 的含义： 是 将 x 分类为 y类的 概率。即：p(y|x) = 1/(1+e^(-x))  可以看作 对输入空间进行 线性分割，而分割线的两边的区域概率递增方向相同，一边的概率>1/2，分割线上的点的分类概率为1/2。。显然，输入空间上的每个点 对 这个模型 都输出一个 概率值--表示分类为这个类别的概率。自然的，多个逻辑回归，就是对输入空间的多次线性分割，就输出属于多个类的各自类别的概率，即一个点 就输出 属于 多个类的分别的概率，显然希望 这些概率之和=1，因此对它们归一化，得到的输出 就是 x属于各个类的概率，就是 属于类的概率分布:P(y|x)。 
					>逻辑回归代价函数：	
				>模型：f(w^T*x + b)	或者直接  w^T*x + b 
				>代价函数： argmax argmin yi*(w^T*xi + b)/||w||  即 内层先寻找函数距离最小的点i, 再外层寻找使得这个最小的函数距离 最大的 w,b; 但是 显然 外层必须把 内层的约束带上。(里层等价于yi*(w^T*xi + b)/||w||>=λ,因为错误分类，所以λ为负值，又因为线性组合可以乘以系数，所以不妨λ=-1,所以等价于yi*(w^T*xi + b)>=-||w|| 可以构造拉格朗日函数L(w,b) = -1/2*w^2 + Σαi*(yi*(w^T*xi + b)+1) ,从而对w,b求偏导数 得出 ,w=Σαiyixi, Σαiyi=0)
				>代价函数2：采用迭代方式；假设初始给了w0,b0, 则 得出误分类点的集合M, 那么 误分类点到 直线的距离总和 = -Σyi*(w^T*xi + b)/||w||,  再把w,b当作未知变量，来最小化这个误分类的点到直线的距离总和，不能采用批量梯度下降，只能采用随机梯度下降SGD；即一个误分类点一个误分类点的调节，对一个误分类点来说，误差距离的减小方式 就是 沿着梯度的反方向运动，所以更新w = w0 + η*yi*xi, b = b + ηyi ,那么 调整 了所有的误分类点之后，大概率 又产生了新的误分类点，所以继续循环，直到没有误差点 。
				>多层感知机：
					
			>隐藏单元：问题-如何选择隐藏单元的类型；			
				>隐藏单元激活函数的不可微性：左导数和右导数 一般都有。
				>隐藏单元共同过程：接收输入向量x,计算仿射变换z = W^T*x + b , 然后 带入 非线性函数 g(z)  结果就是输出。
					>整流线性单元使用激活函数：g(z)=max{0,z},  	h = g(W^T*x+b)
			>架构设计：			
				>万能近似定理：
					>Borel可测函数：
					>前馈网络的导数：
					--分类的本质：是在学习  一个 函数。单层前馈网络 来表示 出任意一个函数： W^T*x, 这个W行数非常多>>x的维度； 以 二维函数 f(x,y)的模拟为例，如果在某个小块区域上，f取值为z,那么可以有若干个线性组合在这里交叉形成的小区域-，取值在这里正好也是z,而在其他地方取值强制为0，则 所有的 小块上都这样模拟，将所有的结果 相加 得到的输出 就是 函数的输出了。
					>更深层的网络可以更好的泛化：每层学习出 上一层的 特征。上一层可以构成的结构。	
						>旋转矩阵 的存在： 如[0 -1;1 0] 可以让平面上的点逆时针旋转90°，所以如果 最开始有一个 第二象限的长方形内的点被分为A类，那么在神经网络前面增加这个 旋转矩阵后，则在第一象限内的可以旋转90°导第二象限和它重复的长方形 则 现在 也是 被分为A类了。同理 平移向量[a a]则直接作用在输入上 ，从而，所有的同一个形状的区域 经过平移或者旋转 后 都可以 被 分类为 同一类。 直角坐标系 下的放缩，则是[a,0;0 a]放缩矩阵相乘的结果。截图-放缩-旋转 而 把 目标区域 重新放置在坐标系中心。
							>一种识别数字的方法：比如识别8， 1.先在平面上 构造形成8的带状区域(这个可以用15个带状区域组合构成;每个区域显然需要4条直线相交并取逻辑与构成)，在这个区域上的点输出都是“8”类。2.遍历图像上的点，根据沟壑/数字相邻关系 而找到 有效构成数字的路径点 集合，并进行-截图-放缩-旋转 而规范化。3.将集合上的点 输入 神经网络，看有多少 点 在 这个网络表示的区域上，并且各个子区域上 分别有多少，如果每个子区域上都有若干个点(甚至占总子区域点的某个分数)，且 点集合整体落在区域上的点的比例有xx%, 这些可以计算出来，另外，如果还有0-9的其他的区域 表达的神经网络，则也输入比较，满足一定比例，则认为 是 有可能为 某个区域，排个序，输出最可能的属于的k个区域/k个类。
				>微积分链式法则：普通标量x,y,z: 如果有 y = f(x), z=g(y) 则 有  зz/зx = зz/зy * зy/зx ,  扩展开来 ，x∈R^m,y∈R^n, z还是标量，y = g(x), z = f(y),第一个关系式子表明yi和xj有关系,第二个式子表示z和yi有关系 , 那么 зz/зxi = зf(y)/зxi = зf(y1,y2,y3...yj...)/зxi = Σ зz/зyj * зyj/зxi 对j求和。那么 写成矩阵的形式，则 зz/зx = ▽xZ = (▽xY)^T*▽yZ  其中 ▽xY 就是 j为行标，i为列标的n*m 矩阵：Jacobian矩阵。而▽xZ 是 z对x的梯度，▽yZ是 z对y的梯度； 这个方程的解释 就是 z对x的梯度 可以 通过 z对y的梯度和 Jacobian矩阵 的乘积 来表示。Jacobian矩阵 这样写 正好 和 神经网络 中间层 的 第 j个节点的输出yj 对 第i个输入分支xi的偏导数/变化率的特征(i个变化率 横向 铺展开)。  也可以说 输出层的值 对 每个输入层的值 的变化率 =  中间层每个节点对它的每个输入分支的变化率 * 输出层 的值 对 每个输入分支的 变化率。
					>同理2层隐藏层的输出对输入求偏导：зq/зxi = Σзq/зzk*зzk/зxi = Σзq/зzk * Σзzk/зyj * зyj/зxi  右边项 第一个求和对k，第二个求和对j,。矩阵形式：▽xQ = (▽xY)^T*(▽yZ)^T*▽zQ  从 这个形式来看，则 可以扩展到 无限深度的神经网络上去，都可以计算 最后输出层的值 对某个输入层的变量的变化率。
						>真正要计算的：误差/目标函数/代价函数  对  权重矩阵 的 偏导数。
					>自动微分：以算法方式 计算 导数。
					>反向模式累加：比 反向传播算法 更广泛。
					>Krylov方法：	
		>深度学习中的正则化：设计用来 减少 测试误差 的策略--正则化策略。 是 机器学习的中心主题之一。模型->正则化->优化
			>正则化：即对学习算法的修改--以达到 减少 泛化误差 而不是 训练误差 的目的。
				>策略1： 向 学习模型 添加 限制参数值的 额外约束；
				>策略2：向 目标函数 添加 额外项 来对 参数值 进行软约束；J(θ, X,y)~ = J(θ, X, y) + α*Ω(θ)  其中α是超参数。
			>参数范围惩罚：权重衰减。Ω(θ) = 1/2*||w||^2,2; 称 L^2 参数范围惩罚；岭回归 或 Tikhonov正则；
				>线性回归的代价函数添加 正则项 的影响：(Xw - y)^T*(Xw -y) + 1/2*α*w^T*w 
					>添加前：则 根据 矩阵微分 可知， 求导数 = X^T*2(Xw-y) = 0 ,则 w = (X^T*X)^(-1)X^T*y
					>添加后： 同理  因为添加项的 导数 = αw ,  因此合起来  w = (X^T*X + 1/2*αI)^(-1)X^T*y
				>反向传播算法：
					>背景：神经网络 作为 非线性/线性 模型，模型参数一般是 不知道的(权重/偏置未知)，但是模型的输入输出关系/方程 本身 确定的(方程中的参数未确定)。而确定 这些参数 的方式 需要 根据 更多的 具体信息和 条件，即更多的约束关系(直接的约束和间接的约束关系)。而实际可用的信息 就是 样本(输入值-期望值-实际值)，这几乎是 唯一的 新增的 约束。
					>目标： 确定 神经网络 模型 的  参数 ： 权重 参数 和 偏置 参数， 使得 对 旧的 输入，模型的输出 和 期望值 几乎无差别，对 新的 输入， 模型的输出 和 预期值 也十分接近。
					>已知条件：假设的样本集合{(xi,yi)}。神经网络模型。 
					>基于的思路和信息和基本定义：模型的偏差/误差/目标函数 表达式/函数 可以明确，这个表达式函数可以认为只有每层的w,b是变量(未确定的,xi/yi都是可以确定的), 那么误差由w,b决定，显然误差越小越好，所以就是待定w,b而计算误差函数的最小值：argmin J(w,b); 实际实施时，由于模型输出值的确定需要知道w,b, 所以不妨如果一开始定了w,b--尽管不是最优的, 但 至少可以算出(所有的样本的总)误差是多少，且 从模型本身可以得到误差关于w,b的梯度表达式，则利用反梯度*步长 来施加到w,b而更新w,b则新的w,b会使得输出误差更小，不断的计算最新输入点处的梯度，来更新w,b 并同时查看最新的误差--如果达到了极限值 则 结束返回w,b；否则继续将w,b作为最新的模型的参数 而计算 模型对所有样本的误差，继续更新w,b。。  
						>l层第j个神经元输入值：z^l,j = Σ w^l,jk * a^(l-1),k + b^l,j 
						>l层第j个神经元输出值：a^l,j = σ(z^l)
						>误差表达式：代价函数： 误差平方和的平均值：C = 1/2n*Σ||y - a^L||  其中 y就是实际的分类值，a^L 就是 预测的输出值。
						>误差对第l层j个神经元的输入 的 偏导数： зC/зz^l,j = δ^l,j 
							>误差对第L层j个神经元的输入的偏导数：зC/зz^L,j = δ^L,j = зC/зa^L,j * зa^L/зz^L,j , 则矩阵化，认为 δ^L = зC/зa^L ⊙ зa^L/зz^L = ▽aC ⊙ σ'(z^L)
							>误差对第l层j个神经元的输入的偏导数 和 误差对第l+1层的各个神经元的输入的误差的偏导数的关系: зC/зz^l,j = δ^l,j = ΣзC/зz^(l+1),k * зz^(l+1),k / зa^(l),k * зa^(l),k / зz^(l),j =  Σ δ^(l+1),k  * σ'(z^(l),k) * w^(l+1),kj , 对 k求和，则 δ^l = ((w^(l+1))^T * δ^(l+1) ⊙  σ'(z^(l)))
						>误差对第l层的权重w^l,jk的的偏导数： зC/зw^l,jk = зC/зz^l,j * зz^l,j/зw^l,jk =  δ^l,j * a^(l-1),k    矩阵化，则 右边 是第一项 是 列，第二项是列，形成j*k矩阵
						>误差对第l层的偏置b^l,j的 偏导数： зC/зb^l,j = зC/зz^l,j * зz^l,j/зb^l,j = δ^l,j
						>w的更新公式： w^l <- w^l - η/m *Σ δ^l * a^(l-1)^T  对样本求和。
						>b的更新公式： b^l <- b^l - n/m * Σδ^l  对 样本求和。
				>向量导数的本质：标量和标量的所有组合，然后 标量分别对标量求导。
					>求导布局：
						>分子布局：
							>向量对标量求导：是一个 由 向量的各个 元素 分别对 标量进行求导 而 构成的 同维度 的 向量(且都是行向量/列向量)
								>标量对向量求导：类似。
							>矩阵对标量求导：是一个 由 矩阵的各个元素 分别对 标量进行求导 而 组成的 同维度的 矩阵。 
								>标量对矩阵的求导：类似。 
							>向量对向量的求导：向量A的每个标量元素 都 和 向量B的每个标量元素 进行求导：这样 构成 m*n 个 导数，组成一个m*n的矩阵。
								>按分子布局：则 第一个维度 以分子为准，所以 是 зyi/зxj 其中  i为行标，j为列标。y,x都是向量。结果 矩阵 是 Jaccobian 矩阵。 
								>按分母布局：则 第一个维度 以分母为准， 所以 是 зyj/зxi 其中 i是行标，j是列表。y,x都是向量。结果 矩阵 是 梯度矩阵。
						>分母布局：
							>向量对标量求导：是一个 由 向量的各个 元素 分别对 标量进行求导 而 构成的 同维度 的 向量(且向量是行向量则结果是列向量，反过来也一样)
						
				>向量导数的简便方法：
					>目标：将  向量表达式/矩阵运算表达式  对 向量/矩阵求导， 结果 和 原向量表达式/矩阵运算表达式 之间 有什么 直接的关系？
						>归纳例子：
							>结果为标量的向量表达式 对 向量求导： з(a^T*x)/зx = [з(Σaixi)/зxj] = [a1 a2 ...aj..]^T = a  这里取 分母布局。 同理  з(x^T*a)/зx = a^T 
								>拓展1： з(x^T*x)/зx = [з(Σxi^2)/зxj] = [2x1 2x2 ...2xj..]^T = 2x 
								>拓展2：з(x^T*A*x)/зx = [з(Σ(Σxi*aik)*xk)/зxj] = [з(ΣΣxi*xk*aik)/зxj] = 四部分(i!=k时i=j或者k=j; i=k时i=j或者i!=j) = [ Σxk*ajk + Σxi*aij + 2xj*ajj ] = [Σxk*ajk + Σxi*aij] = [x^T*Aj; + x^T*A;j] = Ax + (x^T*A)^T = Ax + A^T*x = (A+A^T)x  如果 A = E 则 退化为了上式的 2x 
							>结果为标量的矩阵表达式 对 矩阵 求导：з(a^T*X*b)/зX = [з(Σ(Σai*Xij)*bj)/зXkc] = [з(ΣΣai*Xij*bj)/зXkc] = 因为对i,j求和,所以一定有k=i,c=j的项，且仅有一项 = [akbc] = a*b^T = 
							--矩阵函数：exp(Xb) 是一个列向量,每个元素为Xb的第i个元素作为exp指数的值。这里X是矩阵，b是列向量。	
							>结果为向量的表达式 对 向量 求导： зAx/зx = [з(ΣAij*xj)/зxk] = 分子肯定包含分母项 =  [Aik] = A 
						>整体求导：而非分量求导 再 组合 为向量的 方法：即 基本 的 向量 求导 法则  。
							>标量对向量求导法则：
								>线性法则：з(c1*f1(x) + c2*f2(x))/зx = c1*зf1(x)/зx + c2*зf2(x)/зx
								>乘法法则：з(f(x)*g(x))/зx = f(x)*зg(x)/зx + g(x)*зf(x)/зx 
								>除法法则：з(f(x)/g(x))/зx = 1/g^2(x) * (f'(x)*g(x) - g'(x)*f(x))
					>矩阵微分：
						>预备：多变量函数f 的微分：df = Σзf/зxi * dxi = (зf/зx)^T * dx  这里使用了标量对向量求导的 定义，x是一个向量。dx = [dxi]^T
						>同理：将矩阵元素 看作多变量，则矩阵函数f 的微分： df = ΣΣ зf/зxij *dXij = 同规模的两个矩阵的对应元素之积的和 --如果一行一行对应的来看--就会发现和A*B^T的结果矩阵和这个目标和的关系即：tr(A*B^T)=目标和。所以： ΣΣ зf/зxij *dXij = tr((зf/зX)^T*dX) 其中 tr()就是迹函数，标量的迹函数就是它本身，所以 tr((зf/зx)*dx) = зf/зx * dx  从而形式就统一了。而注意 前面 dX 是 一个矩阵--且就是直接对每个矩阵元素进行微分。
						>矩阵微分的性质：注意 f 是 多变量 的函数，取值是标量，而зf/зX 则是一个矩阵。df也是标量。
							>加减法：d(X+Y)= dX + dY 
							>乘法/转置/迹：d(XY) = dX*Y + X*dY ;  dX^T = (dX)^T;  d tr(X) = tr(dX)
							>哈马达乘积：很好理解 d(X⊙Y) = X⊙dY + Y⊙dX
							>矩阵函数微分/逐元素微分：dσ(X) = σ'(X) ⊙ dX = tr((σ'(X))^T*dX) 从上面知道 σ'(X) = [зσ/Xij]
							>矩阵的逆矩阵的微分：dX^(-1)的推导： d(X*X^(-1)) = dI = 0 = dX*X^(-1) + X*dX^(-1)   ， 所以  0 = X^(-1)dX*X^(-1) + dX^(-1) 所以 dX^(-1) = - X^(-1)dX*X^(-1)
							>行列式微分：d|X| 的推导： 因为 |X| = ΣXij * Xij的代数余子式Aij或者说伴随矩阵的元素X*ji,这里对j求和 = ΣXij * (-1)^(i+j)*det(X^ij) 而代数余子式 就是删除i行j列后的元素构造的新矩阵的行列式*(-1)^(i+j)的值，跟Xij无关；因此  з|X|/зXij = Xij的代数余子式 = (-1)^(i+j)*det(X^ij) =Aij=X*ji,  因此 d|X| = ΣΣз|X|/зXij * dXij = ΣΣ(-1)^(i+j)*det(X^ij) * dXij = ΣΣAij*dXij = ΣΣX*ij*dXij , 因为 X*X* = det(X)*E , 即 X*X的伴随矩阵 = X的行列式为对角元素的对角阵。这里要证明：ΣXij*X*jk = 0 当i!=k时成立----推导方法：利用行列式某一行加到另一行上 后 行列式值不变 ,即 ΣXij*X*ji = Σ(Xij + Xkj)*X*ji = ΣXij*X*ji + ΣXkj*X*ji 因此 ΣXkj*X*ji=0。所以 X* = X^(-1)*det(X)*E , 因此 ΣΣX*ij*dXij = tr(X**dX) = tr(X^(-1)*det(X)*E*dX) = det(X)*tr(X^(-1)*dX)(假象可以这样认为：ΣΣ |X|*dXij/Xij = |X|*ΣΣdXij/Xij = |X|tr(X^(-1)*dX))
							---由上述运算构成的函数f:	
							>迹函数性质：转置不变：tr(A^T) = tr(A) ;  交换律：tr(AB) = tr(BA) 都容易证明。(简答的指标带入即可)。矩阵乘法和迹交换：tr((A⊙B)^T*C)  = tr(A*(B^T⊙C)) 也容易证明，指标带入即可。
							---利用上述性质: 再次对 标量 对 矩阵 求导进行 推导： 
								>例子1： y = a^T*X*b, 则 зy/зX = ? , 先进行微分：dy = d(a^T*X*b) = da^T*(Xb) + a^T*d(Xb) = a^T*d(Xb) = a^T*dX*b = ΣΣai*dXij*bj = ΣΣai*bj*dXij = (a*b^T)⊙dX ,则 dy/dXij = ai*bj ;根据 标量函数对 矩阵求导的定义：dy/dX = [dy/dXij] = [ai*bj] = a*b^T , 这个形式就是 (a*b^T)⊙dX的左边的项的形式；所以 将微分 表示成这样就 可以 得出 标量对 矩阵的求导的结果。
								>例子2：y = a^T*exp(Xb), 则 зy/зX = ? ,同理，先进行微分：dy = d	(a^T*exp(Xb)) = a^T*dexp(Xb) = a^T*exp(Xb)*d(Xb) = 	a^T*exp(Xb)*dX*b = ΣΣai*exp(Xij*bj)*dXij*bj = ΣΣai*exp(ΣXik*bk)*bj*dXij = a⊙exp(Xb)*b^T⊙dX = diag(exp(Xb))*a*b^T⊙dX , 所以 dy/dX = a⊙exp(Xb)*b^T
								>例子3：y = tr(A), 则 зy/зA = ? , 同理， dy = dtr(A) = tr(dA) = ΣdAii, 所以 dy/dAij = I , 即 是单位矩阵。这是迹函数的特殊特征。
								>例子4： y = tr(AB) ，则 зy/зA  = ? 则 dy = dtr(AB) = tr(d(AB)) = tr(dA*B + A*dB) = tr(dA*B) = ΣΣdAij*Bji , 所以 dy/dAij = Bji , 所以 dy/dA = [Bji] = B^T ; 同理 dtr(AB)/dB = A^T (注意矩阵相乘的表达式中的指标 是 绑定指标之间的关系而已,什么名称无所谓)
								>例子5： y = tr(W^T*A*W),зy/зW  = ? 则 dy = dtr(W^T*A*W) = tr(d(W^T*A*W)) = tr(dW^T*(AW) + W^T*d(AW)) = tr(dW^T*(AW) + (W^T*A)*dW) = ΣΣΣdW^Tij*Ajk*Wki + ΣΣΣW^Tij*Ajk*dWki = ΣΣΣdWji*Ajk*Wki + ΣΣΣW^Tij*Ajk*dWki = ΣΣΣdWij*Aik*Wkj + ΣΣΣW^Til*Alj*dWji = ΣΣΣdWij*Aik*Wkj + ΣΣΣW^Tjl*Ali*dWij, 所以 dy/dWij = ΣAik*Wkj + ΣW^Tjl*Ali , 而 [ΣAik*Wkj + ΣW^Tjl*Ali] = [ (AW)ij + (W^T*A)ji ] = [(AW)ij + (W^T*A)^Tij] = AW + A^T*W = (A+A^T)W
								>例子6： y = tr(B^T*X^T*CXB), зy/зX = ? 则 dy = dtr(B^T*X^T*CXB) = tr(d(B^T*X^T*CXB)) = tr(B^T*d(X^T*CXB)) = tr(B^T*(dX^T*(CXB) + X^T*C*dX*B)) = tr((CXB)^T*dX*B) + tr(B^T*X^T*C*dX*B) = tr(B*(CXB)^T*dX) + tr(BB^T*X^T*C*dX), 根据定义，则 dy/dX = 迹函数中dX前面的部分的转置 = (B*(CXB)^T + BB^T*X^T*C)^T = (BB^T*X^T*(C+C^T) )^T = (C+C^T)*X*B*B^T
							>标量对多个向量的链式法则：x->y->z 这样的依赖关系，x是m*1维向量，y是n*1维向量，z是标量，则 зz/зx = зz/зy * зy/зx  但是  因为按照定义 зy/зx 是 n*m维度的Jaccobian矩阵，所以无法直接相乘的：一种则是：m*1 = m*n * n*1, 即 зz/зx = (зy/зx)^T*зz/зy , 这就是 多个标量对多个向量的链式法则。当 y1->y2->...->yn->z , 则 利用 前述关系，则 зz/зy1 = (зyn/зy(n-1)*зy(n-1)/зy(n-2)*...*зy2/зy1)^T*зz/зyn
								>例子1： 最小二乘法的目标 是 优化 损失函数：l = (Xθ - y)^T*(Xθ - y) , 显然第一个子目标就是要 求 导数 зl/зθ,  虽然可以直接展开右边，但是形式特征上看，不妨 令 z = Xθ - y, 则 l = z^T*z , 从而θ->z->l  зl/зθ = (зz/зθ)^T*зl/зz = (X)^T* зl/зz = X^T * 2z = 2X^T*z = 2X^T*(Xθ-y)
							>标量对多个矩阵的链式法则：X->Y->z 这样的依赖关系， X是m*n维度，Y是n*p维度,z是标量， 则 зz/зxij = Σ зz/зYkl * зYkl/зXij (对kl积分)= 很明显是个哈马达相乘:зz/зY ⊙ зY/зXij 再对结果矩阵的所有元素求和, 而这个和表达式就是 矩阵表达式 зz/зY * (зY/зXij)^T 的结果矩阵的对角线上的元素的和相等：即 哈马达乘积后的所有和 等于 矩阵乘积的迹函数映射的值；所以 зz/зxij = tr(зz/зY * (зY/зXij)^T) = tr((зY/зXij)^T * зz/зY)
								>例子1： z = f(Y), Y = AX + B , 则 зz/зX = ?  先看对每个元素的偏导数，зz/зxij = Σ зz/зYkl * зYkl/зXij = Σзz/зYkl*з(AX+B)kl/зXij ,因为 з(AX+B)kq/зXij = з(ΣAkc*Xcl)/зXij = зAki*Xil/зXij = Aki*δlj ,所以 Σзz/зYkl*з(AX+B)kl/зXij = Σзz/зYkl*Aki*δlj = Σзz/зYkj*Aki*δlj 这里只对k求和,此时已经使用了l=j则δlj=1 ，所以=Σзz/зYkj*Aki = ΣA^Tik*зz/зYkj = (A^T*зz/зY)ij。  因此  зz/зXij = A^T*зz/зY 
								>例子2： z = f(y), y = Ax + b, 其中 x,y都是m维度的向量，则 зz/зx = ? 类似上面 ,зz/зxi = Σзz/зyk * зyk/зxi = Σзz/зyk * з(ΣAkc*xc)/зxi = Σзz/зyk * Akc*xc*δci = Σзz/зyk*Aki = ΣA^Tik*зz/зyk = (A^T*зz/зy)i ,所以 зz/зx = A^T*зz/зy 
								>例子3： z = f(Y), Y = XA + B, 则 зz/зX = ? 同上，可知 зz/зXij = Σ зz/зYkl * зXkj*Ajl/зXij = Σ зz/зYkl * Ajl*δki = Σзz/зYil*Ajl = Σзz/зYil*A^Tlj = (зz/зY*A^T)ij , 所以 зz/зX = зz/зY * A^T 
								>例子4： z = f(y), y = Xa + b, 则 зz/зx = ? 同理 ， зz/зxij = Σзz/зyk * зyk/зxij = Σзz/зyk * з(ΣXkc*ac)/зXij = Σзz/зyk * з(Xkj*aj)/зXij = зz/зyk* aj*δik = зz/зyi*aj = ((зz/зy)*a^T)ij , 所以 зz/зX = (зz/зy)*a^T
							>矩阵对矩阵求导：F:p*q, X:m*n, 不会 使用 зFij/зX 或者 зF/зXij的方式。而是矩阵向量化。也不是 зFi; / зX;j  而是 зvec(F)/зvec(X)  即直接将F,X拉成向量。 即 vec(F) 是 (pq)*1 规格的向量,vec(X) 是 (mn)*1的向量； 而 зF/зX 是 (p*q)*(m*n)的矩阵 ,或者 (pq*1)*(mn*1)的矩阵：зvec(F)/зvec(X)，  因为  dFij = tr((зFij/зX)^T * dX) ， 即 是 F的第ij位置元素生成的偏导数矩阵和dX的哈马达乘积的全和,则 dF就是 F的所有的位置的元素的这样的全和的 和，想象这个加法的过程，是一个巨大的p*q个m*n的矩阵的全和，如果把зF/зX 表示为 pq行*mn列的矩阵，即第一行表示第一个vec(F)第一个元素 对 vec(X)的第j个个元素求偏导数,  则 vec(dF) = зF/зX * vec(dX)
								>矩阵分析和应用：张贤达的书。
								>克罗内克积：vec(ab^T) = b ⊕ a 
			>正则化策略Dropout: 
			>数据集增强：噪声-高斯噪声 用于输入/隐藏单元/权重。
				>深度学习的目的：也是 学习 函数/模型。  即 输入区域 划分， 各块区域上的样本点的函数输出值(各类型的各概率值/直接对应到类型的值) 基本是 一致的/一类的/类似的。
					>常见整理输入样本的方法：
						>将原空间中的点映射到新空间中：
					>生成模型：P(x) 输入样本的分布概率(即样本也不是随便任意取的/设计的,也是有定义域的,也是生成的/获取的)			
					>判别模型：P(y|x) 输入x时输出的各类型的概率值。
					>L^2正则化：
					>L^1正则化：
					>参数共享： 相同的特征(隐藏层权重) 在 输入的不同位置  的 输出 应该一样。 图像中的特征 平移 不变。
					>稀疏表示：即许多参数为0或者接近为0
					>Bagging: 结合几个模型 降低 泛化误差。模型平均。
					>正切传播：
					>流形正切分类器：
		>深度模型中的优化：解决 神经网络训练 问题 的 优化技术：寻找一组θ--使得显著的降低代价函数J(θ)。
			>目标函数：J(θ) = E(x,y)~pdataL(f(x;θ), y)  其中 pdata(x,y)是真实的潜在分布，通常不知道--但知道训练集中的样本。-->要最小化期望损失，不妨最小化 经验风险---即用 训练集上的经验分布p~data 代替真实分布p(x,y) : 1/m*ΣL(f(x;θ),y) 。但容易导致过拟合，因此深度学习里很少使用。
			>代理损失函数：替代0-1损失之类的损失函数。如负对数似然。
			>批量算法和小批量算法：说的都是如何减少计算步骤、计算量的事情。
				>随机梯度下降：	沿着 随机 挑选的 小批量 数据 的 梯度下降方向。是 一般机器学习 中 应用 最多的 优化算法，尤其深度学习中。
					>样本的梯度的均值：为什么是 梯度的无偏估计。
					>样本梯度的均值的计算：g = 1/m * Σ▽θL(f(xi,θ), yi)  即 每个 样本点 在误差函数 上 的梯度值的和的均值；
					>初始参数θ:
					>更新参数θ: θ = θ -εg   保证收敛 则  Σεk = ∞ ,  且 Σεk^2 < ∞   ，  εk = (1-α)ε0 + α*εt ,  α=k/t
				>动量算法：使用了 参数 在参考空间的 移动方向和速率；
					>更新动量方向：v = αv - ε*▽θ(1/m*ΣL(f(xi,θ),yi))   即 有 加速 过程---如果 不断沿着一个方向走 则 后面会加大步长；
					>更新参数θ: θ = θ + v 
					--Nesterov动量：
				>其他优化算法：RMSProp,AdaDelta,Adam;SGD;动量SGD;动量RMSProp	
			>神经网络优化中的挑战：
				>病态：H矩阵的病态。g^T*g 和 g^T*H*g 的增长变化趋势的不同导致。
				--迭代公式和Δx有关系：认为 x(n+1) - xn = Δx
				--泰勒级数的推导和解释：
					>推导：用假定f(x)可以表示为n阶多项式的和，然后待定系数法求出系数即可；
					>解释：需要逐步推广的的方式。先假定[x,x+Δx]内一阶导数不变，则f(x) = f(0) + f'(0)Δx;  拓展一步，假定	[x,x+Δx] 内一阶导数是变化的，但是二阶导数不变，则为了利用后一个性质，将[x,x+Δx]区域分割为n份,n->∞, 认为一阶导数只有在分割出的子区间[xi,xi+1]之间不变,相邻子区间上的一阶导数不一样,但整个区间上二阶导数一样，则重新表示f(x+Δx) = f(x) + f'(x)(Δx/n) + f'(x+(Δx/n))*(Δx/n) + f'(x+(Δx/n)*2)*(Δx/n) + ... + f'(x+(Δx/n)*n)*(Δx/n) ,将 f'(x+(Δx/n)*i)用上一个位置的二阶导数表示,逐个递推下去到x处，则 整理 可得 f(x+Δx) = f(x) + f'(x)*Δx + f''(x)*(Δx/n)^2*n*(n+1)/2 , 当n->∞时， f(x+Δx) = f(x) + f'(x)Δx + 1/2*f''(x)*Δx^2   同理，继续假设 每个长度为(Δx/n)的子区间内 二阶也要变，但是三阶不变，这样 可以继续 更精准的 表示 处 f(x+Δx) , 直到   n阶不变,n->∞ ，这样，就 完整精确的表示了f。
				>牛顿法 迭代 和 梯度下降法 迭代的不同：
					>相同和不同：基于 函数的泰勒展开： f(x+Δx) = f(x) + f'(x)Δx + 1/2*f''(x)*Δx^2 + ... 则损失函数 = f(x+Δx) - f(x) =  f'(x)Δx + 1/2*f''(x)*Δx^2 + ... 即关于Δx的函数。如果只考虑到一阶，即认为Δx范围内一阶是不变的，那么 损失函数 = f'(x)Δx, 那么损失函数 跟步长Δx是线性关系，越小越好，每次前进显然对Δx的要求都一样, 没有其他约束关系 --就是越小越好，但这样显然 前进就慢。这就是 梯度下降法的思想。 如果考虑到2阶，即认为Δx范围内一阶是变化的而二阶不变，则损失函数 =  f'(x)Δx + 1/2*f''(x)*Δx^2 ，那么 损失函数和Δx之间不是线性关系，而是二次关系，损失函数关于Δx就有一个极小值, 求导得到取得极小值时,Δx = -f'(x)/f''(x) ，这说明 前进的步长是变化的， 前进到每处，下一步前进多少 都是可以计算出来的；这样可以比梯度下降更快的速度收敛到极值。很明显，只要考虑的阶数更高,3阶,4阶，则计算更复杂，而得出的Δx也更精准，但是这 提升 或许 不是非常的明显---没有从考虑1阶到考虑2阶那么明显。
					>高维下的牛顿 迭代公式： Δx = -[H(f)]^(-1)*▽f , n>=0		
				--二次型：f = x^T*A*x 。  如果 f(x)>0则 f为正定二次型。f(x)<0恒成立则 f为负定二次型。 A一定是方阵, 如果A可以进行 特征值分解，进一步如果A是对称阵，那么可以进行分解为特征向量构成的正交矩阵*特征值对角阵*特征向量构成的正交矩阵。 所以 f = Σλi*xi^2 ， 因此 如果特征值都是>0,则 一定 f是正定的；特征值都<0则 f一定是负定的。
				>Hession矩阵： 就是 多变量函数f(x1,x2...xn)的全部二阶偏导数构成的矩阵， 显然是对称矩阵。A = A^T, 所以 A*(A^T)^(-1)=E, 即 A^T = A^(-1) 所以 此时 上述的 高维下的牛顿迭代公式 Δx = - H(f)^(-1)*▽f 
				>独立推导多变量函数的泰勒级数展开形式 和 二阶近似形式： 仍然用 假定 f(x) 可以表示 为 多变量的组合多项式 表示的形式，然后待定系数法求出。容易得出 一般形式为 f(x + Δx) = f(x) + ΣΣΣ...Σ1/(nn!*n(n-1)!*...n1!)*зf(x)/(зΔx1^n1*зΔx2^n2*...зΔxn^(nn))* Δx1^n1*Δx2^n2*...Δxn^(nn)  ; 求和是对n1,...nn,都是从0->+∞; 如果只是进行二阶近似，则 f(x+Δx) = f(x) + Σзf(x)/зxi * Δxi + 1/2*Δx^T*H(f)*Δx = f(x) + Δx^T*▽xf(x) +1/2* Δx^T*H(f)*Δx  这里的 H(f) 就是 Hession矩阵。
					>多变量下的梯度下降和牛顿法：误差函数还是 f(x+Δx) - f(x),  一阶近似下： J(Δx) = ▽xf(x)*Δx , 此时加上 ||Δx|| = 1的L^2模长=1的约束，则J(Δx)关于Δx的极值在 J'(Δx) = 0 = ▽xf(x) + Δx  时 取得，则此时Δx = -▽xf(x), 这就是 使得 等步长下 变化量变化最大的方向；。。同理，如果近似到二阶不变，则 J(Δx) = ▽xf(x)*Δx + 1/2*x^T*H*x , 则极值 在 J'(Δx) = ▽xf(x) + 1/2*(H+H^T)Δx = 0 约束下,此时 Δx = -H^(-1)*▽xf(x)  这就是 使得 变化量最大的步长方向---因为考虑了在Δx范围内一阶要变化而二阶才不变，所以这个步长 必然是 更优的-更快更短路径到达目标的运动步长。
					>牛顿法的鞍点问题：牛顿法可能寻找到的是鞍点。		
				>梯度消失和爆炸：循环网络会遇到。前馈网络无。				
			>训练深度神经网络的二阶近似方法： 
				>目标函数：平均误差：J(θ) = 1/m * ΣL(f(xi;θ),yi)
				>牛顿法：只适用于 Hession 矩阵 为 正定的情况；鞍点问题的解决，导致 需要 对 Hession矩阵 进行处理：即 对角线上 增加 αI,   即新的迭代公式 : θ	= θ - [H + αI]^(-1)*▽f   
					>挑战2：计算H的逆 复杂。
						>共轭梯度：有效避免Hession矩阵求逆。 方法 是 考察 按照 梯度 前进 的 缺点。
							>第t次的搜索方向dt: dt = ▽θJ(θ) + βt*d(t-1)   这说明t次的方向依赖于 t-1次的方向，并且 可以调节依赖程度；如果 dt^T*H*d(t-1) = 0  则称 dt和 dt-1是 共轭的。
								>Fletcher-Reeves 设计的 βt =  	▽θJ(θt)^T*▽θJ(θt)/(▽θJ(θ(t-1))^T▽θJ(θ(t-1)))
							>最速下降法用在二次代价：
						>共轭梯度算法：
						>非线性共轭梯度算法：
							>BFGS 算法： 拟牛顿法。使用Mt来近似 H的逆。
			>优化策略和元算法：
				>批标准化：
				>坐标下降：一次优化一个坐标，找到一个坐标对应函数的极值；
				>Polyak平均：输出是前t个梯度下降的点的均值：θ^t =  1/t*Σiθ^i   用在非凸问题时，则为 指数衰减计算平均值：θ = αθ^(t-1) + (1-α)^t
				>贪心监督预训练：
				>延拓法：
				>课程学习策略：
		>卷积网络：卷积是一种特殊的线性运算；可以替换隐藏层的矩阵运算，和工程数学上的卷积定义有区别。CNN
			>卷积运算：给定一个函数f(x), 求相关的 另一个函数g(t), 它是对f(x)的自变量小于t的每个函数值进行加权后求和得到的，假设已经到对x1的函数值进行加权, 则此时应该加的权重的大小就设计为要求和t有关 ,如权重的大小和x1到t的间隔有关w = w(t-x), 因此 这个加权函数是w 是一个间隔函数/间隔加权函数/对间隔加权的函数；因此g(t) = ∫f(x)*w(t-x)dx , g(t) = (f*w)(t)自然的积分范围从-∞到t。这种加权平均是一种平滑估计, 是对任意t时刻时对t之前的所有函数值进行一致的加权(无论t为何值,只要和t间隔为a,则受到的加权权重就是固定的w(a))。函数可以当作是一个无限长的向量 或者2*∞的矩阵。 
				>离散形式的卷积：s(t) = (f*w)(t) = Σf(x)*w(t-x) , 这是 x只取离散值，整数值，因此dx=1。
				>二维卷积运算：类似上述的间隔加权原理，s(t1,t2) = ∫∫f(a1,a2)*w(t1-a1,t2-a2)da1da2 = (f*w)(a1,a2) ,  对于离散情况，s(i,j) = ΣΣF(x,y)*w(i-x,j-y)
				>卷积可交换：s(t) = Σf(x)*w(t-x) = 令t-x=a,则x=t-a,即把间隔当作自变量,从而 = Σf(t-a)*w(a) 此时的求和范围改变了，如果都是-∞到+∞，则未变，从而 = Σf(t-x)*w(x)。
				>互相关函数: 顺间隔加权：即 s(i,j) = ΣΣI(i+m,j+n)K(m,n)  前述 则是对核函数进行了翻转flip,然后进行的加权。这里是不翻转加权。
				--用途：检测图像的边缘(图像的像素值看作函数值，则有像素梯度)。稀疏连接。
					>检测图像的边缘：在原图中，i,j位置的点，发现将这个点的周围的像素点的像素值和这个点的像素值的差值总额，如果越大，则越可能是边缘，如果把这个差值总额再当作一个像素值，那么 对每个i,j都这么操作，则可以得到一幅新的图像，显然新图像上，像素值大的，基本就是原图中的明显边缘，像素值小的，则就是一块相同色的区域--则不是边缘；因此这种操作可以把边缘找出来。。回想总结 从 原图到新图的i,j点的转换过程: 减去周围像素值，假设就是3*3，即周围就8个像素，则减去这8个像素的差值和 可以表示为： 8I(i,j) + Σ(-1)*I(i-a,j-b) 进行合并，则相当于i,j为中心的3*3的元素分别和[-1 -1 -1;-1 8 -1;-1 -1 -1] = K矩阵的对应位置元素相乘而求和得到，即哈马达乘积后的所有和，也即 tr(A⊙B);  = ΣΣI(i+a-2,j+b-2)*K(a,b)  其中a,b都是1->3
			>神经网络中使用卷积运算层的特征：
				>稀疏连接：每个输出只和 部分输入有关(核数相等的输入数),直接连接是稀疏的，但是 多层卷积之后则几乎和所有输入都相关。
				>参数共享：每个输出 因为只和特定数目的输入有关，而 这些数目定的输入的权重矩阵 都是一个相同的矩阵--核函数矩阵。这也使得 该层网络具有 平移等变性质，即如 图像向右平移后 再进行 卷积运算得到的结果 和 先卷积运算再进行平移 的结果一样；或者说 输入平移了，则输出 相当于对原来没平移的输出进行平移 的结果，即输出 也会跟着平移。
					>卷积对平移等变：但对 旋转和放缩 不等变。
			>池化：如最大池化，即取输出的 相邻范围内的最大输出，这样 输入做少量平移时，输出保持不变/近似不变。   即最大池化 对 平移 也近似有 等变性。
				>所属层：卷积--非线性变换-池化。
				>用途1：对大小不同的输入 进行 统一规格：
			>卷积和池化作为一种无限强的先验：来帮助了解 卷积网络 如何工作的。如了解到 卷积 和池化可能导致欠拟合。	
			>基本卷积函数的变体：单核卷积提供一种特征。
				>分形和张量的关系：
				>多通道卷积：Z(i,j,k) = ΣΣΣV(l,j+m-1,k+n-1)*K(i,l,m,n)  很自然的得出 зZ(i,j,k)/зK(i,m,n) = ΣV(l,j+m-1,k+n-1) , зZ(i,j,k)/зV(l,m,n) = K(i,l,m-j+1,n-k+1) 
				>下采样：步幅s,	Z(i,j,k) = ΣΣΣV(l,(j-1)*s+m,(k-1)*s+n)*K(i,l,m,n) 步幅=1就是没有采样时候的公式；
				>零填充加宽输入：这样在 计算到边界时，不够宽则补充0元素 再 和 核矩阵运算。来让输出输入保持相同的大小。如果对每个边界都这样操作，那么输出就会更大。
				>非共享卷积：输入的不同位置的权重核不一样。
				>平铺卷积：有t个不同的核；依次作用于输入，然后循环；。传统卷积就是 t=1的平铺卷积。
			>结构化输出：输出高维的结构化对象；。。不仅仅是预测 分类任务的 类标签 或 回归任务 的实数值。
			>高效的卷积算法：利用卷积 乘法的 特征，而用傅里叶变换 处理 这个 卷积表达式，然后进行具体的计算，然后 再 执行 傅里叶逆变换。	
			>随机或者无监督的特征：使用无监督的标准来学习 特征。	
				>监督的贪心逐层预训练：大多数卷积网络 以纯粹监督的方式训练，每次训练迭代中使用通过整个网络的完整的前向和反向传播。	
				>Garbor函数所描述的权重：w(x,y) = αexp(-(βx*x'^2 + βy*y'^2))cos(fx'+Φ)  其中  x' = (x-x0)cost + (y-y0)sint;   y' = (x-x0)sint + (y-y0)cost;  x'可以看作是 以 (x0,y0)作为坐标轴原点，平行于原轴建立x-y轴，的新坐标系下 点到 直线 sqrt(2)/2*(x+y)=0的距离, 同理,y'则看作是点到垂直于这条直线也经过原点的直线的距离，而 exp(-(βx*x'^2 + βy*y'^2)) 甚至可以看作 新坐标系下 以新原点为中心的椭圆线为等值线,					
		>序列建模-循环和递归网络：RNN  。在几个时间步内 共享相同的权重(无需学习序列的每个位置的所有规则)。							
			>在一维时间序列上使用卷积：						
			>展开计算图：动态系统的经典形式：s(t) = f(s(t-1);θ)  循环定义。带外部信号x(t) 驱动，则 s(t) = f(s(t-1), x(t),θ)		或者  h(t) = f(h(t-1),x(t),θ)			
			>循环神经网络：	在不同的时间步上共享参数。
				>模型1： 
					>隐藏单元到隐藏单元：h(t) = tanh(W*h(t-1) + U*x(t) + b)  ;  
					>输出: o(t-1) = V*h(t-1) + c;  
					>误差函数: L(t-1) = y(t-1) - softmax(o(t-1)); y^(t-1) = softmax(o(t-1)); 总的误差= ΣL(t) 目标仍然为最大似然误差函数 并且 最小化。
				>模型2：
					>输出到隐藏层的反馈连接：h(t) = tanh(W*o(t-1) + U*x(t))
					>输出：o(t) = h(t)*V
					>误差函数：	L(t-1) = y(t-1) - softmax(o(t-1))
				>梯度的计算：目标：	зL/зU, зL/зV, зL/зW,зL/зb, зL/зc 。方法：反向传播。 最后一级的梯度：зL/зL(t) = 1, 然后是对o(t)： (зL/зo(t))i = зL/зL(t)*зL(t)/зo(t)i = y^(t)i - 1 ,  因为h(t)->h(t+1)->L, h(t)->o(t)->L; 即两条线相关到L,所以 зL/зh(t) = V^T*▽o(t)L + (зh(t+1)/зh(t))^T*зL/зh(t+1) = V^T*▽o(t)L + W^T*diag(1-(W*h(t))^2)*▽h(t+1)L  接着就是计算具体的：
					>зL/зc = Σ(зo(t)/зc)^T*▽o(t)L = Σ▽o(t)L  对时间t求和，因为 c经过所有t的路径来和L相关联--即都有贡献。
					>зL/зb = Σ(зh(t)/зb)^T*▽h(t)L = Σdiag(1-h(t)^2)▽h(t)L 
					>зL/зV = ΣΣ(зoi(t)/зV)^T*▽oi(t)L = ΣΣ(δim*h(n))▽oi(t)L = Σh(n)*▽om(t)L = Σ▽o(t)L* h(t)^T  先对i,t求和
					>зL/зU = ΣΣ(зhi(t)/зU)^T*▽hi(t)L = ΣΣ(δim*(1-hi(t)^2)*xj(t)*δjn)▽hi(t)L = Σ(1-hm(t)^2)*xn(t)*▽hm(t)L = Σdiag(1-h(t)^2)*▽h(t)L*x(t)^T
					>зL/зW = ΣΣ(зhi(t)/зW)^T*▽hi(t)L = ΣΣ(δim*(1-hi(t)^2)*hj(t-1)*δjn)▽hi(t)L = Σ(1-hm(t)^2)*hn(t-1)*▽hm(t)L = Σdiag(1-h(t)^2)*▽h(t)L*h(t-1)^T
			>双向循环神经网络：应用如 手写识别、语音识别。						
				>模型1：
					>隐藏单元到隐藏单元1：h(t) = tanh(W*h(t-1) + U*x(t) + b)  ;  
					>隐藏单元到隐藏单元2：g(t) = tanh(K*g(t+1) + U2*x(t) + b2)
					>输出: o(t-1) = V*h(t-1) + V2*g(t-1) +  c;  					
					>误差函数: L(t-1) = y(t-1) - softmax(o(t-1)); y^(t-1) = softmax(o(t-1));				
			>基于编码-解码的序列到序列的架构： 将输入序列映射到 不一定 等长的 输出序列。应用如语音识别、机器翻译。						
				>编码器RNN: 只有隐藏层没有输出层，对于输入序列{xi},最终的隐藏层状态h(t) 就是 编码器RNN的输出C：即 输入序列的语义概要。C是固定大小的向量。
				>解码器RNN: C+上一时间输出+上一时间隐藏层状态  三大信息 来作为 当前时间的隐藏层的输入；而输出 是隐藏层+C
				--注意力机制：将输出和序列C相关联。					
			>深度循环网络：多层循环网络组合。
			>递归神经网络：树状结构。用途：学习推论。输入是数据结构，如 自然语言处理和计算机视觉。	
			>回声状态网络：避免学习W/U的 设定循环隐藏单元的方法；。流体状态机 也是这种思想。形成了 捕获输入历史的不同方面的 临时特征池。
				>储层计算循环网络：类似 核 机器。
				>反向传播中的Jaccobian矩阵的谱半径：与收缩向量有关的矩阵的特征。
				--连续矩阵乘法 可能导致的 特征值爆炸或者消失： 通过 饱和非线性单元 如 双曲正切函数 tanh  的稳定作用 的 使用 而 避免。
			>渗漏单元和其他多时间尺度的策略：
				>渗漏单元：模拟 滑动平均行为的 线性自连接 的 隐藏单元。滑动平均：μ(t) = αμ(t-1) + (1-α)*v(t) 
				>门控RNN: 包括：基于长短期记忆LSTN 和 基于门控循环单元GRN的网络。
					>LSTN ：
						>模型1： 输入：Ct-1, ht-1, xt, 偏置bf,bi,bo, 输出：Ct,ht 
							>Ct和Ct-1的关系： Ct = C(t-1)*σ(Wf*[h(t-1),xt] + bf) + σ(Wi*[h(t-1),xt] + bi)*tanh(W*[h(t-1),xt] + b)  门的目的：增加或者去除信息到细胞状态。
							>ht和ht-1的关系： ht = σ(Wo*[h(t-1),xt] + bo)*tanh(Ct)
					>GRU: 
						>模型1： 输入：ht-1,xt, 偏置br,bh,bz, 输出：ht 
							>ht和ht-1的关系： ht = h(t-1)*(1-σ(Wz*[h(t-1),xt]+bz)) + tanh(Wh*[h(t-1)*σ(Wr*[h(t-1),xt]+br),xt]+bh)*σ(Wz*[h(t-1),xt]+bz)
				--全连接的本质：实现了 对输入空间的划分 ，不同区域不同标签。 
				--卷积连接的本质：实现了 对输入张量 的 每一处的某种局部特征的提取/度量(通过对每一处的周围的分量值的分析计算)，如每一点的边缘特征的度量/提取。
				--循环连接的本质(根本效果/根本作用/根本功能)： 输入 n个有序的向量xi,  对每个xi的输出yi是 一个 各类别的概率的向量， 如果 当前输出类别的概率yi 不只是取决于当前的输入向量xi, 而还和上一个输入向量x(i-1)有关系--占一定的比例，那么就有必要把上一个输入向量x(i-1)变换后的中间量h(i-1)保留下来, 而在这次 输入xi时以一定的比例W作用后 和 xi变换后的结果 加起来 一起 经过 V的作用 而作为改进后的输出，则新的输出  就 是 由 本次的输入xi又和上一次的输入x(i-1) 共同 各以一定的比例来决定的了。同理，上一步，上k步也可以考虑进来；未来一步,未来k步也可以考虑进来，认为可以影响 当前输入xi时的输出。
																实现了把 第 i次输入的 前后k次输入都考虑进来 共同 决定 当前的 类别输出概率 向量 的 想法。(即类别跟上下文有关;类别跟前后k个输入有关)(普通神经网络的想法是/则认为 对应 当前输入的 类别概率 输出 仅仅 由 当前输入 决定)(隐藏层的输入包含了上一次的隐藏层的输出的某个比例处理后的值--即不是把上一次的隐藏层输出值原原本本输入)
				>向量对向量的作用：归结到用线性组合 来解释：1.点到直线的函数距离。2.向量到另一个向量上的投影/坐标。3.到各个特征上的距离(一条直线代表一个特征)。
				>矩阵对向量的作用：1.信息的丢弃。2.对向量的放缩和旋转。3.对任何向量都进行一样的对每个维度进行不同程度的放缩(对角矩阵)。4.对点进行了坐标系变换。
					>旋转作用：用2阶矩阵可以推导，x^T*A只发生旋转作用时，A是一个关于行向量的单位正交矩阵，可以推广到n维向量。反向旋转复原：则即 x^T*A*B = x^T, 则 A*B = I, 自然的，B=A^(-1), 因为B也只发生了旋转作用，所以B也是一个单位正交矩阵；又因A是关于行向量的单位正交矩阵，所以根据定义可以得出A*A^T = I, 则 B = A^T = A^(-1), 因此 复原 矩阵 就是 原单位正交矩阵的转置矩阵。
						>复原矩阵：是原关于行向量的单位正交矩阵的转置矩阵。因此：关于列向量的单位正交矩阵 也只具有旋转作用。简单的实验：可以推导出二维下的 旋转矩阵：[sinθ cosθ; sinΦ cosΦ] ,其中θ-Φ=Π/2+nΠ 其转置矩阵[sinθ sinΦ; cosθ cosΦ]也是旋转矩阵，且旋转方向刚好相反。
						>特征向量：是 乘以旋转矩阵刚好转到 某一条轴上的向量；即和轴的夹角 刚好等于Π/2-旋转角度。
						>旋转角度的决定因素：等于 单位正交矩阵表达的新坐标系 相对于 原坐标系 旋转的角度。
					>放缩作用：用2阶矩阵推导，x^T*A 只发生放缩作用，且各个分量都发生相等的放缩，则A只能是对角元素相等的对角阵aI, 而如果恒定能对任何向量各个维度发生固定的不相等的放缩作用，则A是对角矩阵即可。
					--而矩阵A如果是实对称矩阵，则可以做单位正交分解：A = QΛQ^T , 即 对向量先旋转k度 后 各个分量放缩 再旋转回去k度。	
				>激活函数可以过滤信息的原因：压缩了两端的信息,使得输出都差不多,即对范围外的数据不敏感。让输入域内的某个范围内的数据的对系统的输出都几乎一样-很大或者很小,总之没有梯度/几乎消除了这部分区域上的梯度/这部分数据的梯度接近于0，就属于一种过滤。
				--RNN的用途：1.根据上下文-输出下一个词语/下一句话/下一篇文章。
				--RNN的想法：		
				>梯度截断：	想法 大约是 让步长 受到限制。				
			>外显记忆：			
		>实践方法论：模型、目标、训练算法。
			>常见动作：收集更多的数据、增加或减少模型容量、添加或删除正则化项、改进模型的优化、改进模型的近似推断、调整超参数、
		>应用：深度学习的基本思想/想法是 ：基于联结主义：尽管 单个神经元 不是智能的， 但是 	大量的 神经元 或者 特征 作用在一起 往往表现出 智能。						
			>语音识别模型：输入 20ms采集的语音向量 X: 输出 最有可能的语言序列： argmax P(y|X=X)  待定y  ，即在给定X的条件下  y的分布 确定了，然后取 概率最大的y; 。。受限玻尔兹曼机 + 整流线性单元 + Dropout
				>其他模型：隐马尔可夫模型(对音素序列建模) 和 高斯混合模型(对声学特征和音素之间的关系建模)。
		>深度学习研究：							
		>线性因子模型：描述一个数据生成过程：隐向量随机变量h, 满足 h ~ p(h) = Πp(hi) 即各个因子出现xx模式/空间点的概率；	而 x = Wh + b + noise 其中 noise服从高斯分布，则x随机变量的分布为：						
			>为什么说矩阵乘向量 是对向量的线性变换：因为 Ab = x, Ab2 = x,  如果 det(A) != 0, 则 b=b2, 因此是一一变换；即两个原空间的点不会变换到新空间上的同一点；
			>概率PCA和因子分析： 噪声和 h的先验分布 不同 可以确定 多种 模型；
				>因子分析：潜变量的先验分布h~N(h;0,I);  而 噪声满足正态分布，且期望为0，则 E(x) = b,  方差σ^2矩阵 = E(x^2)-E(x)^2 =  E[(Wh+b+noise)(Wh+b+noise)^T] - b^2  = E[Whh^T*W^T + noise*noise^T] 即其中如果有h和noise的单项的，因为它们独立且期望为0，所以都是0，就只剩下了两项； = E[WE(hh^T)W^T] - Φ = WW^T - diag(σ^2) 如果每个噪声分量的范畴都相等，则 = WW^T - σ^2I; 所以  x~N(x; b,WW^T+σ^2I) 同时x = Wh + b + σz, z是一个N(z,0,I)分布的高斯噪声； 
			>独立成分分析：	建模线性因子的方法，目的在将观察到的信号分离为 许多 潜在信号，它们通过缩放和叠加 可以恢复原信号；ICA。 可以推广到 非线性生成模型；				
				>Garbor滤波器：
			>慢特征分析：SFA。使用来自时间信号的 信息 学习 不变特征 的 线性因子模型；是输入空间到 特征空间的一个线性映射；	
				>SFA算法：定义线性变换f(x;θ), 目标：min Et[f(x(t+1)i - f(x)i)]^2  满足约束 Et[f(x)i] = 0 即 每个分量在时间维度的期望都是0；(隐含: 分量是随机变量) 且有 Et[f(x)^2i]=1
			>稀疏编码：使用线性解码器 + 噪声 获得 一个 x的重构；p(x|h) = N(x; Wh+b;1/β*I)			
			>PCA的流形解释：学习一个流形。
				>训练任何有W/V的线性自编码器的目标：使得重构的x尽可能接近于原始的x: argmin E[||x - x^||^2]
					>编码器：h =f(x)= W^T(x - μ)
					>解码器：x^=g(h)= b + Vh  
		>自编码器：类似过滤器。选择性的输出输入，并且对输入进行了重构后输出(消除了噪声?)。			
			>传统自编码器：用于降维或者特征学习。是前馈网络的一个特例；：训练算法为：小批量梯度下降算法；
			>欠完备自编码器：目的 使 h 获得有用的特性；
				>方法：限制h的维度比x小；强制自编码器捕捉训练数据中最显著的分布特征；有关数据分布的特征；容量不能太大；
				>目标：最小化损失函数 min L(x, g(f(x))) L 可以是均方误差；
			>正则自编码器：目标在训练任何架构的自编码器。特性如：稀疏表示、表示的小导数、对噪声和输入缺失的鲁棒性；
				>变分自编码器VAE：学习高容量和过完备的模型；被训练为 近似 训练数据 的 概率分布。GAN也有类似功能。target=argmin||x-x'|| = argmin ||x - DEx|| 其中E就是编码矩阵,D就是解码矩阵。
					>线性自编码器：学习到的 基 不一定是正交的(因为E矩阵的行向量不一定是正交的)。而PCA计算得到的最佳子空间的基则是正交的。
					>深度网络编码器：输入数据 经过 深层网络编码后--即乘以层层的矩阵后,输出的值只有某个分量不是0，其他分量基本都是0或者非常接近0.  如果其他分量都是0那么 这可以说是最佳的压缩了/只有唯一的主成分/某种奇怪的新的度量(可以区分所有的数据),并且逆变换后可以完全恢复数据 而 没有损失； 或者压缩到几个维度(通过这几个维度也可以完全的区分数据)(维度更多则暴露的信息量也更多)，但是维度过低 会导致严重的过拟合---隐空间/编码空间中的很多点解码后毫无意义。----因此要增加隐空间组织结构的约束 到 训练中--方法就是 训练中 显式的 正则化/正规化---这就是变分自编码器的思路。
					>核心思路：将输入 编码 为 以 Ex 为均值的 一定方差的高斯分布 上 随机采样的一点，而不是 直接就是Ex 。具体做法就是 让 损失函数 loss = ||x - DEx|| + KL(N(μx,σx), N(0,1))  后面一项 就是 KL散度 正则化项。KL散度: 相对熵,衡量两个分布的差异/距离;KL(p(x),q(x)) = Σp(x)log(p(x)/q(x)) 可以认为是概率比在p下期望。	
						>正则化之后会产生的效果：1.连续性：隐空间中相邻的点解码后内容基本一致/相似.2.完整性: 针对给定的分布，从隐空间采用的点在解码后都是有意义的内容。
					---一个函数的期望表达式的正确理解：Ez~q(z|x)[logp(x|z)] 包含 3部分信息。正好对应函数的期望定义的三部分信息：随机变量,随机变量的分布,随机变量的函数。或者说 函数表达式，自变量是哪个随机变量，随机变量满足什么分布。在这个式子里，随机变量/函数自变量 就是 z, 服从的分布就是p(z|x), 函数是 logp(x|z)
					>目标：给定x的条件下推出z的分布：p(z|x)  
						>思路：用一个可解的分布q(z|x)来近似 p(z|x), 近似的标准：K-L散度尽可能的小: min KL(q(z|x)||p(z|x))
							>已知:p(x|z), 所以需要将条件和目标进行联系转换。从目标开始: KL(q(z|x)||p(z|x)) = ∫q(z|x)*log(q(z|x)/p(z|x))dz = ∫q(z|x)*log(q(z|x)*p(x)/(p(x|z)*p(z)))dz = ∫ + ∫ + ∫ = logp(x) + ∫q(z|x)*log(q(z|x))dz - ∫q(z|x)*log(p(x|z)*p(z))dz = 则很明显最小化这个则第一项是常量固定无可调整；因此后面 = ∫q(z|x)log(q(z|x)/p(x|z))dz - ∫q(z|x)*log(p(z))dz = KL(q(z|x)||p(z)) - Ez~q(z|x)[log(p(x|z))] = Ez~q(z|x)[log(p(x|z)) - KL(q(z|x)||p(z))] 注意此时 z是服从q(z|x)分布的, 而这个分布是人为相似确定的-且输入x已经确定，所以可以进行采样，采样后则 logp(x|z) 表示的是一个重构概率-已知z时出现x的概率-显然第一项的目标就是让它最大化(重构出x的概率的期望最大化)；第二项则是要让 后验分布p(z|x)和先验分布p(z)尽可能的接近。如果pq都是高斯分布，那么第一项的含义 就是  平方误差的高斯分布均值了。
				>生成随机网络：
				>稀疏自编码器：
				>去噪自编码器：
				>收缩自编码器：误差函数增加了一个 正则项: 鼓励编码函数的导数尽可能小:Ω = ||зf(x)/зx||^2,F 值 相当于一个 矩阵的元素的平方和，这个矩阵就是 jaccobian矩阵。
				>预测稀疏分解：学习近似推断的特例。优化目标完全不同了：||x - g(h)||^2 + λ|h|1 + γ||h - f(x)||^2
				---区别概率和分布：P(yt'|y1,...y'(t-1),c) 表示在yi,c这些随机变量的值确定后随机变量yt'的分布； P(A|B)则就表示B事件发生条件下A事件发生的概率；yt‘是随机变量,A是事件。
				---seq2seq 模型的 编解码器：功能：输入一个序列，输出另一个序列；如机器翻译从一个语言的句子 翻译为 另一种语言的句子。
					>编码器：循环神经网络。目的：把不定长的输入序列 变换为 定长的背景变量C;  
						>过程： 生成隐藏状态序列 ht = f(xt,ht-1)   然后计算背景变量  c = q(h1,...hT)
					>解码器：循环神经网络。目的：利用背景变量和t时间步之前的输出 来 计算 t时间步 的输出 yt; 这需要先计算t时间步的输出yt的分布：P(yt|y1,y2...yt-1, C)
						>过程： 生成隐藏状态序列 st = g(yt-1, c, st-1)， 然后计算 yt的分布：P(yt|y1,y2,...yt-1,c)  即 每个时间步都可以计算输出的分布(利用此时间步之前的输出)，并选择一个作为这个时间步的输出--显然选择的这个输出对应的概率是明确的；。那么到时间T为止，所有输出步的概率之积就是 这个输出序列的出现的概率，显然对于不同的输出序列，输出序列出现的概率是不同的；其中最大的那个概率对应的输出序列 就是 预测的最有可能的输出序列----来作为预测的输出序列；
							>上述表达了预测的输出序列应该如何计算：而这是在 f,g 已经明确了的前提下；如果f,g本身就是未知的就是求解的目标，那么fg的计算方法是什么，fg的计算过程 就是 模型训练的目标。训练的结果就是一个具体的f,g; 训练方法，自然 也应该和上述有关，就是这个模型 的 所有的输出序列中的最大概率 应该尽可能的大；而所有的输出序列的概率的平均值也应该尽可能大---相较之下 把 这个 平均值 的最大化 作为 训练目标 可能更好； 输出序列的概率=P(y1...yt|x1...xt) = ΠP(yi|y1..yi-1,x1,...xt) = ΠP(yi|y1,y2...yi-1,c) , 损失函数 = -logP(y1,y2...yt|x1,x2,...xt) = -ΣlogP(yt|y1,y2,..yt,c)
							>寻找概率最大的输出序列的方法/尝试：
								>贪婪搜索：在t时间步上选择输出yt的算法： yt = argmax P(yt|y1,y2,...yt-1, c) 即 分布中的概率值最大的变量值作为yt。时间复杂度O(|y|*T)
								>穷举搜索：穷举所有的输入序列，找出条件概率最大的序列。	时间复杂度O(|y|^T)
								>束搜索：在t时间步上选择概率值最大的k个变量值 来作为候选值，对每个候选值都进行试探,来找出 这个约束下的 所有可能的输出序列的概率的最大值对应的输出序列。时间复杂度O(k^T*|y|), 当k=1时，公式重新计算，得时间复杂度=O(T*|y|)
							>Blue得分： 对预测序列的效果/精确度的一种评价度量。先定义：词数为n的子序列的预测精确度Pn = 预测序列中长度为n的子序列也在输入序列中的个数/预测序列中长度为n的子序列的总个数 ，  显然 当预测序列如果很短 那么 预测精确度都会较高，但是可能有大量的输入单词都没有预测到，所以要纠正一下/惩罚一下，这个惩罚则跟 len(label)/len(pred)呈负相关,如定义为exp(min{0,1 - len(label)/len(pred)});    所以定义 精确度 Blue得分： = exp(min{0,1 - len(label)/len(pred)}) * Πpn^(1/2^n) 是比较合理的。
				---用来 降维：
				---用作 数据去噪：
				---用来 作为 特征检测器：应用于 深度神经网络的预训练。
				---用作 生成模型： 生成 和 训练数据 类似 的 数据。如  输入人脸图片 输出新的人脸图片。
				---主成分分析 得到的 数学理解：以前用n个属性来区分数据，仔细分析后发现k个属性就能区分了,(且排在前面的对数据的区分能力越强,说着说数据在这个维度上分散得很开,熵比较大;而不是取值都差不多而聚集在一起)。
					>两个维度相关 的表现： 画出 这两个维度 的 直角坐标系 下 数据的分布，则会发现数据 呈现条状分布---即在某条直线周围。而用回归方法找出的那条直线 就认为是 两个维度的 线性关系。而将数据点到这条直线的距离 定义为 新属性/特征。	
					>一组数据/一组样本/一组数据样本：已经说明了数据是多维度的，也说明了数据是有分布的---否则应该全部是相同的。即数据是 随机变量的取值，如果数据是多维度的，那么数据就是随机向量的取值。即数据集 是随机向量的 n次取值。
					>标量的期望和方差：E(x)= Σp(x=xi)*xi  即所有可能的取值乘以该取值的概率，然后求和。var(x)=E((x-E(x))^2)= E(x^2) + E(x)^2 - 2E(x)^2 = E(x^2) - E(x)^2 ，不管正负-和均值点的距离的平方的期望；既然是距离的期望，所以这个距离期望越大，则表象就是数据分散的越开，两者正相关, 所以 方差反应 随机变量分布的分散程度；
						>样本集的均值和均方差：
					>两个标量的协方差：cov(x,y)= E((x-E(x))*(y-E(y))) = E(xy)-E(x)E(y), 在这两个标量形成的直角坐标系上分布数据点, 计算过程 就是 取值点到期望中心点的连线构成的平行于以期望中心点为原点的直角坐标系K的矩阵的函数面积的均值；显然，如果两个数据点关于坐标系K的某条坐标轴对称,则它们的这个函数面积和就是0；所以，如果数据点均匀分布在坐标轴周围, 那么协方差就是0, 根据函数面积=dxdy=k的特征，则关于轴对称的两条反比例曲线上的分别一点作为数据点则函数面积和都是0；；考虑到期望中心点基本 都是 使得数据点关于它原点对称的，所以在原点对称的条件下还能划分为关于坐标轴对称，则数据点至少关于原点旋转90°对称；越和这种特征相似，则协方差就越小；而越是仅仅关于原点对称,或者说关于原点旋转越大于90°对称，则协方差绝对值越大，且关于原点对称的数据点如果分布在第一三象限则协方差为正，分布在二四象限则协方差为负，而关于原点对称正好让数据点回归的直线的斜率的正负和协方差的正负一一对应，而回归直线代表了数据点x-y维度的线性关系，在这个线性关系的周围都说明了其中一个维度的随机变量的取值确定的情况下会影响到另一个随机变量的取值分布(分布骤然收缩到某个值的周围;比如y=kx+b+Ω 即多的一个噪声分布;而一般情况下y是服从类似x的分布的一个分布)；在有线性相关分布特征--即仅原点对称的条件下，对同样的数据进行绕原点旋转,k趋近于0或者∞协方差都小，而在k=1周围的时候即x和y的取值接近的时候协方差更大(尽管间隔距离相同)；相同直线周围，而原点对称点之间距离相隔越远则协方差也越大，所以也体现了分散程度的特征；
						>协方差体现出的数据点之间的特征：协方差可以反应/刻画的数据的分布：线性程度、分散程度、偏向程度(偏离y=±x这个中心线的程度)。线性强，分散大，偏向低，则协方差的量值就大。比如如果 y=kx+b,则 cov(x,y)=k*var(x) 这里解释偏向低(分散程度和线性程度固定)：当k很小时则因为x分布更分开则方差更大,同理k很大则x分布更窄则方差更小,两种情况都有偏向,因此相关关系认为不明显,反而是在中心的时候k不大方差也适中--导致乘积反而相对更大--相关关系更明显。显然，当k=1时 数据的分散、偏向、线性都体现的很明显，此时的结果就是x的方差。即相关程度好比x和x即自己和自己的相关程度。
					>向量的期望和方差：
						>多维样本集的均值和均方差：
						>向量的不同分量之间的相关性：则需要cov(xi,xj) 共有n*n个量/协方差来描述，自然的将它们形成一个协方差矩阵C。而对角元素的值就是 各个维度变量的方差。而已知 对角矩阵 可以分解为 QUQ^T 其中 U是对角阵，而Q是单位正交矩阵。而如果x是A的特征向量，显然Ax=λx=QUQ^Tx ,很显然从Q是旋转矩阵的角度看，则必然x在旋转之后旋转到了某一条轴上,然后*U就是对这条轴上的值进行放缩特征值λ,然后旋转回来，旋转到某条轴上，则x必然和Q的某行平行,而后面放缩时系数为λ，则x必然和Q的平行的那一行相等,从而Q^T的每一行就是特征向量。当协方差矩阵C被分解为这个形式后，则看出一些性质。
						>降维： 数据点 原始 在 某个坐标系下就是原始的坐标取值，当在原点旋转坐标系到某个角度时, 数据点在这个新的坐标系下的坐标点的各个维度分别称为 主元1，主元2,...如果数据呈现线性强,那么此时数据点的坐标会发现某个主元的值都很大,另一个主元的值则都很小且几乎都是0/都相等。则用这个值都大的主元值 可以一定程度上表示数据点的特征,可以 用来代表 各个数据，那么数据的维度就降低了1维度了。
							>主成分的含义：因为数据点到原点/某一点的距离是固定的，所以旋转坐标系之后，新坐标系中某个分量大，则另一个分量则小；
							>用新坐标系下的数据恢复出原坐标系下的数据：比如新旧坐标系的关系：e'=Pe, 则 P^(-1)e'=e;  那么 原坐标系下的数据[a b]e 在新坐标下则表示为 [a b]P^(-1)e' 如果 [a b]P^(-1) = [m n] = [m 0] + [0 n] 形式，即分解为两个成分，其中[m 0]为主成分，那么 恢复到原坐标系中则 =[m n]*Pe= ([m 0] + [0 n])*Pe = [m 0]Pe + [0 n]Pe 那么显然 如果[0 n]不是主要的成分，比如n非常小或者为0，那么[0 n]Pe 的值就近似为0 或者是各个分量值都非常小的向量，即对结果的贡献很小，那么构成结果值的过程中，显然[m 0]的比例就最大,因此认为是形成结果值的主要原因主要因素--主要成分。因此，目标就是找到一个合理的变换P,使得新坐标系下 数据点的成分区分程度较大 而 明显显示出 主成分。标准就是区分度 达到最大,或者说分量值尽量集中在某个轴上,而轴无区分意义不大所以不妨x轴上。 
							>怎样的分量才能被认为是主成分：旋转坐标系后新坐标下的数据的横坐标的值的平方和 达到最大时 ,此时数据点在坐标系下的分量才是各个数据点的主成分。因此首先计算 整个目标 坐标系，通过 表示出 新坐标系的基(e1,e2...) 再表示出 基下的第一轴坐标分量的平方和(ΣXi^2=Σ(e1*[a1i,a2i,...]^T)^2)=Σakie1k*amie1m=ΣΣΣakiami*e1ke1m=Σe1^T*Mi*e1=e1^T(ΣMi)e1, 其中Mi就是单个点的原坐标系下的各个坐标值互相乘形成的矩阵--显然是对称的,令ΣMi=P,则P也是对称的,而 新坐标下第一轴的平方和=e^T*P*e,将P的对称性利用进来:即P=U^T*Σ*U其中U是单位正交矩阵,所以Ue仅仅对e进行了旋转--结果也是单位向量,令n=Ue则|n|^2,2=1,即Σni^2=1, 则原平方和=n^T*Σ*n=Σσi*ni^2 其中σ就是对角阵的对角元素,显然假设ni^2=ki>=0那么就会看出原平方和是个线性组合的和,而且有约束Σki=1,则就是在一个超平面区域求一个线性组合的最大值最小值-则一定在边界上-很明显各个分量都很对称-没有其他关系-所以一定在轴上,因此最大值=max{Σσi}最小值=min{σi},而此时的n的取值=[0 0 0 ...1(ni) 0 0...0],则可以计算e1=U^T*ni即是第U^T的第i列--即是P的第i个特征向量:或者说σi对应的特征向量, 或者说最大特征值对应的特征向量, 就可以发现 它的最大值 就是P的最大特征值,以及此时的新坐标系的这个最大分量的轴:最大特征值对应的特征向量。同理最小值和最小分量的轴。得到最大最小主元 后 自然可以得到各个数据点在这两个主元上的坐标值。
								>观察P可以得出的新结论：注意到P的对角线上的元素 其实 是平方和, 想到方差中也有平方和，而非对角线的相乘和，则形如协方差,实际计算则发现 对角线元素=n*数据点在原坐标系下第i轴的取值的均值为0时的方差,非对角线元素 则是=n*数据点在原坐标系下第ij轴的取值在两个轴的均值都是0时的协方差，因此 P = n*原坐标系下数据点的各个维度值在均值为0时的协方差矩阵Q。。。而注意P其实正是 在 均值为0的条件下推导出来的，所以 均值为0的条件下的结论，其实可以推广到均值不为0的条件下----此时最大化目标就是减去均值后的平方和最大: 即 Σ(Xi-X_)^2 不妨假设 将X_分解为n份:X=Σmi*ei或者说这才是本来均值点的本来面目,则 Xi=Σei*(a1i-mi) , 因此 P中的每一项akm都重写为ak1-ml , 因此 P任然可以解释为 n*原坐标系下的数据点的协方差矩阵Q。即P=nQ
						>最小二乘法：使得数据 到某个值的 误差的绝对值的 总和 最小的 那个值认为是均值是合理的，为方便计算，误差的绝对值 改为 误差的平方 不会改变 均值的计算，因此 计算 使得 误差平方和 最小的 值 就是 均值。得出均值=1/n*Σyi。。。推广到 多维数据，就是数据的拟合，找到一条曲线，使得点到曲线的距离总和最小。可见：最小二乘法 是 让 总误差函数最小 的参数值为均值；而最大似然法 则是 让 数据出现的概率最大的参数为参数实际的取值(如均值)。(高斯推导随机误差分布 没有使用最小二乘法,而是在最后引进了xi具体化约束条件-x,-x...-mx来得出了-f'(x)/f(x)=g(x)的线性关系: m*g(x) = g(mx)两边同时求导得出g(x)=kx的结论)(实际也是常见的对求和的简化处理:一种分支情形:每一项相等/多项相等)
						>谱定理：V上的算子 关于 某个 正交基 有 对角阵  当且仅当 这个正交基 是由 该算子 的特征向量 组成的。
							>零空间：Ax=0的所有x构成的空间；显然每个0特征值对应的特征向量 就构成了 零空间。
							>伴随矩阵：如果算子/矩阵 T: V->W 即实现将V空间的向量v变换到W空间的向量w; 如果 还有T*: W->V 即实现将W空间的向量w变换到V空间的向量v; 且满足内积关系：<Tv,w> = <v,T**w> = ΣΣTij*vj*wi = ΣΣ(T^T)ji*wi*vj对所有的v,w都成立；则T*是T的伴随。A如果是实矩阵则意味着T*=T^T
							>等距同构：是一种线性变换运算/算子： 使得变换前后2范数不变(只旋转不放缩)：||Av||=||v|| 容易证明 实内积空间中就是 正交算子/正交矩阵(直接利用2范数相等可以得出矩阵的行向量是单位的且正交的,再利用3关系4个变量而得出列向量也是单位的且正交的;并且行向量和列向量之间的关系表明它是一个对称矩阵;再利用行向量单位且正交的性质得出A*A^T=I;即逆矩阵就是矩阵的转置,因此综合得A^2=I,A^n=I)。
							>极分解：任何方阵可以分解为 正交阵 和 半正定阵的 乘积(即变换方向*变换长度)。T = S * sqrt(T**T)  平方根定义：R^2=T,则R是T的平方根。
							>谱分解：任何对称矩阵的分解。T = Σλi*Ti 其中Ti就是T的特征向量(它们是单位正交的)
							>奇异值分解：任意矩阵的分解。任意矩阵T的奇异值 就是 sqrt(T**T)的特征值；即：sqrt(T**T)*ei = λiei = siei,  同乘等距同构的旋转矩阵S,则 S*sqrt(T**T)ei = si*S*ei = si*fi 因为对ei的旋转还是一个规范正交基；
							>舒尔分解：任何方阵可以在复数范围内分解为：A=QUQ' 其中U是上三角矩阵。Q是酉矩阵,Q’是Q的共轭转置。即 每个复方阵都 酉相似于 某个上三角矩阵。如果A是对称的，则上三角矩阵退化为对角阵。矩阵相似的意思为 有 相同的特征值。 
								>酉矩阵：是正交矩阵往复数域 上的 推广。幺正矩阵。即如果方阵U的厄米共轭 U*  和U满足关系：U**U=I, 则 U就是酉矩阵。对于实矩阵，厄米共轭就是转置。即U的厄米共轭是U的逆矩阵。酉矩阵 不改变两个复向量的内积：<Ux,Uy>=<x,y>; 如果酉矩阵的元素是实数，则U是正交矩阵，则很显然只有旋转作用，所以不改变两个实数向量的内积：<Ux,Uy>=<x,y>
									>正交矩阵：前面已经证明 行向量单位且正交的矩阵 实现 了 对 行向量的旋转：x^T*U , 而很明显 U*x 则是x在行向量形成的单位坐标系上的投影,因为行向量是正交的和单位的，所以投影的平方和就是x的模长的平方和，因此投影的模长还是没有发生改变,因此Ux还是只对x发生了旋转的作用,因此将U展开为列向量的格式，利用模长不变，则得出 U的列向量也是单位的且是正交的。但是，不能得出U是对称的，因为往往U不是对称的。
								>n阶方阵的相似方阵：假设有两组基 {e1,e2,...en},{e1',e2',,...en'} 都张成了n维空间；则两组基应该能互相表示：所以 ei'= Σpik*ek 所以基底之间的变换关系为：e' = P * e , 很明显因为e'的各行是线性无关的，所以P的行列式不为0，即P的逆存在，所以 P^(-1)e'=e , 假设 用e来表示空间中的一个向量v,则 不妨 假设 v = [a1 a2 ...an]e = a e ,则在e'上为：a * P^(-1)e' 进一步如果进行了变换A,则 结果为  aP^(-1)Ae', 再变换回e基系，则为 aP^(-1)APe, 显然 这相当于在e基上也进行了类似A的变换,一个向量变换到另一个基系后进行了一个变换然后再变换回原基系,则形式上发生的变换和在另一个基系上的变换是不同的---是原变换的包裹：即 P^(-1)AP ，令它=B， 又因为P是基之间的变换，是固定的，而A是某种变换，任意的，和两个基都没有关系，显然A和B之间的关系是固定的；所以知道A则可以知道B，知道B也可以知道A,因此称A和B是相似的。A和B之间有什么深刻的关系/相关性：特征值和行列式。有时A是很复杂的，但是选择好P,则B的形式是很简单的对角阵。很明显P^(-1)*P = I 即一个向量从一个基变换到另一个基再立即变换回来 则 形式是不变的。
								>A^n 的计算： A^n = QUQ'*QUQ'... = QU^nQ' 而 U是上三角矩阵，U^n计算会比较简便；
								>Ax 的计算： Ax = λx, 即x是特征向量，则 QUQ'x = λx 成立，左乘Q', 则 UQ'x = λQ'x , 记Q'x=v 即相当于对x做了一次旋转，则 Uv=λv, 说明了 U和A有相同的特征值，且同一个特征值的特征向量只相差Q'---即一个旋转。
								---行列式： |A|
									>余子式：Mij = 去除i行j列元素的剩下的元素构成的矩阵的行列式。
									>代数余子式：cij = (-1)^(i+j)*|Mij|
									>普通行列式公式：|A| = Σcij * aij 对 j求和。 i任意。
									>一般非递推公式：det(A) = Σ (-1)^k * a1k1*a2k2*a3k3*....ankn 其中  k1,...kn是 1,2,3,...n的一个排列, 则有n!项,k为 对排列交换k次--且每次都是有效交换 而最终排列为顺序排列的 交换次数。
									>行列式的含义：n维空间的n个向量的超体积--显然只要有个向量可以用其他向量表出，则相当于缺一个独立的维度，所以超体积就压缩为0，行列式就是0。反过来，行列式为0，则一定至少缺1个维度。
									>矩阵的基本行变换对行列式的影响：行列式的值不变。因为 叉乘的缘故：比如 矩阵的两行a,b; 现在 b变为ka+b, 则 a x (ka+b) = axb 结果不变。 但是本质的原因，还是基于 行列式的两个基本性质：线性关系1： 某行乘以t 则行列式变为t倍。 线性关系2：某行增加一个向量k=[k1 k2 ...] ,则 行列式可以分解为两个行列式的和，一个还是原来的行列式，另一个则是替换为k之后的新的矩阵的行列式------这里扩展出的结论就是如果这个k是其他某行的k1倍，那么显然这个新的矩阵的行列式就是0，因此 整体还是和原行列式的值一样；因此 行变换不改变行列式的值。
										>任何矩阵经过至多n(n-1)/2次初等行变换之后变为 三角矩阵：此时行列式就是 对角元素的积。
									>行列式性质：
										>矩阵乘以矩阵的行列式：如果把第一个矩阵当作是线性变换，第二个矩阵B当作一组列向量/基，那么 如果第一个矩阵A 是 压缩变换，即变换之后向量都会少一个维度，那么自然结果的行列式是0. 此时可以写为|AB|=|A|*|B|; 如果A不是压缩变换-即 |A|!=0, 那么 此时的方阵A 可以经过初等行变换 变为 I, 而初等行变换是可逆的，所以 A=P1P2P3...Pn, 且初等行变换的行列式-当某行加到另一行则行列式对应的初等行变换行列式为1，而某行*k倍则行列式为k, 且 初等行变换过程中--需要某行独立乘以ki来得到1,则k1就是A的特征值的倒数,变换到右边后即特征值,而|A|=Πλi=Π|Pi|; 而 容易知道对所有初等行变换矩阵都有|Pi*B|=|Pi|*|B|, 因此 |AB|=|P1...PnB|=Π|Pi|*|B|=|A|*|B|。
										>矩阵转置的行列式：同样 也把矩阵进行分解：A = ΠPi ; 因为在三种情况下 |Pi^T|=|Pi| 都成立， 因此 |A^T| = |ΠPi^T| = Π|Pi|=|A| 所以 |A^T|=|A|。。。第二种方法，还是用定义式： 转置了 ，对 同样的元素构成的多项式：变为了 (-1)^k ak11 * ak21 * ...akn*n, 而交换次数k显然也和原来的一样多，因此 整体行列式不变。
										>矩阵交换2行后的行列式增加了一个负号: 因为 |Pk|=-1 即存在 一个 an1和a1n为1其他行对角元素aii=1的子矩阵，但行列式始终=1 。另一种思路，从行列式的定义式子看，当交换两列，则对每个多项式有两个元素交换了位置,因此要多一次交换才能变回原样，因此整体多一个(-1)系数；同理交换两行也是。
									--叉积：在2维时，假设a=(m,n),b=(e,f)则可以得出 |a|*|b|*sinθ=mf-ne, 外乘以内。这个值和 叉积的值一样。定义叉积的值:axb=|a|*|b|*sinθ, 可视化看 |b|*sinθ 就是b向量和a向量构成的平行四边形以a为底的高，因此整体值 就是 这个平行四边形的面积--符号和向量夹角有关。当推广到n维时，则|a|*|b|*sinθ = sqrt((Σai^2)*(Σbl^2)-((Σakbk)(Σajbj))) = sqrt(Σaibj*(aibj - ajbi)) = sqrt(Σ(aibj-ajbi)^2) 此时 i<j  因此当 2维时,i=1,j=2，所以 = a1b2-a2b1 正好就退化为 二维的公式。里面的求和总项数=(n-1 + ...+ 1) = n*(n-1)/2因此当n=2时只有1项符合2维场景。当3个向量进行叉乘后点乘时：axb*c 很明显就是形成的平行六面体的体积。
										>使用基底计算：a = a1i + a2j + a3k, b = b1i + b2j + b3k, 再利用ixj=k,jxk=i,kxi=j, 相同的叉乘为0，得出 axb = (a2b3-a3b2)i + (a3b1-a1b3)j + (a1b2-a2b1)k 仔细观察这个向量的模长，正好就是 和上述的当n=3时展开的|a|*|b|*sinθ 值是一摸一样的，这也证明|axb| = |a|*|b|*sinθ 即为什么是sinθ的原因。或者如果是sinθ则叉乘会是什么样子。
											>行列式和叉乘的联系：从上式可以看出 axb的三个分量就是三阶行列式的表达式的三项中的cij部分，因此只要 aij形成的向量和axb相点积 ，则结果就是行列式，即 (a31,a32,a33)*((a11,a12,a13)x(a21,a22,a23))=|A|, 也说明了 axb*c 的结果就是 把 a,b,c构成一个矩阵A 然后计算它的行列式。当然，这是在n=3的条件下才有意义。但是更高维度下，会产生更多的额外维度无法消除，因此和行列式的关系比较不直接。
									>代数余子式矩阵：A的代数余子式矩阵= C,则 Cij = cij = (-1)^(i+j)*|Mij| 。显然，(A*C^T)ii = Σaij* cji = |A| 对j求和。即A*C^T的对角元素都是A的行列式。而当 (A*C^T)ik,即 k!=i时，从n=3时获得启发，借助上述的axb*c的思想，k!=i时，相当于 axb*b或者axb*a，显然因为叉乘的性质，结果和参与向量是垂直的，所以结果为0; 当维度>3时，再次利用行列式的性质，即将这个乘法过程转换为 等效为  计算 一个 矩阵的 行列式：显然， i!=j, 相当于 从1->n-1行向量中平移某行到第n行 来覆盖第n行原来的内容，这样 来计算 新的矩阵的行列式 就是 和 i!=j时的 结果一摸一样，或者说形式上看一摸一样，而显然第n行 就和 前面某行一样了，或者说可以被线性表出了，那么这个矩阵就肯定缺一个维度，按照 行列式 是 超体积的 本质，因此 缺一个维度的 矩阵的行列式为0 。 另一个角度，因为 矩阵经过行变换 不改变 矩阵的 行列式，因此 新的矩阵 的最后一行可以变换为全为0，显然这样的矩阵的行列式就是0。综上所述，(A*C^T)ii = Σaij* cji = |A|，而 (A*C^T)ik = Σaij* cjk = 0, 此时i!=k, 所以 A*C^T=|A|I 因此 C^T/|A|=A^(-1)
									>伴随矩阵：矩阵A的代数余子式矩阵的转置矩阵。即 C^T。称为adj(A) 根据上述，adjA/|A|=A^(-1)
									>逆矩阵： 
										>叉乘：是一种普通乘法，是多项式叉乘多项式，每个多项式 就是 基的线性组合。最终是 基底的叉乘，结果还是多项式/向量。
										>点乘：是一种函数，也是一种普通乘法，是多项式点乘多项式，每多项式 也是 基底的线性组合。最终还是各个基底的点乘。一种度量函数，计算投影长度的函数。结果是 标量值。
								---矩阵： 形式上是 一组线性关系的 系数部分。价值上 通过矩阵运算体现(通过矩阵运算实现一个又一个价值)。
									>矩阵乘向量的另一种看作：换基 不换 坐标值。 设 x = ai + bj , 而 A = [mi nj; pi qj] 则 Ax = [a(mi+nj);b[pi+qj]] = [ai' bj'] = ai'+bj'  显然 矩阵 就是 在 声明 新基和 旧基 之间的 2组坐标变换关系,推广即n组基表示关系。
										>矩阵就是在声明 一组 共n个 的新基用旧基 来表示的关系： 所以 单位矩阵 就是 新基和旧基 是一比一的关系。
										>当作换基不换坐标的推论：如果A的行列式为0，即Ad的某行可以用其他行进行线性表出--即是线性相关的，而矩阵的每行 代表了 一个 维度的 新基底 用 旧基底如何表示，因此 对于这一行 就完全用其他行可以表出，也就是 这个新基 可以用 其他新基 来线性表出，因此 必然会导致这组 新基 缺一个维度的基底，从而作用于某个向量后结果向量会缺少一个维度，作用于一组向量则它们都缺少一个维度，假设这组向量是一组基/或者构成一个矩阵，那么这组基张成的超几何体 在变换后就少一个维度/压缩了一个维度,这组基都丢失了一个维度，导致变换之后的缺一个维度的那组基底 不能再线性表出原来那组基，所以行列式为0的矩阵 不可逆。
									>矩阵可以看作(当计算行列式时)：一组基/一组基向量/一组向量。
										>行线性相关的矩阵：可以看作 原组基 少 线性组合一次/重复线性组合一次 而成的。	
										>列线性相关的矩阵：可以看作 原组基 线性组合时 少用一个维度/轴 而成的。
										---为什么f(x)是向量，因为f(x)满足 线性空间 元素 要满足的 8条法则。它能做什么 它就是什么。
										---傅里叶变换的 行列式：也可以计算，利用 det(F)*det(F^(-1))=1 和 傅里叶变换/逆变换 的 特征 之间 只相差1/2Π 因此认为 det(F)=2Πdet(F^(-1)) 得出 det(F)=sqrt(2Π)  。 所以 F(k) = 1/sqrt(2Π)*∫f(x)e^(ikx)dx 的行列式为1
										---各种函数变换 都可以当作 矩阵：当作 线性变换---只要这种变换满足+性和k乘性。 因为函数就是线性空间中的一点---根据上述思想。所以  求导 也 有 行列式。
								----Sylvester equation: 	
		>表示学习： 信息如何表示 影响 信息处理任务的复杂度。前馈网络 是 表示学习的 一种形式。未标注数据 可以 学习 出 良好的 表示。 
			>贪心逐层无监督预训练：无监督学习，尝试获取输入分布的形状--然后 把这个表示 输入给另一个任务--一般是监督任务。
				>单层表示学习算法：RBM,单层自编码器,稀疏编码模型。
				>深层网络：可以一层层的表示更抽象的特征/更广范围的特征/组合出的特征/复合型特征。。。注意：函数f(x)其中x是一个向量 不是 说将x的各个分量都进行f(xi)的运算，而是x作为一个整体 映射到另一个点/可能维数都不同了的映射，即矩阵映射，这里的运算 即是 矩阵运算/ 矩阵作为函数/映射。 矩阵是函数，函数也是矩阵，矩阵有逆矩阵，函数也有反函数。矩阵有行列式，函数也有行列式，矩阵有特征向量特征值，函数也有特征函数和特征值；矩阵的特征向量可能正交，函数的特征函数也可能正交..。。机器学习 学习一个 任意的函数 ，自然能够学习 这个 任意的函数的 特征函数。	
			>迁移学习和领域自适应：利用 类别 共享 一些 低级概念，基础概念，则 学习了A，那么A的基础概念可以迁移 到 B的学习中。
			>多模态学习：。前馈网络，长于 学习 轮廓。
			>分布式表示：
			>生成模型：利用 目标 函数 学习出的 多种 有趣的特征。并 将 从A中学到的 特征T 应用到 概念B上。
			--神经网络近似函数/矩阵的目的：1.近似点的分布函数。即这里的函数 是 概率分布函数。因为线性组合的结果就是单值，处理后 归一化 到 对应到概率值 可以。
			>和积网络：
		>深度学习中的结构化概率模型：
			>变量x的有向概率模型：
			>无向模型：马尔可夫随机场/马尔可夫网络。
				>基于能量的模型：p(x) = exp(-E(x))其中E(x)被称为能量函数。这个分布  被 称为 玻尔兹曼分布 的 一个实例。这种模型叫 玻尔兹曼机。
					>配分函数：
				>采样技术：吉布斯采样-Gibbs采样。
				>结构学习：贪婪搜索。
			>结构化概率模型的深度学习方法：
				>受限玻尔兹曼机：用于学习 输入的 表示。E = -b^T*v -c^T*h + v^T*W*h
		>蒙特卡罗方法：近似一个 难以处理的求和或者积分。或者估计函数的梯度.
			>思路:把 和/积分 当作 某分布下的期望. 因为发现 期望的计算公式与  求和公式非常类似.  .. 例如计算 f(x)的期望: s = Σp(x)*f(x) = Ex~p(x)[f(x)] 这个就是 f(x)的期望,和普通的 期望=取值*概率这种 思路 其实是等价的,比如 x1!=x2,但是f(x1)=f(x2), 那么这个求恶化表达式 中有 p(x1)f(x1) + p(x2)f(x2) 这两项,显然按照值同则合并概率 的方式重新整理 求和表达式---那么就可以得出按照f(x)的取值*取值概率的方式 表达的 期望,显然 因为是同一个求和的稍作变化,值没有改变. 因此用Σp(x)*f(x)的方式来表示f(x)的概率是完全可行的. 同理 积分形式: ∫p(x)f(x)dx 也是正确的. ..拓展的,计算 f(x)的方差也可以用这种方式来计算, 也是等效的.
				>虽然不知道p的具体分布: 但是可以 得到采样值xi(第i个随机变量<独立同分布), 那么 因为 采样样本足够多时,sn = 1/n*Σxi 将 服从 正态分布, E(1/n*Σxi) = s ,Var[sn] = E(sn - E(sn))^2 = 1/n^2 * E(Σ(xi - E(xi))^2) = 1/n^2 * (ΣE(xi-E(x))^2 + Σ((xi-E(x)*(xj-E(x))))) = 1/n^2*nVar(x)=Var(x)/n ,很明显 误差随n的增加渐进为0,因此 将 s~ = 1/n*Σxi 作为 s的近似值.   根据中心极限定理,收敛到的 这个正态分布的 均值和方差正是 前面的 E(sn) 和 Var(sn)   ....很容易证明,当拓展到 gn = 1/n*Σf(xi) 的期望和方差时也是 同样的形式.
					>假设无法从p中采样:
						>重要采样: 假设 p(x)f(x) = q(x) * p(x)f(x)/(q(x)) 那么从 E(f(x))的计算形式上看:E(f(x)) = Σp(x)f(x) = Σq(x) * (p(x)*f(x)/q(x)) = Ex~q(x)[p(x)*f(x)/p(x)] 因此 , 采样p可以变为间接采样q, 这样 得出的 关于 原函数f的均值估计和方差估计的都是一致的.: 因为 E[sp] = E[sq] = s , 而 Var[sq] = Var[p(x)*f(x)/q(x)]/ n   使得方差最小, 用xx方式得出 q(x) = p9x)|f(x)|/Z, 其中Z是归一化常数.
							>有偏重要采样: sBIS = Σ(p(xi)/q(xi) *f(xi))/Σ(p(xi)/q(xi))
						>马尔可夫链蒙特卡罗方法: 构建一个 收敛到 目标分布的 估计序列.
							>马尔可夫链: 状态x->x'的转移概率 定义为 T(x'|x) ,  则 q(t+1)(x') = Σ q(t)(x)*T(x'|x) ,  令 Aij = T(x'=i|x=j), 则 v(i)=Av(i-1) = A^t*v(0)  这个矩阵 称为 随机矩阵.显然每一列都是一个概率分布,是从 j转移到各个i的概率;....如果A^t具有一定的性质,比如 当 t达到某个次数之后, 特征值=1的特征向量将会保留下来,而特征值<1的则会指数递减,接近于0,(都需要假设A分解出的特征向量矩阵是单位正交矩阵:UΣ^tU^T) 假设只有一个特征值为1的特征向量Vi,则Σ是一个只有一个对角元素为1其他都是0的对角阵,或者说 A^t=B, 并且显然的,当 任意一个变量v乘以B之后要么为v要么再次乘以B后为v... 或者从 旋转角度,v乘以U^T得到一次旋转,乘∑则直接其他维度压缩而只留下一个维度的值---自然这个结果向量就是在某个轴上的单位向量了,因此再旋转,则结果就是 第i个特征向量*vi倍了.一旦变成了特征向量,则乘以A始终为v自己,  从而形成了稳定分布.----第i个特征向量Vi. 这也被称为 均衡分布. 合适的选择T则Vi会是我们希望的采样的分布p....这样就可以从 这个 均衡分布中 抽取 无限多数量的 样本序列. 
								>最初目标: 处理 从大量变量中 采样的 问题.  p(x|h)其实可以用一个转移矩阵来表示.. 同样 p(h|x)也可以用一个转移矩阵来表示.
							>不同峰值之间通过回火来混合:  ....用 p(h|x)来编码x, 用 p(x|h)来采样出x.
		>配分函数: 将未归一化的概率分布 归一化. .是 所有 状态的概率求和/积分.   p(x;θ) = p~(x;θ)/Z(θ)
			>对数似然梯度: 需要计算参数,于是计算相对于 θ的对数似然: ▽θlogp(x;θ) = ▽θlogp~(x;θ) - ▽θlogZ(Θ) 这是正负相分解. 对最后一项 专门处理: ▽θlogZ =▽θZ/Z = ▽θΣp~(x;θ)/Z = (Σ▽θp~(x;θ))/Z  ,等效变换, 因为 p~(x;θ) = exp(logp~(x;θ)) ,  所以  (Σ▽θp~(x;θ))/Z = (Σ▽θexp(logp~(x;θ)))/Z = (Σ(p~(x;θ)*▽θlogp~(x;θ)))/Z = Σ(p(x;θ)*▽θlogp~(x;θ)) = Ex~p(x;θ)[▽θlogp~(x;θ)]  
				>上述既有:  ▽θlogZ(θ) = Ex~p(x;θ)[▽θlogp~(x;θ)]   右侧可以看作是 logp~(x;θ) 对 θ的梯度在 p分布下的均值.  均值 就需要 抽样 来 近似.
				>一种朴素的MCMC算法: 整体思路:θ <- θ - ε▽θlogp(x;θ)  其中梯度项 可以分解为两项:根据上述, 从训练集中采第一组xi来带入第一项表达式计算的值再求平均 作为第一项的值;  第二组xi需要对每个进行Gibbs更新 然后带入第二项的期望公式等效的均值公式得出的值作为第二项的值; 那么 整体梯度就计算出来了.   然后不断的重复这个采样-更新θ的过程.
			>对比散度:  真实负相的一个近似.  对比散度算法:和上述的区别在  第二组xi的来源----这里则直接来源于第一组=直接等于第一组::后面的Gibbs采样一样.	
				>持续性对比散度算法:  和 第一种算法相比,只初始化了一次第二组的xi , 后面 每次循环都用上次的
			---蒙特卡洛方法: 通过大量随机样本,来了解一个系统, 而 计算系统的各种重要的值....特点:通过样本 来计算,而不是 通过 某些精确间接直接相关的条件量的值.
				>原理: 某个量 从几何等角度可以等效的 看作: 1.量所基于的空间--看作 随机变量的取值区域, 2. 量值--看作 在这个随机变量取值区域上 采集n个样本中 A的个数/B的个数 这个比值(样本个数比值) 来 近似描述/等价描述,  则 通过 样本中A的个数 和 样本中B的个数 
					>其中随机变量在区域上应该服从什么分布: 看量.  比如 估计 正方形内圆的面积和正方形的面积的比: 则随机变量的取值区域为 正方形,分布为均匀分布.  A=到中心点的距离<=r的样本的个数, B=总样本的个数 
			---随机模拟抽样算法: 1.接收-拒绝 抽样 . 2.重要性抽样 . 3.MCMC方法(马尔可夫链蒙特卡罗方法)
				>接受-拒绝 抽样: 因为 p(x) 不明确 或者 太复杂 很难程序直接模拟, 直接抽样无法做到. 所以 解决思路为:  容易可抽样的分布q(x)(如高斯分布) , 采样出若干个样本 然后拒绝其中某些样本  来 近似/接近 p(x) 分布  		
					>拒绝方法的根据: 发现的 一串处理q(x)的动作事件后得到的深层变量 和p(x) 的关系:  尽管对p(x)采样太复杂,但是对q(x)采样很方便; 假设 q(x)>=p(x) 恒成立,  也假设对q(x)采样到了一个样本a,显然得到这样样本的事件的概率=q(a), 信息完全了, 需要新信息来推动推理, 则不妨将 接下来的事务和目标 设定为  再构建一个采样事件,来让 整体事件的概率和 p(a)有联系:  自然的 要和p(a)有联系 的 采样事件,直接的想到 在 a点到函数q的a点的值的线段和 p(x)有相加的点p(a),如果对a到q(a)这段线段上进行均匀采样点,则 采样到 在 a到p(a)之间的点 这件事的概率为 p(a)/q(a),  那么 整体的事件, 从q采样到点a 这件事发生且 接着从均匀中采样到a到p(a)之间的点 这件事 也发生  的 概率 就是: q(a) * (p(a)/q(a)) = p(a),  因此 这个组合事件发生的概率 就是 p(a), 显然这个组合事件是非常容易模拟的,因此 实验 n次, 记这个组合事件发生的次数为k, 那么 这个组合事件发生的概率=k/n , 就可以作为p(x)的近似. 这个组合事件 发生或者不发生 容易用实验来呈现--然后用样本频率来近似计算这个组合事件的概率, 而样本 本身就可以看作是 从 这个概率分布 得出的 样本.
						>方法: 构造一个可行的 多步骤的  采样实验,   且 实验结果(多步骤的递进采样事件) 发生的概率 正好就是 目标概率,   那么 对 这个目标概率分布的 进行采样的 样本 就可以 用 这个采样实验 来得出. ..这个方法  可以对 任意一个分布 进行采样 ...最简单的q还是正态分布(均匀分布可能不能归一化)
					--问题: 高维情况下 q分布难以找到且 拒绝率很高
				>重要性采样: 不是 对任意 分布进行抽样 的方法. 而是 计算 一个 自变量服从 任意 分布 的 函数  的 期望 的 实际近似方法.  根据: I = ∫f(x)*p(x)dx = ∫ f(x)*p(x)/q(x) * q(x)dx = ∫f(x)*w(x)*q(x)dx 其中 w(x)=p(x)/q(x)称为重要度,而q(x)是一个容易采样的分布.  因此 对 q采样到一组xi,则可以计算每个xi的概率,对应的函数值 f(x)w(x), 从而积分计算出 期望. 
				>Gibbs采样: 目的 是 获得一个样本 ,(对高维随机变量采样). 先t时刻的x的所有属性值,然后计算t+1时刻的x的各个属性的值:= 从 t时刻其他属性的值的条件下得到的这个属性的分布 中 采样一个值 
					>平衡方程: F(x)*p(y|x) = F(y)*p(x|y)
					>可行根据:
						>马尔可夫链收敛定理: 假设x有n个状态,初始时候x的状态分布为v=[p1 p2 ...]其中pi代表x为状态为i的概率, 经过1个单位时刻即一次状态转移矩阵P之后,x的状态分布改变为: Pv,  则经过t个单位时刻之后x的状态分布变为:P^t*v, 因为P是状态转移矩阵,所以列向量的元素和是1,每个元素都<=1>=0, 所以P^t 一定也是每个元素<=1>=0的, 分解之后 特征值也<=1>=0, 假设只有一个特征值λi为1, 那么 P^t进行特征分解可以表示= ni*ni^T 这是一个对称矩阵. 但是这个假设意义不大. 
							>P^t(i,j) 元素的含义: i状态经过t步后转移后到达j状态的概率. 自然的 P(i,j) 就代表i状态经过一步后转移到j状态的概率.
							>P^t(i,j) 元素的含义2: 第t-1步时的状态i转移到第t步时的状态j的概率:即是一个条件概率: P(Xt=j|X(t-1)=i)  其中Xi代表第i步之后的状态(显然是一个分布), 则 P(X(n+1)=j) = Σ(P(Xn=i)*P(X(n+1)=j|Xn=i)) = Σ(P(Xn=i)*Pij), 对i求和,  取极限得 πi = Σπj*Pij; 写成向量形式: v(n+1)^T = v(n)^T*P   如果t步达到稳态,则表示 v(n+1)=v(n), 或者对于任何向量u来说,u^T*P^t=k*h^T, 其中 k=Σui, h是一个向量,满足Σhi=1, 那么显然 k*h^T*P^t = k*h^T 这样 状态就不会再改变了.因此任何状态都可以转移到稳定态,稳定态=h ,显然从计算过程知道,P^t的元素特征应该满足 第j列的值都是cj, 即cij=cj对所有i都成立,而 且 cj就是hj元素, 自然的有 Σcj=1;  这就是稳态P^t矩阵具有的特征; P^t=[1 1 1...1]^T*h , P^t的结构特征说明了,初始时两个不同的状态c,d, 在经过t步之后 它们转移到各个状态的概率彼此都是相同的---对初始状态没有区别; h 就是平稳分布. 这样,假设此时(马式链的状态)有状态i0, 则转移到j的概率=hj,  实际得到状态i1之后,再次转移, 则转移到j的概率还是=hj, 即接下来每次转移得到的状态 都 相当于 对h采样得到的状态. ...这说明了 马尔可夫链转移得到的状态 正式 对h采样得到的状态;  因此可以用 马尔可夫链稳态后 的 每次转移的 转移状态 来 等效代表 对h的采样 样本 . 这也正是 Metropolis-Hastings 采样算法的 想法.
							--上述得到了: 马尔可夫链 稳态后的每次的转移状态 都是 对 分布h的采样 样本.   接下来的问题 是 如何构造P才使得 稳态分布 h 是 我们事前希望的p(x)
								>细致平稳条件: 从i转移到j的概率质量=从j转移到i的概率质量  : π(i)Pij = πj*Pji 对任意ij都成立, 从这个平稳条件 可以推出 	πP=π,  推理过程: 直接 计算 Σπ(i)Pij = Σπ(j)Pji = π(j)ΣPij=π(j) 写成向量形式 就是 πP=π 说明 π是稳态分布, 即 π 就是 h ;这就是 不通过计算P^t 而直接计算出h的方法.
									>对于一个已知的马尔可夫链的转移矩阵Q: 对任意一个分布p(i)显然 不会满足 p(i)*qij = p(j)*qji 对任意ij都成立, 但是假设 给两边乘以一个不同的系数--但跟ij有关,来假定这个系数能保证 使得等式对所有ij都成立,即: p(i)*qij*αij = p(j)*qji*αji 对ij恒成立, 自然的可以让αij = pj*qji,而αji=pi*qij, 那么 qij*αij 作为 新的 马尔可夫链的转移矩阵的元素,其中αij可以认为是接收率--从i转移到j时接受这个转移的概率, 就会有 p 就是 它的 稳态分布. 这就是构造一个任意分布 作为 马式链的 稳态分布的 马尔可夫链转移矩阵的方法.....为保证 Σqij*αij=1, 则 在对aii*qij设定的时候,自然的就不能少约束设置,而应该设置为 αii*qii = 1 - Σqij*αij 其中求和i!=j
										>已知马式链的一个状态Xt=xt: 计算 转移到的下一个状态X(t+1): .  那么直接的, xt状态对应到的转移概率矩阵的分布为P(y|xt), 从这个分布采样到一个状态y, 则 接着看是否接受, 所以要走一个均匀概率的采样事件--这个事件发生的概率=α(xt,j), 自然的只需要在均匀分布(0,1)上采样,如果值<α(xt,j)则表示接受,否则就是不接受; 接受 则 X(t+1)=y, 不接受 则 X(t+1)=xt ... 这就是MCMC采样算法
										--上述就是MCMC采样算法: 计算 马尔科夫链的下一个状态的算法;  而根据上述,稳态下,下一个状态 就是 对稳态分布的采样的值. 因此 是 采样算法.;								
									>提升接受率的改进算法:  观察到上述算法中 αij 和 αji在计算中,即便同时增大k倍--保证kαji<=1,kαij<=1, max{αij,αji}=1则 增大α之后可以 提高 采样中的 跳转接受率.  这就是 Metropolis-Hastings 算法;  其中 αij可以等价表示为: αij = min{1, p(i)*qij/(p(j)*qji)} 
									--当状态是向量的时候, Metropolis-Hastings 一样有效---只要仍然满足细致平稳条件 p(x)*Q(x->y)=p(y)*Q(y->x)		
								>对高维随机变量的采样: 即Gibbs抽样; 前提必须知道: 去除一个维度下,其他所有维度构成的空间中所有的点 作为条件下 被去除维度的分布函数 需要知道: 即 P(xi|x1,x2,...xi+1,xi+2...xn), 即x是一个向量,那么某一个维度的分布 由其他维度的取值 决定, 显然其他维度的取值张成的空间 非常大, 所以一般 这个条件概率 应该有统一的公式;  假设 初始时候 的向量x(0) = [...] 则 利用条件概率 可以 一个个采样出 1时刻的各个分量的值: x(1), 自然的可以得出 t时刻的 x的样本值, 直到稳态分布下的x的采样到的值;
									>二维平面上的两点X,Y: 如果满足细致平稳条件, 则二维空间上的马尔可夫链 将 收敛到 平稳分布p(x,y), 这个平稳分布 正好是 目标 二维随机变量的分布.
										>二维空间上的马式链: 从一个二维状态 转移 到另一个二维状态; 这个转移矩阵显然是(R^2)^2, 则一个n维状态转移到另一个n维状态的状态转移矩阵规格=(R^n)^2, 转移n-1次则规格=R^n^2
										>二维状态平面上的一个点转移到另一个点的概率: 假设 一个点A(x1,y1), 转移到另一个点B(x1,y2), 转移的概率= P(x1转移到x1 且 y1转移到y2) = P(x1转移到x1)*P(y1转移到y2) = P(x1|x1)*P(y2|y1), 出现在A点且转移到B点的概率 = P(x1,y1)*P(x1|x1)*P(y2|y1) = P(x1)*P(y1|x1)*P(x1|x1)*P(y2|y1) 发现其实不满足细致平稳条件;但是 发现: 当经过转移阶段后,  x=x1上的一点转移到这条直线上的另一点的转移概率 如果只跟 终点的纵坐标有关而跟初始点的纵坐标无关, 那么 这种转移 就达到了细致平稳条件. 即: 如果经过某个转移阶段后, p(x1,y1)*p(y2|x1) 就可以代表 从x1,y1转移到x1,y2的概率; 那么 p(x1,y1)*p(y2|x1) = p(x1,y2)*p(y1|x1) 显然这就是细致平稳的条件;同样道理,  任意两点A(x1,y1),C(x2,y2) 之间如果进行沿着坐标轴满足前面的转移概率--只跟目标点的不同轴值有关 ---那么 A发生且转移到C的概率 也等于C发生且转移到A的概率;;;P(x1,y1)*P(y2|x1)*P(x2|y2) = P(x2,y2)*P(x1|y2)*P(y1|x1)   这样 A点和B点之间的转移概率和点发生的概率之间就满足: P(A)*P(A->C) = P(C)*P(C->A)  其中 P(A) = P(x1,y1) ; P(A->C) = P(y2|x1)*P(x2|y2)  这说明了 高维向量的转移概率 等于 依次转移到各个分量的概率之积;  扩展到 三维空间上的平行轴的两点之间的转移 稳态 自然也要求 只跟 目标点的轴坐标值有关, 所以P(x1,y1,z1)*P(y2|x1,z1)=P(x1,y2,z1)*P(y1|x1,z1), 拓展到 三维空间上任意两点则为:P(x1,y1,z1)*ΠP(π2i|π21,π22,π2(i-1)..,π1(i+1)..π1n) = P(x2,y2,z2)*ΠP(π1i|π11,π12,π13...π1(i-1),π2(i+1)...π2n)  扩展到n维空间上任意两点之间的确定和转移概率的关系;自然也满足这种形式; 因此 统一写成向量形式:P(X)Q(X->Y)=P(Y)*Q(Y->X) 这就是马尔可夫链转移一定阶段后达到稳态时 细致平稳的 特征; 服从这个关系之后, 后面所有步转移出的 状态点 都是 稳态分布的 样本点;  这个稳态分布就是 P(X) , 而马尔可夫链的转移矩阵 就是 Q----显然 就是左边描述的一个很长的由条件概率构成的连乘表达式;.......而这个Q因此是可以根据已知的条件概率计算出来的, 从而从一个初始状态A出发,不断的经过这个马尔可夫链的转移,转移n步之后达到稳态,之后每次转移出的 点 就可以认为 是 P(X)分布的采样点了;  这就是 Gibbs Sampling 采样算法;
											>总结整个关联过程: P的条件概率-->构造出马尔可夫链的转移矩阵-->随机找一个初始点转移n步到马尔可夫链的稳态-->n+1步开始的转移出的点就是稳态分布P(X)的采样点;
											---什么是达到稳态的马尔可夫链: 就是 一个 有n层每层权重由转移矩阵代表的网络的结构; 它使得 无论输入 是什么非0向量,n层网络之后/n步转移之后都达到稳态, 输出 都是 一个服从稳态分布的随机变量的值;.....比如 输入是 一个某分量=1而其他分量=0的向量,那么相当于就是输入某个状态,那么输出的 也是一个服从稳态分布的随机变量的值;
											---马尔可夫链的每个点的产生过程: 下一个状态点 是 当前的状态点确定的 分布/转移概率分布 中 采样的结果 点;;初始的点则是从一个任意的初始分布中采样到的...
												>下一个点 的分布概率: 预测.  因为 初始步的点的分布是随机的, 并且 尽管后面每次下一步的结果得出过程都明确肯定从固定的转移概率分布中采样得出; 一步步的得出每一步的状态点是容易的.  但是想在初始时就预测未来第 n步的时候各个状态的分布概率, 而在计算过程中发现规律,不变性---就是 分布概率居然稳定了; 那么就意味着 从第一步随机分布开始,一直按上一步的采样值的对应的条件转移概率分布采样下去, n步之后每步采样到的点都会是对一个固定分布/稳态分布的采样的点;
													>上面正是表明了马尔可夫链上n步之后的点 正好就是 马尔可夫链转移矩阵 确定的 稳态分布的 采样点: 稳态分布 的 采样 是难以直接进行的.
														>应该知道什么信息才能确定一个 满足要求的稳态分布的 马尔可夫链转移矩阵: 已知稳态分布;求 每个状态的转移出去的概率分布;(当然不能都是 稳态分布--因为认为稳态分布是难以采样的)...假设 为了方便采样,则每个状态转移出去的概率分布/条件转移概率为一个简单的分布比如 高斯分布, 再进行 类似前面的"接受-拒绝"方法 的处理: 即从简单分布采样到的点决定接受还是不接受---根据还是均衡分布U(0,N)上采样,<α(i,j) = q(i|j)*p(j) (其中q就是简单分布,而p就是期望分布;) 则接受,否则不接受;---不接受则自然要重新开始采样--也是原来的那个条件概率分布q(j|i)因此 不接受 表示 还是原来的状态点; 从而 继续从 这个简单分布q(j|i)上进行采样;;;; 而根据 计算 i->j的转移出去的概率 和 j->i的概率 只要 乘以了接受率 那么 就同样会 有 转移出去的概率总量等于 转移进来的概率总量. (概率当作质量/能量/价值一样的事物/守恒量/概率守恒定律), 从而 带 接受率的 普通分布 作为 转移概率的 马尔可夫转移矩阵 的 稳态分布 就是 p, 即从 p分布出发, 经过带接受率的马尔可夫转移矩阵后的的点的分布还是 p;  ...  这种带接受率的阵的稳态分布的 微观特征:即 细致平稳条件: p(i)q(j|i)*α(i,j)=p(j)*q(i|j)*α(j,i)  转出转入相等, 说明了正确性.-----即i=1,i=2,i=3的状态的概率的分布,转移后,还是这个分布---因为转出转入相等, 每个点都转出转入, 因此 分布不变;
									
https://www.zhihu.com/question/20695804 张量 解释。					
https://www.docin.com/p-1150499820.html Christoffel 的推导 学报。
https://zhuanlan.zhihu.com/p/116461924 协变导数的推导 
https://zhuanlan.zhihu.com/p/30483076 高斯混合模型 
https://www.cnblogs.com/neopenx/p/5590756.html 前馈网络求导
https://www.cnblogs.com/huangyc/p/9706575.html 感知机 随机梯度下降算法 计算 w,b
https://blog.csdn.net/u014313009/article/details/51039334 较好的 反向传播算法推导。
https://zhuanlan.zhihu.com/p/144255438 行列式的本质解释。
https://www.cnblogs.com/pinard/p/10930902.html 矩阵微分的 极佳解释。
https://blog.csdn.net/u012102306/article/details/75905570?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-3.base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-3.base  LSTN/GRU的解释。
https://blog.csdn.net/qq_39422642/article/details/78676567 RNN原理简单介绍
https://www.cnblogs.com/mantch/p/11433829.html seq2seq模型理解
https://zhuanlan.zhihu.com/p/149458380 奇异值分解的较好解释。
https://www.zhihu.com/question/41120789/answer/474222214 最佳的PCA主成分分析文章。
https://zhuanlan.zhihu.com/p/83865427 VAE变分自编码器 的 较好阐述。
https://github.com/nathanhubens/Autoencoders 自编码器 的实现代码。
https://youzipi.blog.csdn.net/article/details/51373090?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-6.base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-6.base  从随机采样到 Gibbs采样/高维随机变量的采样 的 非常好的解释. 并有高级进阶主题.
