---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。
>学习要有战略和格局：过于零散、孤立、末支、叶子的知识点就没必要化太多时间，懂得放弃。聚焦在根节点、枢纽节点、产生分支的节点、主干知识点、枢纽知识点、关联到很多知识点的知识点上。
 >对于主干知识点：要有全新的认识视角和进行精细的描述，大量的提问和好奇和进行必要的实验以获取支撑观点/导出观点的数据。
 >规范是工程最独特的特征.
 >慢慢读：
>一种新技术的学习：
 >它面对的情况和问题、它的世界观、它的方案、它的方案验证/论证/能处理的解决的所有情况及能成功处理的理由/功能边界
  >所有的软件：都可以看作是向上封装一层接口，根据自己的世界观封装底层而向上/对外提供统一的(统一的更简单的更直观的更业务的更少底层信息的)接口，底层包含一系列的第三方的插件/构件/组件；内部则去做兼容和调用(对底层)(对上层则做逻辑分解和底层实现)。
>知识混乱就是因为没有组织：
	>组织就是关键字树：几个单词就是每层上的每个节点的内容；
	>组织也可以看作逻辑树：有逻辑关系，逻辑顺序，逻辑联系的关键字的层层集合。层层囊括更精细的范围，层层划分范围。
>举动-痛点：亿级流量网站的高可用高并发原则，落地这些规则；
	>解法/解决方案：负载均衡、降级、超时重试;限流、隔离/解耦;回滚机制、压测与预案、缓存、池化、异步化、扩容、队列。OpenRestry(Nginx+Lua)
		>高并发：缓存、池化、异步化、扩容、队列。Nginx+Lua 
			>高并发原则：
				>无状态：无状态的应用方便水平扩展；应用无状态，配置文件有状态----不同机房里的节点读取不同的数据源；
				>拆分: 按照业务闭环；
					>系统维度：即对象维度；商品、购物车、订单；
					>功能维度：流程的环节分别为一个系统；如优惠券创建、领券、用券；
					>读写维度：按流量类型分；读商品、写商品；读：则可以进一步采用缓存、数据异构存储；写：分库分表；
						>数据异构：按哪个维度异构？先是订单号，则后面是按照用户ID异构；来提高查询性能；。有的无序异构，直接异步并行加载即可；
							>历史数据：归档处理，提升服务的性能和稳定性；
						>数据闭环：
							>对于数据来源很多的某个页面：保证稳定性，需要聚合后先存储，直接使用聚合后存储的--利用使用缓存的方式；这样，问题在于更新会有积压---延后；
							>数据异构： 接收变更数据，将数据原子化存储到redis/持久化KV存储；
							>Hash Tag机制：相关数据，聚合到一个实例上--利用 相同的productId，尽管具体信息方面不同；
						>缓存： 
							>浏览器缓存：问题：实时性比较高的则有问题；
							>APP客户端缓存：托底数据，离线缓存；
							>CDN缓存：静态页面和图片；
							>接入层缓存：URL重写-->一致性hash-->proxy_cache-->shared_dict(lua+nginx方案中用)
							>分布式缓存：即redis缓存集群；注意：tomcat项目堆内也有本地缓存；
						>并发：
							>并发读：对于无数据依赖的各个服务，可以并行获取；
					>AOP维度：CDN+页面渲染系统 来拆分；CDN就是AOP系统；
						>CDN数据的更新：推送机制或者拉取源数据覆盖；
					>模块维度：是一个系统内部的架构层次，本身按照这个层次 再次 的切分分割；
				>服务化：既做集群，也做隔离，按照调用方隔离(调用不同节点组/分组)，避免一个调用方影响其他调用方；
					>服务限流：治理。
					>黑白名单：治理。
					--->影响服务质量的其他因素：超时时间、服务路由(动态切换不同的分组)、重试机制、故障补偿；；
				>消息队列：解耦和异步
					>服务解耦：一对多消费；这个数据很多 下游系统都关心都需要；
					>流量削峰/缓冲：
					---->队列问题：消息发送失败、重复接收必须解决；发送--应该持久化才算成功；防重处理。
							>消息丢失：数据校对和修正，保证数据的一致性和完整性；如用Worker定期去扫描业务表/原始表；
					---->大流量缓冲：先改快的，后同步到慢的；同步时，可以并发和重试；
					>应用层面用来解决哪些问题：
						>异步处理：异步写日志、写任务、写消息、写缓存/数据(先源后缓存)，提高主程序响应速度，而队列的消费者异步的消费，可以大量消费者一起进行；
						>数据迁移/同步：变更消息的发送，有序入队；数据增量同步，canal,otter---分布式数据库同步系统；全量离线数据同步---kettle
						>系统解耦：一个事件发生，可能未知的多个系统都要同时响应，则主线程发送这个事件/消息到队列(而不是直接调用这些系统)，而多个系统都消费这个消息--每个订阅者都有自己的一份副本；
						>流量削峰：系统瓶颈在数据库，则请求入队列等待访问数据库；排队和限流；缓冲区队列；
						>扩展性：双写---先写数据库后写消息系统；构建数据异构系统；订阅消息，来进行数据的重新分片存储；
						>缓冲：Log4j 缓冲队列，字节缓冲区，日志写入，满了刷磁盘；；；缓冲队列，可以队请求缓冲、批量处理、异步处理、平滑处理；
						>具体队列：
							>Redis+Disrupter:
								>redis: 等待队列-->本地队列-->失败队列/备份队列；
							>订单系统下单：	订单放入缓冲队列/缓冲表，然后任务同步到订单中心/订单表，来把下单逻辑和操作订单逻辑分开到不同的系统上；(缓冲队列/表+订单表+订单缓存)
								>worker取缓冲表的数据时：发送到 Disrupter RingBuffer里，然后Handler处理这些消息事件；
					>不同维度的查询：分库分表之后，采用 数据异构；
						>一个维度分库分表之后：其他维度来查询：全表(无用功多性能差)、双写(可用性无法保障)、订阅数据库变更日志binlog再解析分库分表到另一个维度的分库分表里(即一个维度成功后其他维度才进行分库分表，就像接变更消息一样，更多维度都可以进行分库分表)(这种思想可以用来间接实现分布式事务的功能---就是先一个成功后另一个接收变更消息而处理)
							>分布式事务的一种实现：A->B转账；先写流水-->流水表消息变更发送到消息系统-->其中之一消费者接收到这个消息,开始处理-->先做A的事务：写A流水表+总资产表；如果成功则继续写B的事务：写流水表+总资产表；如果都成功了，则回写总事务流水表；--->如果A成功而B失败了，则A回滚；回滚失败，则服务不可用，手动修复；如果A成功而突然断电了，则恢复电之后查询发现A有流水已经成功但总事务表中还没有成功-B没有成功，则开始重做B事务，后续则如同之前；如果AB都成功了，而回写总事务失败，则重试；如果回写时断电，则来电之后 检查总事务的子事务是否都已经成功，都成功则更新总事务状态。
						>ES异构：
						>缓存异构：
						>Canal的使用：Canale订阅binlog日志，进行消费：数据索引、异构、缓存同步更新、数据镜像；(数据变更后通知其他系统；任务系统监听变更而发送到消息系统)
							>canal server:主备模式；zk监听切换。读取的binlog事件写入内存，可以再写入消息系统？ActiveMQ允许同一份数据供多个消费者镜像消费；
							>canel client:主备模式；zk监听切换。消费canal server的事件；
						>binlog记录模式：row为数据的变化情况记录下来；statement则是记录修改的sql;这种模式在sql中有系统函数时可能造成主从的不一致；mixed混合模式	--一般sql用statement模式，系统函数用row模式；show binary logs 查看当前有哪些二进制日志文件及其大小；
				>扩容：线性扩容和弹性扩容；
					>硬件扩容：升级服务器cpu+mem,磁盘扩容HDD--SSD,RAID10--裸盘；
					>水平扩容：增加更多实例；应用实例；
					>应用拆分：前后端分离；服务拆分；
						>提升数据库连接数：mycat,corbar;
						>缓存/限流/防刷: 单独放到nginx接入层；
						>前端页面：引入CDN。
						>业务系统：多机房多活(多地等同部署)；BIND--->根据用户IP将不同区域的用户路由到离他最近的机房来提供服务；
					>数据库拆分：
						>业务维度：垂直拆分。直接分到不同的数据库实例上；。至于宽表拆分为小表---在数据库设计时一般就考虑了；
							>跨库join: 全局表+ES搜索等异构数据机制实现；
							>分布式事务：
							--读写不均衡：读多写少，那么就读写分离；部署主从架构，多个从来读；提升吞吐量；
						>水平拆分：分库分表 
							>取模： 即分库分表的常用方案；
							>分区：时间维度，一个月一张表；一年一个库；地域分区，一个地域一个库；先hash，后分区也可以；
					>数据迁移方案：
						>异步增量方案：写继续；但t1时刻开始迁移，将更新时间<t1的用户的数据都进行迁移，直到t2迁移完毕；然后继续下一轮迁移，此时迁移更新时间在[t1,t2]之间的所有用户，直到t3迁移完成；然后开始下一轮迁移：更新时间在[t2,t3]的用户....这个时间间隔不断减少，直到为最小间隔；稳定下来，此时刚好追到最新的更新；那么此时只需要重新启动一次服务的暂停时间---就可以直接追上而迁移完成了！！切换服务！！
						>异步增量+切换方案：上面的方案继续执行，且每次迁移完毕的时间要写到数据库A表里，业务的save方法执行中，先看自己的更新时间<A表里的迁移时间，是则向B表插入迁移完成标记--自己的userid,如果是新用户save,那么直接插入B表迁移完成；则随着A表的更新时间不断提前和B表的用户不断增加，当B表的用户新增的都是新增用户时，那么迁移完成了；或者说业务save方法上线开始前的所有注册用户都已经迁移到B表中了，那么就动态迁移完毕了；！！！
					>分布式事务方案：
						>先写交易日志--然后才真正增扣双方资产--双方都成功再更新这笔交易为成功！：每一方有自己的子事务来保证成功-失败回滚即事务的ACID；来保证强一致和可对账；
							>主事务信息写入流水数据库，子事务也写入流水数据库；如果A子事务成功而B子事务失败，则A要回滚(前向反做)或者B要重试(一方要成功，一方成功即可，先B后A)；主事务没有成功，那么也就是这件事没有成功--里面的子事务也都回滚了；则保持了一致性；
							>分布式事务：死锁检测。同一时刻，相互转账；
							>主事务控制器：负责启动两个事务的start,...commit 。问题在于有的如果commit失败？需要回滚！
						>事务表/补偿机制/TCC模式/Sagas模式(拆分事务+补偿机制)：业务尽量设计为最终一致性，而不是强一致性；
					>读缓存：双写；同步双写/异步(变更消息)双写；
					>写后读：
						>Hint机制：HintManager强制读主库；Sharding-jdbc在执行更新的时候，会记录，而让下次读强制路由到主库；写后读主库，同步完成才开始读从库；
					>数据异构：全局表、ES搜索、异构表来实现；
						>查询维度异构：即两轮的分库分表；或者订阅变更消息/binglog(基于Canal实现基于数据的异构) 实现重新分布(分库分表)，或者被消费转移到ES里形成新的维度的倒排索引；
						>聚合数据异构：ES像wlist,key-list键值对数据库；
					>定时任务：
						>单机版本：ScheduleExecutorService实现con表达式不方便？使用Spring Task配置简单，有Quartz的cron表达式；
						>分布式：elastic-job-lite,分布式任务调度，动态扩容缩容、任务分片、失效转移、运维平台等；
				>异步并发：
				>多级缓存：让数据更接近于使用者；
					>缓存使用监控指标：命中率；读取到数据次数/总访问次数
					>缓存策略：基于空间---存储空间；基于容量；基于时间--TTL过期时间/TTI空闲时间；基于java引用：缓存适合软引用--避免OOM;
					>回收算法：FIFO;LRU;LFU--最近最不频繁使用；
					>java缓存：堆内：Guava/ehcache/MapDB; 堆外：Ehcache/MapDB；需要序列化反序列化；
					>磁盘缓存：
					>分布式缓存：redis集群；
					---堆内-热数据；堆外-相对频繁；分布式-全量数据；
						>guava: 缓存策略：写后多久过期而删除；写后多久没有访问而删除；最大缓存容量多少超过而删除；还可以设置软引用缓存；put时回收失效--而不是一失效立刻其他线程去删除；
						>空缓存：专门存储数据库里没有的key;那么缓存没命中时，就从空缓存里查看，看是否存在；	
						>强制刷新数据：
						>失败统计、延迟报警：
					>缓存同步：canal订阅binlog
					>缓存代码和业务代码的编写：
						>Cache-Aside: 缓存代码和Sor获取源数据在主逻辑下，先取缓存，没有则系统，后缓存写；。。对于写场景，则先写源数据库，后写redis/或者失效redis;
							>实现：适合aop模式，读 就是 环绕通知，写就是后向通知/前向通知；
							>问题：非用户维度数据；并发读取和并发更新的问题；。单机事务。(或者相同数据路由到同一个节点上)(节点挂了，重新开始)
						>Cache-As-SoR: Cache看作Sor,所有操作对缓存操作；	
							>Read-through实现：把从数据库获取数据的动作封装到 cache.get()方法里，即从数据库获取数据当作一个接口的匿名内部类的方法实现里。可以代码通用，各个地方读取这个数据都不用再写一套了，共用即可；且在一处就可以做同步了--同步的从数据库获取数据--仅仅一次；get(key, Callable)
							>write-through实现：穿透写模式，类似上述；将写数据库的动作封装/注入到 cache.set()方法里，先写Sor,成功后写缓存；update/delete/add都是这个写方式；
							>write-behind： 回写；异步写；先写缓存，后写入队列，由线程池异步从队列拉取任务来写数据库；通过Sempere控制并发度；
							>copy pattern: Copy-on-Read: 读时复制；Copy-on-Write:写时复制；
					>性能测试：@Benchmark @Warmup ..JMH
					>http缓存：浏览器缓存
						>页面返回时：响应报头里有：缓存设置；浏览器就会根据设置将响应内容缓存到浏览器；下一次则直接使用，或者去服务器端验证文档是否过期即可；---第二次就会返回304
							>响应报头设置：Last-Modified -- 最后修改时间，浏览器下次会带上(if-modified-since)；	Cache-Control: max-age=20 表示内容在浏览器缓存20s，过期后再从服务器请求返回(浏览器带上Cache-control:max-age=0)；	
								>强制刷新：ctrl+F5, 则导致发送：Cache-control:no-cache 和 Pragma: no-cache，没有if-modified-since; 通知服务器提供最新的数据；
								>新tab: 请求也是先从浏览器里获取数据；
								>Age: CDN缓存代理层中创建的内容的生命时间
								>Vary: 告诉CDN缓存代理层取不同版本的数据；
								>Via: 经过了哪些代理层，是否命中缓存；
								>ETag: 发送到服务器端进行内容变更验证；文档内容摘要；
					>httpclient客户端缓存：职责链模式实现缓存；引入httpclient-core包；
					>Nginx http缓存设置：expires,etag,if-modified-since指令来实现浏览器缓存控制；
						>expires配置：location /img{expires 1d;} 代表有效1天；
							>返回报头：Cache-Control: max-age=86400 ETag:"xxx"  Expires: xxx.xxx.xxx.xxx Last-Modified: xxx
						>整体流程：用户-->全国各地的CDN节点(ATS/Squid实现)，如果CDN没命中，回源到中央Nginx集群，没命中缓存，则回源到后端应用集群；
						>proxy_cache_use_stale: 配置缓存过期了也可以的内容；返回给客户；如http_502/http_500/http_503 
						>缓存清理：location ~ /purge(/.*){allow 127.0.0.1 ; deny all; proxy_cache_purge cache$1$is_args$args;}
					>Nginx 代理层缓存配置：
						>http模块设置：proxy_buffering on;
					>Nginx本地缓存：应对热点数据有效；
					>分布式缓存：减少访问回源率；
						>写库之后再写分布式缓存：Cache-Aside 模式；。定期全量同步；
						>缓存数据：维度化 分开存；方便区别地增量更新；而不是缓存一个超大的Value；至少需要压缩；
						>分片缓存为主：可以考虑使用redis-cluster分布式集群方案；
						>热点数据造成某台分片服务过重：则从一致性hash降级为轮询；或者数据推送到接入层Nginx,直接响应给用户；
						>热点数据与更新缓存：
							>热点数据问题的解法：单机全量缓存+主从。从Lvs+Haproxy--->nginx+lua-->从redis集群 ，即Nginx也会访问redis缓存，而主redis将数据同步给这些从redis集群，且tomcat查询了数据会异步发送给主redis集群；
								>两级Nginx实现：lvs+Haproxy-->接入层nginx+lua-->应用层nginx-->本地缓存，没有则分布式缓存-->回源到tomcat集群
							>更新缓存与原子性：redis可以原子更新、版本对比，canal专门来负载数据一致同步；
						>缓存崩溃与快速修复：
							>取模：先新建一个缓存集群-->迁移数据-->流量迁移；
							>一致性hash实例坏了之后的快速恢复：尽管部分缓存不命中；主从切换；
					>Tomcat本地堆缓存；用于防止相关缓存失效/崩溃之后的冲击；
						>应用负载均衡：轮询则跟业务无关，同一个请求都会打到不同服务器上；而 一致性hash则会打到同一个节点上；
				>池化：
					>对象池：对象复用
					>连接池：
					>线程池：数据连接、redis连接、http连接；复用tcp连接；
						>等待队列大小和等待时间长短：失败回退，安全切换的失败机制；
						>连接的关闭：一定要JVM停止或者重启时调用；Runtime.getRuntime().addShutdownHook(()->{httpclient.close();});
						>开启长连接：才是真正的连接池；连接实现了复用；Htppclient线程安全，不必每次使用创建一个；
						>超时机制-线程隔离机制-快速失效机制：。tomcat有最大连接数+最大线程数两个请求并发控制；
					>并发：降低了总体流程时间；同步非阻塞；Future
					>回调：提高了吞吐量，没有提高性能；异步非阻塞；CompletableFuture
						>函数式编程和回调：函数式编程本身a().b().c()，分成并列的两条线看，每个方法的调用看作一条线上的各个点，其中的入参就是并列的另一条线上的点，这些入参lamda函数就是注册的回调函数；回调函数是被另一个线程/IO线程异步在事件发生后调用的；所以函数式编程本身就像是注册事件编程！！！！类似前端！！！
							>函数式编程就是注册事件编程：！！！！注册 “事件-回调处理方法” 给第三方独立运行的如在网络端口监听的线程 可以访问到的地方/仓库/注册中心；
								>事件发生之后回调A：如果要求A被回调之后再执行C, 那么写法上就要继续注册，从而函数级联，而每级返回的都是CompletableFuture,是这个也代表可以继续级联下去；。。因为如果在A的实现里嵌入代码调用C，显然植入性太强，不适合；且会形成调用层次太深，不像流程顺序调用那么自然而简单！！！！
									>且并行注册+串行注册都可以：！！！看要实现的效果和调用的API匹配即可！！
									>注册的事件-handle方法 就像发送的消息，等待被消费处理；
						>CompletableFuture: 可以轻易实现主等待子线程的并发模式；supplyAsync(), allOf().get()
				>负载均衡和反向代理：
					>流程：浏览器-->DNS获取ip--->访问LVS/F5软件硬件负载均衡器(接入层，四层)--->HAProxy-->Nginx七层负载均衡--->tomcat
					>Nginx提供的负载均衡：
						>上游负载均衡：
						>故障转移：
						>失败重试：
						>容错：
						>健康检查：
						--配置：upstream 均衡器名{...}配置Ip和权重
						--OpenRestry:更智能：热点非热点流量分离、正常流量爬虫流量分离；
					>Nginx提供的反向代理：
						>对响应结果缓存、压缩：
						--配置：location / {proxy_pass http://均衡器名;} 请求反向代理到 均衡器名指定的server
					>负载均衡算法：round-robin轮询，ip-hash客户端Ip相同则同一个upstream server,一致性hash---可以配置nginx变量实现/lua脚本实现， least_conn:最少活跃连接的上游服务器；
					>超时-重试配置：upstream backup{server xxx.xxx.xxx.xxx:xxxx max_fails=2 fail_timeout=10s weight=1} 在fail_timeout时间内失败了max_fails次，则会被摘掉，等fail_timeout之后才开始重试；
						>重试配置： proxy_connect_timeout 5s; proxy_read_timeout 5s; proxy_write_timeout 5s; proxy_next_upstream error timeout;proxy_next_upstream_timeout 10s;proxy_next_upstream_tries 2; 遇到配置错误，则重试下一台上游服务器；
					>Nginx健康检查：惰性，非主动；需要nginx_upstream_check_module 才能 tcp心跳+http心跳 来进行健康检查；
						>配置： upstream backup{... check interval=3000 rise=1 fall=3 timeout=2000 type=tcp;}成功1次则活跃，失败3次则不存活；一次请求超时时间；两次请求间隔时间；
							>也可以http报文：需要额外添加 check_http_send  ... check_http_expect_alive 
					>Nginx备份和不可用上游服务器：server xxx weight=2 backup; server xxx weight=2 down; 不可用--即摘掉机器可以这样做；
					>Nginx配置和上游服务器的长连接：keepalive 100; nginx的每个worker进程和上游服务器的最大空闲连接个数限制，最大则不限制---超过的LRU关闭；
					>Nginx缓存配置：减少上游服务器的压力：
						>proxy_buffering on; proxy_buffer_size 4k; ... proxy_cache_lock on; proxy_cache_lock_timeout 200ms; ...proxy_cache_path ...
						>设置一致性hash负载均衡key: set_by_lua_file xxx;
						>gzip支持： gzip on; gzip_min_length 1k;
					>upstream动态注册到nginx: 实现upstream服务自动发现
						>Consul: 服务注册和服务发现：Http/Dns方式；
							>故障检测：http/tcp健康检查机制，有故障自动摘除
							>K/V存储：存储配置，使用http长连接实现配置更改触发通知；
							>多数据中心：只消费本地机房服务；
							>Raft算法：实现集群数据一致性；
							---部署方式：客户端Agent和nginx在一起，实现Nginx配置更改和Nginx重启；比etcd3好---多数据中心-故障检测-web管理界面；
						>过程：
							>consul-template: 部署在nginx相同节点，和consul server建立长连接，监听配置变更，有则更新ngnix的upstream列表，调用重启nginx的脚本
							>consul server: 上游服务器 注册-以便consul server监听服务器状态从而自动修改upstream配置信息，以及 consul-template长连接 监听配置，本身还提供web管理后台来修改配置；
							--启动consul: ./consul agent -server ...通过HTTP API注册服务；也可以摘除服务---比如当JVM停止时回调摘除服务---Runtime.getRuntime().addShutdownHook();
							--启动consul-template: ./consul-template -consul 127.0.0.1:8500 ... 指定模板位置.
						>非重启Nginx方式更新upstream配置：动态负载均衡；
							>Upsync+Consul:
							>Consul+balancer_by_lua:
								>进行负载均衡的是balancer_by_lua: 而它使用 shared_dict 里存储的upstream列表进行动态负载均衡；而这个共享字典是可以被动态修改的---init_worker_by_lua，
								>定期拉取配置和更新到shared_dict的模块：init_worker_by_lua。
								>启动时拉取配置和更新到shared_dict的模块：init_by_lua;
							>Dyups:
					>静态负载均衡：
						>Nginx四层负载均衡：配置在stream{}下，而不是http{}指令下
							>upstream{}子指令模块：类似
							>server{}子指令模块：listen 3308; proxy_next_upstream on;proxy_next_upstream_timeout 0; proxy_next_upstream_tries 0;等等连接到上游服务器的配置：关于端口、失败重试(次数/超时时间)、限速、上游服务器组
						>Nginx七层动态负载均衡：nginx-stream-upsync-module
							>nginx-upsync-module: 无需重启nginx, 基于consul+etcd动态更新上游服务器；
						--HAProxy进行四层负载均衡：	
		>高可用：负载均衡+故障监控+故障转移/消除单点; 限流、隔离/解耦; 降级、超时重试;压测与预案；回滚机制；。。。还是解决节点挂了怎么处理的问题
			>降级：别人挂了，我访问它，那么我现在该怎么做？降级、重试、重试另一台？
				>降级数据：本地缓存、分布式缓存、默认降级数据；
				>开关前置化：如到nginx,则到tomcat的流量就少了；
				>业务降级：为保证高可用，需要做异步化，如保证用户能下单、能支付，保证数据最终一致性，则完全同步则不能提高高可用性，所以要异步，把特殊和高优先级的数据处理了，后面的异步处理完成；合理分配进入系统的流量，保证高可用；
				---实现部分可用、有损服务：
				--形象理解降级：就像一个时钟指针，顺时针流转，这个过程，不断下降的过程，就像降级---每个位置对应一种方案，保证服务可用；
				>降级目的：保证服务可用；有的服务无法降级---如加入购物车-结算；
				>降级预案：日志级别设置预案：
					>一般：服务上线、网络抖动而超时；自动降级；
					>警告：服务成功率有下降；自动降级/手动降级，报警；
					>错误：服务成功率低于90%,访问量猛增到系统最大承受y阈值，数据库连接池不够，；自动或人工降级；
					>严重错误：特殊原因数据出现错误，紧急人工降级；
				>降级分类：
					>读服务降级和写服务降级：
					>系统层次：多级降级；
				>哪里需要降级：用户访问的服务调用链路；
					>页面降级：整个页面紧急情况下不提供服务；
					>页面片段降级：商品详情页 商家部分 因为 数据错误，对其降级；
					>页面异步请求降级：推荐信息等异步加载的，可以降级；
					>服务功能降级：热销榜，不重要的服务；
					>读降级：只读缓存；
					>写降级：秒杀抢购，只进行cache的更新，然后异步扣减库存到DB,保证最终一致性；
						>扣减redis-->扣减db:降级为发消息-->
					>爬虫降级：
					>风控降级：根据用户风控级别降级处理，直接拒绝高风险用户；
					>自动开关降级：根据系统负载、资源使用情况、SLA等指标；
						>超时-重试机制：最后降级；
						>统计失败次数降级：次数达到，则直接熔断；异步线程去请求看，是否恢复；恢复则取消降级；
						>故障降级：远程服务挂了，网络故障-DNS故障-http服务返回错误：直接降级：默认值-兜底数据-缓存；
						>限流降级：达到限流阈值，后续请求直接降级处理：返回错误、默认值；
					>人工开关降级：
						>开关配置：保存在zk/分布式配置；人为主动的降级；
							>Consul配置管理：人工降级；但是不能获取增量更新；
						>新开发的服务灰度测试：设置开关，有问题通过开关切换回老服务；数据库数据有问题，需要切换回老服务；
					>读服务降级：先降级到缓存-->暂时屏蔽读；接入层缓存；
					>hystrix实现降级：
						>实现：继承一个类，实现两个抽象方法；原理：应该是调用另一个方法，方法里主方法负责监控，子线程负责执行；主线程会短时间暂停/等待，看并发数和子线程是否完成；来决定是否中断线程和让线程继续执行；---线程可以semphere信号量控制并发数---不用主线程控制；
						>失败量组成：失败-异常、超时、线程池拒绝、信号量拒绝 数量的总和；
						>休眠时间窗口：即开关打开后，多久时间内需要重试一次，看是否成功而打开开关；一般5s,以便快速成功；
						>采样统计：10s,每秒采集一次，采集成功、失败、超时等各个的次数，然后覆盖写，来做10s内的平均统计；
							>一组内最值统计：command/threadpool; 
							>时延排序统计：排序后，前50%,90%,99%的均值是多少
			>限流：我提供服务，别人调用，量太大，我该怎么处理？识别，限流；
				>防止恶意请求流量：只让这种流量访问到cache
				>穿透到后端：则Nginx的limit模块处理
				>恶意IP：Nginx 的deny进行屏蔽 
				------限制流量穿透到后端薄弱的应用层；免受雪崩；
				>限流指标：
					>限制总并发数：数据库连接池
					>限制瞬时并发数：Nginx的limit_conn模块
					>限制时间窗口内的平均速率：Guava的Ratelimiter, Nginx的limit_req模块，用来限制每秒的速率；
					>限制远程接口调用速率、限制MQ的消费速率等：另外还可以根据网络连接数、网络流量、cpu或内存负载等来限流；
						>令牌桶算法：周期为T地向桶内添加一个令牌，则速率为1/T； 业务线程则向桶中取出令牌。
						>漏桶算法：限制流出速率；即取值api需要等待时间到了才能取到，早到则没有，即只有固定时间点上才有；因此需要保存上次取值的时间，然后和这次取值的时间比较，如果时间延迟t不够，那么需要等到足够时，1/t就是漏水的速率；
				>应用级别限流：
					>系统极限：QPS；超过阈值，过载保护，拒绝请求；
					>限制总资源数：数据库连接；超过则排队等待，或者放弃；其实是并发连接数；
					>限制接口总并发数：也是并发同时在处理的连接数：就像信号量一样；Hystrix在信号量模式下也是使用Semaphore限制接口的总并发数；
					>对其他系统调用的限流：比如用Guava的Cache，定时2s重新设置缓存值为0，而业务中取缓存自增值，如果大于limit，就被限流了；
						>平滑突发和平滑预热限流：Ratelimiter;可以设置启动时-到稳定时的周期时间；
					>分布式限流：高性能和高并发：Redis+Lua,Nginx+Lua
					>接入层限流：Nginx:<使用OpenRestry开发高性能web应用>： 
						>接入层功能：负载均衡、非法请求过滤、请求聚合、缓存、降级、限流、A/B测试、服务质量监控等；
							>连接数限流模块：ngx_http_limit_conn_module用来对某个key对应的总的网络连接数进行限流；可以按照如IP\域名维度进行限流；
								>如IP维度的并行连接数：配置： location /limit{limit_conn perip 2;}
								>或者按照某个服务域名来限制某个域名的总连接数：
								--具体配置：http{}下的键：limit_conn_xxx
							>漏桶算法实现的请求限流模块：ngx_http_limit_req_module用来对某个key对应的请求的平均速率进行限流，平滑模式+允许突发模式；
								>配置：http{limit_req_zone ...} 桶算法，请求入桶(延迟模式)；桶满拒绝；通过的最大速率是被限制了的--为滴水速率；
								>ip维度限流：location /limit{limit_req zone=test;}
							>OpenRestry提供的Lua限流模块：lua-restry-limit-traffic应对更复杂的限流场景；
								>可以利用动态特性：
			>节流：限制时间窗口内相同事件的处理次数，为1次；
				>前端开发常用：throttleFirst 处理第一个事件，throttleLast 处理最后一个事件；滑动和点击事件；
				>throttleTimeout: 相邻两个事件的间隔，如果小于最小间隔，则上一个事件忽略，这个事件等待时间内要么被消除，要么被执行---即等待时间超过最小间隔；如搜索关键词自动补全；
					>先将这些事件发送到一个队列里，然后逐个的处理(需要记录上一个的时间)。
			>切流量：机房、机架、机器都有可能挂；
				>DNS：切换机房入口
				>HttpDNS: 在客户端分配好流量入口，更精准的流量调度；
				>LVS/HAProxy: 故障切换的Nginx接入层
				>Nginx:故障切换的应用层 
			>可回滚：版本化设计的目的：实现可审计、可追溯、可回滚；
				>事务回滚：
				>代码回滚：
				>部署版本回滚：
				>数据版本回滚：
				>静态资源版本回滚：
			>隔离：故障隔离；调用方隔离；
				>系统隔离：限定传播范围和影响范围，出问题服务不可用之外，其他服务仍然可用；
				>资源隔离：减少资源竞争；互相不影响；cpu/mem/磁盘/网络的隔离；
				>线程隔离：线程池隔离，降低影响；对业务线程池监控、运维、降级；
				>进程隔离：拆分为多个业务子系统
				>集群隔离：每个业务子系统集群部署；
				>三机隔离：每个机房里的服务是一个分组，机房内的服务一般只调用机房内的服务，不跨机房调用；机房前端有DNS路由，出问题可以检测到而失败策略执行---如切换到其他机房；
				>读写隔离：也是缓存隔离，不同的服务，有不同的redis集群；读按照：先从后主；
				>快慢隔离：
				>动静隔离：静态资源CDN存放，动态资源才部署在服务上；CDN下一层才是Nginx;Nginx下一次可以访问Js/Css更新的；
				>爬虫隔离：Nginx层识别和限流，ip+cookie识别用户；
				>热点隔离：秒杀抢购，独立系统来服务；读热点：多级缓存；写热点：缓存+队列 削峰，
				---隔离分布式服务的故障组件：Hystrix
					>线程和信号量隔离：减少不同服务之间资源竞争。不同的服务用不同的线程池；
					>优雅降级机制：超时降级、资源不足降级；调用降级接口返回托底数据；
					>熔断机制：服务可以快速失败；失败策略；失败率达到阈值触发降级；熔断器触发的快速失败也会快速恢复；
					>请求缓存和请求合并：
			>超时与重试：避免请求堆积造成雪崩；
				>代理层超时重试：HAProxy和Nginx实现请求的负载均衡，Twempproxy实现redis的分片代理；需要设置代理和后端真实服务器之间的网络连接/读/写超时时间；
					>nginx:
						>客户端超时设置：请求头/体超时、发送响应超时、长连接超时；避免长期长连接的占用；如：服务器主动向客户端发送FIN主动关闭连接；
							>读写超时：在 location / {}里 如 proxy_connect_timeout , proxy_read_timeout....
							>代理超时：在 upstream {} 里， 如 fall_timeout, max_fails 
							--状态码：http_502...等都是上游服务器返回特定的状态码；error timeout则是与上游服务器建立连接 出错或者超时；失败会被摘掉，error ,timeout,invalid_header被认为是失败；
								>500/502/503/504, 且该次请求耗时在200ms内，进行一次重试；			
						>DNS解析超时：OpenRestry使用时，如果 upstream 中配置的域名，如果Ip要动态改变，则 用 lua-restry-dns进行DNS解析；这个解析里有超时配置；
					>web容器超时：tomcat 超时:连接超时、请求超时
					>中间件客户端超时与重试：
						>服务注册/订阅：连接超时 、心跳包超时；
						>消息中间件：收发消息超时
						>httpclient工具：连接收发数据超时
				>web容器超时：tomcat/jetty可以设置的socket网络连接读写超时；
				>中间件客户端超时与重试：Httpclient之类，需要设置客户的网络连接读/写超时时间与失败重试机制；
				>数据库客户端超时：mysql jdbc连接，读写,事务超时时间；
					>连接超时、连接池中线程空等待超时时间；
					>spring配置事务级别超时时间：
					--数据库操作：慢查询 服务出问题的杀手；
				>Nosql客户端超时：
				>业务超时：Future.get()
				>Ajax超时：前端网络连接超时；
				---重试-->缓存-->错误页；；；网络连接都有超时：初始化握手建立连接超时、普通一次请求超时；
			>回滚机制：快速修复错误版本；
				>事务回滚：事务提交冲突；mysql服务器挂了；
					>最终一致性：事务表、消息队列、补偿机制(执行/回滚)、TCC模式(预占/确认/取消)、Sagas模式(拆分事务+补偿机制)等实现最终一致性；
						>定时任务：扫表，进行一致性确认；不一致的回滚，或者补偿；记录事务日志表；流水表；
				>部署版本回滚： 版本灰度-->用户灰度到节点上新功能-->Nginx流量切换到新集群；
					>失败降级：nginx的error page配置： location ~* "^/(\d+)\.html$"{proxy_pass xxx.$1.html; error_page 500 502 503 =200 /fallback_version/$1.html}
				>数据版本回滚：全量版本化，或者增量版本化；就像mysql中MVCC涉及的数据版本；undo中有归档；
				>静态资源版本回滚：
			>压测与预案： 
				>大促来临之前：系统梳理，发现系统瓶颈和问题；通过系统压测来发现这个瓶颈和问题；发现之后：通过系统优化和容灾(单机房多机房容灾)；
					>不稳定因素：网络、依赖服务的SLA不稳定等；出现之后的预案：路由切换+降级处理；
					>大促之前：预案演习；确保预案的有效性；
				>压测：性能压力测试；评估系统的稳定性和性能；压测数据进行系统容量评估，决定是否需要扩容或缩容；	
					>压测方案：压测接口、并发量、压测策略(突发、逐步加压、并发量)、压测指标(机器负载、QPS/TPS、响应时间)，产出压测报告(压测方案、机器负载、QPS/TPS、响应时间(平均、最大、最小)、成功率、相关参数JVM-压缩参数)，最后根据压测报告分析的结果进行系统优化和容灾；
					>线下压测：仅仅用Jmeter,ab之类的测试接口；
					>线上压测：
						>读压测：如产品价格服务；
						>写压测：下单服务；注意数据的隔离；
						>混合压测：
						>仿真压测：人工构造、Nginx访问日志、引流压测--TCPcopy复制线上真实流量---引流到压测集群--还可以将流量放大N倍，来观察服务器的负载能力；
						>单机压测：可以看到单机的能力，但是集群环境下，和其他机器竞争缓存、其他服务、数据库，会使得性能其实更糟糕；
				>系统优化和容灾：在得到压测报告之后；
					>硬件升级：
					>系统扩容：扩容和系统容灾一起；分组部署、跨机房部署；
					>参数调优：
					>代码优化:
					>架构优化：缓存、读写分离、历史数据归档；
					>应急预案： 容灾之后的存在的风险：网络抖动、某台机器负载过高、某个服务变慢、数据库Load值过高等，防止这些问题导致的系统雪崩；需要制定应急预案；
						>系统分级：不同的质量保证；保证高可用还是暂时不可用；
						>全链路分析：梳理关键路径：网络接入层、应用接入层、Web应用层、服务层、数据层
							>网络接入层: 机房不可用、DNS故障、VIP故障；机房公网入口挂---摘除DNS上的某机房入口，vip网络抖动---切换到备用VIP
							>应用接入层：IP访问量太大导致上游web服务器负载过高--IP限流；上游应用异常：硬件故障、负载高、GC慢响应慢---摘除异常节点、切换路由到其他节点；上游服务异常---超时网络故障--超时自动降级-查缓存；爬虫多---爬虫降级为静态资源返回；机房升级---流量路由到机房B
							>web层/服务层：服务响应慢--其他分组机房；线程池不够--动态调整？请求量太大--限流-降级；服务压力大--限流；网络异常--切换分组机房；调用量大--限流；
							>数据层：主库硬件故障---主从切换--后验证应用；redis挂掉--主从切换；机房不可用--路由切换到其他机房缓存；
						>配置监控报警：服务器监控；系统监控---端口存活；接口监控--qps、接口性能-Top99、告警策略；
						>制定应急预案：
		>业务设计原则：防重、幂等、流程可定义、状态与状态机、后台系统操作可反馈、后台系统审批化、文档与注释、备份。
			>防重设计：下单防重，支付防重；可以 建立 订单号-支付状态， 来做是否进一步操作的状态存储；
			>幂等设计：重复消息消费时进行幂等的处理；
		>特定方面的原则：
			>容量规划：流量、容量；
			>SLA制定：吞吐量、响应时间、可用性、降级方案；
			>压测方案：线上、线下；
			>监控报警：机器负载、响应时间、可用率
			>应急预案：容灾、降级、限流、隔离、切流量、可回滚；
				>限流：避免雪崩；
				>降级：部分可用、有损服务；		
			>分流方案：负载均衡、反向代理--故障监控和故障切换；
		>京东内部的一些项目架构和实现：
			>商品详情页架构：
				>商品详情页系统：静态部分 
					>前端结构：商品基本信息、商品详情信息、分类信息、商家信息、店铺信息等；实时价格、实时促销、广告词等异步加载；。。服务端响应时间<38ms
					>缓存结构：浏览器缓存+CDN+Nginx(Nginx+Lua架构), 服务端：local cache+磁盘+分布式缓存
					>tomcat请求处理：利用异步模型，直接发送到请求队列，tomcat线程专门请求解析，而用户线程池专门从请求队列里进行拉取，进行业务处理；
					>同城机房：一主三从
					>线上压测：TcpCopy,导流、流量放大；分发到压测服务器；
					>键值存储选型压测：LMDB随机写随机读，；数据归并时会产生抖动；1w/s+1k/s读写；
					>数据过滤、URL重写、防爬在Nginx层处理为重点：
				>商品详情页统一服务系统：动态部分
				>商品详情页动态服务系统：给内网其他系统提供一些数据服务；
			>双十一商品详情页架构：
				>整体流程： Lvs+HAProxy-->nginx->Lua逻辑过滤-->本地缓存过滤(HttpLuaModule的shared dict)-->redis集群过滤-->回源tomcat请求/写redis集群(主集群同步到从redis集群)； 其中nginx和从redis集群在一个机房；不同的nginx则在不同的机房；
					>命中率：一般 本地缓存25%, redis集群过滤-- 28%, 回源tomcat 47%
					>nginx缓存：抗热点；
					>nginx访问的从redis集群：保存大量离散数据，抗大规模离散请求；
					>tomcat本地缓存：堆缓存 存储量小频繁访问的数据；缓存时间为redis缓存时间的一半；
					>tomcat读取主redis集群：防止前端缓存失效后大量请求涌入，进而导致后端服务压力太大而雪崩；本地缓存 之后 要在超过一定阈值才读取主redis集群；
				>nginx接入层做的一些逻辑：
					>数据校验/过滤逻辑：cookie等报头的取数据和验证和整理；url的重新拼装为统一格式；可以用url进行缓存；
					>业务逻辑前置：将业务逻辑改动通过Nginx+Lua推送到tomcat,重启tomcat只需要秒级；没有性能抖动的问题；
					>降级开关前置：init_by_lua_file 初始化开关数据，功能切换开关、非核心服务超时自动降级；
					>A/B测试：lua中根据请求的信息调用不同的服务；或者通过upstream分组；请求分流；
					>灰度发布/流量切换：也是设置多个upstream分组，根据需要切换分组；
					>监控服务质量：对请求进行代理，记录status,request_time,response_time来监控服务质量；根据调用量、状态码是否为200、响应时间来告警；
					>限流：使用token方式，每个请求必须带上token,而token的获取必须从前端获取，且每次获取需要消耗一定的时间，带上token则又只有一定时间的效果--且需要交叉验证--只能一个用户使用；每个页面的速率？
					>静态资源css/js放到CDN:动态资源Jsonp; 解决js缓存问题；	
					>聚合数据：一次url请求调用多个后端服务，将数据聚合后返回；Lua协程机制并发调用多个相关服务，对服务进行合并；
			>使用OpenRestry开发高性能web应用:
				>Nginx: 每个工作进程：单线程；非阻塞I/O;更改配置重启毫秒级；
					>动态重载nginx配置：
					>可以http负载均衡/tcp负载均衡：
					>内容缓存、web服务器、反向代理、访问控制：
				>Lua: 提供协程并发；即同步调用而异步执行；
				>nginx_lua: 提供了lua和nginx交互的api,
				>openresty: nginx + lua库+nginx第三方模块
					>使用例子：
						>web应用：访问mysql/redis/http服务 获取数据，产生json;
						>接入网关：数据校验前置、缓存前置、数据过滤、API请求聚合、A/B测试、恢复发布、降级、监控；
						>web防火墙：进行ip/url黑名单、限流；
						>缓存服务器：
					>使用架构：
						>两层Nginx:
							>第一层：无状态，直接轮询转发；限流、缓存、过滤、故障切换
							>第二层：A/B测试、降级，跟业务相关；通用逻辑；无状态设计；水平扩容；灰度发布；
								>流程1：请求解析-->读取JIMDB/回源tomcat-->业务处理-->4.渲染模板-->返回响应
								>配置： events{woker_connections 1024;}配置工作线程数；http{lua_package_path xxx}配置lua依赖路径；
									>本地缓存配置：lua_shared_dict item_local_shop_cache 600m;
						>单机闭环：
						>分布式闭环：
							>细粒度限流：lua-resty-limit-traffic 通过编程实现更灵活的降级逻辑。
							>A/B测试，灰度发布：在业务nginx上部署lua脚本，实现不同的人看到不同的版本接口实现；	
					>写后读主库的强制实现：
						>如果读写分服务部署：那么写服务写mysql后失效redis，那么这个失效是可以被读服务发现的，发现xxkey的失效不是超时失效、不存在，而是被写服务更为失效，从而读服务专门从主库读取数据 后更新到redis; 或者说干脆写服务不是失效，而是直接将redis的key值更新为最新的---代劳了！！