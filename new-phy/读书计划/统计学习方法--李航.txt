---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。
>学习要有战略和格局：过于零散、孤立、末支、叶子的知识点就没必要化太多时间，懂得放弃。聚焦在根节点、枢纽节点、产生分支的节点、主干知识点、枢纽知识点、关联到很多知识点的知识点上。
 >对于主干知识点：要有全新的认识视角和进行精细的描述，大量的提问和好奇和进行必要的实验以获取支撑观点/导出观点的数据。
 >规范是工程最独特的特征.
 >慢慢读：
>一种新技术的学习：
 >它面对的情况和问题、它的世界观、它的方案、它的方案验证/论证/能处理的解决的所有情况及能成功处理的理由/功能边界
  >所有的软件：都可以看作是向上封装一层接口，根据自己的世界观封装底层而向上/对外提供统一的(统一的更简单的更直观的更业务的更少底层信息的)接口，底层包含一系列的第三方的插件/构件/组件；内部则去做兼容和调用(对底层)(对上层则做逻辑分解和底层实现)。
>知识混乱就是因为没有组织：
	>组织就是关键字树：几个单词就是每层上的每个节点的内容；
	>组织也可以看作逻辑树：有逻辑关系，逻辑顺序，逻辑联系的关键字的层层集合。层层囊括更精细的范围，层层划分范围。
>推进理解的属性发展拓展、问题延展：重要方式；
>什么是架构：架构也是从抽象到具体的考虑和描述；树形延展开来，可以写满非常大的黑板和巨大的脑图！！sharding-jdbc,dubbo,spring都可以这样方式来展现它的架构！！它的抽象到具体的考虑---本身才是架构！！！而不是什么模块、模式之类！！
>抽象设计：则某一层就不管上一层的含义和下一层的含义，即更抽象的含义或者更具体的含义；而是实现本层的含义；完成本层的含义指定的功能；。如网络协议的架构设计；	
>面向设计来理解，面向架构设计来理解，面向架构问题一层一层来理解它：面向设计来理解，所以按照面向对象设计的方式，看其中的对象、行为属性、流程环节逻辑。	
>找不到知识/描述 所对应的问题 ， 那么看书将没有条理纲领，变得零散琐碎没有组织。	
>不是按概念方式组织，而是按架构、问题方式来组织 笔记，书本内容。架构顺序，问题层次顺序。	
>架构不是设计出来的，也不是演进出来的(甚至不是迭代出来的--尽可能避免迭代)：而是问出来的。	
>每个方法方案都从属于一颗树，所以找到一个方法巧妙方法仅仅是第一步--找到从属的层次树 有更大的价值；(无论是谁想到的方法/概念，都要这样更进一步)
>解决问题的办法就是提出问题：类似递归和动态规划；。。权衡就是线性规划；	优势劣势在一定场景下也是劣势优势；
>一个词，到一句话，两句话，一段话，一篇文章；这个就是抽象总结，层次总结；越简洁，站得越高
--在网络、搜索引擎、推荐系统 三方面的专家；作为系统方面的独特优势/拔高优势；(网络-查询-推荐)
	
----有且只有响应，通信端才知道连接是否成功；。浏览器自动扩展。
----维持连接，并发连接，都是软件的实现，物理上看都是一条出口；从响应就是维护连接的角度看，不存在需要维护什么连接，维护就是维护连接数据而已；只要发送响应，连接就活了；在网络端口出口，可以连续发送不同目的地的响应报文，这就是并行；所以完全可以用队列来接收请求数据包；而用队列缓存发送响应数据包；多核使用起来，来并行大批量的发送和接收；不存在要维护和持续占用“端口”网络出口这种概念---完全没必要，用完即走 就可；	
	>或者不存在连接这个概念：所有的事情就是接收数据包和发送数据包(接送/发送缓冲区)。(连接 是 软件臆造出来的概念，不要和物理对应；和物理对应就会束缚思想，就会很多事情理解不了不知道原因)
	>连接的状态转移图；
	>应用的固定端口：实际上是建立新TCP连接的请求的处理的端口，请求到达这个端口--后面建立一个独立的TCP连接---来负责和客户端通信-交互数据；
----UML：为什么类继承关系图---因为这就是具体到一般的概念对象抽象过程。自上而下是能力顺序，能力组合；	
----说话和介绍：语速不要快，快就是掩盖问题，掩盖过程步骤；直接导致别人认为思路不清晰，表达不清楚，东拉西扯；也不利于自己思路的成长和扩展和自己主动发现问题，且必然导致不简洁--废话很多；
	>介绍需要先纲目后具体：抽象到具体；而不是张口就是罗列枚举---内容没有结构--全是线性结构；
	>描述更精简：一个字一个词，一句话，两句话，一段话；
----大事和吏治：大事 就像西天取经；吏治就像管理四人；


--计划：nginx/tomcat-->计算机系统-->架构-->自己的系统架构方案:专题研究、大提问、大总结。大简化/模型图化；
>一个进程看作一个消息，代码计划/任务计划；；都是异步隔离；	
	>程序启动点/执行点：可以多个，看作是并行的；(一个机器上，多个程序文件里)；可以留下执行点/新增执行点；可以删除执行点/减少执行点；
	>函数式编程为什么好：因为每个精细环节清晰描述确定了下来；使得充分配置和指明动作；
	>如何看待对象的方法：所有的对象都是被动的；主动的只有cpu/并行点；
	>程序的执行要想象为人在执行；多线程则是交接执行权给其他人执行；
	>抽象编程与具体填充：假设编程和实现假设。面向对象中一个对象属性就假设已经填充好了，一个接口的实例就假设已经在容器中有了。代码编码层面和虚拟机执行层面，都规定/定义/设计为 将 接口和实例分开，系统启动时/甚至具体接口调用时才去容器里找接口的实例(启动时 一方面建立实例容器，另一方面 对接口寻找匹配的实例和链接到实例---进行连接和关联)。资源填充 和 接口调用(抽象调用)，资源--接口的映射对。领域抽象，资源抽象，功能抽象。
----未知和迷惑的地方：痛点	；
----关键和核心的地方：要点

--混乱的答案，宁可不说；只回答真正掌握的；。没有逻辑，因为没有进行抽象；没有找到所在的流程环节、模型中的位置
--系统、中间件的介绍，不是一来就是组成结构；这层次已经太细了太具体了太里面了，必须要从最简洁最抽象最上层开始；最表面最近开始；务实，不僵硬，不突然，要自然，不要忽视和没注意没意识和跳过很多步；而是从问题出发、从困难出发、从疑惑出发
	>从问题出发：先明确问题；先明确表达出问题、疑惑点、黑箱、痛点、矛盾点、难点，表述的范围可以很大(完全不知道是什么怎么办)后面逐渐具体问；无论多少问题，先明确下来；尤其要全部且完整的描述下来；
		>问题的提出：先明确背景，自然衍生、过渡、转折、演化，逻辑关系上，什么时候什么事情什么人，事情什么阶段遇到的什么问题、阻碍、阻挠、缺陷、瓶颈、不够简洁、不够简便、不够方便、不够优雅、离目标远、离理想情况远、离期望/极限效果远；不够抽象的地方；把它们充分描述完整叙述结构式组织起来。
			>问题抽象：归结为一类问题；去除具体和细节而明确问题模型；
		>问题产生原因：过程；条件；	
		>问题导致的恶果：阻碍、损失；
		>理想的方案特征/效果/必做必不做的动作&事情/应当改变的环节: 
			>这种特征/必做必不做的前提、必要条件、必然要求、必然说明、必然指示、必然可以确定的更多的事情/结论：
				>一系列结论、约束得到之后(结合条件/问题/情况本身)逐渐可以清晰看到/归结出该具体问题符合的/满足的通用/一般的/一类的问题模型/函数模型/服务模型/IO模型/请求响应模型的轮廓：若干个具体模型
					>方案的装饰/补充: 补充可靠性/稳定性/高性能(从而高可用/高并发)：因为暂时只是一个裸机、容易受到伤害、有功能但没有抵抗力(仅为打火机的火而不是熊熊大火)
							>方案的用法规则：在请求缓存前使用
		>能将具体方案进行分类的维度/情况/模型/环节/流程/抽象表述 的确定：然后使用 抽象-具体 的方法来得到新方案；					
		>普通的方案：已有的方案；方案的抽象，方案的取值选择评价；方案的表象缺陷、劣势；。。模型、数据结构和算法、协议约定分担 维度 上考虑；
	>任何事情/事物都有顺序/逻辑：且几乎都是几种常见逻辑中的一种: 时间先后、空间远近、因果环扣、程度递增
--大总结：含义包括：重新 深刻理解：
--通用的建模架构能力+Flink深度强化学习的推荐系统。。。。而不是做简单的业务逻辑开发；用深度强化学习来做应用/解决实际问题(用户的识别问题和抉择问题)；用抽象建模架构出逻辑完整的方案(工具方案/服务方案)；
--一体两翼的发展模型：底座：增强操作系统、网络、搜索、推荐能力； 两翼：普通项目：则架构建模；特殊项目：则深度强化学习；。。四大基础+两大实践(应用/使用)。。基础：是为了解决自己的问题；实践：为了解决别人的问题(用户的问题/大众)；
--一次彻底弄懂，而不是 反复低温加热。看架构书和源码书，不看使用书。
--对话中胜利：一个是提问，二个是不断的输出-高能输出。
--牢固的观念: 系统都是被使用的。
--任何一个系统、产品、服务、方案、东西、事情，它要解决的核心问题是什么？理想的形态效果影响应该是什么？市面上对应的哪些产品达到了或者没达到或者很难达到？没达到是为什么是否有我们的机会？是否还有我们可以满足的缺口。找市场缺口。
--从缝纫机原理看方案所属的分类和方案内的环节。抽象出分类和环节。
--把自己当做cpu,调/使用各个服务/接口/类/系统。
--全新学习掌握方法：先定问题体系，先提问，定逻辑路径，后开始找答案--推导和思考和查阅资料。
	>旧方法：还是盲目阅读，还是从头到尾的阅读一遍的阅读。
--所有系统和方案的理解/研究/制定的出发点：都是 出发顶点/问题顶点/概念顶点，找到了顶点才能出发，出发要从顶点出发，才顺畅，没有后顾之忧
--计算机解决问题的方法：采集，记录，计算，展示，通知，跟踪 数据/信息。

--商业经营才能：(全新的解决问题的方法：提出问题)
--什么问题 是可以用互联网来解决的？ 只要满足什么特征，该问题就可以用互联网方式来解决? 该问题的本质是什么问题(如是信息传递问题？)该问题抽象一般化后是什么问题(资管、记录、通知、计算、采集识别调动、代理/中介/数据中心/信息中心/信息模拟线下过程中心、信息传递)(信息展示/发布/搜索/推荐 平台；信息池；信息可以是映射描述的现实事物的信息(如商品/价值物/人)，也可以是纯虚拟信息(游戏/知识/新闻/视频))
	>能用信息发布展示/活动追踪-信息记录/通知/协作/推荐/搜索 解决的问题：信息可以是 编辑的信息/映射的信息/等价现实的信息/等价现实任何事任何物的信息。内涵BAT的老业务(电商/聊天/游戏/信息流/搜索)。
	>能用数据驱动决策抉择与调度分配/识别与推理/观测采集与调动机电-生物 而解决的问题：如alphazero/图像识别/语音识别/视频识别/云计算/智慧城市/自动驾驶。内涵BAT新业务(云计算/视频语音识别-下棋打游戏智慧体/智能音箱/自动驾驶)
--码农之家和脚本之家、异步社区。从 面向对象-->模式-->架构：由小到大。
--给其他非软件行业的工程师/人员开发软件。三高三可(高可用高并发高性能,可扩展性可维护性可重用性)。
--最佳：一边看书，一边独立思考；(比起纯看书和纯思考都好)。因为 看书 要有沉淀，积累，独立思考 结论，总结。而不是流经书，翻翻而已，左进右出。
--说话：社交场合，最重要的是：要有自己的认知、决策和行动；对形势的判断和分析和预测。而不是被别人牵着走，或者置身事外。
--什么是最重要的? 就是你想做出一个什么东西...  这是最有价值的事情,是最重要的目标(比起学位和金钱和看多少书都重要);
	>不讲在公司的项目: 只讲自己 业余时间 做出的东西; ..  和 理解的东西... 总结起来 就是 四个方面: 架构 + 源码 + 算法 + 产品/作品
--不必指责和愤怒: 只需 战胜 和提防.
--莫忘理性决策：详细的明确和计算，一点也不能含糊。看看这次搬家时机的决策失误和搬家房间选择的失误，造成了金钱和时间上的巨大浪费。又比如以前自大没有投资股市---导致损失很大！！	拒绝感情用事和感动别人。
	>不能武断：不能一厢情愿。不能只见树木不见森林。
--兴趣不仅决定能力，而且决定性格，比经历更加影响性格；总结和思考 是兴趣带来的根本也是兴趣导致的根本活动；而性格能很大程度上影响命运---作。韩信(仰仗自己聪明而希望别人只记得我的好不记得我的坏)、赵括(自己负责精彩，麻烦交给别人--希望别人帮自己解决；只知顶层逻辑不知道低层逻辑)、李广(总觉得别人对自己不好，发现不了自己的错，不按程序办事，爱自己来，当别人按程序来--总觉得别人是让自己受气，受不了别人的指责)	
--在大部分有钱人和机构都将资本+利润存入股市的时候：说明股市是会升值的，股市就是最大的公司。多余的所有钱+利润 只有在股市才能分享。
	>既要有信心，也要有担忧： 激情和忧虑 都有 ，才是 理性的人。而不是两者都没有的麻木不仁、冷血和装。
	>做人不要尖酸刻薄，说话要说别人的好，坏处知道 但是不要说出来。有的人不喜欢被戴高帽，或者认为是反讽，这只是 好 没有说到点上而已。
	>面对别人指出的问题: 改就完了,且深刻反思和各种改进, 大量的收获和扩展.更强!!!错误的反思总结得到的东西让人更强!!!
	>已经是青年人了：要敢于争取
	>控制欲望、总结反思验证、勤于积累和使用：控制无收益的欲望-事情白白浪费时间；总结反思-站在第三方检查和痛批评价自己的想法和行为-从细节到方向目标-做得不行的要改变；勤于积累和使用-阅读和实践都要彻底掌握弄通；
		>读书不破、万卷何益：不会有长板；
		>不痛苦痛批、不是反思：
	>择人：最忌讳的是好吃懒做的人。喜欢比较的人--一上来就卡脖子；眼睛没必要跟鼻子比。
	>不要轻易拒绝做 看不起的事情、还早的事情、不重视的事情、觉得自己不需要的事情-觉得世俗/自大的事情(世俗也要同流合污)：买车摇号、社保缴纳、公积金缴纳、找女朋友、找好的房子、考在职硕、炒股-投资。多少件事情--明明顺手就可以做，却没有做，一直拖，导致损失惨重！！
		>每一件瞧不起、不重视、还早的事情：相反，都要早点做！！
		>机会错过 难再有：往往影响是决定性的。没把握机会 就是 决策性失误！
		>其他事情：私活-阿里云-本地开发-换电脑。
	>想到就要做，错过不再有：这是最重要的名言！！只有这样才能发挥才智 并且 赢得 胜利！！！	最重要的就是时机、机会，把握机会把握时机。
		>时机、战机 转瞬即逝：稍纵即逝。错过就是一辈子，错过就是奉上身家性命。犹豫不决和瞧不上和不需要而暂时不做延后做 都是错失良机、贻误军机。
			>说小了是 酸，说理性点 就是 后悔遗憾 承受损失。
	>商业机会：大部分来自于持续观察大家的生活---什么社会事件政策会怎样的影响人们的生活/会让人们的生活发生哪些改变； 人们生活中遇到了什么瓶颈/矛盾/困难/问题/需求/不足/遗憾/不开心的事情。		
--理论观点/方法缺乏层次系统的组织，还是在等别人分配赚钱任务。概念、观点 还是片段、不连续的。
	>基本认知方式：提炼、总结、简化、抽象、封装 。
	>问题为 元素 进行组织：这次不以概念为元素，来更方便于实际使用、进行设计训练---工作也当作是设计训练。
	>填充所有的逻辑漏洞：
	>分层问题森林\分层问题 群: 一个n层的问题,其顶元素 只是 另一个n层问题的一个元素. 因此 k个 分层问题  构成 了 一个最顶层分层问题 , 整体 是一个 问题森林.
	>每一种策略/设计/具体化： 都必须明确它到底是为了解决什么问题。
	>范畴不明确：就会 陷入 “还有什么” 的困惑和迷途。范畴的向下具体划分和向上抽象统一更大范围。
	>讲一个东西的内容时：可以看出是否结构化思考，是否很明确范畴，范畴化思考，不断的抽象上升又具体下降--树上遍历。因为问题，产生目标，想到思路，立刻执行。
	>输入不必多：关键是转化 为 总结、实践  有 多少。	
	>可视化：为最佳的理解方法。
	>深入一个事物：就是问它的什么的什么的什么的的的的的的的的的的的的的的。
--领域、问题、逻辑。(路径/方案/思路)(条件/目标/路径)	范畴。顶级思路/顶层思路。顶层领域，顶层问题。范畴与逻辑。范畴关系(同一范畴内、不同范畴内)、比较关系、规则关系、联动关系。
	>不能只以概念入手，也要从问题、思路入手。
	>谈话永远不能泛泛而谈：回答而已，别总想着怎么回答会好-显得聪明有创意有见解之类。必须要有目标，要预见和引导 话题走势走向，进而从当前条件 寻找 路径/思路。
	>开发时：反思总结 而 扭转观念，不要一来就想着实现代码，而应该想着本层逻辑、本层事务、具体交给下层实现-下层再进行拆解-完成它要做的那部分工作-塔式调用/直到具体的专门的一个个的实现//这个也是逆封装过程，类似塑造过程。
		>链式调用：默认实现Filter 放在 最后，用户提供的放在前面；从而用户提供了实现如果想提前返回则可以直接先返回不走默认。
		>插件的发现/用户回调类的注入：往往都是自动配置类bean  注入了外部的 beanFactory，而自动配置bean里就有本框架的核心类，而获取了bf,则各种规范的实现bean就都可以获取了，从而用在框架各个位置。
	>一切技术都是简便方法：更快速的方法、更省的方法、更安全的方法。更可靠、更通用的方法。
	>调用 就是 询问：询问就是调用。
--顶级关注点：任何事物 寻找到它 最有价值的一面  对我有用有好处，如果有 则 认识到通和使用到精。没有看到组织结构，必然混乱和觉得复杂、含糊不清、仍然不懂、没有消化--分解/拆解出有营养可以被利用来构造系统的基本元素出来。
	>于社会：形势和机会：最理性的分析 -->最准确的预测 --> 最周全的方案 --> 最简便的验证。很多人不相信完美的十全十美的方案的确存在，也就不愿思考和制作和逼近，而是找了一个草率的方案，执行后失败而亏损。
		>人际关系：在不能够失败的事情上取得胜利 才叫胆子大胆略胆识,其他不能产生直接或间接作用用处的事情 失败了也无所谓--就当作给对方一个面子--做个厚道人-而不是尖酸刻薄没人接近没朋友-朋友就是大量小事上免费的互相帮助。办事：对方道德好-好心人，道德一般讲利益-合情合理，道德差坑蒙拐骗一把-就损失了。
	>于痛点：目标和思路：最理想的样子 -->最真实的现状 --> 最顶层的思路 --> 最简便的做法。
	>于业务：领域和关系：帮助用户解决的问题是什么(用户只需要有什么条件、只需要做什么)(概括) --> 最理想的方案的最顶级的思路是什么 --> 要展示什么信息 、后端计算要什么信息 --> 领域的属性结构、领域在某活动中关联的领域(如人和商品在购买活动中关联了起来)--活动的间接结果/结果描述：就是将领域和领域绑定在一起--划归到一个活动领域中而成为一个活动领域的两个属性--毕竟一个活动本身也有领域/流程/规则--单独的领域是独立无关的,只有在活动中才和无关的领域关联起来;反过来发现两个无关领域关联起来了则一定在某个活动中(如人与商品-在购物活动中；商品和地点-在物流活动中)；领域是基本不变的有限的，真正大量的不断产生的同样需要记录的是活动数据--一是活动参与者多二是活动步骤多三是活动多，此外，展示独立的领域信息 其实价值有限且固定难以增长，而不断新增的活动让领域参与者有新的参与感、活动服务了参与者、给参与者带来了新的服务价值、解决了新的问题 满足了新的需要--这些都是在活动中完成的；所以创造活动并记录活动 数据 才产生巨大价值。纯线上活动、活动的线上部分、线下活动的线上模拟活动 ，寻找、熟悉和创造 新的 活动，是对一门业务能力的三个阶段。
						 --->明确 所有的领域(属性结构)、活动和活动领域(其属性为若干个独立领域+活动本身信息) --> 各个活动领域 将各个独立领域 直接或者间接 关联了起来，则可以建立起一张关联起所有独立领域的大表；也可以判断是否存在和 存在则找出 两个独立领域的关联路径；帮助数据分析和数据挖掘。---> 使用 活动领域 记录和重新梳理 活动的过程。记录了一个活动过程，就是记录了一个服务过程(展示-交易-消费-评论)  。例子：一个购物：独立领域的关联过程： 人和商品(下单) --> 人和商家(支付) --> 商品和地点 (物流) --> 人和地点(固定信息) --> 商品和人(物流终点) --> 商品和评论(消费者对服务打分)
		>澄清 业务流程是什么-->每一步需要展示什么信息、要上传什么信息
			>信息分成哪几部分: 本质上可以归结为 哪几个独立的 领域(有明显的自己的边界)，这些领域的属性结构树是怎样的。
		>有 存储、筛选、推荐和展示海量数据 功能的 系统：供看、消费、互动。
			>展示的目的：>展示的东西：展示有价值的信息、宣传(广告)、引导用户购买、引导用户提供信息(生产/上传)、引导用户交互信息。
	>于系统：：拆解和组建。本身的规律和统一的路径。范畴式创新(补集/包集/子集)
		>认识：
			>系统一定不是一个单调的整体，而一定是组装起来的。单调的整体 只是一个部件，甚至不能用来构造复杂稳定的系统，没有这个扩展性、接口。
			>系统的运转：组装起来之后，启动系统，开始触发系统，向多米诺骨牌一样 传递下去。
			>系统的逻辑结构：假设...则有。如果...并且...,那么对...情况,则有...。假设有几个关键的暴露关联关系并好度量的事实、实验、条件、不变性，则可以得出xx间接的关联关系。具体关系直接向上抽象，将具体量消除,推导更一般的关系；。或者将关系和关系封装在一起，得出更间接更远距离的关联关系，得出对一个事物的完备的关系集合。或者规律本身特征的概括，来得出其他未知规律也一定满足的特征-抽象特征(如不变性)
			>系统的元素：系统的元素可以用来构成其他系统。因为它职责单一，可以替换别人也能被别人替换，可以共享(越单一越能被共享,越复合越不能被共享)(共享就是拿来使用,避免重复劳动)，可以互换，各个系统的若干个位置上都需要，从而用这些元素构成的若干个这类系统，一个系统报废了拆分出的组件元素 还可以放在其他系统中，从而元素利用 最节省 最经济、利用率高-浪费少。单一职责 是 有限的 ，构成一个 有限的集合。承载单一职责的元素 种类 就是 有限的，构成有限的类别的集合。从而每类元素可以批量生产，缓存起来，按需提取。
				>元素的类别：职责类别：特殊的元素--骨架型元素(支撑连接其他元素)。业务型元素(完成自己的输入输出模型)。
				>元素的接口：元素 的 可以和其他元素连接咬合起来 而传递/接收 刺激/物质/信号 从而发挥自己的作用 的部分。
				>元素的连接：一个元素的接口和另一个元素的接口咬合在了一起，形成 信号/刺激/物质 可以从一个元素传递到另一个元素的状态、复合形态、复合物形态。新的复合物有新的接口。不同于两个元素的输出输出模型 的 新的输入输出模型 产生了，这是连接起来 最大的意义和作用和目的，还具有新的状态转移图---也有使用用途。
				>元素的组建：在一个框架型/架子型/骨架型元素上(丰富的接口)(本身甚至就简单的仅仅是一个接口集合) 连接 接入 若干 功能型元素 和 骨架型元素。
					>组建的目标-条件-路径：最顶层的目标--产生一种更间接更长路径关联起输入输出的高度更高长度更长远度更远的输入输出模型的子系统/子模块(以进一步组装出更强的系统/让本系统更强)
				>元素的封装：用边界将 若干个临近的元素(无论是否连接起来) 包装起来，隔离其他 集群元素，而有自己的独有的基础资源、基本元素资源池。形成独立的环境、独立的上下文、独立的数据中心(信息中心)，统一的对外接口、复合的多模式的输入输出模型。
			>系统的状态转换：可以从外部施加给系统的动作，系统接收该动作后向内传递 而改变内部形态 最终传递反馈到外部(表面) 输出 响应，施加的每个动作每种动作都会引起系统状态的往不同路径上的迁移转化。这个状态迁移图 往往 固定的，即从某状态到另一状态可复现/永远不变的，从而沿着这些路径找到最短路径达到我们希望的具有某特征的状态---往往也是我们的目标 和有用处的事情。
			>系统的拆解分解：(自然界的系统没有飞地--即没有可以控制感知的物理隔离的另一部分; 生的属于后代属于另一个系统;甚至不能产生和发送控制用途/通讯用途的电磁信号;声音等是否算)
				>系统要解决的问题本身的拆解：问题的树形结构。问题模型。
				>系统的目标的拆解：功能的拆解，职责的拆解。从整体 最顶层 的 一句话概括  到 最底层的 若干个 元素的单一职责的表述/表示。(最高目标就是最高层的目标,而非平等的目标中最重要的目标;区分两种概念表述)
					>顶层目标的确定过程-就是需求明确的过程:
						>最简理想模型功能的明确过程：最简单目标、简单模型功能、理想模型功能、最理想条件下的模型功能、各种因素都不考虑进来的模型(正相关影响因素/负相关影响因素/微扰因素)、理想实验模型、实验功能、脆弱功能、简化功能、本质功能、核心功能的明确过程、澄清过程、细化过程、精准描述的过程。还只是一个实验理想模型。
							>理想条件的特征：其他阶段不考虑-只考虑核心阶段、其他影响变量不考虑-只考虑主要影响变量、具体细节情况不考虑-只考虑最抽象上层的情况。其他功能不考虑-只考虑单一功能。
						>其他一个个平级约束条件逐个考虑进来后 模型逐次扩展后的新而又新的模型的明确过程：新目标、新的主体架构、演化后的主体架构。扩展对基本功能的表述，将其中的具体的东西向上抽象化一般化。最苛刻条件下的模型功能。
							>约束条件的特征：限制条件、故障、条件变量值变为极值特殊值、险恶的环境、多变复杂的环境。必然会自然会发生的遇到的新需求，第一版现在就会遇到的其他需求。把这些需求明确下来--为了需求的完整和深刻 先只讨论需求有什么是什么 此环节不考虑实现。
							>考虑的目的/必要性：让系统健康成长、稳定成长、能适应险恶的环境、自愈自修复。生命力更顽强。
						>目标模型的向上抽象过程：功能增强过程。封装进其他目标模型，和其他目标模型封装到一个新的目标模型。
							>增强这个目标模型：依赖这个模型、监控这个模型、统计分析监控量并得出结论发出控制信号调节/指挥/引导/服务/治理/保护/优化/自动化/智能化 这个目标模型/系统。
							>考虑的目的/必要性：考虑到各方面各线可能增加的需求和功能-进行预留和预备、让系统扩展性更好、性能更好、功能更多还不乱、更稳定可靠、可扩展也可卸载替换功能模块、更好的可控性-更细更广可控。
						>目标模型的向下细化过程：向下一层层具体化展开明确确定。
				>系统的解决思路的拆解：方案思路的拆解。思路模型。
					>思路中的有限类型元素集合：
					>思路中的当前可以使用的条件的集合(组织成结构)：
				>系统的组件的拆解：组件模型。分解出下一层的构件，下一层的构件再到下一层的构件。一层层分解下去(如肉分解出蛋白质-蛋白质到氨基酸,来到可以被吸收的层次)(如汽车拆解出底盘-底盘拆解出发动机系统)，拆解到元素层级，最广泛被使用/重复利用的 元素级别。
		>使用：	利用和扩展。训练(它)和增强(它)。整合(到一个方案中)与改造(更适配一个方案)。
			>该系统如何应用在方案中：哪些类 哪些问题 的 方案  会使用 到 这种系统。就像用人 一样 用系统。
			>这些方案如何应用在业务中：这些方案又可以用在哪些业务中。
				>方案的评估和论证：
			>这些业务如何解决人们遇到过和将遇到的系统型的个人又难以独立解决的希望有组织-公司提供专业的服务来解决服务的问题/痛点： 
				>动作的条件、目的和规范：如伸缩 这个动作。
	>于创业：人们当前遇到了什么问题 --> 人们会遇到什么问题 --> 人们想要解决而无法解决想别人提供可以解决的哪怕是有偿的也要解决的问题是什么 --> 先 概括出用户想要而缺乏且看到则感到惊喜的服务是什么
>使人不惑：
	>结构：将系统呈现为结构形式来表达，可以帮助分清主次，找到本质入手，立刻看到关键、看到本质的轮廓、要义的切入点、概念。
		>一个变量 可能 也是 一个结构：
	>大量的困惑和不理解：都是 理解推进过程中 没有建立中间概念 ，即 没有找到可以 对某 事物的 理解作用起到增强作用的 中间概念、桥梁概念，通过桥梁概念来找到前后的联系，上文和下文的联系，导致结果突兀难以理解不明所以 。所以 理解的推进过程 中 中间概念的 找到 至关重要。	
		>困惑的根本原因：联系是断开的。上文和下文的联系 是断开的。或者说是 强硬 联系起来的。联系的一点也不自然。
		>没有归结到根本：没有将上层抽象的概念和 底层 基本基础的 牢固的概念 建立 联系，建立 坚强 牢固的 联系。上下之间缺少 中间概念 也会悬空 而 难以理解 抽象概念。
		>强硬理解：就是 不知道 条件到目标的路径，而强行让自己认为条件到目标就是必然的/直接的/一步的/显然的/不关心中间过程的。完全就是记。只知结论。
>目标：对一种系统 进行 领域建模，系统架构。总结出新的 一类架构，新的一类问题/共同问题。		
	>写研究报告、策划方案、完整讲清楚-八面受敌：
	>验证方法的进步和改进：
>公式推导的问题/痛点、想法、目标、起点、条件和路线：每次公式推导必须明确，从而清晰思路。
	>问题的提出：推荐算法中 通过 相似用户、相似物品计算 用户对 物品的 喜好值 并不是彻底的一般的方法，更像是一种临时的方法。所以想寻找一种通用的统一的一般方法。
	>想法的得出：评分矩阵里的每个评分 为什么是这个 值，思考这种必然性、确定性，不那么显然的显然性，任何一个确定的一定的取值则要么是公理要么就可以继续分解用更基础的概念集合更本质的元素集合 来解释来计算来表达出计算过程来表达相互作用得出该结果的过程，任何一个值 要么 是 公理值 要么就是 计算结果。
		>把评分值当作计算结果 反推计算过程：假设是一个简单的线性组合得到的计算结果，而线性组合可以分解为两个向量的乘积，而这个评分值显然是由用户的特性和物品的特性决定的，而特性相乘，必然一个是数量-一个是含量，显然用户要求的是数量，而物品有的是含量；所以两个特性向量，用户的是用户的特性数量/偏好数量/需求数量，物品的是物品的特性含量。
	>转换后的目标：计算最优的用户特性数量矩阵和物品的特性含量矩阵。最优的标准：明确为 方差最小。在这个约束/要求下 计算PQ矩阵。
	>起点条件和顶级想法层层细化：在这个约束/要求下 计算PQ矩阵的数学方法：梯度下降法/最小二乘法。用差量的方式 甚至可以用 迭代法：Puf(t+1)=Puf(t) + α * зLossui/зPuf(t)	, 而 ^rui = ∑Puk*Qki, Loss = ∑(rui - ^rui)^2, 为防止overfitting,添加正则项控制过拟合：Loss = ∑(rui - ^rui)^2 + λ(∑Puf^2 + ∑Qfi^2) = f(P,Q)
	>方案的证明和优化：
		>对于 ^rui = ∑Puk*Qki ： 此表达式计算^rui 显然只考虑了 Pu和Qi相互作用的部分，即用户和物品共同决定的部分--通过需要数量*具有含量 来 共同确定的部分，实际情况中 可能还有 只跟 用户、只跟物品有关的因素 bu, bi, 甚至跟用户和物品都没关系的因素μ ，所以 修正后的 ^rui = ∑Puf*Qfi + μ + bu + bi ; bu往往可以用用户的性格打分习惯严格、温和来决定；bi往往和物品质量有关；
		>对Puk: 是用户对各个特性的需求量，而用户实际 对这个特性 的 需求量 可能 会 因为 用户 购买了 越来越多的物品后 而 有所改变--比如历史买了富含维生素C这个特性的西红柿 而 下次对 黄瓜这个同样富含维生素C这个特性的物品 需求量 就更低了，所以需求量 需要和用户的购买历史物品 关联起来，即 Puf 替换为  Puf + ∑Yjf/sqrt(N(u)), ∑Yjf就是已购买的物品对此特性的需求量的影响力之和。SVD++
		>再次优化：则是 考虑时间因素了。rui的各个因子都是时变的了。
	>符号的拆解和封装：符号也可以拆解开 用 更基本的 符号来表示，直到 +- 等。其他符号都是简化某个使用基本符号构造的复杂运算过程 而 定义的；就像 计算机里封装出一个个函数一样 封装出一个个符号。
		>积分符号 也是 一种简化符号，是对 某个区间 分割 出的 无数个 连续的无限小量 求和 这个过程 的简化 表示。Σx*f(x)dx = ∫x*f(x)dx 
		>乘法符号 是一种简化符号：是对 k个 相同值m的 求和 这个 计算过程 的 简化 表示，等值表示。C = m+m+..+m = k*m。。。数量和单位数量值--方便总量值的计算。(计算总量 和 可以摆出多少种组合 都是最原始的需求，野人时代就有)
			>乘法第二种含义：同时性/两个集合的元素能够组合出的所有结果构成的集合-方便组合数的计算。比如两个独立事件同时发生的概率。一个事件发生的概率 等于 这个事件代表的情况数 占 全部情况数 的比例，(一个事件 是若干个情况的集合)(一件事情 全部可能出现的情况)(一个实验 全部可能出现的结果)(一件事情 出现了某种情况(即所有可能情况中的某种) 是 一个 概率事件)。
				>当两件独立的事情联合构成/视作 一件新的事情时：则这个新的事情 全部可能出现的情况数 就是 两个子事情的情况数之积(组合出的结果)。
				>同时性：代表了组合空间的增大。
		>触发符号的来源：计算比例、占比。计算倍数则是另一种 源头。概率 是 表达 情况的占比。		
	>推导这件事情也需要一步步推进：每一步都走的很小，很连贯，来保持思路的连贯。
		>
>举动-痛点：要解决的问题、目标、条件/起点/底层/原子、思路。最顶点的思路，最上层的确定性/认识。
	>解法/解决方案：要点：利用架构/封装/塑造思想，以基本的定义为核心，每个关系都必须找到 值相等的原因，每个乘法都必须找到可以相乘的原因和应该相乘的原因和必须相乘的原因,其他运算符号也是、变换操作也是--来找到隐藏的前提条件/理由/根据，用更基本的概念/不变性 来解释，直到最底层、公理层。来建立起自底向上的基本到抽象、原子简单到复合抽象的层次概念体系  和 从内向外的 关联网络体系。实现 表象到内核 内核到另一个表象的自由简便推理。
		>表象：数据
			>表象特征：表象度量。表象规律。数据的特征；数据的结构，数据的模型。
		>假设：同类数据具有一定的统计规律性。X和Y具有联合概率分布。
			>同类：共同性质。
			>规律性：相似性。共同特征。统计规律。
				>具体假设：训练数据和测试数据 是 按照联合概率分布P(X, Y)独立同分布产生的。
				>具体假设2： 模型的输出值 和 样本的y 值 之间的差值 越小 则模型 越优。
		>监督学习：选模型，定参数。假设空间 + 策略空间 + 算法空间
			>假设：数据独立同分布。要学习的模型属于某个函数的集合--假设空间。
			>过程：应用某个评价准则，从假设空间选择出一个模型，如果它对已知训练数据和未知测试数据 在给定的评价准则下有最优的预测，则它是最优模型。确定模型的参数则需要算法实现。
				>模型：
					>输出可以确定具体值：条件概率分布P(Y|X)  
					>输出只能确定其取值分布：决策函数 Y = f(X)
				>策略：怎么选模型 --期望损失最小的模型；
					>损失函数：一次预测的好坏的度量，度量一次预测的错误程度，即模型输出值和样本值的差距。所以这个损失函数/代价函数 应该为 f(X)和Y的函数/非负实值函数：L(f(X),Y) ,具体形式，最简单到最复杂：可以是：0-1损失函数，平方损失函数(Y-f(X))^2，绝对损失函数|Y-f(X)|；对于分布函数和输出值的差距，可以用 对数损失函数/对数似然损失函数 来描述L(Y,P(Y|X)) = -logP(Y|X), 即如果这个分布函数在y1是有最大值，而实际取值y2微微小于y1,那么对应的分布函数值则要小,取对数后为绝对值小的负数,所以还有一个负号,结果就是小正数，正好可以表达偏离不大的含义---相当于概率密度大则误差小。
					>风险函数：平均意义下模型预测的好坏，定义为 损失函数的期望。期望 = Σ 随机变量的取值 * 取值概率  , 所以 Rexp(f) = Ep[L(Y,f(X))] = ∫L(y,f(x))*P(x,y)dxdy  但联合概率分布是未知的且不应该事先知道(否则就可以从它直接计算出条件概率P(Y|X)来了), 因此需要一个不使用联合概率密度 而 计算 平均误差的计算表达式， 最简单的 就是 直接计算误差的平均值：1/N * Σ L(yi,f(xi)) ---这个记作 经验风险/经验损失。Remp(f) 当N逼近无限大时，经验风险逼近期望风险Rexp(f)
					>经验风险最小化和结构风险最小化：
						>经验风险最小化策略：认为使得经验风险最小的f就是最优模型： 即 计算  min 1/N ΣL(yi,f(xi)) 
							>例子: 极大似然估计：模型是条件概率分布，损失函数是 对数损失函数，经验风险最小化 就是  min -log(P(Y|X))*P(Y|X) 即 当概率P取值越大越接近于1则log(P(Y|X))取值就越小,而取这个值的概率就是P,因此 按照期望定义 的 经验风险最小化 计算表达式 就是：min -P(Y|X) * log(P(Y|X)
								>极大似然估计：认为 在 给定的 条件下 ，一件事情 发生了 xx 情况，则认为 这种情况发生的概率比发生其他的情况的概率更大，即发生这个情况的概率是最大的。如果这个情况是个复合情况，则将这个情况发生的概率用 其 子事件 发生的概率 和 表达事情条件的参数和xx情况的度量 运算起来表达，则得到了一个关于 子事件概率和事情条件和xx情况的度量的表达式，而这个情况发生的概率最大，则就表示通用表达式有极值，且在xx情况时取得，即如果 实验/实际上 条件为yy，xx情况的度量取值为kk, 表达式有极值，有极值代表可以求导数/从而得到了 关于情况的度量 + 子事件概率 + 事情条件的定量关系式。
									>例子：一个口袋里若干个球，取出10次，得到了7个红球3个白球，则 下次取出红球的概率。显然，取出10次 是事情，出现7红3白是情况，取出红球的概率是子事情。记复合事情为A,则用子事件概率表达为：P(A)= p^a * (1-p)^(10-a), 有极值在a=7处，首先有极值代表可导，得出关系式--先不改单调性用对数处理，可以得到导数=0，简化为 p=a/10
							>当样本容量小时：计算的误差就大；产生 过拟合 over-fitting 现象。
						>结构风险最小化策略：避免过拟合。在经验风险上增加 模型复杂度： Rsrm(f) =1/N ΣL(yi,f(xi)) + λJ(f), J(f)是定义在假设空间F上的泛函。模型越复杂则J(f)越大。λ用来权衡经验风险和结构复杂度的影响。
							>例子：贝叶斯估计 中的 最大后验概率估计。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示，此时结构风险最小化就是 最大后验概率估计。
								>概率：
									>条件概率：P(B|A) 定义为 A发生后发生B的概率，即已知A已经发生取值为a，则此时B发生取值为b的概率。显然 A=a时 通过联合概率分布 则 B的取值总情况 是 可以确定的,甚至此时B有可能什么值都取不到/即总情况就是0，但一般总情况还是分立的有，这个总情况数就是 ∫P(a,B)db, 而B=b代表的情况数则是P(a,b)db 所以A=a时B=b发生的概率=此时的取值情况/此时的总情况= P(B|A)= P(a,b)db/∫P(a,B)db, 而 ∫P(a,B)db = P(A)定义为边缘概率；所以 P(B|A) = P(A,B)/P(A)
										>贝叶斯公式：因为 P(B|A) = P(A,B)/P(A), 同理，P(A|B)=P(A,B)/P(B) 从而有 ：P(B|A)=P(A|B)*P(B)/P(A) , 将 P(A|B)/P(A) 称为标准似然比， 为 A为B事件发生提供的支持程度。可以>1=1<1, 表示A事件的发生 增大了/不影响/降低了事件B的发生。P(B)为先验概率。
										>概率函数：P(x|θ) 已知θ,x是变量。表示x取各个值的概率。(分布概率)
										>似然函数：P(θ|x) 已知x,θ是变量。表示不同θ下，x出现的概率(随机变量取值为x时的概率)(取值概率) 。似然函数 L(x;θ) 是关于实验结果X=x1,x2,x3...xn的函数。由概率函数，知道x1发生的概率,同理xn发生的概率。一轮n次的实验，这个事件，其实际发生a次红,n-a次白的概率：P(x|a,p) = ∏P(xi|a,p)*∏P(xi|a,p) = p^a *(1-p)^(n-a) ,没做实验的时候，a,p都是不确定的，即到底出现几次红是不确定的，但是认为实验后实际出现的那个次数，应该是最可能出现的，即是那个取值会使得P(θ|x1,x2...xn)这个似然函数取得最大值。
										>概率函数和似然函数的关系：似然函数 是 实际 取值 的 概率， 是 n次实验 X 实际取值x1,x2,...xn 这个大的事件 发生的概率(像这样的结果的作为一个事件它发生的概率,似然事件)(引入一个似然事件的概念/这个中间概念/桥梁概念 来辅助理解)，发生的和参数相关的概率。即一般为 ∏P(xi), 是用n次结果的概率值算出来的(包含未知参数,但参数是值类型而不是随机变量-当作值而不是当作分布)。也可以认为是 取值带入其概率密度函数而相乘的结果，就是似然事件的概率密度。所以 准确的说 叫似然密度。
											>概率函数 则 是 实验前 就 对 X 取值的分布 的估计，分布概率估计。其实是 密度函数。
											>似然函数 是 似然事件 发生的概率， 是 θ未知时 实验出现x1,x2...xn这个似然事件的概率。(可以认为是P(X|θ))。。θ虽然不确定取值是多少，但它的取值一定会使得这个似然函数取值最大---形成最有可能发生的特点，否则似然事件就不是最有可能发生的--和假设矛盾(假设为似然事件最可能发生)。不知道θ的取值，也不知道θ的分布，等效含义 就是 θ可以取任何值，各个值事先不知道取值概率--或者认为概率相等。即此时的 θ 分布函数为 1/取值区间长度；但这种假设，隐含假设 可能和实际是差别很大的，即实际可能是正太分布，β分布，那么最大似然估计的结果就会产生较大的偏差(比如实际θ服从的正太分布瘦高且顶点在1,而最大似然估计函数的顶点在Θ=5;而事实上由θ的正态分布知道几乎不可能取值这个；因此θ的取值既要兼顾它的分布也要兼顾似然函数取极值；由于分布函数和似然函数的横轴/自变量都是θ，因此实际取值为这两个因素综合起来最大更加合理，综合起来的方法--加法不好--因为是概率-结果就可能超过1，乘法则有同时成立性的意思-两者相乘有期望的单项表达式的特征-即单项(对期望的)贡献值/贡献量的意思--贡献量最大的单项则认为是事实中最可能发生的,对应的θ的取值也就是最合理的；这就是最大后验分布估计)。
									>边缘概率：两张边缘图。所以叫。
									>事件同时发生的概率：P(A)*P(B) 是因为 两个事件发生与否对 另一个事件的分布没有影响。
									>全概率：从情况占比来理解非常自然。P(B) = ΣP(B|Ai)P(Ai)
								>最大后验概率估计：认为参数θ 具有某种概率分布，称为先验分布。从贝叶斯公式，得 P(X|θ)= P(θ|X)*P(X)/P(θ);其中P(θ|X)称为后验分布。θ满足一种分布，那么似然函数 作为θ的函数，则也满足一种分布--但知道这个分布意义不大；另一方面，由于约束了θ事先要满足特定的分布，因此似然函数作为似然事件的概率也必然是在θ已经服从这种分布下做的实验会产生的结果的概率，因此θ的最可能取值可能不是使得似然函数取得最大值的那个值，但似然函数也不一定就是θ取得它的先验分布的极值时的结果--毕竟这个先验分布只是个假设---并不是真实的分布--而似然函数则一定是θ满足真实分布的结果。θ既要/会处于自己分布概率大的位置 也要/会处于使得似然函数值尽可能大的位置，对两者的贡献都是正相关则进行综合考虑--最合理简单不是加法而是乘法(毕竟两者单位不同)---而概率和函数值的相乘 代表含义为期望--而 θ取某值时的概率函数值是对实际期望结果的贡献量，显然 实际取值应该是 使 贡献量最大，即 argmax{P(X|θ)P(θ)}。这个就是更加合理的，考虑了θ的分布的 对基本事件概率的估计。
									>思想：假设Θ实验前已经知道它是个随机变量且服从某个确定的分布--称为先验分布(先验--就是先于实验)，在似然函数上 把 这个先验分布考虑进来，
									>后验概率和似然函数的关系：后验概率 ∞ 似然函数*正则项P(θ)---这里不是加法。
								>贝叶斯估计：估计θ的分布。在引入实验X/大事件X后 对θ分布概率的重新估计。先验估计为P(θ), 做实验X后的后验估计为P(θ|X), 注意P()都是概率的意思--P(θ)里面的θ不是指事件而是指随机变量---事件是随机变量的一个取值-概率就是随机变量取该值的概率；所以随机变量是一场实验的意思，实验可能的结果就是 随机变量可能的取值；谈实验重点要谈可能的结果/每个结果对应的概率。
									>符号术语：P(θ) 是随机变量θ的概率(即它的分布)；P(θ|X) 就是在实验A/事情A结果明确为X之后/结果发生之后随机变量θ的取值概率(分布)(这个概率中就可能夹杂X的结果值)。。同理 P(X|θ) 就表示 已知存在参数θ但不知具体取值 时 实验A出现结果X的概率。所以：P(X|θ),P(θ) 横轴都是θ的取值，但纵轴第一个是事件X的概率,第二个是随机变量θ的概率。P(θ|X)这里X仍然是事件，而θ是随机变量，表达事件X发生之后随机变量θ的概率(可能受X的影响而和P(θ)不同)。
									>得出等效估计目标：即估计 P(θ|X) , 即估计事件X发生后随机变量θ的全新的分布概率。根据贝叶斯公式，P(θ|X) = P(X|θ)P(θ)/P(X) 和 边缘概率条件概率的关系：P(X)=∫P(θ,X)dθ=∫P(X|θ)P(θ)dθ  从而 得出 ： P(θ|X) = P(X|θ)P(θ)/∫P(X|θ)P(θ)dθ  这就是贝叶斯估计。而先验分布P(θ)是假设出来的，而P(X|θ)则是实验结果X可以表达出来的。从而可以估计受X影响下的θ的分布了。
									>估计新值x: P(x)=∫P(x|θ)P(θ)dθ, 但是用P(θ)这个先验估计是不准的(当然如果是真实分布肯定是准的)，因为没有考虑实验结果X的影响，实验结果影响了P(θ)的估算，即应该替换为P(θ|X), 考虑了X的影响，则左边P(x)应该改为P(x|X), 所以：P(x|X)=∫P(x|θ)P(θ|X)dθ 这样 P(x|θ)可以表达出来而P(θ|X)可以贝叶斯估计出来，从而就可以估计新值了。
				>算法：求解最优模型的计算方法。最优化的问题的解析解不存在时，数值解的计算方法。
			>模型评估使用的损失函数：
				>训练误差：不断减小
				>测试误差：先减小后增大
				>误差率：
				>准确率：
				>过拟合：对训练数据预测的好，对测试数据预测的差。模型参数过多了。典型的用多项式拟合，当项数足够多时，甚至可以过每个样本点，而使得误差为0，
				>泰勒级数/多项式：可以看作是 线性组合，系数和指数项的线性组合。
				>模型选择的方法：
					>正则化：经验风险 上 增加一个 正则化项	
					>交叉验证：数据分为 训练集、验证集、测试集。
					>S折交叉验证：N-1份训练集，1份测试集，来回N次。
				>泛化能力：模型对未知数据的预测能力。
					>泛化误差：模型输出对实际的误差。为 期望风险？
					>泛化误差上界：比较两种学习模型的优劣。为 经验风险 + ε
					---训练误差小，则泛化误差小。
				>生成模型与判别模型：
					>生成方法：学到的模型为：生成模型。通过学习 联合概率分布，通过 边缘概率关系 而计算 条件概率P(Y|X)
						>朴素贝叶斯法：
						>隐马尔可夫模型：
					>判别方法：学到的模型为：判别模型。直接学习 P(Y|X) 作为预测的模型。准确率更高，对数据各种程度的抽象、定义特征和使用特征，简化了学习问题。
						>感知机：
						>决策树：
						>支持向量机：
						>条件随机场：
						>最大熵模型：
				>分类模型：称为 分类器。对新的输入进行的预测输出，称为分类。可能的输出为类。
					>学习出分类器：
					>对新的输入进行分类：
					>二类分类问题：存在100%精确率，而1%召回率的。
						>精确率：对于分类器预测的所有正类中，正确的比例。
						>召回率：对于事实中所有正类中，分类器预测中的比例由多少。
						>调和均值：P+R 叫和，  1/P + 1/R 叫 调和。
					>常见分类：客户分类，日志分类，手写数字分类，网页分类。文本分类(输入文本的特征向量，输出文本的分类)
				>标注问题：自然语言处理中的词性标注。从文章中抽取基本名词短语。
				>回归问题：输入变量到输出变量之间的映射函数 的寻找 ，即函数拟合。P(Y|X)输入为值时输出的是分布概率；Y=f(X) 输入为值时输出为值。
					>多元回归：输入变量多个，输入输出变量关系的类型---线性回归和非线性回归。常用损失函数：平方损失函数。此时回归问题的求解：最小二乘法。
					>例子问题：投资风险分析 可以形式化 为 回归问题。股价预测(样本：(当天营收，当天利润，当天股价), 过去30天的数据)---关键是影响股价的因素量化的好。
		>假设空间：定义为 决策函数的集合。 F = {f|Y=f(X)}  这个表达式 读作：/表示的 含义为： F 是一个函数集合(最外层) , 元素为函数f, 元素满足的条件为：Y=f(X) 。即 集合 = {元素|元素满足的条件}		
			>假设空间的细化： F = {f|Y=fθ(X), θ∈Rn} , 依然： F是一个函数集合，f是元素，元素满足的条件为： Y=fθ(X), θ∈Rn  这个逗号 就是 其中  的 意思，子条件的意思，条件的条件的意思；其中 θ满足它是一个实数空间的向量。定义为参数向量。
			>假设空间用概率分布表示： F = {P|P(Y|X)} ，依然：F是一个分布函数集合，P是元素/分布函数，元素满足的条件为：P(Y|X) 即 当输入为X时输出Y的分布函数。同理，F = {P|Pθ(Y|X), θ∈Rn}	 需要由一个参数向量θ决定。
		>特征空间：
			>特征向量：一个特征向量代表一个特征，代表特征空间中的一个特征。
		>输入空间：输入实例由特征向量表示，而输入空间可以映射到特征空间。
			>输入实例的特征向量表示：x = {x1,x2,...xn}
		>样本：输入输出对
		>回归问题：输入变量和输出变量都是连续变量。
		>分类问题：输出变量为有限个离散变量。
		>标注问题：输入输出都是变量序列。
		>非监督学习：
		>强化学习：
		>感知机：
			>输入：实例的特征向量。为输入空间中的一点。输入空间则为R^n中的一部分。
			>输出：实例的类别，+1，-1两值。
			>感知机：输入空间 中 将实例 分为 正负两类 的 分离超平面。属于判别模型。
				>模型： y = f(x) = sign(wx + b) 其中 w为权值向量，b为偏置。所有的线性分类器：{f| f(x)=wx+b} 
				>几何解释：wx+b=0, 当作 R^n空间中的一个超平面，w是法向量,b是截距。例如，假设这个超平面上有任意两点x1,x2, 则满足 wx1+b=0, wx2+b=0; 则 w(x1-x2)=0, x1-x2表示两个点之间的连线向量，则属于这个超平面，而w点乘它=0，则w始终垂直于超平面上任意两点的连线，所以w是法向量。偏置 就是 输入都是0的时候 的输出，就是b.
					>超平面将空间分为了2部分：这个超平面称为 分离超平面。		
				>数据集线性可分：超平面可以完全将两类分开。	
			>损失函数：误分类。
				>选择：误分类点 到 超平面的 总距离。
					>点x1到超平面的距离：通过法向量w, 假设点所在的平行于超平面S的超平面为S1: 则 wx1 + b1 = 0, 则S和S1之间的距离 可以通过截距差来间接计算，截距是其他维为0,最后一维的取值的差：显然这个截距差= -b1/wn - (-b/wn) = (b-b1)/wn = (b+wx1)/wn, 截距向量=(0,0,0... (b+wx1)/wn), 截距向量在法向量方向的投影就是超平面之间的距离，而投影=两个向量的点积/||w|| ,而w=(w1,...wn) 从而 点积=b+wx1, 所以投影长度= |(b+wx1)|/||w|| 即为点到超平面的距离。
					>误分类数据(xi,yi)的特征： -yi * (wxi + b) > 0  即模型输出和样本值矛盾/相反。从这个等式有：|-yi * (wxi + b)| = |-yi| * |(wxi + b)|= -yi * (wxi + b)	= |(wxi + b)|, 所以 误差点到超平面的距离 等价为 :  -yi * (wxi + b)/||w||  。更一般的，带入为正值 则是误差点，带入为负值则为正确分类点。
					>误差点到超平面的总距离：-1/||w||Σyi(wxi+b) 即为损失函数L(w,b) = -Σyi(wxi+b) 显然，误分类点越少，这个正值就越小。实际操作：所有样本带入，只取单项为正的 进行求和，使得最小的取出来。假设已经找到了误分类点集合。
			>损失函数极小化算法：梯度下降法 对 损失函数进行极小化。原始形式和对偶形式。
				>函数极值的算法：因为函数的极值 出现在 使得函数的导数=0的地方，所以初始点要朝着 这个位置去，而最快的路线 就是 使得取值变化最快的方向。对于超平面 z = ax1 + bx2 + c, 在某点 变化最快的方向，可以假设该点附近有一点，两点相距单位1长度，则可以证明，使得z取值最大的这个附近的点(m,n)就是 满足 m/n=a/b, 即(a/sqrt(a^2+b^2), b/sqrt(a^2+b^2)) 也即 (a,b)方向；同理可以扩展到n维度的场景，也是这个方向是变化最快的方向--偏导数构成的向量的方向。
					>同理扩展到一般的函数f:  f(x,y) 同理在P点 附近 有一个点 f(x+d1,y+d2) 其中 d1^2+d2^2=d^2=1,  则 该点的函数值= df/dx * d1 + df/dy * d2 , 则 这个函数值的最大值的取得时机？其中 d1,d2 是未知的，使用 导数=0，可以得出 同样的关系，df/dx / df/dy = d1 / d2 从而变化最快的方向还是：(df/dx, df/dy) 同理可以扩展到一般的n维函数。
						>简化计算： 这个附近点的函数值 =  df/dx * d1 + df/dy * d2 = (df/dx, df/dy) * (d1, d2) 表示为点积的形式，显然 (d1,d2)是个单位向量，因此 = |(df/dx, df/dy)| * cosθ * 1, 显然，只有当θ=0时 取得极值，即两个向量平行，也就是 偏导数方向 向量 是 变化最大的方向。更一般的，两个n维度向量相交 则必然处于同一 二维平面，所以同样是 可以分解为 模长和夹角的积，因此仍然是夹角=0时取最大值，所以仍然是同面平行，仍然是偏导数向量方向 是 变化最大的方向。
					>损失函数L(w,b)的偏导数向量：对w: зL/зw = -Σyixi, 对b: зL/зb = -Σyi ; 所以w要沿着 зL/зw 方向变化：如果初始w=w0, 则下一个w'= w0 - зL/зw, 同理 b'=b - зL/зb 这样 就会使得L的值朝着极值变化，很显然这个L的极值 就是0，即朝着L不断减小的方向变化--L减小 就是 误差点到超平面的距离更小。当然，前提要求 这个点 集合 是 误分类的点集合，误分类 就是 yi 和 wxi+b 的值符号相反；因此当初始w,b确定，则可以确定误分类点集合，从而得出w',b', 然后又可以得出误分类点集合，就这样不断的迭代下去，直到L 不再减小为止，就得到了最好的w,b。为了小步前进，增加因子η, w'= w0 - η * зL/зw = w0 - Σηyixi, b'=b0 - Ση *yi
				>算法的收敛性：
				>学习算法的对偶形式:  上述 迭代算法中 下一个w,b的计算中：w'= w0 - η * зL/зw = w0 - Σηyixi, b'=b0 - Ση *yi 初始w0,b0应该是随意的，不妨为0，不妨假设经过了ni次迭代，那么最终的w = Σniηyixi, b=Σniηyi, 令 α=nη, 则 w=Σαiyixi, b=Σαiyi ;从而 以使用 样本点的方式来表示 模型： f = sign(Σαyixi * x + Σαiyi) 为保持变化，最后一项不替换：f = sign(Σαyixi * x + b) , 所以显然：计算最后的α,b: 只需要：α' = α + η ， 而 b' = b + ηyi  从而出现了 xixj 形式-Gram矩阵   好处在于 对偶形式 迭代次数更少 就达到了收敛----但是在物理几何含义 上 就丧失---很不明显了。
			>用感知机对新的输入实例分类：
		>k临近法：分类问题中的k临近法。
			>输入：实例的特征向量。特征空间中的点。
			>输出：实例的类别。
			>训练集：实例类别已定。
			>对新的实例的预测：根据其k个最相邻的训练实例的类别，通过多数表决等方式 进行预测。
				>相邻的定义：两个实例点的距离--欧式距离/Lp距离/Minkowski距离。Lp就是两个向量的 p范数。 p=2就是二范数，就是 欧式距离。 p=∞时，Lp=max|xi-xj|
				>单元：距离实例点 比 其他实例点 更近的所有点的集合-即一个区域。所有的单元就形成了一个对特征空间的划分。
			>参数：k值，定义为临近需要的距离界限，分类决策规则；
				>k值选择：太小 则可能 临近的都是噪声 ，容易 过拟合。k大，则估计误差小，而近似误差大；模型会忽略大量的局部信息有用信息--预测变得笼统。 ---交叉验证法 来选取 最优k值。
			>kd树：二叉树，节点就是实例/样本点。
				>k维超矩形：k就是样本的维度；超矩形 要包括 所有的样本。
				>超矩形切分点：所有样本点的某一维度的分量值的中位数。
				>切分操作迭代结束的标准：子区域内没有 样本点/实例。
				>实例归属区域的划分标准：左节点为 实例的该维度值 小于 中位数，右节点保存 实例的该维度值 大于 中位数，根节点保存 实例的该维度值 等于 中位数。
				>给定节点下搜索它的临近节点：因为兄弟节点是比较近的，而父子节点也是比较近的，所以一般只需要依次往上几代就可以 认为 在这个范围内已经找不到更近的点了，缩小了搜索的范围。而中间节点的临近节点则需要寻找它的所有子类。
				>给定可能不在kd树上的点，搜索kd树中离它最近的点：则可以从根节点出发找到 与 该点 同属一个 叶子节点区域 的 实例，计算和这个叶子节点的距离作为最邻近，然后 向上和父节点/兄弟节点...比较，更新 最邻近，比较的时候可以取巧---就是以目标点为圆心，与当前最邻近点的距离为半径的圆--是否与该节点表达的区域相交。
		>朴素贝叶斯法：基于贝叶斯定理 + 特征条件 独立假设。
			>样本：当作是 X,Y独立同分布 产生的 一组输出。  在X产生的条件下Y的分布= P(Y|X) = P(X,Y)/P(X) = P(X|Y) *P(Y)/P(X) 其中，先验概率P(X)肯定是可以知道的，所以在计算P(Y), P(X|Y); 即计算P(Y=ck), P(X|Y=ck)。
			>学习输入输出的联合概率分布：基于特征条件 独立 假设。
				>条件独立假设：一次 取得 n个值的实验，之所以取得是这n个值，或者说刚刚好 取 得的n个值是 这 n个 ，的 概率 是 跟 这 n个 每个自己有关系的，和 不受前后取值的影响；则称这n个取值 条件独立。假设 一个n次的实验，产生了 X1,X2,...Xn, 结果y为y1,...yk中的某个，则 yk对应的是X1....Xn中的一个子集，n次取值出现这种取值 组合/排序 概率 和 单次取得组合中的每个值的概率 之间 是有关系的，假设n次中每次之间没有影响，那么出现这种组合/排序的概率 就等于 依次 独立取xi值的概率 之积 (即同时取),严格的说 是出现 这种 排序的概率(用n=2证明;依次取3,5;或者取5,3概率一样)：整个排序事件的概率，和单独取值的多个事件 之间的关系： 即 P(X1=x1,X2=x2,...) = P(X1=x1)*P(X2=x2)... ，假设n次的输出都是yk,相当于加了条件，则排序事件和 单独取值事件之间 仍然有关系：P(X1=x1,X2=x2,X3=x3...|Y=ck) = P(X1=x1|Y=ck) *P(X2=x2|Y=ck)*... = P(X=x|Y=ck) X是实验，x是取值序列/排序
			>基于上述模型，利用贝叶斯定理 求出后验概率最大的输出y: 	
			>贝叶斯定理：条件概率-边缘概率 关系 ， 中 边缘概率 替换为 全概率公式即可：P(Y=ck|X=x) = P(X=x|Y=ck)*P(Y=ck) / P(X=x),  而 根据全概率公式，P(X=x) = ΣP(X=x|Y=ck)*P(Y=ck) k为索引。	即：P(Y=ck|X=x) = P(X=x|Y=ck)*P(Y=ck) / ΣP(X=x|Y=ck)*P(Y=ck), 上式带入，则有 = P(Y=ck)*∏P(Xi=xi|Y=ck) / ΣP(Y=ck) * ∏P(Xi=xi|Y=ck), 而对于后验概率的分布，显然认为 分布概率最大的ck 认为 就是 x属于的分类，所以最有可能的分类 就是：argmax P(Y=ck)*∏P(Xi=xi|Y=ck) / ΣP(Y=ck) * ∏P(Xi=xi|Y=ck) = argmax P(Y=ck)*∏P(Xi=xi|Y=ck) 因为分母对k是一样的。这个公式的解释：右边表达式 中 连乘表达式 部分：就是Y=ck时的那一个纵切面上，实验X做n次恰巧取得x1,x2,...xn这样的排序/序列取值的概率，显然，ck 不同取值时，这个序列的概率乘积是不同的，P(Y=ck)取值也是不同的，现实发生的就应该对应的是理论表达式上最有可能出现的应该是 概率最大的, 即 因ck不同取值 而 使得表达式 P(Y=ck)*∏P(Xi=xi|Y=ck) 最大时 的那个 ck 就是 参数ck的取值，就是x的分类值。 
				>argmax: 之类 符号 的目标 都是 求参数， 约束条件是 使得结果最大/最小。
				>期望风险：损失函数的期望。	E[L(Y,f(X))] 此时未知为f, 待确定为f, 使得 argmin E[L(Y,f(X))] 期望风险最小的f;。。但如果 f已经确定了，那么 对于输入x 待 确定的 就是 ck了，此时的期望风险 还是那个关系，但未知/待确定已经变为了ck, 且问题表明 里面的取值此时Y=y,f(x)=ck, X=x，问题即变为： argmin E[L(y,ck)], 而期望就是 概率和取值的累加，所以转换为：argmin ΣL(y,ck)*P(ck|X=x)
					>L(Y,f(X))的准确理解：是一个 二维分布，可以想象为 离散的 横轴为f(X),纵轴为Y，取值都是整数，对应(f(X),Y)处的取值为0/1, 那么E(L(Y,f(X))) 就可以转换为 先计算出 横轴f(X)的边缘分布，然后再计算这个边缘分布的期望。这两步表达为：,,,E(L(Y,f(X))) = ΣΣL(Y=ck,f(X)=y) * P(f(X)=y,Y=ck) , 又 g(y) = ΣL(Y=ck,y)* P(Y=ck|f(X)=y), 即g(y)看作是了f(X)这个随机变量的期望的单项，从而 Σg(y) = Ex(Y=ck|f(X)) = E(L(Y,f(X))) 则 g(y) 是一个随机变量，且E(g(y)) = E()
					>想法2：一个二维随机变量的期望 =  这个随机变量的一维固定取有限的离散值时 分别的形成的新的 一维随机变量的 期望的 概率概率组合：这个关系 可以证明，也可以形象的表示：E(X,Y) = ΣE(X,Y=ck)P(Y=ck|X)。总随机变量 分解为 有限个子随机变量时 总体和局部的关系。这是一个坚固的基础关系。可以用二维平面上的离散点的分布规律来作为例子说明-并触发证明。
						>上述思想：就是 局部均值 和 全局均值的关系！！最形象的解释，局部面积大小 占 全局的面积大， 显然就会使得全局均值受到较大的影响。上述中：分解出的 三行离散数  构成的三个子随机变量 就是 局部。概率占比 就相当于 面积占比。
						>使用上述基本局部均值和全局均值的关系：来对 E(L(Y,f(X))) 进行解释的结果就是 E(L(Y,f(X)))=ΣEx(L(Y=ck,f(X)))*P(Y=ck|f(X))=ΣEx(L(Y=ck,f(X)))*P(Y=ck|X), 其中注意：P(Y=ck|X) 代表作n次实验X时 输出Y=ck的概率---是实验统计的/理论序列计算的，而P(Y=ck|X=x)则代表作一次实验时输出 Y=ck的概率--这就是理论概率了。
							>求期望风险最小：就是 min ΣEx(L(Y=ck,f(X)))*P(Y=ck|X) 此时的未知是 f,待定是f, 即怎样的f才能使得 期望风险R(f) 最小？ 如果f已经确定，则 对于一次输入x,则可以计算f(x)=y, 那么再看风险，则为：min ΣEx(L(Y=ck, y))*P(Y=ck|X=x) 此时 未知的就是实际的输出ck了，认为能够使得 风险最小的 ck 就是 实际的输出值，因此  风险表达式再次变为：argmin ΣEx(L(Y=ck, y))*P(Y=ck|X=x) = argmin ExΣ(L(Y=ck, y))*P(Y=ck|X=x) ,根据L(ck=y,y)=0, L(ck!=y, y)=1, 从而 转换为： argmin ExΣ(L(y!=ck, y))*P(Y=ck|X=x) = argmin ExΣP(y!=ck|X=x) = argmin ExΣ(1 - P(y=ck|X=x)) = argmax ΣP(y=ck|X=x) 这个就是 y的后验概率，即使得 后验概率最大。
				
				
				
				
>统计和概率：统计  由数据推 概率，概率则 由概率 推 取值。	
https://github.com/ShangtongZhang/reinforcement-learning-an-introduction	
https://www.jianshu.com/p/9c153d82ba2d  贝叶斯估计。