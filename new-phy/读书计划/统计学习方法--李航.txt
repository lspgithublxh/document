---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。
>学习要有战略和格局：过于零散、孤立、末支、叶子的知识点就没必要化太多时间，懂得放弃。聚焦在根节点、枢纽节点、产生分支的节点、主干知识点、枢纽知识点、关联到很多知识点的知识点上。
 >对于主干知识点：要有全新的认识视角和进行精细的描述，大量的提问和好奇和进行必要的实验以获取支撑观点/导出观点的数据。
 >规范是工程最独特的特征.
 >慢慢读：
>一种新技术的学习：
 >它面对的情况和问题、它的世界观、它的方案、它的方案验证/论证/能处理的解决的所有情况及能成功处理的理由/功能边界
  >所有的软件：都可以看作是向上封装一层接口，根据自己的世界观封装底层而向上/对外提供统一的(统一的更简单的更直观的更业务的更少底层信息的)接口，底层包含一系列的第三方的插件/构件/组件；内部则去做兼容和调用(对底层)(对上层则做逻辑分解和底层实现)。
>知识混乱就是因为没有组织：
	>组织就是关键字树：几个单词就是每层上的每个节点的内容；
	>组织也可以看作逻辑树：有逻辑关系，逻辑顺序，逻辑联系的关键字的层层集合。层层囊括更精细的范围，层层划分范围。
>推进理解的属性发展拓展、问题延展：重要方式；
>什么是架构：架构也是从抽象到具体的考虑和描述；树形延展开来，可以写满非常大的黑板和巨大的脑图！！sharding-jdbc,dubbo,spring都可以这样方式来展现它的架构！！它的抽象到具体的考虑---本身才是架构！！！而不是什么模块、模式之类！！
>抽象设计：则某一层就不管上一层的含义和下一层的含义，即更抽象的含义或者更具体的含义；而是实现本层的含义；完成本层的含义指定的功能；。如网络协议的架构设计；	
>面向设计来理解，面向架构设计来理解，面向架构问题一层一层来理解它：面向设计来理解，所以按照面向对象设计的方式，看其中的对象、行为属性、流程环节逻辑。	
>找不到知识/描述 所对应的问题 ， 那么看书将没有条理纲领，变得零散琐碎没有组织。	
>不是按概念方式组织，而是按架构、问题方式来组织 笔记，书本内容。架构顺序，问题层次顺序。	
>架构不是设计出来的，也不是演进出来的(甚至不是迭代出来的--尽可能避免迭代)：而是问出来的。	
>每个方法方案都从属于一颗树，所以找到一个方法巧妙方法仅仅是第一步--找到从属的层次树 有更大的价值；(无论是谁想到的方法/概念，都要这样更进一步)
>解决问题的办法就是提出问题：类似递归和动态规划；。。权衡就是线性规划；	优势劣势在一定场景下也是劣势优势；
>一个词，到一句话，两句话，一段话，一篇文章；这个就是抽象总结，层次总结；越简洁，站得越高
--在网络、搜索引擎、推荐系统 三方面的专家；作为系统方面的独特优势/拔高优势；(网络-查询-推荐)
	
----有且只有响应，通信端才知道连接是否成功；。浏览器自动扩展。
----维持连接，并发连接，都是软件的实现，物理上看都是一条出口；从响应就是维护连接的角度看，不存在需要维护什么连接，维护就是维护连接数据而已；只要发送响应，连接就活了；在网络端口出口，可以连续发送不同目的地的响应报文，这就是并行；所以完全可以用队列来接收请求数据包；而用队列缓存发送响应数据包；多核使用起来，来并行大批量的发送和接收；不存在要维护和持续占用“端口”网络出口这种概念---完全没必要，用完即走 就可；	
	>或者不存在连接这个概念：所有的事情就是接收数据包和发送数据包(接送/发送缓冲区)。(连接 是 软件臆造出来的概念，不要和物理对应；和物理对应就会束缚思想，就会很多事情理解不了不知道原因)
	>连接的状态转移图；
	>应用的固定端口：实际上是建立新TCP连接的请求的处理的端口，请求到达这个端口--后面建立一个独立的TCP连接---来负责和客户端通信-交互数据；
----UML：为什么类继承关系图---因为这就是具体到一般的概念对象抽象过程。自上而下是能力顺序，能力组合；	
----说话和介绍：语速不要快，快就是掩盖问题，掩盖过程步骤；直接导致别人认为思路不清晰，表达不清楚，东拉西扯；也不利于自己思路的成长和扩展和自己主动发现问题，且必然导致不简洁--废话很多；
	>介绍需要先纲目后具体：抽象到具体；而不是张口就是罗列枚举---内容没有结构--全是线性结构；
	>描述更精简：一个字一个词，一句话，两句话，一段话；
----大事和吏治：大事 就像西天取经；吏治就像管理四人；


--计划：nginx/tomcat-->计算机系统-->架构-->自己的系统架构方案:专题研究、大提问、大总结。大简化/模型图化；
>一个进程看作一个消息，代码计划/任务计划；；都是异步隔离；	
	>程序启动点/执行点：可以多个，看作是并行的；(一个机器上，多个程序文件里)；可以留下执行点/新增执行点；可以删除执行点/减少执行点；
	>函数式编程为什么好：因为每个精细环节清晰描述确定了下来；使得充分配置和指明动作；
	>如何看待对象的方法：所有的对象都是被动的；主动的只有cpu/并行点；
	>程序的执行要想象为人在执行；多线程则是交接执行权给其他人执行；
	>抽象编程与具体填充：假设编程和实现假设。面向对象中一个对象属性就假设已经填充好了，一个接口的实例就假设已经在容器中有了。代码编码层面和虚拟机执行层面，都规定/定义/设计为 将 接口和实例分开，系统启动时/甚至具体接口调用时才去容器里找接口的实例(启动时 一方面建立实例容器，另一方面 对接口寻找匹配的实例和链接到实例---进行连接和关联)。资源填充 和 接口调用(抽象调用)，资源--接口的映射对。领域抽象，资源抽象，功能抽象。
----未知和迷惑的地方：痛点	；
----关键和核心的地方：要点

--混乱的答案，宁可不说；只回答真正掌握的；。没有逻辑，因为没有进行抽象；没有找到所在的流程环节、模型中的位置
--系统、中间件的介绍，不是一来就是组成结构；这层次已经太细了太具体了太里面了，必须要从最简洁最抽象最上层开始；最表面最近开始；务实，不僵硬，不突然，要自然，不要忽视和没注意没意识和跳过很多步；而是从问题出发、从困难出发、从疑惑出发
	>从问题出发：先明确问题；先明确表达出问题、疑惑点、黑箱、痛点、矛盾点、难点，表述的范围可以很大(完全不知道是什么怎么办)后面逐渐具体问；无论多少问题，先明确下来；尤其要全部且完整的描述下来；
		>问题的提出：先明确背景，自然衍生、过渡、转折、演化，逻辑关系上，什么时候什么事情什么人，事情什么阶段遇到的什么问题、阻碍、阻挠、缺陷、瓶颈、不够简洁、不够简便、不够方便、不够优雅、离目标远、离理想情况远、离期望/极限效果远；不够抽象的地方；把它们充分描述完整叙述结构式组织起来。
			>问题抽象：归结为一类问题；去除具体和细节而明确问题模型；
		>问题产生原因：过程；条件；	
		>问题导致的恶果：阻碍、损失；
		>理想的方案特征/效果/必做必不做的动作&事情/应当改变的环节: 
			>这种特征/必做必不做的前提、必要条件、必然要求、必然说明、必然指示、必然可以确定的更多的事情/结论：
				>一系列结论、约束得到之后(结合条件/问题/情况本身)逐渐可以清晰看到/归结出该具体问题符合的/满足的通用/一般的/一类的问题模型/函数模型/服务模型/IO模型/请求响应模型的轮廓：若干个具体模型
					>方案的装饰/补充: 补充可靠性/稳定性/高性能(从而高可用/高并发)：因为暂时只是一个裸机、容易受到伤害、有功能但没有抵抗力(仅为打火机的火而不是熊熊大火)
							>方案的用法规则：在请求缓存前使用
		>能将具体方案进行分类的维度/情况/模型/环节/流程/抽象表述 的确定：然后使用 抽象-具体 的方法来得到新方案；					
		>普通的方案：已有的方案；方案的抽象，方案的取值选择评价；方案的表象缺陷、劣势；。。模型、数据结构和算法、协议约定分担 维度 上考虑；
	>任何事情/事物都有顺序/逻辑：且几乎都是几种常见逻辑中的一种: 时间先后、空间远近、因果环扣、程度递增
--大总结：含义包括：重新 深刻理解：
--通用的建模架构能力+Flink深度强化学习的推荐系统。。。。而不是做简单的业务逻辑开发；用深度强化学习来做应用/解决实际问题(用户的识别问题和抉择问题)；用抽象建模架构出逻辑完整的方案(工具方案/服务方案)；
--一体两翼的发展模型：底座：增强操作系统、网络、搜索、推荐能力； 两翼：普通项目：则架构建模；特殊项目：则深度强化学习；。。四大基础+两大实践(应用/使用)。。基础：是为了解决自己的问题；实践：为了解决别人的问题(用户的问题/大众)；
--一次彻底弄懂，而不是 反复低温加热。看架构书和源码书，不看使用书。
--对话中胜利：一个是提问，二个是不断的输出-高能输出。
--牢固的观念: 系统都是被使用的。
--任何一个系统、产品、服务、方案、东西、事情，它要解决的核心问题是什么？理想的形态效果影响应该是什么？市面上对应的哪些产品达到了或者没达到或者很难达到？没达到是为什么是否有我们的机会？是否还有我们可以满足的缺口。找市场缺口。
--从缝纫机原理看方案所属的分类和方案内的环节。抽象出分类和环节。
--把自己当做cpu,调/使用各个服务/接口/类/系统。
--全新学习掌握方法：先定问题体系，先提问，定逻辑路径，后开始找答案--推导和思考和查阅资料。
	>旧方法：还是盲目阅读，还是从头到尾的阅读一遍的阅读。
--所有系统和方案的理解/研究/制定的出发点：都是 出发顶点/问题顶点/概念顶点，找到了顶点才能出发，出发要从顶点出发，才顺畅，没有后顾之忧
--计算机解决问题的方法：采集，记录，计算，展示，通知，跟踪 数据/信息。

--商业经营才能：(全新的解决问题的方法：提出问题)
--什么问题 是可以用互联网来解决的？ 只要满足什么特征，该问题就可以用互联网方式来解决? 该问题的本质是什么问题(如是信息传递问题？)该问题抽象一般化后是什么问题(资管、记录、通知、计算、采集识别调动、代理/中介/数据中心/信息中心/信息模拟线下过程中心、信息传递)(信息展示/发布/搜索/推荐 平台；信息池；信息可以是映射描述的现实事物的信息(如商品/价值物/人)，也可以是纯虚拟信息(游戏/知识/新闻/视频))
	>能用信息发布展示/活动追踪-信息记录/通知/协作/推荐/搜索 解决的问题：信息可以是 编辑的信息/映射的信息/等价现实的信息/等价现实任何事任何物的信息。内涵BAT的老业务(电商/聊天/游戏/信息流/搜索)。
	>能用数据驱动决策抉择与调度分配/识别与推理/观测采集与调动机电-生物 而解决的问题：如alphazero/图像识别/语音识别/视频识别/云计算/智慧城市/自动驾驶。内涵BAT新业务(云计算/视频语音识别-下棋打游戏智慧体/智能音箱/自动驾驶)
--码农之家和脚本之家、异步社区。从 面向对象-->模式-->架构：由小到大。
--给其他非软件行业的工程师/人员开发软件。三高三可(高可用高并发高性能,可扩展性可维护性可重用性)。
--最佳：一边看书，一边独立思考；(比起纯看书和纯思考都好)。因为 看书 要有沉淀，积累，独立思考 结论，总结。而不是流经书，翻翻而已，左进右出。
--说话：社交场合，最重要的是：要有自己的认知、决策和行动；对形势的判断和分析和预测。而不是被别人牵着走，或者置身事外。
--什么是最重要的? 就是你想做出一个什么东西...  这是最有价值的事情,是最重要的目标(比起学位和金钱和看多少书都重要);
	>不讲在公司的项目: 只讲自己 业余时间 做出的东西; ..  和 理解的东西... 总结起来 就是 四个方面: 架构 + 源码 + 算法 + 产品/作品
--不必指责和愤怒: 只需 战胜 和提防.
--莫忘理性决策：详细的明确和计算，一点也不能含糊。看看这次搬家时机的决策失误和搬家房间选择的失误，造成了金钱和时间上的巨大浪费。又比如以前自大没有投资股市---导致损失很大！！	拒绝感情用事和感动别人。
	>不能武断：不能一厢情愿。不能只见树木不见森林。
--兴趣不仅决定能力，而且决定性格，比经历更加影响性格；总结和思考 是兴趣带来的根本也是兴趣导致的根本活动；而性格能很大程度上影响命运---作。韩信(仰仗自己聪明而希望别人只记得我的好不记得我的坏)、赵括(自己负责精彩，麻烦交给别人--希望别人帮自己解决；只知顶层逻辑不知道低层逻辑)、李广(总觉得别人对自己不好，发现不了自己的错，不按程序办事，爱自己来，当别人按程序来--总觉得别人是让自己受气，受不了别人的指责)	
--在大部分有钱人和机构都将资本+利润存入股市的时候：说明股市是会升值的，股市就是最大的公司。多余的所有钱+利润 只有在股市才能分享。
	>既要有信心，也要有担忧： 激情和忧虑 都有 ，才是 理性的人。而不是两者都没有的麻木不仁、冷血和装。
	>做人不要尖酸刻薄，说话要说别人的好，坏处知道 但是不要说出来。有的人不喜欢被戴高帽，或者认为是反讽，这只是 好 没有说到点上而已。
	>面对别人指出的问题: 改就完了,且深刻反思和各种改进, 大量的收获和扩展.更强!!!错误的反思总结得到的东西让人更强!!!
	>已经是青年人了：要敢于争取
	>控制欲望、总结反思验证、勤于积累和使用：控制无收益的欲望-事情白白浪费时间；总结反思-站在第三方检查和痛批评价自己的想法和行为-从细节到方向目标-做得不行的要改变；勤于积累和使用-阅读和实践都要彻底掌握弄通；
		>读书不破、万卷何益：不会有长板；
		>不痛苦痛批、不是反思：
	>择人：最忌讳的是好吃懒做的人。喜欢比较的人--一上来就卡脖子；眼睛没必要跟鼻子比。
	>不要轻易拒绝做 看不起的事情、还早的事情、不重视的事情、觉得自己不需要的事情-觉得世俗/自大的事情(世俗也要同流合污)：买车摇号、社保缴纳、公积金缴纳、找女朋友、找好的房子、考在职硕、炒股-投资。多少件事情--明明顺手就可以做，却没有做，一直拖，导致损失惨重！！
		>每一件瞧不起、不重视、还早的事情：相反，都要早点做！！
		>机会错过 难再有：往往影响是决定性的。没把握机会 就是 决策性失误！
		>其他事情：私活-阿里云-本地开发-换电脑。
	>想到就要做，错过不再有：这是最重要的名言！！只有这样才能发挥才智 并且 赢得 胜利！！！	最重要的就是时机、机会，把握机会把握时机。
		>时机、战机 转瞬即逝：稍纵即逝。错过就是一辈子，错过就是奉上身家性命。犹豫不决和瞧不上和不需要而暂时不做延后做 都是错失良机、贻误军机。
			>说小了是 酸，说理性点 就是 后悔遗憾 承受损失。
	>商业机会：大部分来自于持续观察大家的生活---什么社会事件政策会怎样的影响人们的生活/会让人们的生活发生哪些改变； 人们生活中遇到了什么瓶颈/矛盾/困难/问题/需求/不足/遗憾/不开心的事情。		
--理论观点/方法缺乏层次系统的组织，还是在等别人分配赚钱任务。概念、观点 还是片段、不连续的。
	>基本认知方式：提炼、总结、简化、抽象、封装 。
	>问题为 元素 进行组织：这次不以概念为元素，来更方便于实际使用、进行设计训练---工作也当作是设计训练。
	>填充所有的逻辑漏洞：
	>分层问题森林\分层问题 群: 一个n层的问题,其顶元素 只是 另一个n层问题的一个元素. 因此 k个 分层问题  构成 了 一个最顶层分层问题 , 整体 是一个 问题森林.
	>每一种策略/设计/具体化： 都必须明确它到底是为了解决什么问题。
	>范畴不明确：就会 陷入 “还有什么” 的困惑和迷途。范畴的向下具体划分和向上抽象统一更大范围。
	>讲一个东西的内容时：可以看出是否结构化思考，是否很明确范畴，范畴化思考，不断的抽象上升又具体下降--树上遍历。因为问题，产生目标，想到思路，立刻执行。
	>输入不必多：关键是转化 为 总结、实践  有 多少。	
	>可视化：为最佳的理解方法。
	>深入一个事物：就是问它的什么的什么的什么的的的的的的的的的的的的的的。
--领域、问题、逻辑。(路径/方案/思路)(条件/目标/路径)	范畴。顶级思路/顶层思路。顶层领域，顶层问题。范畴与逻辑。范畴关系(同一范畴内、不同范畴内)、比较关系、规则关系、联动关系。
	>不能只以概念入手，也要从问题、思路入手。
	>谈话永远不能泛泛而谈：回答而已，别总想着怎么回答会好-显得聪明有创意有见解之类。必须要有目标，要预见和引导 话题走势走向，进而从当前条件 寻找 路径/思路。
	>开发时：反思总结 而 扭转观念，不要一来就想着实现代码，而应该想着本层逻辑、本层事务、具体交给下层实现-下层再进行拆解-完成它要做的那部分工作-塔式调用/直到具体的专门的一个个的实现//这个也是逆封装过程，类似塑造过程。
		>链式调用：默认实现Filter 放在 最后，用户提供的放在前面；从而用户提供了实现如果想提前返回则可以直接先返回不走默认。
		>插件的发现/用户回调类的注入：往往都是自动配置类bean  注入了外部的 beanFactory，而自动配置bean里就有本框架的核心类，而获取了bf,则各种规范的实现bean就都可以获取了，从而用在框架各个位置。
	>一切技术都是简便方法：更快速的方法、更省的方法、更安全的方法。更可靠、更通用的方法。
	>调用 就是 询问：询问就是调用。
--顶级关注点：任何事物 寻找到它 最有价值的一面  对我有用有好处，如果有 则 认识到通和使用到精。没有看到组织结构，必然混乱和觉得复杂、含糊不清、仍然不懂、没有消化--分解/拆解出有营养可以被利用来构造系统的基本元素出来。
	>于社会：形势和机会：最理性的分析 -->最准确的预测 --> 最周全的方案 --> 最简便的验证。很多人不相信完美的十全十美的方案的确存在，也就不愿思考和制作和逼近，而是找了一个草率的方案，执行后失败而亏损。
		>人际关系：在不能够失败的事情上取得胜利 才叫胆子大胆略胆识,其他不能产生直接或间接作用用处的事情 失败了也无所谓--就当作给对方一个面子--做个厚道人-而不是尖酸刻薄没人接近没朋友-朋友就是大量小事上免费的互相帮助。办事：对方道德好-好心人，道德一般讲利益-合情合理，道德差坑蒙拐骗一把-就损失了。
	>于痛点：目标和思路：最理想的样子 -->最真实的现状 --> 最顶层的思路 --> 最简便的做法。
	>于业务：领域和关系：帮助用户解决的问题是什么(用户只需要有什么条件、只需要做什么)(概括) --> 最理想的方案的最顶级的思路是什么 --> 要展示什么信息 、后端计算要什么信息 --> 领域的属性结构、领域在某活动中关联的领域(如人和商品在购买活动中关联了起来)--活动的间接结果/结果描述：就是将领域和领域绑定在一起--划归到一个活动领域中而成为一个活动领域的两个属性--毕竟一个活动本身也有领域/流程/规则--单独的领域是独立无关的,只有在活动中才和无关的领域关联起来;反过来发现两个无关领域关联起来了则一定在某个活动中(如人与商品-在购物活动中；商品和地点-在物流活动中)；领域是基本不变的有限的，真正大量的不断产生的同样需要记录的是活动数据--一是活动参与者多二是活动步骤多三是活动多，此外，展示独立的领域信息 其实价值有限且固定难以增长，而不断新增的活动让领域参与者有新的参与感、活动服务了参与者、给参与者带来了新的服务价值、解决了新的问题 满足了新的需要--这些都是在活动中完成的；所以创造活动并记录活动 数据 才产生巨大价值。纯线上活动、活动的线上部分、线下活动的线上模拟活动 ，寻找、熟悉和创造 新的 活动，是对一门业务能力的三个阶段。
						 --->明确 所有的领域(属性结构)、活动和活动领域(其属性为若干个独立领域+活动本身信息) --> 各个活动领域 将各个独立领域 直接或者间接 关联了起来，则可以建立起一张关联起所有独立领域的大表；也可以判断是否存在和 存在则找出 两个独立领域的关联路径；帮助数据分析和数据挖掘。---> 使用 活动领域 记录和重新梳理 活动的过程。记录了一个活动过程，就是记录了一个服务过程(展示-交易-消费-评论)  。例子：一个购物：独立领域的关联过程： 人和商品(下单) --> 人和商家(支付) --> 商品和地点 (物流) --> 人和地点(固定信息) --> 商品和人(物流终点) --> 商品和评论(消费者对服务打分)
		>澄清 业务流程是什么-->每一步需要展示什么信息、要上传什么信息
			>信息分成哪几部分: 本质上可以归结为 哪几个独立的 领域(有明显的自己的边界)，这些领域的属性结构树是怎样的。
		>有 存储、筛选、推荐和展示海量数据 功能的 系统：供看、消费、互动。
			>展示的目的：>展示的东西：展示有价值的信息、宣传(广告)、引导用户购买、引导用户提供信息(生产/上传)、引导用户交互信息。
	>于系统：：拆解和组建。本身的规律和统一的路径。范畴式创新(补集/包集/子集)
		>认识：
			>系统一定不是一个单调的整体，而一定是组装起来的。单调的整体 只是一个部件，甚至不能用来构造复杂稳定的系统，没有这个扩展性、接口。
			>系统的运转：组装起来之后，启动系统，开始触发系统，向多米诺骨牌一样 传递下去。
			>系统的逻辑结构：假设...则有。如果...并且...,那么对...情况,则有...。假设有几个关键的暴露关联关系并好度量的事实、实验、条件、不变性，则可以得出xx间接的关联关系。具体关系直接向上抽象，将具体量消除,推导更一般的关系；。或者将关系和关系封装在一起，得出更间接更远距离的关联关系，得出对一个事物的完备的关系集合。或者规律本身特征的概括，来得出其他未知规律也一定满足的特征-抽象特征(如不变性)
			>系统的元素：系统的元素可以用来构成其他系统。因为它职责单一，可以替换别人也能被别人替换，可以共享(越单一越能被共享,越复合越不能被共享)(共享就是拿来使用,避免重复劳动)，可以互换，各个系统的若干个位置上都需要，从而用这些元素构成的若干个这类系统，一个系统报废了拆分出的组件元素 还可以放在其他系统中，从而元素利用 最节省 最经济、利用率高-浪费少。单一职责 是 有限的 ，构成一个 有限的集合。承载单一职责的元素 种类 就是 有限的，构成有限的类别的集合。从而每类元素可以批量生产，缓存起来，按需提取。
				>元素的类别：职责类别：特殊的元素--骨架型元素(支撑连接其他元素)。业务型元素(完成自己的输入输出模型)。
				>元素的接口：元素 的 可以和其他元素连接咬合起来 而传递/接收 刺激/物质/信号 从而发挥自己的作用 的部分。
				>元素的连接：一个元素的接口和另一个元素的接口咬合在了一起，形成 信号/刺激/物质 可以从一个元素传递到另一个元素的状态、复合形态、复合物形态。新的复合物有新的接口。不同于两个元素的输出输出模型 的 新的输入输出模型 产生了，这是连接起来 最大的意义和作用和目的，还具有新的状态转移图---也有使用用途。
				>元素的组建：在一个框架型/架子型/骨架型元素上(丰富的接口)(本身甚至就简单的仅仅是一个接口集合) 连接 接入 若干 功能型元素 和 骨架型元素。
					>组建的目标-条件-路径：最顶层的目标--产生一种更间接更长路径关联起输入输出的高度更高长度更长远度更远的输入输出模型的子系统/子模块(以进一步组装出更强的系统/让本系统更强)
				>元素的封装：用边界将 若干个临近的元素(无论是否连接起来) 包装起来，隔离其他 集群元素，而有自己的独有的基础资源、基本元素资源池。形成独立的环境、独立的上下文、独立的数据中心(信息中心)，统一的对外接口、复合的多模式的输入输出模型。
			>系统的状态转换：可以从外部施加给系统的动作，系统接收该动作后向内传递 而改变内部形态 最终传递反馈到外部(表面) 输出 响应，施加的每个动作每种动作都会引起系统状态的往不同路径上的迁移转化。这个状态迁移图 往往 固定的，即从某状态到另一状态可复现/永远不变的，从而沿着这些路径找到最短路径达到我们希望的具有某特征的状态---往往也是我们的目标 和有用处的事情。
			>系统的拆解分解：(自然界的系统没有飞地--即没有可以控制感知的物理隔离的另一部分; 生的属于后代属于另一个系统;甚至不能产生和发送控制用途/通讯用途的电磁信号;声音等是否算)
				>系统要解决的问题本身的拆解：问题的树形结构。问题模型。
				>系统的目标的拆解：功能的拆解，职责的拆解。从整体 最顶层 的 一句话概括  到 最底层的 若干个 元素的单一职责的表述/表示。(最高目标就是最高层的目标,而非平等的目标中最重要的目标;区分两种概念表述)
					>顶层目标的确定过程-就是需求明确的过程:
						>最简理想模型功能的明确过程：最简单目标、简单模型功能、理想模型功能、最理想条件下的模型功能、各种因素都不考虑进来的模型(正相关影响因素/负相关影响因素/微扰因素)、理想实验模型、实验功能、脆弱功能、简化功能、本质功能、核心功能的明确过程、澄清过程、细化过程、精准描述的过程。还只是一个实验理想模型。
							>理想条件的特征：其他阶段不考虑-只考虑核心阶段、其他影响变量不考虑-只考虑主要影响变量、具体细节情况不考虑-只考虑最抽象上层的情况。其他功能不考虑-只考虑单一功能。
						>其他一个个平级约束条件逐个考虑进来后 模型逐次扩展后的新而又新的模型的明确过程：新目标、新的主体架构、演化后的主体架构。扩展对基本功能的表述，将其中的具体的东西向上抽象化一般化。最苛刻条件下的模型功能。
							>约束条件的特征：限制条件、故障、条件变量值变为极值特殊值、险恶的环境、多变复杂的环境。必然会自然会发生的遇到的新需求，第一版现在就会遇到的其他需求。把这些需求明确下来--为了需求的完整和深刻 先只讨论需求有什么是什么 此环节不考虑实现。
							>考虑的目的/必要性：让系统健康成长、稳定成长、能适应险恶的环境、自愈自修复。生命力更顽强。
						>目标模型的向上抽象过程：功能增强过程。封装进其他目标模型，和其他目标模型封装到一个新的目标模型。
							>增强这个目标模型：依赖这个模型、监控这个模型、统计分析监控量并得出结论发出控制信号调节/指挥/引导/服务/治理/保护/优化/自动化/智能化 这个目标模型/系统。
							>考虑的目的/必要性：考虑到各方面各线可能增加的需求和功能-进行预留和预备、让系统扩展性更好、性能更好、功能更多还不乱、更稳定可靠、可扩展也可卸载替换功能模块、更好的可控性-更细更广可控。
						>目标模型的向下细化过程：向下一层层具体化展开明确确定。
				>系统的解决思路的拆解：方案思路的拆解。思路模型。
					>思路中的有限类型元素集合：
					>思路中的当前可以使用的条件的集合(组织成结构)：
				>系统的组件的拆解：组件模型。分解出下一层的构件，下一层的构件再到下一层的构件。一层层分解下去(如肉分解出蛋白质-蛋白质到氨基酸,来到可以被吸收的层次)(如汽车拆解出底盘-底盘拆解出发动机系统)，拆解到元素层级，最广泛被使用/重复利用的 元素级别。
		>使用：	利用和扩展。训练(它)和增强(它)。整合(到一个方案中)与改造(更适配一个方案)。
			>该系统如何应用在方案中：哪些类 哪些问题 的 方案  会使用 到 这种系统。就像用人 一样 用系统。
			>这些方案如何应用在业务中：这些方案又可以用在哪些业务中。
				>方案的评估和论证：
			>这些业务如何解决人们遇到过和将遇到的系统型的个人又难以独立解决的希望有组织-公司提供专业的服务来解决服务的问题/痛点： 
				>动作的条件、目的和规范：如伸缩 这个动作。
	>于创业：人们当前遇到了什么问题 --> 人们会遇到什么问题 --> 人们想要解决而无法解决想别人提供可以解决的哪怕是有偿的也要解决的问题是什么 --> 先 概括出用户想要而缺乏且看到则感到惊喜的服务是什么
>使人不惑：
	>结构：将系统呈现为结构形式来表达，可以帮助分清主次，找到本质入手，立刻看到关键、看到本质的轮廓、要义的切入点、概念。
		>一个变量 可能 也是 一个结构：
	>大量的困惑和不理解：都是 理解推进过程中 没有建立中间概念 ，即 没有找到可以 对某 事物的 理解作用起到增强作用的 中间概念、桥梁概念，通过桥梁概念来找到前后的联系，上文和下文的联系，导致结果突兀难以理解不明所以 。所以 理解的推进过程 中 中间概念的 找到 至关重要。	
		>困惑的根本原因：联系是断开的。上文和下文的联系 是断开的。或者说是 强硬 联系起来的。联系的一点也不自然。
		>没有归结到根本：没有将上层抽象的概念和 底层 基本基础的 牢固的概念 建立 联系，建立 坚强 牢固的 联系。上下之间缺少 中间概念 也会悬空 而 难以理解 抽象概念。
		>强硬理解：就是 不知道 条件到目标的路径，而强行让自己认为条件到目标就是必然的/直接的/一步的/显然的/不关心中间过程的。完全就是记。只知结论。
		>明确定义构成整件事情的所有 没有 清晰准确定义的 基本概念：都要 明确 提出 和定义 一遍。并构成 概念树。
			>要基于实验模型 进行思考：上面的概念页来源这个实验模型；没有实验模型 那么 就没有任何图像，只剩下逻辑了。
		>任何的总结都必须总结到最顶层：形成最简洁扼要的一句话的总结。由它可以层层推出到最底层的 整个 树形细节过程。	
		>使人困惑和书上东西看不懂的原因：一个动作、操作、名词、式子的定义/含义 理解错了，或者根本不理解。第二，才是 跳跃，中间过程 和 联系 省略了/显然了。 如 min max L的含义。
>目标：对一种系统 进行 领域建模，系统架构。总结出新的 一类架构，新的一类问题/共同问题。		
	>写研究报告、策划方案、完整讲清楚-八面受敌：
	>验证方法的进步和改进：
>公式推导的问题/痛点、想法、目标、起点、条件和路线：每次公式推导必须明确，从而清晰思路。
	>问题的提出：推荐算法中 通过 相似用户、相似物品计算 用户对 物品的 喜好值 并不是彻底的一般的方法，更像是一种临时的方法。所以想寻找一种通用的统一的一般方法。
	>想法的得出：评分矩阵里的每个评分 为什么是这个 值，思考这种必然性、确定性，不那么显然的显然性，任何一个确定的一定的取值则要么是公理要么就可以继续分解用更基础的概念集合更本质的元素集合 来解释来计算来表达出计算过程来表达相互作用得出该结果的过程，任何一个值 要么 是 公理值 要么就是 计算结果。
		>把评分值当作计算结果 反推计算过程：假设是一个简单的线性组合得到的计算结果，而线性组合可以分解为两个向量的乘积，而这个评分值显然是由用户的特性和物品的特性决定的，而特性相乘，必然一个是数量-一个是含量，显然用户要求的是数量，而物品有的是含量；所以两个特性向量，用户的是用户的特性数量/偏好数量/需求数量，物品的是物品的特性含量。
	>转换后的目标：计算最优的用户特性数量矩阵和物品的特性含量矩阵。最优的标准：明确为 方差最小。在这个约束/要求下 计算PQ矩阵。
	>起点条件和顶级想法层层细化：在这个约束/要求下 计算PQ矩阵的数学方法：梯度下降法/最小二乘法。用差量的方式 甚至可以用 迭代法：Puf(t+1)=Puf(t) + α * зLossui/зPuf(t)	, 而 ^rui = ∑Puk*Qki, Loss = ∑(rui - ^rui)^2, 为防止overfitting,添加正则项控制过拟合：Loss = ∑(rui - ^rui)^2 + λ(∑Puf^2 + ∑Qfi^2) = f(P,Q)
	>方案的证明和优化：
		>对于 ^rui = ∑Puk*Qki ： 此表达式计算^rui 显然只考虑了 Pu和Qi相互作用的部分，即用户和物品共同决定的部分--通过需要数量*具有含量 来 共同确定的部分，实际情况中 可能还有 只跟 用户、只跟物品有关的因素 bu, bi, 甚至跟用户和物品都没关系的因素μ ，所以 修正后的 ^rui = ∑Puf*Qfi + μ + bu + bi ; bu往往可以用用户的性格打分习惯严格、温和来决定；bi往往和物品质量有关；
		>对Puk: 是用户对各个特性的需求量，而用户实际 对这个特性 的 需求量 可能 会 因为 用户 购买了 越来越多的物品后 而 有所改变--比如历史买了富含维生素C这个特性的西红柿 而 下次对 黄瓜这个同样富含维生素C这个特性的物品 需求量 就更低了，所以需求量 需要和用户的购买历史物品 关联起来，即 Puf 替换为  Puf + ∑Yjf/sqrt(N(u)), ∑Yjf就是已购买的物品对此特性的需求量的影响力之和。SVD++
		>再次优化：则是 考虑时间因素了。rui的各个因子都是时变的了。
	>符号的拆解和封装：符号也可以拆解开 用 更基本的 符号来表示，直到 +- 等。其他符号都是简化某个使用基本符号构造的复杂运算过程 而 定义的；就像 计算机里封装出一个个函数一样 封装出一个个符号。
		>积分符号 也是 一种简化符号，是对 某个区间 分割 出的 无数个 连续的无限小量 求和 这个过程 的简化 表示。Σx*f(x)dx = ∫x*f(x)dx 
		>乘法符号 是一种简化符号：是对 k个 相同值m的 求和 这个 计算过程 的 简化 表示，等值表示。C = m+m+..+m = k*m。。。数量和单位数量值--方便总量值的计算。(计算总量 和 可以摆出多少种组合 都是最原始的需求，野人时代就有)
			>乘法第二种含义：同时性/两个集合的元素能够组合出的所有结果构成的集合-方便组合数的计算。比如两个独立事件同时发生的概率。一个事件发生的概率 等于 这个事件代表的情况数 占 全部情况数 的比例，(一个事件 是若干个情况的集合)(一件事情 全部可能出现的情况)(一个实验 全部可能出现的结果)(一件事情 出现了某种情况(即所有可能情况中的某种) 是 一个 概率事件)。
				>当两件独立的事情联合构成/视作 一件新的事情时：则这个新的事情 全部可能出现的情况数 就是 两个子事情的情况数之积(组合出的结果)。
				>同时性：代表了组合空间的增大。
			>乘法的第三种含义：映射。如 [0,c]*[0,b], 如果 每个当作轴上的点区间，则两个集合相乘 就是映射到 相交的一片平面区域，这也可以看作两个集合元素完全组合的结果。	
		>触发符号的来源：计算比例、占比。计算倍数则是另一种 源头。概率 是 表达 情况的占比。		
	>推导这件事情也需要一步步推进：每一步都走的很小，很连贯，来保持思路的连贯。
		>合理的充分的代数表示：非常重要的一步，表示事情、过程 应该完全代数化，没有具体变量的痕迹。
	>数学：就是在基本概念和定义(元素、目标、条件、路径) 下 向上 封装、抽象 、推导推论 而 建立起来的 关联系统结构。如线性代数、概率论。
		>抽象：就是省略具体特征。
	>物理：也是基本定义和概念、不变性等 上 进行抽象、封装、推导推论 而建立的 关联系统结构。
>举动-痛点：要解决的问题、目标、条件/起点/底层/原子、思路。最顶点的思路，最上层的确定性/认识。
	>解法/解决方案：要点：利用架构/封装/塑造思想，以基本的定义为核心，每个关系都必须找到 值相等的原因，每个乘法都必须找到可以相乘的原因和应该相乘的原因和必须相乘的原因,其他运算符号也是、变换操作也是--来找到隐藏的前提条件/理由/根据，用更基本的概念/不变性 来解释，直到最底层、公理层。来建立起自底向上的基本到抽象、原子简单到复合抽象的层次概念体系  和 从内向外的 关联网络体系。实现 表象到内核 内核到另一个表象的自由简便推理。
		>表象：数据
			>表象特征：表象度量。表象规律。数据的特征；数据的结构，数据的模型。
		>假设：同类数据具有一定的统计规律性。X和Y具有联合概率分布。
			>同类：共同性质。
			>规律性：相似性。共同特征。统计规律。
				>具体假设：训练数据和测试数据 是 按照联合概率分布P(X, Y)独立同分布产生的。
				>具体假设2： 模型的输出值 和 样本的y 值 之间的差值 越小 则模型 越优。
		>监督学习：选模型，定参数。假设空间 + 策略空间 + 算法空间
			>假设：数据独立同分布。要学习的模型属于某个函数的集合--假设空间。
			>过程：应用某个评价准则，从假设空间选择出一个模型，如果它对已知训练数据和未知测试数据 在给定的评价准则下有最优的预测，则它是最优模型。确定模型的参数则需要算法实现。
				>模型：
					>输出可以确定具体值：条件概率分布P(Y|X)  
					>输出只能确定其取值分布：决策函数 Y = f(X)
				>策略：怎么选模型 --期望损失最小的模型；
					>损失函数：一次预测的好坏的度量，度量一次预测的错误程度，即模型输出值和样本值的差距。所以这个损失函数/代价函数 应该为 f(X)和Y的函数/非负实值函数：L(f(X),Y) ,具体形式，最简单到最复杂：可以是：0-1损失函数，平方损失函数(Y-f(X))^2，绝对损失函数|Y-f(X)|；对于分布函数和输出值的差距，可以用 对数损失函数/对数似然损失函数 来描述L(Y,P(Y|X)) = -logP(Y|X), 即如果这个分布函数在y1是有最大值，而实际取值y2微微小于y1,那么对应的分布函数值则要小,取对数后为绝对值小的负数,所以还有一个负号,结果就是小正数，正好可以表达偏离不大的含义---相当于概率密度大则误差小。
					>风险函数：平均意义下模型预测的好坏，定义为 损失函数的期望。期望 = Σ 随机变量的取值 * 取值概率  , 所以 Rexp(f) = Ep[L(Y,f(X))] = ∫L(y,f(x))*P(x,y)dxdy  但联合概率分布是未知的且不应该事先知道(否则就可以从它直接计算出条件概率P(Y|X)来了), 因此需要一个不使用联合概率密度 而 计算 平均误差的计算表达式， 最简单的 就是 直接计算误差的平均值：1/N * Σ L(yi,f(xi)) ---这个记作 经验风险/经验损失。Remp(f) 当N逼近无限大时，经验风险逼近期望风险Rexp(f)
					>经验风险最小化和结构风险最小化：
						>经验风险最小化策略：认为使得经验风险最小的f就是最优模型： 即 计算  min 1/N ΣL(yi,f(xi)) 
							>例子: 极大似然估计：模型是条件概率分布，损失函数是 对数损失函数，经验风险最小化 就是  min -log(P(Y|X))*P(Y|X) 即 当概率P取值越大越接近于1则log(P(Y|X))取值就越小,而取这个值的概率就是P,因此 按照期望定义 的 经验风险最小化 计算表达式 就是：min -P(Y|X) * log(P(Y|X)
								>极大似然估计：认为 在 给定的 条件下 ，一件事情 发生了 xx 情况，则认为 这种情况发生的概率比发生其他的情况的概率更大，即发生这个情况的概率是最大的。如果这个情况是个复合情况，则将这个情况发生的概率用 其 子事件 发生的概率 和 表达事情条件的参数和xx情况的度量 运算起来表达，则得到了一个关于 子事件概率和事情条件和xx情况的度量的表达式，而这个情况发生的概率最大，则就表示通用表达式有极值，且在xx情况时取得，即如果 实验/实际上 条件为yy，xx情况的度量取值为kk, 表达式有极值，有极值代表可以求导数/从而得到了 关于情况的度量 + 子事件概率 + 事情条件的定量关系式。
									>例子：一个口袋里若干个球，取出10次，得到了7个红球3个白球，则 下次取出红球的概率。显然，取出10次 是事情，出现7红3白是情况，取出红球的概率是子事情。记复合事情为A,则用子事件概率表达为：P(A)= p^a * (1-p)^(10-a), 有极值在a=7处，首先有极值代表可导，得出关系式--先不改单调性用对数处理，可以得到导数=0，简化为 p=a/10
							>当样本容量小时：计算的误差就大；产生 过拟合 over-fitting 现象。
						>结构风险最小化策略：避免过拟合。在经验风险上增加 模型复杂度： Rsrm(f) =1/N ΣL(yi,f(xi)) + λJ(f), J(f)是定义在假设空间F上的泛函。模型越复杂则J(f)越大。λ用来权衡经验风险和结构复杂度的影响。
							>例子：贝叶斯估计 中的 最大后验概率估计。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示，此时结构风险最小化就是 最大后验概率估计。
								>概率：
									>条件概率：P(B|A) 定义为 A发生后发生B的概率，即已知A已经发生取值为a，则此时B发生取值为b的概率。显然 A=a时 通过联合概率分布 则 B的取值总情况 是 可以确定的,甚至此时B有可能什么值都取不到/即总情况就是0，但一般总情况还是分立的有，这个总情况数就是 ∫P(a,B)db, 而B=b代表的情况数则是P(a,b)db 所以A=a时B=b发生的概率=此时的取值情况/此时的总情况= P(B|A)= P(a,b)db/∫P(a,B)db, 而 ∫P(a,B)db = P(A)定义为边缘概率；所以 P(B|A) = P(A,B)/P(A)
										>贝叶斯公式：因为 P(B|A) = P(A,B)/P(A), 同理，P(A|B)=P(A,B)/P(B) 从而有 ：P(B|A)=P(A|B)*P(B)/P(A) , 将 P(A|B)/P(A) 称为标准似然比， 为 A为B事件发生提供的支持程度。可以>1=1<1, 表示A事件的发生 增大了/不影响/降低了事件B的发生。P(B)为先验概率。
										>概率函数：P(x|θ) 已知θ,x是变量。表示x取各个值的概率。(分布概率)
										>似然函数：P(θ|x) 已知x,θ是变量。表示不同θ下，x出现的概率(随机变量取值为x时的概率)(取值概率) 。似然函数 L(x;θ) 是关于实验结果X=x1,x2,x3...xn的函数。由概率函数，知道x1发生的概率,同理xn发生的概率。一轮n次的实验，这个事件，其实际发生a次红,n-a次白的概率：P(x|a,p) = ∏P(xi|a,p)*∏P(xi|a,p) = p^a *(1-p)^(n-a) ,没做实验的时候，a,p都是不确定的，即到底出现几次红是不确定的，但是认为实验后实际出现的那个次数，应该是最可能出现的，即是那个取值会使得P(θ|x1,x2...xn)这个似然函数取得最大值。
										>概率函数和似然函数的关系：似然函数 是 实际 取值 的 概率， 是 n次实验 X 实际取值x1,x2,...xn 这个大的事件 发生的概率(像这样的结果的作为一个事件它发生的概率,似然事件)(引入一个似然事件的概念/这个中间概念/桥梁概念 来辅助理解)，发生的和参数相关的概率。即一般为 ∏P(xi), 是用n次结果的概率值算出来的(包含未知参数,但参数是值类型而不是随机变量-当作值而不是当作分布)。也可以认为是 取值带入其概率密度函数而相乘的结果，就是似然事件的概率密度。所以 准确的说 叫似然密度。
											>概率函数 则 是 实验前 就 对 X 取值的分布 的估计，分布概率估计。其实是 密度函数。
											>似然函数 是 似然事件 发生的概率， 是 θ未知时 实验出现x1,x2...xn这个似然事件的概率。(可以认为是P(X|θ))。。θ虽然不确定取值是多少，但它的取值一定会使得这个似然函数取值最大---形成最有可能发生的特点，否则似然事件就不是最有可能发生的--和假设矛盾(假设为似然事件最可能发生)。不知道θ的取值，也不知道θ的分布，等效含义 就是 θ可以取任何值，各个值事先不知道取值概率--或者认为概率相等。即此时的 θ 分布函数为 1/取值区间长度；但这种假设，隐含假设 可能和实际是差别很大的，即实际可能是正太分布，β分布，那么最大似然估计的结果就会产生较大的偏差(比如实际θ服从的正太分布瘦高且顶点在1,而最大似然估计函数的顶点在Θ=5;而事实上由θ的正态分布知道几乎不可能取值这个；因此θ的取值既要兼顾它的分布也要兼顾似然函数取极值；由于分布函数和似然函数的横轴/自变量都是θ，因此实际取值为这两个因素综合起来最大更加合理，综合起来的方法--加法不好--因为是概率-结果就可能超过1，乘法则有同时成立性的意思-两者相乘有期望的单项表达式的特征-即单项(对期望的)贡献值/贡献量的意思--贡献量最大的单项则认为是事实中最可能发生的,对应的θ的取值也就是最合理的；这就是最大后验分布估计)。
									>边缘概率：两张边缘图。所以叫。
									>事件同时发生的概率：P(A)*P(B) 是因为 两个事件发生与否对 另一个事件的分布没有影响。
									>全概率：从情况占比来理解非常自然。P(B) = ΣP(B|Ai)P(Ai)
								>最大后验概率估计：认为参数θ 具有某种概率分布，称为先验分布。从贝叶斯公式，得 P(X|θ)= P(θ|X)*P(X)/P(θ);其中P(θ|X)称为后验分布。θ满足一种分布，那么似然函数 作为θ的函数，则也满足一种分布--但知道这个分布意义不大；另一方面，由于约束了θ事先要满足特定的分布，因此似然函数作为似然事件的概率也必然是在θ已经服从这种分布下做的实验会产生的结果的概率，因此θ的最可能取值可能不是使得似然函数取得最大值的那个值，但似然函数也不一定就是θ取得它的先验分布的极值时的结果--毕竟这个先验分布只是个假设---并不是真实的分布--而似然函数则一定是θ满足真实分布的结果。θ既要/会处于自己分布概率大的位置 也要/会处于使得似然函数值尽可能大的位置，对两者的贡献都是正相关则进行综合考虑--最合理简单不是加法而是乘法(毕竟两者单位不同)---而概率和函数值的相乘 代表含义为期望--而 θ取某值时的概率函数值是对实际期望结果的贡献量，显然 实际取值应该是 使 贡献量最大，即 argmax{P(X|θ)P(θ)}。这个就是更加合理的，考虑了θ的分布的 对基本事件概率的估计。
									>思想：假设Θ实验前已经知道它是个随机变量且服从某个确定的分布--称为先验分布(先验--就是先于实验)，在似然函数上 把 这个先验分布考虑进来，
									>后验概率和似然函数的关系：后验概率 ∞ 似然函数*正则项P(θ)---这里不是加法。
								>贝叶斯估计：估计θ的分布。在引入实验X/大事件X后 对θ分布概率的重新估计。先验估计为P(θ), 做实验X后的后验估计为P(θ|X), 注意P()都是概率的意思--P(θ)里面的θ不是指事件而是指随机变量---事件是随机变量的一个取值-概率就是随机变量取该值的概率；所以随机变量是一场实验的意思，实验可能的结果就是 随机变量可能的取值；谈实验重点要谈可能的结果/每个结果对应的概率。
									>符号术语：P(θ) 是随机变量θ的概率(即它的分布)；P(θ|X) 就是在实验A/事情A结果明确为X之后/结果发生之后随机变量θ的取值概率(分布)(这个概率中就可能夹杂X的结果值)。。同理 P(X|θ) 就表示 已知存在参数θ但不知具体取值 时 实验A出现结果X的概率。所以：P(X|θ),P(θ) 横轴都是θ的取值，但纵轴第一个是事件X的概率,第二个是随机变量θ的概率。P(θ|X)这里X仍然是事件，而θ是随机变量，表达事件X发生之后随机变量θ的概率(可能受X的影响而和P(θ)不同)。
									>得出等效估计目标：即估计 P(θ|X) , 即估计事件X发生后随机变量θ的全新的分布概率。根据贝叶斯公式，P(θ|X) = P(X|θ)P(θ)/P(X) 和 边缘概率条件概率的关系：P(X)=∫P(θ,X)dθ=∫P(X|θ)P(θ)dθ  从而 得出 ： P(θ|X) = P(X|θ)P(θ)/∫P(X|θ)P(θ)dθ  这就是贝叶斯估计。而先验分布P(θ)是假设出来的，而P(X|θ)则是实验结果X可以表达出来的。从而可以估计受X影响下的θ的分布了。
									>估计新值x: P(x)=∫P(x|θ)P(θ)dθ, 但是用P(θ)这个先验估计是不准的(当然如果是真实分布肯定是准的)，因为没有考虑实验结果X的影响，实验结果影响了P(θ)的估算，即应该替换为P(θ|X), 考虑了X的影响，则左边P(x)应该改为P(x|X), 所以：P(x|X)=∫P(x|θ)P(θ|X)dθ 这样 P(x|θ)可以表达出来而P(θ|X)可以贝叶斯估计出来，从而就可以估计新值了。
				>算法：求解最优模型的计算方法。最优化的问题的解析解不存在时，数值解的计算方法。
			>模型评估使用的损失函数：
				>训练误差：不断减小
				>测试误差：先减小后增大
				>误差率：
				>准确率：
				>过拟合：对训练数据预测的好，对测试数据预测的差。模型参数过多了。典型的用多项式拟合，当项数足够多时，甚至可以过每个样本点，而使得误差为0，
				>泰勒级数/多项式：可以看作是 线性组合，系数和指数项的线性组合。
				>模型选择的方法：
					>正则化：经验风险 上 增加一个 正则化项	
					>交叉验证：数据分为 训练集、验证集、测试集。
					>S折交叉验证：N-1份训练集，1份测试集，来回N次。
				>泛化能力：模型对未知数据的预测能力。
					>泛化误差：模型输出对实际的误差。为 期望风险？
					>泛化误差上界：比较两种学习模型的优劣。为 经验风险 + ε
					---训练误差小，则泛化误差小。
				>生成模型与判别模型：
					>生成方法：学到的模型为：生成模型。通过学习 联合概率分布，通过 边缘概率关系 而计算 条件概率P(Y|X)
						>朴素贝叶斯法：
						>隐马尔可夫模型：
					>判别方法：学到的模型为：判别模型。直接学习 P(Y|X) 作为预测的模型。准确率更高，对数据各种程度的抽象、定义特征和使用特征，简化了学习问题。
						>感知机：
						>决策树：
						>支持向量机：
						>条件随机场：
						>最大熵模型：
				>分类模型：称为 分类器。对新的输入进行的预测输出，称为分类。可能的输出为类。
					>学习出分类器：
					>对新的输入进行分类：
					>二类分类问题：存在100%精确率，而1%召回率的。
						>精确率：对于分类器预测的所有正类中，正确的比例。
						>召回率：对于事实中所有正类中，分类器预测中的比例由多少。
						>调和均值：P+R 叫和，  1/P + 1/R 叫 调和。
					>常见分类：客户分类，日志分类，手写数字分类，网页分类。文本分类(输入文本的特征向量，输出文本的分类)
				>标注问题：自然语言处理中的词性标注。从文章中抽取基本名词短语。
				>回归问题：输入变量到输出变量之间的映射函数 的寻找 ，即函数拟合。P(Y|X)输入为值时输出的是分布概率；Y=f(X) 输入为值时输出为值。
					>多元回归：输入变量多个，输入输出变量关系的类型---线性回归和非线性回归。常用损失函数：平方损失函数。此时回归问题的求解：最小二乘法。
					>例子问题：投资风险分析 可以形式化 为 回归问题。股价预测(样本：(当天营收，当天利润，当天股价), 过去30天的数据)---关键是影响股价的因素量化的好。
		>假设空间：定义为 决策函数的集合。 F = {f|Y=f(X)}  这个表达式 读作：/表示的 含义为： F 是一个函数集合(最外层) , 元素为函数f, 元素满足的条件为：Y=f(X) 。即 集合 = {元素|元素满足的条件}		
			>假设空间的细化： F = {f|Y=fθ(X), θ∈Rn} , 依然： F是一个函数集合，f是元素，元素满足的条件为： Y=fθ(X), θ∈Rn  这个逗号 就是 其中  的 意思，子条件的意思，条件的条件的意思；其中 θ满足它是一个实数空间的向量。定义为参数向量。
			>假设空间用概率分布表示： F = {P|P(Y|X)} ，依然：F是一个分布函数集合，P是元素/分布函数，元素满足的条件为：P(Y|X) 即 当输入为X时输出Y的分布函数。同理，F = {P|Pθ(Y|X), θ∈Rn}	 需要由一个参数向量θ决定。
		>特征空间：
			>特征向量：一个特征向量代表一个特征，代表特征空间中的一个特征。
		>输入空间：输入实例由特征向量表示，而输入空间可以映射到特征空间。
			>输入实例的特征向量表示：x = {x1,x2,...xn}
		>样本：输入输出对
		>回归问题：输入变量和输出变量都是连续变量。
		>分类问题：输出变量为有限个离散变量。
		>标注问题：输入输出都是变量序列。
		>非监督学习：
		>强化学习：
		>感知机：
			>输入：实例的特征向量。为输入空间中的一点。输入空间则为R^n中的一部分。
			>输出：实例的类别，+1，-1两值。
			>感知机：输入空间 中 将实例 分为 正负两类 的 分离超平面。属于判别模型。
				>模型： y = f(x) = sign(wx + b) 其中 w为权值向量，b为偏置。所有的线性分类器：{f| f(x)=wx+b} 
				>几何解释：wx+b=0, 当作 R^n空间中的一个超平面，w是法向量,b是截距。例如，假设这个超平面上有任意两点x1,x2, 则满足 wx1+b=0, wx2+b=0; 则 w(x1-x2)=0, x1-x2表示两个点之间的连线向量，则属于这个超平面，而w点乘它=0，则w始终垂直于超平面上任意两点的连线，所以w是法向量。偏置 就是 输入都是0的时候 的输出，就是b.
					>超平面将空间分为了2部分：这个超平面称为 分离超平面。		
				>数据集线性可分：超平面可以完全将两类分开。	
			>损失函数：误分类。
				>选择：误分类点 到 超平面的 总距离。
					>点x1到超平面的距离：通过法向量w, 假设点所在的平行于超平面S的超平面为S1: 则 wx1 + b1 = 0, 则S和S1之间的距离 可以通过截距差来间接计算，截距是其他维为0,最后一维的取值的差：显然这个截距差= -b1/wn - (-b/wn) = (b-b1)/wn = (b+wx1)/wn, 截距向量=(0,0,0... (b+wx1)/wn), 截距向量在法向量方向的投影就是超平面之间的距离，而投影=两个向量的点积/||w|| ,而w=(w1,...wn) 从而 点积=b+wx1, 所以投影长度= |(b+wx1)|/||w|| 即为点到超平面的距离。
					>误分类数据(xi,yi)的特征： -yi * (wxi + b) > 0  即模型输出和样本值矛盾/相反。从这个等式有：|-yi * (wxi + b)| = |-yi| * |(wxi + b)|= -yi * (wxi + b)	= |(wxi + b)|, 所以 误差点到超平面的距离 等价为 :  -yi * (wxi + b)/||w||  。更一般的，带入为正值 则是误差点，带入为负值则为正确分类点。
					>误差点到超平面的总距离：-1/||w||Σyi(wxi+b) 即为损失函数L(w,b) = -Σyi(wxi+b) 显然，误分类点越少，这个正值就越小。实际操作：所有样本带入，只取单项为正的 进行求和，使得最小的取出来。假设已经找到了误分类点集合。
			>损失函数极小化算法：梯度下降法 对 损失函数进行极小化。原始形式和对偶形式。
				>函数极值的算法：因为函数的极值 出现在 使得函数的导数=0的地方，所以初始点要朝着 这个位置去，而最快的路线 就是 使得取值变化最快的方向。对于超平面 z = ax1 + bx2 + c, 在某点 变化最快的方向，可以假设该点附近有一点，两点相距单位1长度，则可以证明，使得z取值最大的这个附近的点(m,n)就是 满足 m/n=a/b, 即(a/sqrt(a^2+b^2), b/sqrt(a^2+b^2)) 也即 (a,b)方向；同理可以扩展到n维度的场景，也是这个方向是变化最快的方向--偏导数构成的向量的方向。
					>同理扩展到一般的函数f:  f(x,y) 同理在P点 附近 有一个点 f(x+d1,y+d2) 其中 d1^2+d2^2=d^2=1,  则 该点的函数值= df/dx * d1 + df/dy * d2 , 则 这个函数值的最大值的取得时机？其中 d1,d2 是未知的，使用 导数=0，可以得出 同样的关系，df/dx / df/dy = d1 / d2 从而变化最快的方向还是：(df/dx, df/dy) 同理可以扩展到一般的n维函数。
						>简化计算： 这个附近点的函数值 =  df/dx * d1 + df/dy * d2 = (df/dx, df/dy) * (d1, d2) 表示为点积的形式，显然 (d1,d2)是个单位向量，因此 = |(df/dx, df/dy)| * cosθ * 1, 显然，只有当θ=0时 取得极值，即两个向量平行，也就是 偏导数方向 向量 是 变化最大的方向。更一般的，两个n维度向量相交 则必然处于同一 二维平面，所以同样是 可以分解为 模长和夹角的积，因此仍然是夹角=0时取最大值，所以仍然是同面平行，仍然是偏导数向量方向 是 变化最大的方向。
					>损失函数L(w,b)的偏导数向量：对w: зL/зw = -Σyixi, 对b: зL/зb = -Σyi ; 所以w要沿着 зL/зw 方向变化：如果初始w=w0, 则下一个w'= w0 - зL/зw, 同理 b'=b - зL/зb 这样 就会使得L的值朝着极值变化，很显然这个L的极值 就是0，即朝着L不断减小的方向变化--L减小 就是 误差点到超平面的距离更小。当然，前提要求 这个点 集合 是 误分类的点集合，误分类 就是 yi 和 wxi+b 的值符号相反；因此当初始w,b确定，则可以确定误分类点集合，从而得出w',b', 然后又可以得出误分类点集合，就这样不断的迭代下去，直到L 不再减小为止，就得到了最好的w,b。为了小步前进，增加因子η, w'= w0 - η * зL/зw = w0 - Σηyixi, b'=b0 - Ση *yi
				>算法的收敛性：
				>学习算法的对偶形式:  上述 迭代算法中 下一个w,b的计算中：w'= w0 - η * зL/зw = w0 - Σηyixi, b'=b0 - Ση *yi 初始w0,b0应该是随意的，不妨为0，不妨假设经过了ni次迭代，那么最终的w = Σniηyixi, b=Σniηyi, 令 α=nη, 则 w=Σαiyixi, b=Σαiyi ;从而 以使用 样本点的方式来表示 模型： f = sign(Σαyixi * x + Σαiyi) 为保持变化，最后一项不替换：f = sign(Σαyixi * x + b) , 所以显然：计算最后的α,b: 只需要：α' = α + η ， 而 b' = b + ηyi  从而出现了 xixj 形式-Gram矩阵   好处在于 对偶形式 迭代次数更少 就达到了收敛----但是在物理几何含义 上 就丧失---很不明显了。
			>用感知机对新的输入实例分类：
		>k临近法：分类问题中的k临近法。
			>输入：实例的特征向量。特征空间中的点。
			>输出：实例的类别。
			>训练集：实例类别已定。
			>对新的实例的预测：根据其k个最相邻的训练实例的类别，通过多数表决等方式 进行预测。
				>相邻的定义：两个实例点的距离--欧式距离/Lp距离/Minkowski距离。Lp就是两个向量的 p范数。 p=2就是二范数，就是 欧式距离。 p=∞时，Lp=max|xi-xj|
				>单元：距离实例点 比 其他实例点 更近的所有点的集合-即一个区域。所有的单元就形成了一个对特征空间的划分。
			>参数：k值，定义为临近需要的距离界限，分类决策规则；
				>k值选择：太小 则可能 临近的都是噪声 ，容易 过拟合。k大，则估计误差小，而近似误差大；模型会忽略大量的局部信息有用信息--预测变得笼统。 ---交叉验证法 来选取 最优k值。
			>kd树：二叉树，节点就是实例/样本点。
				>k维超矩形：k就是样本的维度；超矩形 要包括 所有的样本。
				>超矩形切分点：所有样本点的某一维度的分量值的中位数。
				>切分操作迭代结束的标准：子区域内没有 样本点/实例。
				>实例归属区域的划分标准：左节点为 实例的该维度值 小于 中位数，右节点保存 实例的该维度值 大于 中位数，根节点保存 实例的该维度值 等于 中位数。
				>给定节点下搜索它的临近节点：因为兄弟节点是比较近的，而父子节点也是比较近的，所以一般只需要依次往上几代就可以 认为 在这个范围内已经找不到更近的点了，缩小了搜索的范围。而中间节点的临近节点则需要寻找它的所有子类。
				>给定可能不在kd树上的点，搜索kd树中离它最近的点：则可以从根节点出发找到 与 该点 同属一个 叶子节点区域 的 实例，计算和这个叶子节点的距离作为最邻近，然后 向上和父节点/兄弟节点...比较，更新 最邻近，比较的时候可以取巧---就是以目标点为圆心，与当前最邻近点的距离为半径的圆--是否与该节点表达的区域相交。
		>朴素贝叶斯法：基于贝叶斯定理 + 特征条件 独立假设。
			>样本：当作是 X,Y独立同分布 产生的 一组输出。  在X产生的条件下Y的分布= P(Y|X) = P(X,Y)/P(X) = P(X|Y) *P(Y)/P(X) 其中，先验概率P(X)肯定是可以知道的，所以在计算P(Y), P(X|Y); 即计算P(Y=ck), P(X|Y=ck)。
			>学习输入输出的联合概率分布：基于特征条件 独立 假设。
				>条件独立假设：一次 取得 n个值的实验，之所以取得是这n个值，或者说刚刚好 取 得的n个值是 这 n个 ，的 概率 是 跟 这 n个 每个自己有关系的，和 不受前后取值的影响；则称这n个取值 条件独立。假设 一个n次的实验，产生了 X1,X2,...Xn, 结果y为y1,...yk中的某个，则 yk对应的是X1....Xn中的一个子集，n次取值出现这种取值 组合/排序 概率 和 单次取得组合中的每个值的概率 之间 是有关系的，假设n次中每次之间没有影响，那么出现这种组合/排序的概率 就等于 依次 独立取xi值的概率 之积 (即同时取),严格的说 是出现 这种 排序的概率(用n=2证明;依次取3,5;或者取5,3概率一样)：整个排序事件的概率，和单独取值的多个事件 之间的关系： 即 P(X1=x1,X2=x2,...) = P(X1=x1)*P(X2=x2)... ，假设n次的输出都是yk,相当于加了条件，则排序事件和 单独取值事件之间 仍然有关系：P(X1=x1,X2=x2,X3=x3...|Y=ck) = P(X1=x1|Y=ck) *P(X2=x2|Y=ck)*... = P(X=x|Y=ck) X是实验，x是取值序列/排序
			>基于上述模型，利用贝叶斯定理 求出后验概率最大的输出y: 	
			>贝叶斯定理：条件概率-边缘概率 关系 ， 中 边缘概率 替换为 全概率公式即可：P(Y=ck|X=x) = P(X=x|Y=ck)*P(Y=ck) / P(X=x),  而 根据全概率公式，P(X=x) = ΣP(X=x|Y=ck)*P(Y=ck) k为索引。	即：P(Y=ck|X=x) = P(X=x|Y=ck)*P(Y=ck) / ΣP(X=x|Y=ck)*P(Y=ck), 上式带入，则有 = P(Y=ck)*∏P(Xi=xi|Y=ck) / ΣP(Y=ck) * ∏P(Xi=xi|Y=ck), 而对于后验概率的分布，显然认为 分布概率最大的ck 认为 就是 x属于的分类，所以最有可能的分类 就是：argmax P(Y=ck)*∏P(Xi=xi|Y=ck) / ΣP(Y=ck) * ∏P(Xi=xi|Y=ck) = argmax P(Y=ck)*∏P(Xi=xi|Y=ck) 因为分母对k是一样的。这个公式的解释：右边表达式 中 连乘表达式 部分：就是Y=ck时的那一个纵切面上，实验X做n次恰巧取得x1,x2,...xn这样的排序/序列取值的概率，显然，ck 不同取值时，这个序列的概率乘积是不同的，P(Y=ck)取值也是不同的，现实发生的就应该对应的是理论表达式上最有可能出现的应该是 概率最大的, 即 因ck不同取值 而 使得表达式 P(Y=ck)*∏P(Xi=xi|Y=ck) 最大时 的那个 ck 就是 参数ck的取值，就是x的分类值。 
				>argmax: 之类 符号 的目标 都是 求参数， 约束条件是 使得结果最大/最小。
				>期望风险：损失函数的期望。	E[L(Y,f(X))] 此时未知为f, 待确定为f, 使得 argmin E[L(Y,f(X))] 期望风险最小的f;。。但如果 f已经确定了，那么 对于输入x 待 确定的 就是 ck了，此时的期望风险 还是那个关系，但未知/待确定已经变为了ck, 且问题表明 里面的取值此时Y=y,f(x)=ck, X=x，问题即变为： argmin E[L(y,ck)], 而期望就是 概率和取值的累加，所以转换为：argmin ΣL(y,ck)*P(ck|X=x)
					>L(Y,f(X))的准确理解：是一个 二维分布，可以想象为 离散的 横轴为f(X),纵轴为Y，取值都是整数，对应(f(X),Y)处的取值为0/1, 那么E(L(Y,f(X))) 就可以转换为 先计算出 横轴f(X)的边缘分布，然后再计算这个边缘分布的期望。这两步表达为：,,,E(L(Y,f(X))) = ΣΣL(Y=ck,f(X)=y) * P(f(X)=y,Y=ck) , 又 g(y) = ΣL(Y=ck,y)* P(Y=ck|f(X)=y), 即g(y)看作是了f(X)这个随机变量的期望的单项，从而 Σg(y) = Ex(Y=ck|f(X)) = E(L(Y,f(X))) 则 g(y) 是一个随机变量，且E(g(y)) = E()
					>想法2：一个二维随机变量的期望 =  这个随机变量的一维固定取有限的离散值时 分别的形成的新的 一维随机变量的 期望的 概率概率组合：这个关系 可以证明，也可以形象的表示：E(X,Y) = ΣE(X,Y=ck)P(Y=ck|X)。总随机变量 分解为 有限个子随机变量时 总体和局部的关系。这是一个坚固的基础关系，中间关系，桥梁关系。可以用二维平面上的离散点的分布规律来作为例子说明-并触发证明。
						>上述思想：就是 局部均值 和 全局均值的关系！！最形象的解释，局部面积大小 占 全局的面积大， 显然就会使得全局均值受到较大的影响。上述中：分解出的 三行离散数  构成的三个子随机变量 就是 局部。概率占比 就相当于 面积占比。
						>使用上述基本局部均值和全局均值的关系：来对 E(L(Y,f(X))) 进行解释的结果就是 E(L(Y,f(X)))=ΣEx(L(Y=ck,f(X)))*P(Y=ck|f(X))=ΣEx(L(Y=ck,f(X)))*P(Y=ck|X), 其中注意：P(Y=ck|X) 代表作n次实验X时 输出Y=ck的概率---是实验统计的/理论序列计算的，而P(Y=ck|X=x)则代表作一次实验时输出 Y=ck的概率--这就是理论概率了。
							>求期望风险最小：就是 min ΣEx(L(Y=ck,f(X)))*P(Y=ck|X) 此时的未知是 f,待定是f, 即怎样的f才能使得 期望风险R(f) 最小？ 如果f已经确定，则 对于一次输入x,则可以计算f(x)=y, 那么再看风险，则为：min ΣEx(L(Y=ck, y))*P(Y=ck|X=x) 此时 未知的就是实际的输出ck了，认为能够使得 风险最小的 ck 就是 实际的输出值，因此  风险表达式再次变为：argmin ΣEx(L(Y=ck, y))*P(Y=ck|X=x) = argmin ExΣ(L(Y=ck, y))*P(Y=ck|X=x) ,根据L(ck=y,y)=0, L(ck!=y, y)=1, 从而 转换为： argmin ExΣ(L(y!=ck, y))*P(Y=ck|X=x) = argmin ExΣP(y!=ck|X=x) = argmin ExΣ(1 - P(y=ck|X=x)) = argmax ΣP(y=ck|X=x) 这个就是 y的后验概率，即使得 后验概率最大。这就是朴素贝叶斯采取的原理。
			>朴素贝叶斯法的参数估计：条件-边缘关系 和 全概率公式 得出的仅仅用另一个维度的条件概率-边缘概率来 计算 P(Y|X) 的公式，并认为使得它 取值最大的时候 的ck就是目标分类。
				>极大似然估计：思想： 实际出现的结果 是 理论运算表达这个结果事件的概率 表达式(变参) 所代表的所有可能的结果/取值 中 概率最大的那个----能得出什么参数关系就是什么参数关系(一定是实验结果量和参数构成的关系表达式)--这是极大似然观点得出的结论。在这里，这个结果事件就是：Y=ck, 理论表达 就是：P(Y=ck)=g(ck), 求 argmax P(Y=ck)
					>实验结果的唯一确定度量 和 实验结果出现这种具体度量取值 的概率的 计算表达式： 结果 和 结果的概率 要分清楚。
					>输入也要明确：输入量 必然是存在的。引入的参数 其实 一般就是 子事件的概率--更基础的构成结果大事件的元素部分/组成部分。例如即便是模球实验，输入也有---就是模球次数n, 参数就是模球一次 摸到红球的概率。
					>子事件构成 结果大事件 的关系：出现这些子事件 就 等于 出现 这个 结果大事件。 认为是 子事件 组合 成了 结果大事件，是组合关系。由于各个子事件的出现是独立的，互不影响的，
					>估计取值概率 就是 在 估计分布：如估计 模球一次取出红球的概率 就是 估计 取值概率， 各个取值的概率都明确了 也就 明确了 分布。
						>已有实验估计出IO模型f：y=f(x) , 则知道不同的x 直接计算出结果f(x)就是估计值。
						>已有估计出OI条件概率P：P(Y=y|X=x),注意这不是一个固定的分布--而是一个以x作为参数的分布， 则知道不同的x ，才确定了输出的分布！！取分布中概率最大的值  就是估计值---或者直接把着分布作为 估计值/作为答案。
							>结果大事件发生的概率 = 一系列构成的子事件 独立同时发生的概率 = ∏P(每个子事件) : 子事件/基本事件不同 ，概率不同，所以要对子事件分类；其次，由于 结果大事件 是确定的 ，所以一定有关于大事件结果本身的特有的度量，这些度量量肯定会用于 子事件概率的联合起来计算 总结果事件的概率中。
								>结果大事件的含义：描述中带上输出结果代数量的 事件，这样才方便定义 结果事件 的 概率。
							>事情的继续推进中：把 极大似然估计 的思想 联系 起来，对于估计条件概率，如果对事件结果 概率取最大，则利用不到取值结果，而如果利用到取值结果，则不再是对概率取最大，具体：对于第一个样本：(xi,yi), 假设输入x则输出y的分布为：P(Y=y|X=x), 则对于第一个样本的输出分布为：P(Y=y|X=xi), 而实际输出结果是yi, 自然考虑将结果乘以分布中当Y=yi时的概率，显然，如果分布是准确的，那么Y=yi的那个位置的概率应该是分布中最大的位置/极大的位置，不准 则不是，准与不准的度量 可以 直接用 这里的概率 来表示：即 P(Y=yi|X=xi) 这个具体的取值，显然对于整个实验，对于整体的输出分布的准确度的度量，这里对各个子概率：可以乘法也可以加法(都是正相关): 但乘法则体现整个事件的概率，则为 ∏P(Y=yi|X=xi), 此时未知参数则是P, argmax ∏P(Y=yi|X=xi)
								>样本的产生：一定要认为是 从 P(Y,X) 这样的联合分布 中产生的，即X也是一个随机变量。而不是说制作样本的时候X可以随便指定，这不是输入指定的X就能得到输出的模型(而是 X,Y同时产生的模型)。
								>样本的含义：二维分布P(Y,X)，或者叫 联合分布 ，分布约束下 采样到的 n个点。
								>此时要估计这个联合分布P(Y,X): 则 第一种方法 是：拟合，  因为 已知了 样本点，假设 P(Y,X)符合某个带未知参数的一般分布模型，如正态分布，则 带入样本点让 使得误差最小的u,σ估计出来，这个分布也就估计出来了。第二种方法：极大似然估计；假设 整个结果事件可以用 子事件的独立不相关的 序列组合 来表示 ，那么 整个事件发生的概率 就是 子事件发生概率的连乘，整个事件的不同 对应到 各个子事件的不同，总之可以用 子事件独立发生的概率 和 整个事件的规度 来表达，而事件发生了则代表它发生的概率很大，所以使得 这个 总事件发生的概率 取极大值 时的 子事件发生的概率 --就是子事件真正发生的概率。对于估计分布的问题中， 总事件  则是 由 每个子事件 在输入值处的分布函数取值为输出值是的概率 的乘积决定的，不同于之前，子事件的概率可以用一个变量表示，而子事件的分布则是一个函数---因此这种方式一般很难成功(有点像泛函一样，要计算函数---这就需要找到微分方程关系/偏微分方程关系--才能解出值是函数 )---因为问题等价为：求f(ck,xi):，满足 зf/зck = 0 时，ck=yi。设 зf/зck = g(ck,xi), 则g(yi,xi)=0
								>P(Y)是分布：P(Y=ck)是概率：其实靠统计也能计算出 P(Y=y|X=x)， 利 条件-边缘概率关系。
								>由样本可以计算什么：先验概率：P(Y=ck) 根据n个样本的输出就可以统计个数出来：P(Y=ck)=ΣI(yi=ck)/N
									>条件概率：即 当 Y=ck时的全部样本中，x的j分量 取值ajl的次数/y=ck的总次数：I(Xj=ajl|Y=ck)/I(Y=ck)
									>输入x取各个分量并y取ck 这个大事件 的概率：分解为两大事件：P(y=ck)和 各个分量取了xj ，所以整体的概率= P(j=ck)∏P(Xj=xj|y=ck) 显然 这个大事件 的概率 和ck有关，既然大事件发生，则应该取最大时，argmax  P(j=ck)∏P(Xj=xj|y=ck), 来计算出ck。。由于条件概率已经算出来了，即y=ck时的x的分布都明确了了，所以就是直接带入x计算的过程，然后取结果最大时的ck。
								>条件概率的贝叶斯估计：P(Xj=ajl|yi=ck) = (ΣI(Xj=ajl,yi=ck)+λ)/(ΣI(yi=ck)+Sjλ) 和上述的条件概率 和先验概率的 结合+调整，或者说是 条件概率计算的 处理后的形式。
									>同样：先验概率的贝叶斯估计：也要这样调整处理： P(Y=ck) = (ΣI(yi=ck)+ λ)/(N+Kλ)
							>继续推进：y的取值是离散 有限的。
		>决策树： 分类 与回归方法。
			>决策树学习：特征选择、决策树的生成、决策树的修剪。
			>决策树模型：通过损失函数最小化建立。
			>预测：对新的数据，利用决策树模型进行分类。
			>算法：ID3\C4.5\CART: 	
			>决策树： 节点 + 有向边。
				>节点：内部节点表示 特征/属性，叶节点 表示 类。兄弟节点是同一特征 的取值，父子节点代表了两个特征--分别代表一个特征。一条n个有向边的路径就有n个特征。
				>每个叶子节点 代表 该叶子到根节点所表示的特征空间中的单元：也绑定 这个单元 属于 正类还是负类。
			>输入：特征空间上 单元 的 集合。
			>单元c的条件概率P：P(Y=+1|X=c) 	
			>条件概率分布的决策树：
			>学习的目标：根据样本数据 构建决策树模型。可以对每个样本 分 维度 分别 统计出 各个 维度 的 取值区间。然后自顶向下 依次 将 各个 维度 对应的取值区间 对应为 有向边 生长出 这一维度 确定的 树枝---从而快速确定一颗决策树？？但是损失函数在哪里？如何优化？最优决策树的得出？。
				>做法：并不是所有的特征维度 都要走完。有的可能几个维度下就划分了属于该路径下的样本，有多个样本，且都是某类:正类/负类。只选择对 训练数据 有足够分类能力 的特征。
				>剪枝：把路径太长找到，兄弟节点是否能合并在一起，即父节点可以不划分，而是增宽父节点对应的条件范围/取值范围。
			>特征选择：好的特征使得 特征树 高度低，分支少。最终叶子节点少 且 路径不长 且 单个含有的样本多。假设叶子节点数n,路径长度k,单个含有的样本c, 则综合效果至少为： 1/N * Σci/ki,  则表示平均单位路径分类数----好一点的算法：argmin 叶子节点总数*有向边数, 参数就是树。似乎是 优先使用 正负分支中 错误率最低 的 特征量？---但太少则 太模糊，太多则过拟合。其他算法：argmin 错分的总数 * 分支数 
				>信息增益：随机变量的熵：定义为 H(X)=Σpi*logpi = H(p) 描述的是不确定程度---即 概率分布的越均匀 则 越不确定 则 H(X)的值就越大。比如X均匀概率取0,1是，H(p)=1 ，而非均匀取则熵更小，当有一个概率=0时 熵为0---表示完全确定了。 -xlogx 这个函数在0，1区间上时，就是正值，先增后减，在1/底数 的地方取得最大值：1/底数，底数一般取2,e。这个也说明了，当x是概率，且接近0或者接近1时取值都很小,而x的和 却可以=1，因此这个函数可以表明当Σxi=1时，如果xi越接近0或者1，那么总熵就越小，从而定义为信息熵 时完全可以的。同理 ： 2x(1-x)  作为信息熵 也可以，但是没有一般性，即二次函数 不能保障x=0/x=1是零点的同时，最高点的位置不在1/2.；假设有这个f存在：能实现，即：f(0,a)=0,f(1,a)=0, 且df/dx = 0时x=a，a在[0,1]上可调；且f(a,a)=a, 则f(x,a)可以是什么？最好就是先假设模型--一个增函数和一个减函数的乘积---且至少一个函数应该满足增长速率先更小而后更大---且都在0,1之间，显然这种最常见的组合就是 线性函数 和 指数/对数函数的乘积，因为指数对数函数天然的有弯曲的过程---且 还与坐标轴相交---因此不难根据特征要求而自行 得出 信息熵的定义。
					>条件熵：X,Y联合概率分布，在X=xi时Y有一个分布，则可以计算它的熵，而xi可以有多个取值，在xi处也有取值概率--边缘概率，即有多少的概率取到这个条件熵，所以总体来看所有的子条件熵的概率组合 就是 整体的条件熵--所有取值下的条件熵的均值即整体的条件熵：P(Y|X) = ΣP(xi)*H(Y|X=xi)  也就是合理的。
					>因为X的引入而会使得 Y的确定性增加：则会使得Y的熵会减少才对。假设此时Y的熵为H(Y), 则因为引入了X=xi之后，Y的熵变为了：H(Y|X=xi)综合看，引入后熵变为：H(Y|X), 从而熵差 = H(Y) - H(Y|X), 所以把这个定义为 信息增益 是合理的。也称为 互信息。自然的，取信息增益大的 即 使得 熵 减少 多的 特征 进行 分类 就非常的合理。
					>数据集D在特征A下重新排布了：因为A取值a1,...an共n类，所以先分成Dn组，当A=ai时 的 所有样本 在 所有分类Ck中都可能有若干个，假设有Dik个，显然 就将样本 分成了 矩形 i*k个区域，每个区域内有 Dik个样本，因此 D的熵 就发生了改变：H(D|A) = Σ pi * H(D|Ai) = Σ |Di|/|D| * (Σ|Dik|/|Di|*log(|Dik|/|Di|)),  而H(D) = Σ|Dk|/|D|*log(|Dk|/|D|) 即按照原来的分类的序列 而计算的 概率分布的 熵， 自然得出了信息增益：H(D)-H(D|A)
						>事先分类 和再分类： 这样组织数据  就自然可以计算信息增益。进一步的分类 而 增加了信息 而更有序 而 使得 信息熵 更小。原始熵 就是 经验熵，再分类后的熵 就是条件熵。
						>信息增益比：gr(D,A) = g(D,A)/H(D)
			>决策树的生成： 
				>ID3算法：从根节点(样本集合)开始，取信息增益最大的特征的取值 建立 子节点(样本子集)；递归 直到 信息增益 很小(小于阈值) 或者 没有特征可以选择为止(或者这个节点的样本都是同一类)。
					>递归中：因为 节点的 样本子集在变化---自然的Di就是不同的，经验熵也就是不同的。	
				>C4.5算法：过程类似同上，仅仅 选择特征时不是取信息增益最大，而是信息增益比 最大的特征。		
			>决策树的剪枝：考虑决策树的复杂度，而对已生成的决策树进行简化。			
				>剪枝算法：
					>决策树整体的损失函数：/代价函数。
						>试探定义：对叶子节点集合，如果按照ck分为k个子集，则每个子集 的熵 其实 可以作为 这个类的分类好坏的度量，那么所有子集的熵 就是整体的好坏：ΣH(T|ck)
						>实际定义：叶节点 熵 大(叶子节点内也ck类分布) 点多  则 损失函数值大，叶子节点数多 也 损失函数值大：即Cα(T) = ΣNt*Ht(T) + α*|T| 后半部分 就是模型复杂度因素考虑进来，前半部分是模型 对训练数据的预测误差。熵太小 或者 叶子节点数太多都不好。
							>损失函数最小化：自底向上，去掉子节点之前 和去掉子节点之后的损失函数值 Cα(T) 如果 相比 更大，则剪枝。可以用动态规划事先。
			>分类与回归树：	CART.
				>输入随机变量X: 输出 随机变量Y的条件概率分布。
				>假设决策树：是二叉树。 左是右否。
				>决策树生成：
					>对回归树 用 平方误差最小化准则：
						>对输入空间的一个划分：R1,R2...RM；在每个单元Rm上有一个固定的输出值cm。回归树模型：f(x)=Σcm*I(x∈Rm), 每个单元上则对应会有若干个样本点，对训练数据的训练误差=Σ(yi-f(xi))^2， 则对每个单元上的最优输出rm' = 该单元上所有样本点的yi的均值
							>输入空间的切分方法：假设选择了维度j,并且维度的取值为s 作为 切分线，则得到两个区域 或者说将输入样本划分为了两个区域：R1(j,s)={x|xj <=s}, R2(j,s)={x|xj>s}	，进一步可以得到 每个 区域上的  样本的总误差：R1: Σ(xi-c1)^2, R2: Σ(xi-c2)^2, 一个好的划分(j,s)应该是 使得 两个区域上的总误差都是最小的-且总和也是最小的：即 argmin{ argminΣ(xi-c1)^2 + argminΣ(xi-c2)^2} 待定的参数：从左至右为：j,s : c1:c2;  对于每个j, 当分界线xj=s时，则都可以计算 出两个 使得方差最小的c1,c2 即取所在区域的样本点的yj的均值；而使得c1+c2也最小的s就是最优的s, 这样可以遍历s的取值而得出最优时s的取值，那么对于j维度的划分,s就是最优值就计算处理了；则对j=1,2....n都这样计算，分别都可以计算出对应的s和对应的最小的c1+c2, 则选择 其中最小的 c1+c2的s及其对应的j, 这就是最优切分点。
								>不断的最优划分：从对原始空间的划分，再对两个子空间的最优划分，不断的迭代下去，直到空间中没有需要划分的维度/点，得到了一个回归树--最小二乘回归树。对于每个最终的区域，自然有一个最优的cm, 从而 整个划分过程 就可以 同步的表示 为 一个二叉树的构成过程。		
					>对分类树 用 基尼系数最小化准则：对于回归，输出yi是不确定的，而对于分类问题，则 yi值域是离散有限的ck。
						>基尼系数：输出ck,如果样本点属于ck的概率为 pk, 则 Gini(p)=基尼系数= Σpk(1-pk) = Σpk - Σpk^2 = 1 - Σpk^2, 显然对于 二分类，则 Gini(p)=2p(1-p), 一般的，对于给定样本集合D, Gini(D)= 1 - Σ(|Ck|/|D|)^2 。而对于D，又可以根据A特征的取值 进行 再分割，这里A只取值 是 或 否，则每个区域Di分为2个区域，而A的取值 是或否 有概率 a , 1-a, 由于基尼系数本质上是 定义在 样本集 序列上的，如输出ck就将整体样本分成了k个样本集-形成k个样本集序列D，而A的引入则将样本集序列划分为了两个样本集序列：A=是的样本集序列和 A=否的样本集序列，则分别有基尼系数Gini(D1)和Gini(D2), Gini(Dj)=1 - Σ(|Akj|/|Dj|)^2 ,基尼系数本质上是 表示 集合D/概率序列的 不确定程度，已经划分为2个集合D1/D2那么步确定程度应该降低了，或者说不确定程度间接 转移 到 子集合 的 不确定程度上了---即它们的平均不确定程度，那么 再分割子样本集序列 后的整体的 基尼系数 和 子样本集序列的关系 应该是 子样本集序列的均值/期望：Gini(D,A)= |D1|/|D|*Gini(D1) + |D2|/|D|*Gini(D2)
						>选择使得基尼系数最小的D的特征A划分: 来构建 决策树 的下一级节点/二分节点。 
							>二分：就是是否等于某个值。如果特征A有k个值，那么可以 计算Gini(D,A=ak) 来确定最优的ak,然后以 是否=ak来进行切分。这就是选择 最优特征 和最优切分点的过程。	
				>决策树剪枝：
					>从决策树T0底端开始不断剪枝：
						>子树的损失函数：= 树的不确定程度 + 树的规模 ， Cα(T)=C(T) + α*|T|, 其中C(T)可以是这个树的基尼系数--来表示D的不确定程度/训练数据的预测误差；。。使得损失函数最小的子树 Tα 就是目标
							>自增α： 计算α1,α2之间的最优子树T1, 计算 剪枝后整体损失函数减少的程度---剪枝是去掉子节点，所以假设去掉了节点t的子节点--同时改变了t节点中的样本数，那么整体损失=(C(t) - C(Tt)) 考虑规模：=(C(t) - C(Tt))/(|Tt|-1),t为内部任意节点，则可以计算各个内部节点剪枝后的整体损失，找出使得损失函数最小的那个t，剪掉后得到新的子树T1, 并将最小的损失函数值 赋值给α1=min(α,g(t))；不断的剪枝下去，直到根节点；产生的所有子树中选择最优的子树Tα。
		>逻辑回归和最大熵模型：对数线性模型。迭代尺度算法，拟牛顿法。		
			>逻辑回归模型：输出 条件概率P(Y|X), 其中X取值为实数，Y为二值。则要估计计算的则是：Y=0的概率和Y=1的概率。这样就得出了 Y的分布，而这两个概率显然应该跟x有关。
				>概率具体计算模型：假设 想  wx+b 的值 > 0 时 映射一个 <1/2的概率，且越大 这个概率越小；也想当 wx+b的值<0时 映射一个 >1/2的概率，且越小则这个概率越大；可以用图想表示出来，就类似一个z形的函数：这个函数满足的性质：f(0)=1/2, f(x) + f(-x) = 2f(0) = 1, f'(x)<0恒成立，f'(x) = f'(-x), x=0时,f''(x) =0 且取得小值,满足这样性质的函数模型 ： 1 / (1+e^x) 就是这样的一个, 所以定义逻辑回归模型：P(Y=1|x)= exp(wx)/(1+exp(wx))。
				>一个事件的几率：定义为 该事件发生的概率 和不发生的概率 的 比值：p/(1-p)，对数几率=log p/(1-p) 如果这件事为 Y=1|x, 则对数几率=logP(Y=1|x)/(1-P(Y=1|x))=w*x,  这就说明了 Y=1|x 这个事件的 对数几率 就是 wx ,  而 wx通过回归模型 就得到了 Y=1|x 这个事件的概率；这样事先了互相转换。且 事件几率越大 则 事件概率越大。
			>模型参数估计：对于逻辑回归模型 P(Y=1|x)= exp(wx) / ( 1 + exp(wx)) , 未知的参数 就是w, 根据 样本 xi,yi ； 使用极大似然法估计：出现这个结果样本事件的概率= ∏P(Y=1|xi)P(Y=0|xj) = ∏P(Y=1|xi)(1-P(Y=1|xi)) = ∏P(Y=1|xi)^yi∏(1-P(Y=1|xi))^(1-yi)=∏P(Y=1|xi)^yi(1-P(Y=1|xi))^(1-yi) 尽管补充项 和 利用结果yi 之后，连乘可以连续进行，但 计算 它的极值还是困难，乘法表达式映射为加法表达式的处理方式 为 对结果 求单调对数：之后 = Σyi*logP(Y=1|xi) + Σ(1-yi)*log(1-P(Y=1|xi)) = Σ(yi*log(P(Y=1|xi)/(1-P(Y=1|xi))) + log(1-P(Y=1|xi)))=Σ(yi*w*b + log(1-P(Y=1|xi))) = Σ(yi*w*b - log(1+exp(wx))) 接着就是 结果 出现概率最大 时的 w 计算出来。
				>梯度下降法估计：即 沿着 一阶偏导数 方向 移动：w1 = w0 + ▽f 
				>拟牛顿法估计：
			>最大熵模型： 
				>最大熵原理：所有的概率模型中，熵最大的模型 是最好的模型。在满足约束条件的 模型 集合 中 选择 熵 最大的 模型。
				>随机变量X的熵：H(X) = -ΣP(x)*log(P(x)) < Σ1 = |X| 另一方面，如果概率取值都相等，则取得最大熵 = -Σ1/n*log(1/n) = -log(1/n)=logn=logN=log|X|
					>例子：在无更多信息的情况下对 随机变量X: 它可能取A,B,C,D,E共5个值，求X的分布，则根据最大熵，则每个取值的概率都是1/5;  但是如果有先验知识，相等于增加了约束条件，则再根据最大熵，则每个的取值概率就不同了。
					>增加约束条件：
						>分类模型：认为是 条件概率分布P(Y|X)
						>给定样本集合：可以确定 经验分布：X,Y的联合概率的经验分布 和 X的概率的经验分布：P1(X=x,Y=y)=v(X=x,Y=y)/N,P1(X) = v(X=x)/N 即通过频率计算就可以得出 两个经验分布。
						>特征函数f(x,y): 是一个二值函数。定义的缘由：因为 x,y之间可能需要满足其他一些简单的关系，假设样本实际满足了，则 取值为1，否则取值为0(所以可以作为损失函数?);又考虑到x,y其实取到是由一定概率的--即P(X=x,Y=y), 所以定义一个总体取值/总体损失/总体实际损失/总体平均损失 由有意义的：ΣP(X=x,Y=y)*f(x,y) = Ep1(f), 此时的待定的变量  就是 x,y对应该满足的关系的是否满足，全部满足则将最大；当全部满足时显然解雇就是1，
							>经验分布P1(x)和实际条件分布P(Y|X)的真实的损失： Ep(f) = ΣP1(X)*P(Y|X)*f(x,y) 之所以真实，因为X是输入肯定是真实的，Y需要再人为的模型 或者 真实的模型 之后才得到,即P(Y|X)假定就是真实的模型之后才得到的，则这个就是真实的。如果人工的模型能从训练数据总获取足够的信息，则两个模型导致的总体平均损失应该是相等的：Ep1(f)=Ep(f) 这个就是模型学习的约束条件。
							>特征函数的个数：假设有n个： fi(x,y) 即 要满足n个 损失函数相等 约束条件。
						>最大熵模型：就是 使得条件熵：= -ΣP(x)*(P(y|x)log(P(y|x))) 最大的 模型集合C中的模型 。模型集合  是  概率约束条件的集合：C = {P∈Р| Ep(fi)=Ep1(fi)} 
							>条件汇总：满足 损失函数 相等、条件概率和=1
							>将条件并入最大熵关系需要引入拉格朗日乘子，后可以构造拉格朗日函数L(P,w)：L(P,w)= -H(p) + w0(1-ΣP(y|x)) + Σwi*(Ep(fi)-Ep1(fi))
								>最优化问题： min max L(P,w) 依次的因子是 P,w 即 使得 结果最大的 w  得到之后 再使得结果最小的条件分布P(y|x)
									>对于全局只有一个极值点的函数则 其对偶问题 的解 是一样的： max minL(P,w), 如果各个维度的极值点都在 全局极值点 的对应维度值 上，则问题解 也 和 对偶问题 的解 一样。可以用 凸函数来证明。
										>对min L(P,w) 部分： 则 就转为计算 对P(y|x)的偏导数为0 是满足的 Pw(y|x):  εL/εP(y|x) = 0 带入上式 L的表达式：则可以得出 ： 这个 偏导数 = ΣP1(x)*(logP(y|x) + 1 - w0 - Σwifi(x,y)) = 0,  从而 得出  logP(y|x) + 1 - w0 - Σwifi(x,y) = 0， 得出P(y|x) = exp(w0 + Σwifi(x,y) - 1)	= exp(Σwifi(x,y))/exp(1-w0) , 再利用 ΣP(y|x)=1, 从而替换 exp(1-w0)为 一个 求和值Σexp(Σwifi(x,y)) 这个求和值 称为 规范化因子：Zw(x), 带入之后 得到的 min Ψ(w), 即 L(P,w)转换为了  Ψ(w),  这个函数的表达式：会发现 和 条件概率P(Y|X)的对数似然函数：即用条件概率 作为 未知 来表示 做P(Y,X)产生值的实验产生了n个样本(xi,xy)这个大事件发生的概率，显然P(x,y)其实可以理解为x,y出出现的频率，P(y|x)值肯定比P(x,y)大，但实际事件发生则由P(x,y)实际决定：所以不妨用P(y|x)^P(x,y)来表示落在(x,y)处的所有样本这个结果事件发生的概率(即本来用P(y|x)表示落在(x,y)处的概率就够了，但是有多少个样本落在这个位置  也考虑进来，假设n个，也就是正比于P(x,y)个，所以这n个同时落在这个位置的概率=p^n=P(y|x)^n=P(y|x)^P1(x,y))，任何点都是如此，所以 Lp(Pw) = log∏P(y|x)^P(x,y) = ΣP1(x,y)*log(P(y|x)), 展开后会发现和Ψ(w) 相等。所以最大熵模型中 对偶函数极大化 等于 最大熵模型的极大似然估计。
									>计算 最大熵模型 Pw(y|x) 的 学习算法：由上可知的 带入Pw(y|x) 后的拉格朗日函数  或者 说 对数似然函数 = ΣP1(x,y)Σwifi(x,y) - ΣP1(x)*Zw(x) 目标就是计算 使得对数似然函数取得极大值的w
										>改进的迭代尺度法：假设 w + δ 使得 对数似然函数值更大；IIS ...略。
										>拟牛顿法： 沿着 梯度方向走：即 计算 ▽L = (зL/зw1,зL/зw3,...) , зL/зwi = ΣP1(x)*Pw(y|x)*fi(x,y)-Ep1(fi)
		>支持向量机：
			>基本模型：定义在 特征空间 上的 间隔最大的 线性分类器。间隔最大 和 感知机不同。引入 核技巧 从而 是 非线性分类器。
			>学习策略：间隔最大化。形式化为 一个 求解 凸二次规划 问题。正则化的	合页损失函数 的最小化 问题。
				>当训练数据线性可分时：通过硬间隔最大化，学习 一个 线性的分类器，即线性可分支持向量机。
				>当训练数据近似线性可分时：通过软间隔最大化，学习一个 线性的分类器，即软间隔支持向量机。
				>当训练数据线性不可分时：通过使用核技巧 及 软间隔最大化，学习非线性 支持向量机。
			>快速学习算法：序列最小最优化算法SMO。	
			>输入空间：欧式空间、离散集合。 通过一个 非线性映射 可以将 输入空间中的点 映射到 特征空间中的 特征向量。
			>特征空间：欧式空间、希尔伯特空间。支持向量机 的学习 在特征空间中 进行的。样本 也是特征空间上的 数据集。(xi,yi) xi是 第i个特征向量，yi是xi的类标记，如+1,-1。
			>学习目标：在特征空间 找到 一个 分离超平面，能将实例 分到 不同的类。							
				>分离超平面：wx + b = 0 其中 w就是分离超平面的法向量；
				>分类决策函数：f(x) = sign(wx + b)	
				>最优超平面标准：准确分开 且 两类 数据 到 超平面的距离 的间隔最小那个样本的间隔可以取得的 最大值：因为 认为 距离 分离超平面 越近的点 越是可能 分类错误的，可信度更低的。
				>点的分类是否准确：计算 yi *(wxi + b)> 0 则分类正确。且 这个 值：样本点 带入超平面表达式 的值 定义 为 超平面关于 点(xi,yi)的函数间隔。yi' = yi *(wxi + b)
					>超平面关于 训练数据集T的 函数间隔 : 为 超平面(w,b) 关于 T中每个样本的函数间隔的 最小值： min yi *(wxi + b)
				>几何间隔：定义为 由正确分类的超平面 和 样本点 确定的： γi = yi * (wxi + b)/||w||	。同理 ，超平面 关于 样本集合T的 几何间隔 为 超平面 关于 T中所有样本的几何间隔的最小值：γ=min γi
				>寻找关于训练数据集T的几何间隔最大的超平面：这个超平面是唯一的。这里是硬间隔。最大间隔，从而最大确信度 对 训练数据 进行 分类。对最难分的实例点--离超平面很近的点 也有足够大的确信度 分开，从而应该能对 未知新实例 能有较好的 分类预测能力。
					>目标问题规范化：max γ,   γ<= yi *(w*xi+b)/||w||,  最大化的 待定参数为 w,b ; 这个问题 也可以 用 函数间隔来表示：max γ'/||w||, γ'<= yi *(w*xi+b),,, 最大化的待定参数依然没变，且此似乎最大化的表达式离已经出现了待定参数之一，函数间隔的倍增 导致γ'的倍增 但是 整个最优化问题没有变，所以选择恰当的倍增，使得γ‘/λ=1，从而问题简化为：max 1/||w||, yi *(wxi+b) >= 1 ; 继续把问题 简化：由于 待定参 在 分母，求导不方便，所以 进行 等效转换，分母上最大化，就是分子上最小化，范式可以加平方 来消除范式 还 不改变单调性：所以为： min 1/2 * ||w||^2,  yi *(wxi+b) - 1 >= 0 形成 一个 凸二次规划问题。
						>凸优化问题：min f(w), gi(w) <= 0, hi(w) = 0 。 这种问题 典型 如 平面圆内 且 嵌入一个 十字架 求 最小的z=f(x,y)  。其中：f 和 gi  是 凸函数，hi 是 超线性函数/仿射函数。
							>如果进一步：f 只是 二次函数，gi只是 仿射函数，  则上述 就是 凸二次规划问题。
							>证明可将样本集中的样本点完全正确分开的最大间隔分离超平面的 唯一性：假设存在两个最优解w1,b1和 w2,b2; 最优解 即满足 上述一个约束条件 和一个min表达式，满足min表达式，则说明最优解的w1模长相等于w2, 第二，构造((w1+w2)/2, (b1+b2)/2) 会发现也满足约束条件，且它的模长还可能小于等于||w1||, 而w1是最优解，因此不会存在更小的，则只可能是相等，即||(w1+w2)/2|| = ||w1|| 直接说明了 w1和w2这两个向量 是 共线且同向，又模长相等，因此w1=w2,所以只有唯一解。
				>支持向量: 是使得约束条件 yi*(w*xi + b) -1 = 0 的点 (xi, yi),显然 这种点有正类和负类的区别：比如 yi=-1,则 w*xi+b= -1, yi=1,则 w*xi+b=1 很明显，这两类点 在 2个平行于最大间隔超平面w*xi+b=0 的边界超平面上	，这是最近的样本点，而其他样本点带进去的值 则会大于1 或者小于-1	.。这两个边界超平面的间隔 容易 知道 = 2/||w|| 称为 间隔边界。很明显，如果反过来知道了 支持向量，那么使得它们之间的间隔最大的两个分离超平面，即 2/||w|| 最大， 就是 使得 目标函数最大，找到了最大间距d,则 ||w||就可以计算出来了。同时+1-1的两个样本点的距离也可以计算出来、构成的向量也可以计算出来，则距离确定其实超平面就确定了，法向量自然就确定了。所以，由两类 支持向量 就可以 得出w,b,而其他点对w,b无影响；因此 称为 支持向量机。		
					>实际计算 约束条件 + 目标函数 确定的 w,b: 在二维平面上(w1,w2)的情况下，比较容易得出；即线性分割的区域中 离 坐标原点最近的点 就是(w1,w2)对应的取值。
			>学习时的对偶算法：
				>极值：不一定在不等约束条件的边界上，也可能就在原目标函数的极值点上(被约束条件域包围)。
				>拉格朗日对偶性：求解对偶问题得到原始问题的最优解。可以自然引入核函数，推广到非线性分类的问题。	
				>构建拉格朗日函数：首先将不等式约束条件表达为：gi(x)<=0 形式，因为是待参 计算 目标函数f(x)的最小值(从而gi(x)尽可能小 才是和 f(x)尽可能小保持同样的变化方向的 而不会导致 因f(x)的尽可能小导致不满足约束条件，即统一变化方向)，对 约束条件引入 拉格朗日乘子αi, 构成拉格朗日函数：L(α,x) = f(x) + Σαi*gi(x) ；在这里，就是：L(α,w,b) = 1/2*||w||^2 - Σαi*yi*(wxi+b) + Σαi*1  使得 拉格朗日函数最小的w,b找出来(ai可以有很多取值)，后 找 使得拉格朗日函数最大的α：理由如下：
					>对目标函数 min 1/2*||w||^2 : 显然 w越小越好，但是对约束条件 gi(x)<=0来说，则w太小会 破坏约束条件，则此时的w不是想要的；因此 这个权衡不如 和 目标函数一起考虑： 即 f(x) + gi(x)作为一个整体考虑， 显然 因为对目标是希望最小，所以 也是求 这个新的整体的最小值， min (f(x) + gi(x)) 因为 f(x)的变化速率 要比 gi(x)变化更快，导致一开始 x减小 目标函数迅速减小，而gi(x)可能还有小幅增加，所以整体在减小，但是当减小到一定程度后，f(x)减小幅度在缩小，还不如gi(x)增加的多，导致整体开始增大，显然这个临界点 是第一步的目标，即找到整体的最小值；这是考虑到条件后能找到的最小值了，即便如此，此时 gi(x)却有可能是 大于0 的，不满足约束条件，即整体减小得太多了，需要匀回去，直到gi(x)小于或者等于0时，则这个 是在满足约束条件下最小的取值，匀的做法就是gi(x)添加调节系数α，这样在min 整体导致x确定了的情况下依然可以匀，匀则会导致整体值的增加，最多只能增加到刚好满足约束条件--否则就不是最优值了，所以此时就是计算 变参α 使得 整体能达到目标时的最大值，：max {f(x0)+αgi(x0)} 整体看：就是： max min {f(x) + αgi(x)}
						>极小值在目标函数梯度和约束边界的法向量相反的时候：所以λ<0；极大值则λ>0;因此：L(x,λ)=f(x)-λg(x)), 对后者整体：-λ>0 是仅仅值扩大作用-不反向。
						>在L(x,λ)=f(x)+λg(x)) 其中λ>0 ,这样的条件下，则 L(x,λ)(其中λ为待定参数)极小值一定出现在g(x)<=0的范围内，且如果g(x)<=0不是一个封闭区间--且f(x)变化率比g(x)快，则极远处的L值是无穷大--λ无法约束的。证明出现在g(x)<=0的范围内：1.如果f(x)的极小值就在g(x)<=0范围内，那么L的极小值自然也在这个范围内；2.如果f(x)的极小值在g(x)>0的范围内，那么这个极小值点到g(x)=0这个边界线的路径上，f(x)的值在增大，而g(x)的值则在减小，如果λ很小则L极小点甚至就在f(x)的极小值点附近，如果λ逐渐增大到很大，则g(x)的相邻的两个等值线的值差就很大，会导致离g(x)=0更近的等值线上的L值更小，从而使得L的极小值点从f(x)的极值点处移动到g(x)=0处，甚至g(x)<0内，但在最远处依然有f(x)变化最快而L为无限大；。。所以L的极小值点，只有先 让λ尽可能大 使得L的值尽可能的大 从而L的最小值点一定落在g(x)<=0范围内 之后才能 在此基础上计算出来。例子：f(x)极值点P1,极值=f(P1),g(x)在这里的极值=g(P1),P2为g(x)=0上的点， 最开始很大可能f(P1)+g(P1)<f(P2) 所以λ=1时都太小了，必须大到f(P1)+λg(P1)>=f(P2) 时 去计算L的最小值 才能保证极值点在g(x)<=0的范围内。然而P1即便好求，P2是不方便计算的，所以λ大到的最低程度难以计算，但最低程度的特征是L非常大，则max L(x,λ)必然也满足；另外，一个带参函数 在 参数变化情况下 的最大值 不是 说 这个函数的最大值，而是参数影响到这个函数它的下界即最小值能够大到的值-即不会比这个值更大。很明显，无论λ有多大，它能使得max L(x,λ) 即下界能达到的最大值就是 在g(x)=0边界上的L的值，不会更大；更正确的理解：不能把min max L 分开理解，这个的计算过程是 先算外层，或者它的含义 是 先算 L在x下的最小值 然后 计算 这个最小值能够达到的最大值，而不是先算最大后算最小。如此理解，才能进一步理解：由max min L(x,λ)=L(x0,λ0)  (含义为：在λ为参量下L的最大值表达式(为L(x,λ0))在以x为参量下的最小值 等于L(x0,λ0)), 得出 L(x,λ0) >= L(x0,λ0) (要说明的即是 上界的最小值),同理min max L(x,λ)=L(x1,λ1)表示下界的最大值，即有：L(x1,λ)<=L(x1,λ1)，  整理得出：L(x0,λ0)<=L(x1,λ1) 说明 上界的最小值 小于等于 下界的最大值。
							>正确的理解：min max L(x,λ) 就是 ：以x为参量的L的最小值表达式 在以 λ为参量时的最大值：即下界的 最大值。由上述分析可知道，L的下界点 开始在λ很小时 就在f(x)的极小值点处，λ的增大导致L的下界点向g(x)=0靠拢--且这个下界值更大了(因为新增了λg(x)的分量)，但当下界点进入到g(x)<0的区域时，这个下界值暴减，且任意远的一个点都能找到对应的λ使得这个下界值暴减到无穷小，因此这个下界值一定存在一个最大值！！因为最小值点出现在：两者 在两处的梯度值(两个取值线值差)差 小于 两者的值差： 当增大λ使得 f(P) + λg(P) = f(P1) + λg(P1) 时,P为f的最小值点,P1为g(x)=0上的点，此时的λ就已经导致L的最小值移动到g(x)=0上了，再次增大λ，则使得点进入到了g(x)<0的区域，此时的点仍是极小值点，但显然对我们的目标帮助意义不大；注意此时λg(x)是<0的，且随着λ增大而不断减小；再次整理：只要计算 min L(x,λ)，就会发现这个最小值 有一个最大取值：1.当最小值点P在f的最小值点P0和P1之间时，则L(P)<L(P1)=f(P1), 则这个最小值不是很大，而当P在P1上时，这个最小值=f(P1)显然更大，当P在g(x)<0的范围内时，则L(P1)>L(P), 即f(P1)>L(P)则这个最小值 也不是很大即也比f(P1)小，所以L的最小值的最大值一定在g(x)=0上！！！且P1点自然是minL(P1) = minf(P1)即g(x)=0上 f最小的那个点。因此转换来的问题就是 计算 min f(x), s.t g(x)=0, 或者说两个问题的解是一样的。反过来， min f(x) , s.t g(x)<=0 的 等效问题 就是 min max L(x,λ)
					>拉格朗日乘子法的来源：对简单的情景，求max f(x), s.t g(x) = 0 ; 其中x是n维的向量；假设为2维，则是g(x)平面上的凸曲线/n维度的超曲面，平面上好理解和扩展；显然，f(x)在平面上的各个点上都有一个梯度向量，沿着梯度的方向变化最大，曲线是光滑连续的，则梯度和曲线法向量之间有夹角，且沿着曲线夹角在连续变化，显然，当沿着夹角<90度 在曲线上走动时，随着夹角的变化，当夹角减小为0时，继续走动 则 函数值会回退--运动方向和梯度相反了--相当于沿着梯度方向已经走到最远的位置了--这里的函数值是极端的，因此这里的点对应的函数值是极值，特征是曲线的法向量和函数的梯度向量平行：▽f(x)=λ▽g(x), 整理得：▽(f(x)-λg(x))=0 , 这个等式相当于 是 计算 f(x)-λg(x)) 取 极值时的点，即说明 对 f(x)-λg(x)) 计算 极值 的解 和 计算 f(x)的极值 其中 x满足g(x)=0 约束条件 的 解  是 一样的，即存在同解函数，并且这个函数更方便计算。
						>同理，对更高维度：也存在同解函数。把这个函数定义为L(x,λ)=f(x)-λg(x)) 称为拉格朗日函数。如果满足的是 g(x)<=0, 则极值存在则一定也在边界上，所以用同解函数可以计算出来则就是极值点；如果满足的不是g(x)=0而是h(x)=0则就是 计算 L(x,λ)=f(x)-λh*h(x)) 的极值；扩展的，如果满足多个约束条件 gi(x)<=0， 如果存在极值点 ，那么极值点也一定在边界上，假设这个点为P,(则P点一定是f单独受gi(x)约束时的极值点 或者 gi(x)的交点--如不是交点则不是极值点(旁边有函数值更大或更小的点)-一定可以滑动到极值或者交点)，n个不等约束条件，得出n个 同解的 拉格朗日函数，然后把这n个拉格朗日函数=0的式子合并，则得出：▽(f(x) + 1/n*Σλi*gi(x)) = 0 = ▽(f(x) + Σαi*gi(x)) 从而得出，多不等约束条件的等值的拉格朗日函数：L(x, αi) = f(x) + Σαi*gi(x)
						>f(x)的等值线：辅助理解。
						>问题：将 不等 约束条件 自然而然 和 目标函数 联系起来：即 放到同一个 函数中，然后 计算这个新的函数的某些特征 来 计算出 目标函数 在不等约束 条件下的 解，这种方式就会转化/很方便的利用到函数的特征特性--而这个更加方便纯代数计算--而不借助几何。
							>目标：找到这样的 将目标函数 和  约束条件 联系起来的 函数 及其计算指标。即找到 同解函数/不同计算指标但解同的函数。
								>条件：目标函数任意。约束条件 划界一个自变量范围。
								>如果进一步知道什么信息则问题转化为标准模型/熟悉的模型/已知解法路径的问题/路径范围内的问题：如果已知 函数 在自定义域内的等值线、值变化时等值线如何变化和等值线上单个点的移动方向。是连续变化的，所以和自定义域边界相切的等值线的值就是极值。推导可知这个函数=L(x,λ)=f(x)-λh*h(x)) ，计算指标 ▽L =0 梯度方向就是函数值增大的方向(则从某个点P开始，增大函数值=P+▽L即点向梯度方向运动),也就是min L, 但是有λ需要处理。
									>函数看作：布满全空间的等值线的集合。对 约束条件 这个带 取值 范围 约束的 函数，则是 布满部分空间的等值线的集合。g(x)<=0 则是 <=0的等值线的集合，则 -λg(x) 这个函数在g(x)<=0的约束下 就是 同等值线但取值相反了且变为了λ倍，
										>两个函数的梯度可以相加：且相加 就是 两个函数相加的梯度。所以从 梯度图上看相互抵消的地方往往就是极值处---尤其是两个函数的极值 在不同 的点处。
						>对偶函数的规律：max min L(x, λ) <= min max L(x,λ)  参数分别：λ,x;  x,λ	即只有计算顺序的区别；求极值的参数不变；。。用代数 +  最值函数定义 容易证明。	
							>前述证明：上界的最小值 小于 下界的最大值。	而 min max L(x, λ) 就可以转换为 max min L(x,λ)
								>已知L(w,b,α) = 1/2 * ||w||^2 - Σαi*yi*(w*xi + b) -Σαi , 先计算对w,b的偏导数，来计算min值：则 ▽wL(w,b,α) = w - Σαi*yi*xi , ▽bL(w,b,α) = -Σαi*yi  令 = 0 则 w = Σαi*yi*xi, Σαi*yi=0 , 带入 L则得到 L(αi) = -1/2* ΣΣ αiyiαjyj(xixj) + Σαi , 接着计算  max L(αi), 一般难以计算--尽管可以尝试用偏导数的方式计算+用边界上取值αi>=0这个边界去尝试，额外的约束条件：Σαi*yi也可以使用。
									>求解 max L : 前述已知 λ是可以找到一个最大值的 是 max L 的解，那么这些最大值中的最大值 假设是C,则 0<=α<=C 是 一个隐藏的约束条件，第二， b约束得到的：Σαi*yi=0， 目标则是 max L ，加一个负号 则是： min L = 1/2* ΣΣ αiyiαjyj(xixj) - Σαi, 其中 yi,xi都是已知的样本，所以只有 αi这一个未知向量。这里已经假设完全正确分类，所以 yi(wxi+b)>0 恒成立的。
										>特殊化计算：如果只有两个样本，则未知变量α1，α2两个，可以 求偏导数=0，而得出新的两个关系：K11α1 + y1y2K12α2 - 1=0,...且仅仅由两个约束条件就可以知道y1y2一定是异号的，所以y1y2=-1,且α1=α2，从而 K11α1 - K12α2 - 1=0 ， 从而αi = 1/(K11-K12), 而 从第二个关系式得出：αi = 1/(K22+K12), 进一步可以得出K12=(K11-K22)/2
										>强力计算：则对每个αi求偏导=0，从而得到 Σyjkijαj = 1/yi, 结合 Σyiαi=0, 则统一为矩阵表达：构造矩阵 M = [K11y1 K12y2 ...K1jyj; K21y1 K22y2 ...K2jyj;...Kjjyj; y1 y2 y3 y4...yj], 构造向量α = [α1 α2 ... αj], 然后构造值 y = [y1 y2 ...yj 0], 则由 Mα=y 从而 解这个矩阵方程 即可得出解了。α = y*M^(-1) , 注意： y有j+1维度，M是(j+1)*j维度。
										>试探性解法/启发式算法：思路：先尝试找α1和α2的最优解。尝试，就是先给初始值, 然后迭代更新。则此时需要把其他变量计算的值当作常量处理，即 α1y1 + α2y2 = ε, 则α1 = (ε - α2y2)y1 , 对目标函数L， 则此时 L = 1/2K11α1^2 + 1/2K22α2^2 + y1y2K12α1α2 + y1α1ΣyiαiK1i +... - Σαi, 然后 带入α1和α2的关系，从而得到 L(α2) = 1/2K11(ε-α2y2)^2 + 1/2K22α2^2 + y2K12(ε-α2y2)α2 -(ε-α2y2)y1 - α2 +v1(ε-α2y2) + y1v2α2 ，其中v1,v2是简写，令Ei=ΣαjyjK(xi,xj) + b - yi = g(xi) - yi, 则vi = g(xi) - α1y1K(x1,xi) - α2y2K(x2,xi) - b 即 Ei是模型系统输出值(第一个和第二个样本的模型系统输出值) 和真实值的差，认为它不是αi的函数，因此对εL/εα2 = 0  来计算极值，则得出了α2的取值表达式： α2(new,unc) = α2(old) + y2(E1-E2)/(K11+K22-2K12) ,再限定范围 [L,H] 即线段的取值范围内，就是α(new)的值。
									>变量的选择方法：	违反 KKT条件 最严重的样本点。
										>具体操作：检验样本点 是否 满足KKT条件：注意 L中 的 αi 确实 是 表示 样本点的系数 ---因为在约束条件中 表示 几何间隔最小的那个样本点对应的几何间隔和其他样本点的几何间隔的关系 而得出的和样本点数n一样多的关系。 	
											>αi = 0 时： 样本的模型系统输出值 g(xi) 和实际值yi应该是同向的，yi*g(xi)>=1, 从拉格朗日乘子法的定义来看，则αi就是对应的 1 - yi*g(xi) <=0 左边部分的系数。αi=0说明 f(x)最小值点就在 1 - yi*g(xi) <=0 这个范围内，即满足这关系：即 yi*g(xi)>=1; 
											>0<αi<C 时： 说明需要调节使得L(x)的最小值移动到边界上，但不是调节强度最大的， 但极值点一定就在>=约束条件函数的范围，因此只能 1 - yi*g(xi) = 0 ， 即 yig*(x)=1 
											>αi=C 时： 说明调节使得L最小值落在 1 - yig(xi) <= 0 的调节力度达到了最大：此时的样本点也一定满足：yig*(x)=1 。。检验如果在ε范围内，则 yig*(x)=1-ε 也满足，则这种点也归结到这一类，此时则 yig*(x)<1。
										>选择第一个：选择支持向量点 看是否满足KKT条件(yig(xi)=1?)然后再看其他点是否满足KKT条件。
										>选择第二个：使得α2有足够大的变化。即从计算表达式α2(new)的计算表达式中发现，|E1-E2|如果更大 则 下一次的α将变化更大。所以选择使得|E1-E2|最大的那个α2. 或则优先从支持向量点中找满足这个计算最大的。
								>阈值b的计算：因为 g(x1)=wx1+b = ΣαjyjK1j + b , 当选择支持向量点 即满足 0<αi<C 对应的样本时， ΣαjyjK1j + b = yi , (yi^(2)=1), 所以 得出b = yi - ΣαjyjK1j	
			>支持向量机和软间隔最大化：训练数据集线性不可分时的做法：					
				>对于原问题：min max yi *(wxi+b)/||w||, s.t. yi *(wxi+b)/||w||>=0 (即服从完全分开)， 而超平面wx+b=0, 足以到wb即便算出来了，则kw,kb也是答案，所以配套即可，w可以取任意值，不妨假设就取了某个配套w,b;使得 min yi *(wxi+b) = 1 这个是完全可以的；从而原问题 变为 max 1/||w|| , s.t. yi *(wxi+b) >= 1; 后面就是使用拉格朗日乘子法来解决问题了。
				>注意到对线性不可分的数据：其 yi *(wxi+b)/||w|| 如果仅仅为超平面附近很短距离内的线性不可分，则这个值为比较小的负数，所以如果给它人为补充一个变量小的εi,使其值为正:yi *(wxi+b)/||w|| + εi/||w|| >=0，(引进很多个其实不方便计算，不妨引进一个最大的ε使得最小的加上后可以为0);以前是希望yi *(wxi+b) >=1 现在是 错误分类的样本已经不能满足了，如果依然调节超平面使得正确分类的点的最小值=1，则错误分类的点带进去值必然为负，但加上一个松弛变量εi则可以继续保证都>=1 : yi *(wxi+b) + εi >=1 ,对 εi的要求则是能小则小，太大了则超过了w,b的影响从而随便的w,b都可以，那么分类就不准了。因此 原目标函数：min 1/2*||w||^2需要加上要求εi尽可能小的特征：min 1/2*||w||^2 + Σεi.  则在计算层面上，又可以使用原来的计算方法了。
					>新的问题模型：min {1/2*||w||^2 + CΣεi} ，s.t. yi *(wxi+b) + εi >=1 , εi >= 0  . 这样得到的 分类决策函数 称为 线性支持向量机。C为惩罚系数。
						>构造拉格朗日函数： L(w,b,εi) = 1/2*||w||^2 + CΣεi - Σαi*(yi *(wxi+b) - 1 +εi） - Σμiεi   分别对三个变量求导取0 可以得出 w,b和以前一样：w=Σαiyixi , Σαiyi=0,  C-αi-μi=0 带入后可得L = -1/2ΣΣαiyiαjyi(xixj) + Σαi  从而 max L 计算即可。
				>合页损失函数：线性支持向量机 的另一种 解释:也是线性支持向量机 的原始最优问题的等价问题： min Σ[1-yi(wxi+b)]++ λ||w||^2 第一项为经验损失函数。	[1-yi(wxi+b)]+ 为合页损失函数：函数值为负的部分映射为0：显然 分类正确且在间隔边界外 则 合页损失=0， 而在间隔边界和分类超平面之间 则 合页损失为正(0,1)，在分隔边界上则为0，分类错误则>1。		
					>等价证明：令 [1-yi(wxi+b)]+ = εi , 则 εi>=0, 当正确分类且在间隔边界外时：1-yi(wxi+b)<=0 ,从而 yi(wxi+b)>=1, 合页损失函数=0=εi, 而正确分类时原最优化问题中的εi也是只能=0； 当在间隔边界内且正确分类，则 1>1-yi(wxi+b)>0	从而 0<yi(wxi+b)<1,合页损失函数得出：1-yi(wxi+b) = εi， 即 yi(wxi+b)=1-εi 正好也是 原最优化问题的形式；当 错误分类时 也是这个关系，也是原最优化的形式	
						>从而合页损失函数分解 得出的2个关系合并后的统一形式就是：yi(wxi+b)>=1-εi , 其中 εi>=0 , 而目标函数形式也一致了。		
			>非线性支持向量机和核函数：
				>非线性分类问题：能用一个超曲面将 正负实例分开。
				>思路：将 实例 经过 一个 非线性 映射 转换为 新坐标系下/新空间下的实例，实例的类别保持不变。
					>将输入空间 对应于特征空间：而将 超曲面模型 对应 到特征空间的 超平面模型。
						>定义映射：φ(x): 是输入空间中的向量x 到 特征空间的一个映射。 则 对于输入空间中的两个向量 x,z : 定义 K(x,z)=φ(x)φ(z) 即映射后的两个向量的内积  为 核函数。直接计算K(x,z)更加容易。且一般 x-->x',x'是更高维度的向量，---至少n+n(n-1)/2项，至多n^2项。同一维度下的φ可能有多个。
						>原目标函数中的内积：如果将实例的x映射到更高维度的空间上的x'后，在新空间上进行线性分类，所构造的拉格朗日函数L的min 得出的梯度=0带入后的L 中 和实例相关的也只有x'*z'内积而已，知道新空间上的内积 实际上 就可以计算出新空间上的线性分类器了，而新空间上的内积x'*z' 表示为：φ(x)φ(z), 这个就是 核函数的表达式K(x,z) ,目标函数中关于实例的x的只有实例间x的内积，因此只需要用核函数替换即可，无需知道φ：min max L = max L(α) = -1/2*ΣΣαiαjyiyj(xi*xj) + Σαi ,新空间中：max L = max L(α,K) = -1/2*ΣΣαiαjyiyjK(x,z) + Σαi	
						>核函数的选择：不用构造φ(x)而判定K(x,z)一定是核函数的特征条件：
							>正定核函数：
								>半正定Gram矩阵：Gram矩阵 是 k个向量互相之间的内积构成 的 矩阵。内积表示两个向量的相关关系。
									>半正定矩阵：如果任意向量x经过矩阵M的变换后得出的新的向量y和x的夹角小于等于90度，则 称M为半正定矩阵。即满足 x^T*M*x >= 0 的M就是半正定矩阵。大于0则是正定矩阵。(把物理爱好放到最后，积累财富为我这一代最优先)
									>协方差矩阵：C = E[(t - t_)*(t - t_)^T]其中t是任意多元随机变量，t_是随机变量的均值。显然(t - t_)*(t - t_)^T 是一个矩阵。。显然由定义：x^T*C*x = E[s^2]其中 s=x^T*(t-t_)=σ是一个数，说明协方差矩阵是半正定的。
									>矩阵的特征向量：Ax=λx, 其中λ是特征向量。
									>对称矩阵的定义和性质：A^T=A则A是对称矩阵。性质：
										>特征值：有n个且为实数。
										>特征向量：n个且互相正交(xixj=0)。证明：如果p1,p2是特征向量，则 构造出 p1^T*p2的结构来看：Ap1=λ1p1, 则 p1^T*A^T=λ1*p1^T=p1^T*A,  所以 λ1*p1^T*p2 =p1^T*A*p2 = p1^T*λ2*p2=λ2*p1^T*p2  ,则 (λ1-λ2)p1^Tp*p2=0, 则 p1^T*p2=0 即特征向量正交。
											>证明相同特征值的特征向量是正交的：相同特征值的特征向量是线性无关的，可以进行正交化。
												>正交化：使用 施密特正交化的办法，对 k个线性无关的向量正交化处理。正交化，就是求 k个线性无关的向量的正交基，用这 k个线性无关的向量进行某种线性组合 可以构造出k个正交基，证明线性无感的向量组和正交基等价。
													>Givens旋转: 另一种正交化的方法。
										>对称矩阵的正交分解：A可以用正交矩阵P进行变换：P^(-1)AP=Λ, 其中Λ是对角元素为A的特征值的对角矩阵。 证明：由上可知A的n个具有不相关特征的特征向量 可以 线性组合出 一组 n个 单位正交基，利用这个单位正交基构成矩阵P, 则带入可以得出 AP=PΛ ，所以 P^(-1)AP=P^(-1)PΛ=EΛ=Λ	
									>正交矩阵的定义和性质：Q^T*Q=E,则Q是正交矩阵。
								>正定核K(x,z)的满足的特征：可以分解：K(x,z)=φ(x)*φ(z)， 因此 由核函数构成的Gram矩阵[K(xi,xj)]m*m 很明显就是一个对称矩阵。而对称矩阵是半正定的。
									>另一种理解：对称函数K(x,z)为正定核的意思是：K(x,z)=φ(x)*φ(z)，而X*X空间中的两个向量x,z映射到希尔伯特空间之后的内积 为 ： φ(x)*φ(z)。。K的Gram矩阵为：[Kij]
										>对称函数K(x,z)升格为正定核的条件：如果K在对称的基础上，加上一个条件：它的Gram矩阵是半正定的，则信息联合之后 可以得出：对于给定的线性空间上的值K(x,z), 可以构造 某个X->H即线性空间到希尔伯特空间的映射φ:x->K(.,x) 即 φ(x)=K(.,x) 即映射H空间之后同样能和H空间中的某个未知变量 通过(内积)方式得到原K值(K函数值)，即K(.,x) 就是x映射到的希尔伯特空间中的那个元素，而由上有：K(.,x).K(.,z)=K(x,z)即H空间中的两个元素通过点积运算后得出的值就是它们在X空间时的K函数值，自然的，K(x,z)=φ(x)*φ(z) 即K是正定核。
										>对称函数K(x,z)的Gram矩阵升格为半正定的特征的条件：当对称矩阵K是 正定核时：有K(x,z)=φ(x)*φ(z) 自然就得出Gram矩阵是半正定的了。
								>再生核：K(.,x).K(.,z)=K(x,z) 注意 K(.,z)  是一个向量，而K(x,z)是一个值/标量。
									>定义φ：x->K(.,x) 是这样的一个映射。即线性空间的一个元素到希尔伯特空间的元素的一个映射。
									>K(.,x)的含义：两参 运算符 定一参 ，要把.当作是 "某 未确定 名称的 变量"。第二种理解方式：不太好：就把.当作运算符，一个函数 的入参 可以是数也可以 是 运算符， 函数内部实现的功能可以理解为 让入参 元素 和 零点元素 作用到运算符上 而得出的结果 作为函数 返回值：这样就很自然的理解了K(.,x).K(.,z)=K(x,z)。(计算机的函数 也可以辅助理解这里的K函数)
										>K(.,x)的最佳理解：“H空间中的 也可以 进行 这样运算(K规则运算过程)的 向量(K中的行为角色功能和X中的x一样)/元素”，这里的.可以立即为x要映射为H中的元素时的一个提升运算符。
									>定义f(.)=ΣαiK(.,xi): 且左边的.和右边的.指的是同一个 "某 未确定 名称的 变量"。
									>定义 f * g = ΣΣαiαjK(xi,xj): 显然 这个 * 就不是数乘乘法，叫做结合，叫做 "确定"“某未确定 名称的变量”的名称的运算---这种运算的规则 就是 左右互相确定 彼此的 未确定名称的变量(同构组合//互补运算//同构填充运算)。
									>K(.,x)*f 的含义：按照上面的定义，则：= K(.,x) * ΣαiK(.,xi) = ΣαiK(.,x)*K(.,xi)=ΣαiK(x,xi)=f(x)
									>K(.,x)*K(.,z)的含义： 由上，则结合的结果为：K(x,z)
									>再生性：定义*=., 则 K(.,x)*f = K(.,x).f = ΣαiK(x,xi)=f(x), 这就是再生性。
								>mercer定理核mercer核：	
								>希尔伯特空间：线性 完备 内积 空间。
									>抽象距离：两个元素的差异程度。这个量定义为：d(X,Y)>=0, d(X,Y)=d(Y,X), d(X,Y)+d(Y,Z)>=d(X,Z);如果d(X,Y)=0则X=Y 。满足这四条性质 的量 称为距离。
									>范数：需要概念：度量空间 + 零点。定义为：元素到 零点 的距离：||X|| = d(X,0), 拥有范数的空间 为 赋范空间。定义 d(X,Y) = ||X-Y|| 自然得出 ||X|| = d(X,0) 零点 就是 X-0=X的点。
									>线性：空间中的一组基 通过 线性组合 可以表示出 空间中的所有元素 则为线性空间。或者一个 集合 对 加法 和 数乘 运算 封闭，则S也构成线性空间/向量空间。
									>内积：在距离基础上 将 两个元素和零点之间形成的夹角 考虑进来，定义 (X,Y)=||X||*||Y||*cosθ,  则 (X,X)=||X||^2
									>完备性：H中存在y使得 lim||x-y||=0 。即 任何一个柯西序列都收敛到此空间中的某个元素。
									>任何一个希尔伯特空间都有一族标准正交基：例如傅里叶分析 中 将一个给定的函数 表示成 一族给定的 基函数的和。
									>柯西-施瓦茨不等式：内积和范数的关系：|(x,y)|<=||x||*||y||
									>变分定理：凸闭子集M中存在元素y, 使得H中的任意元素x 都满足 ||x-y|| 有最小值。
									>正交系：H中的一族 正交 元素： (ei,ej)=0；如果 (ei,ei)=1 则称为规范正交系{ek}。如果包含{ek}的最小闭子空间(张成的空间) 就是H，称{ek}为H的完备规范正交系。
									>贝塞尔不等式: x在H上的一族基的投影和x长度之间的关系：||x||^2<=Σ|(x,ek)|^2
									>傅里叶展开式：H上的一族完备规范正交系{ek}：存在 x=Σ(x,ek)ek
									>帕舍伐尔不等式：H上的一族完备规范正交系{ek}：满足 ||x||^2=Σ|(x,ek)|^2
									>里斯连续线性泛函表示定理：H上的每个连续线性泛函F, 使得 F(x)=(x,y) 成立 且 同时 ||F|| = ||y|| 成立的 y是唯一的。
								>巴拿赫空间：线性 完备 赋范空间。
								>欧几里得空间：有限维 线性 内积 空间。
							>常用核函数：核函数本身 就是普通的函数表达式 且 K的Gram矩阵是正定的 。
								>多项式核函数：
								>高斯核函数：
								>字符串核函数：
									>字符串：就是长度为n的字符串
									>子字符串：字符串的k个字符按原顺序构成的字符串。
									>Σ^n: 定义 长度为n的所有字符串。字符串集合。
									>R^(Σ^n): 定义在Σ^n上的所有元素 为维度 张成的空间。
										>一个字符串A 在 这种 维度上的取值：定义为 A的子串 中所有那些 刚好 等于 该维度 的 子串， 将这些子串分组，按照 在A中的宽度l(t)分，统计每种 宽度的子串的个数n,用λ^(l(t))表示一种子串, 然后求和。总结起来，字符串s在这个维度上的取值= Σn*λ^(l(t))=[φn(s)]μ。。。例如 字符串 [φ3("Iass das")]asd = 2λ^5
									>字符串核函数：kn(s,t)=Σ[φn(s)]μ[φn(t)]μ = ΣΣλ^(l(i))λ^(l(j)) 就是 字符串s和t中所有长度=n的子串  在Σ^n的所有的维度上的取值  所组成的特征向量 (即想象为：想象一个字符串的所有特征向量：纵向 是 所有的子串 ，横向是所有的维度 ，坐标值就是 在维度上的取值；每行 就是一个特征向量)，s和t各自的所有特征向量 的余弦相似度 的计算 公式 就是 这个。 
								
				>非线性支持向量机：将线性支持向量机 的 对偶形式的 内积 换成 核函数。就是计算内积的方式不同，一个直接计算，一个映射到希尔伯特空间之后再计算内积。
				>序列最小最优化算法：SMO算法，高效的实现 支持向量机 学习。
					>SMO算法：将原问题 不断分解为 子问题 并对子问题 求解。
						>原问题： min 1/2*ΣΣαiαjyiyjK(xi,xj) - Σαi , s.t. Σαiyi=0, C>=αi>=0
						>算法思路： 
							>此最优化问题的KKT条件：是否对 所有变量的 解 都满足？
							>过程：选择2个变量，其他变量固定，则原问题 退化为 二次规划问题。
								>比如选择了α1，α2为变量：W(α1,α2)= 1/2 * α1^2 K11 + 1/2 * α2^2 K22 +  α1α2y1y2*K12 - (α1+α2) + y1α1ΣαiyiK(i,1) + y2α2ΣαiyiK(i,2), α1y1 + α2y2 = -Σαiyi = ε ，  注意最后一个公式 还是递推 关系，即 α1(new)y1 + α2(new)y2 = ε = α1(old)y1 + α2(old)y2 
									>则将  α1y1 + α2y2 = ε  这关系 带入 W(α1,α2) 则得到 W(α2)， 然后对它求导 dW/dα2 = 0 就 得出了 α2 的计算表达式----对其中的ε，则用递推关系式  替换：ε = α1(old)y1 + α2(old)y2 ， 则此时 α2 也要替换为 α2(new,unc) 才准, 然后要限制 它的取值边界。那么就得到了 α2(new), 进一步α1(new)
					>KKT条件：计算 min max L时 极值点所在处满足的关系：εL/εx = 0, λi >=0, λigi(x)=0 (因为当f(x)的极小值在 g(x)>0的范围内时，L的极小值假设也在g(x)>0里，则最后求的极大值点就在增大λ后导致极值点到g(x)=0处，此时满足关系。第二，当f(x)极小值在g(x)<0里时，L的极小值点在f(x)右侧即还是 g(x)<0里，而λ变化而L的极小值的极大值在移动，当λ增大时点右移，λ减小时，极值点不断靠近f的极小值点处 且值更大，直到λ逼近0=0时，完全没有了g(x)的影响 而L取得了最大值，此时λ*g(x)=0也成立)。gi(x)<=0
				>概要总结：
					>分离超平面的w: 是 输入的线性组合。
		>提升方法：改变训练样本权重，学习多个分类器，将这些分类器线性组合。
			>提升方法AdaBoost算法：发现 弱学习算法 并 提升 为 强学习算法。弱学习算法-->弱分类器-->组合弱分类器 为 强分类器
				>改变训练数据的概率分布：增大 分类错误的样本的权值，减小 分类正确的样本的权值，来增大错误关注和减小正确关注。
				>组合分类器：则正确率大 的 弱分类器 权值大，在表决中起到更大的作用。
				--这些想法 自然 融合在一个算法里：AdaBoost
				>基本概念： 
					>样本：
						>样本的权重 wi ： Di = wi
							>更新：基本思想 显然是 分类器误差率越低 权重变得越低，输出值的函数间隔(yi*Gm(xi))越大和正确则 权重变得越低(让下一轮关注点在错误分类样本上): 因此 ：w(new,m)i = w(old,m)i * exp(-αm*yiGm(xi))/Σw(old,m)i* exp(-αm*yiGm(xi)),  一般认为 Gm(xi)=+-1
					>分类器：
						>分类器Gm的 误差率 em = Σwmi*I(Gm(xi)!=yi)
						>分类器Gm的 权值系数：αm = 1/2 * log((1-em)/em), 当em=1/2时，系数αm=0, em->0,αm无穷大；em->1则为负∞。
					>分类器的线性组合：f(x)=Σαm*Gm(x)  , 最终的分类器 G(x)=sign(f(x))	
				>AdaBoost算法的训练误差分析：非常经典的推导：1/N * ΣI(G(xi)!=yi) <= 1/N*Σexp(-yi(f(xi)) = ∏Zm    第一个不等式利用 exp(x)是一个增长的函数 和 错误分类时 yi和f(xi)异号的特征即可；第二个 则 直接从 左边推导，利用1/N=w1i, 然后 exp(-yi*f(xi))=∏exp(-αmi*yi*Gm(xi))  从而 左边整体 = Σw1i∏exp(-αmi*yi*Gm(xi)) ,而 w1i*exp(-α1i*yi*G1(xi)) = Z1*w2i, 从而连乘项可以一直缩减下去，得到 ΣZ1Z2Z...exp(-α(M-1)*y(M-1)*G(M-1)(xi))=Z1Z2...Σw(M-1)i*exp(-α(M-1)*y(M-1)*G(M-1)(xi))=∏Zm 
					>进一步利用：正确分类：yi*G(xi)=1 或者错误分类： yi*G(xi)=-1 ， em = Σ wmi (错误分类)， 所以：对于 Zm = Σwmi*exp(-αm*yi*Gm(xi)) = Σ wmi*exp(αm) + Σwmi*exp(-αm) , 再带入αm=1/2 * log((1-em)/em) 的关系，就得出了 Zm = 2sqrt(em*(1-em)), 从而 ∏Zm=∏2sqrt(em*(1-em)) 再利用 γm=1/2-em 得出，∏2sqrt(em*(1-em)) = ∏2sqrt(1-4γm^2), 再利用 泰勒展示在x=0附近有不等式 e^(1/2*x) >= sqrt(1-x) 得出：∏sqrt(1-4γm^2) <= exp(-2Σγm^2)，当m增加1时，可以得出 sqrt(e(m+1)(1-e(m+1))) <= exp(-2*γ^2) 
						>进一步到上上步：则 组合分类器的错误率： 1/N * ΣI(G(xi)!=yi) <=  ∏Zm  <= exp(-2Σγm^2) <= exp(-2Mγ^2)  这说明了，m越大，则 组合分类器的错误率越小。且m增加1，则组合分类器的错误率将减少一个指数倍: exp(-2γ^2) , 这就会使得 组合分类器的错误率会快速减少。
					>算法解释：算法模型：加法模型(输入输出模型)，损失函数(差值为最基本损失函数)：指数函数，学习算法：前向分步算法。二类分类学习算法。
						>前向分步算法：
							>加法模型：f(x) = Σβm*b(x,γm) 很明显，基函数 仅仅 是 γm的区别；βm是未知的系数。
							>损失函数：L(y,f(x))自然指的是 样本值和 模型输出值 的 差 之类的关系； 每个样本都得出一个损失值，所以 总共的损失 = ΣL(yi,f(xi))
							>目标函数：让损失函数的值最小：min ΣL(yi,f(xi)) = min ΣL(yi,βm*b(x,γm)) 此时参数太多，难以计算。而对于加法模型，按照单独求偏导数可以得出极值时的参数 的特征联系，而认为 min ΣL(yi,βm*b(x,γm)) = min ΣL(yi,f(m-1)(xi) + β*b(xi,γ)) = (βm, γm), 即分解出常量，而未知参数大大减少。且认为 f(0)=0, f(m)(xi) = f(m-1)(xi) + βm*b(xi,γm)  。 从而不断的递归下去。最终得出m对(β, γ) 从而： f(x) = Σβm*b(x;γm)
								>当 b(x;γm)= Gm(x) 时： f(x) 则变成了组合 分类器 模型。即最终分类器。f(x) = Σαm*Gm(x) , 前面递推已经得到 f(m)(x) = f(m-1)(x) + αmGm(x), 则 f(m)(x) 在训练集上的指数损失总和 = Σ exp(-yi*f(m)(x)) = exp(-yi*(f(m-1)(x) + αmGm(x))) = exp(-yi*f(m-1)(x))*exp(-yi*αmGm(x)) = wmi*exp(-yi*αmGm(x)), 即指定 wmi = exp(-yi*f(m-1)(x))， 那么使得指数损失总和最小 的 αm，βm 的计算， 需要对这个表达式 进行分解：	Σ wmi*exp(-yi*αmGm(x)) = Σwmi*exp(-α) + Σwmi*exp(α)， 第一项为 相等，第二项为不相等(模型输出和样本实际) 由于Σ只跟样本有关，考虑Σwmi = 1, 则补充项 后 得到 Σwmi*exp(-α) + Σwmi*exp(α) = Σwmi*(exp(α) - exp(-α)) + exp(-α)* Σwmi = Σwmi*(exp(α) - exp(-α)) + exp(-α) = (exp(α) - exp(-α))* Σwmi + exp(-α), 此时  Σwmi 则表示 错误率/误差率，记作em, 则 min这个损失总和L: зL/зα=em*(exp(α)+exp(-α)) - exp(-α) = 0, 整理 得出：α = 1/2*log((1-em)/em)
									>利用 wmi = exp(-yi*f(m-1)(x)) 和 f(m)(x) = f(m-1)(x) + αmGm(x) ， 则可以得出： w(m+1)i = wmi*exp(-yiαm*Gm(xi)) 可见这个关系也是 形式相同的。
							>结论：AdaBoost 算法 时 前向分步加法算法的特例。		
			>提升树：统计学习中性能最好的方法之一。
				>基函数：决策树。
				>提升树模型：f(M)(x) = ΣT(x;Θm)  T为二叉分类树。 	 	
					>递推：f(M)(x) = f(m-1)(x) + T(x;Θm)  本质就是在输出上 不断增加一棵树 来 校正 
				>目标函数：寻找 Θm 使得它是 argmin ΣL(yi, f(M)(x)) = argmin ΣL(yi, f(m-1)(xi) + T(xi;Θm)) 的 解。 T(xi;Θm) = Σcj*I(x∈Rj)  当采用平方误差 损失函数时：L(yi, f(xi)) = (yi - f(xi))^2 = (yi - f(m-1)(xi) - T(xi;Θm))^2 = (r - T(xi;Θm))^2 则 r就是残差，当前模型拟合数据的残差。
					>损失函数：用平方误差损失函数 的回归问题，用指数损失函数的 分类问题。
						>回归问题的提升树算法：初始化 f(0)(x) = 0,---> 计算残差 rmi = yi - f(m-1)(xi); --> 根据argminL 计算 回归树 T(xi;Θm),---> 更新 f(m)(x)=f(m-1)(x) + T(xi;Θm) -->最后将学习到的树类加：ΣT(xi;Θm)
							>如：第一次划分f1：使得 区域1上样本的残差和区域2上的样本的残差和 最小的 划分s：R1:x<=s, R2:x>s,  argmin[ argmin [Σ(yi - c1)^2] + argmin[Σ(yi - c2)^2]] , 则分别对c1,c2求导，令=0，得出 c1 = 1/N1 *Σyi    ,  c2 = 1/N2 * Σyi,  再对不同的s, 则会依次得出 不同的最小值取值，取其中最小的那个s 作为划分，而得出c1,c2 从而第一颗树就明确了，从而得模型输出 和 残差表---每个样本对应的残差；将这个残差表作为 新的 输入-输出，来计算一颗新的树T2(x;Θm), 即新的c1,c2,s, 然后 按照 s1,s2对整个区间的划分，分别将老c和新c在同子区间进行相加，得出新的输出模型f2(x),最后一直到f(M)(x)   最后的平方损失误差 会越来越小。
				>梯度提升：
					>梯度提升算法：将损失函数的负梯度的值 作为 残差的近似值。不同于上面的通过计算出c1,c2而得出 样本的残差，而是用 偏导数计算：rmi = [зL(y,f(xi))/зf(xi)]f(x)=f(m-1)(x), 而由残差表 得出 cmj 的过程 也是利用残差和最小的方式，然后构造树 而 更新f(M)(x)。
		>EM算法及其推广：期望值最大模型。用于含有隐变量 的  概率模型 参数的 极大似然估计/极大后验概率估计。
			>概率模型的变量都是观测变量：则给定数据，可以直接用极大似然估计法，或用贝叶斯估计法估计模型参数。
			>存在未观测数据：除了观测数据之外的数据。
			>观测数据的似然函数：有参数的条件下 观测数据发生的概率：P(Y|θ)， 用类似全概率的公式 来间接的计算/关联计算：则为： ΣP(Z|θ)*P(Y|Z,θ) 即对Z的所有可能取值 进行 累加计算。
				>对于没有解析解的问题：通过迭代的方法求解。
					>EM算法：先选取参数的初始解--->用初始解值 来计算 下一轮的第一个参数的值，然后初始解+已经计算出的新一轮的参数值 来计算下一个 新一轮尚未计算出来的参数值。
						>隐随机变量的取值范围：Σ 
						>每一轮真正要最大化的事件概率：Y发生的概率，Y发生任何一个值的概率(都要最大化)。因为有隐随机变量Z影响Y,所以 P(Y) 是 ΣP(Y,Z)*P(Z),或者对数化联合分布:Σlog P(Y,Z)*P(Z), 加上参数θ待定：所以 ΣP(Y,Z|θ)*P(Z|Y,θ(i))=Q(θ,θ(i)) 所以最大化 argmax Q(θ,θ(i)) = θ(i+1)
							>停止迭代的条件：θ(i+1) - θ(i) < ε 
							>推导过程：
								>极大化：L(θ) = log P(Y|θ)  , 进一步转化为 ： log P(Y|θ) = log P(Y,Z|θ)=log ΣP(Y|Z,θ)*P(Z|θ)
									>如果此时知道了 上一轮的 L(θ),即：logP(Y|θ(i)) 则自然希望 log P(Y|θ) - log P(Y|θ(i)) 这个差能最大，首先认为 这个差 = log(ΣP(Y|Z,θ(i))*P(Z|θ)*P(Y|Z,θ)/P(Y|Z,θ(i))) - logP(Y|θ(i)) , 然后利用Jensen不等式，>= ΣP(Z|Y,θ(i))*log(P(Z|θ)*P(Y|Z,θ)/P(Y|Z,θ(i))P(Y|θ(i))) 最大化L(θ) 就是在最大化这个下界值，就是在最大化 下界值 中 含有 θ 的项： 其他当作常量，(事后诸葛亮)，即为：argmax (ΣP(Z|Y,θ(i))*log(P(Y|Z,θ)*P(Z|θ))) = argmax (ΣP(Z|Y,θ(i))*log(P(Y,Z|θ))) = argmax Q(θ,θ(i))
						>可用于：生成模型的非监督学习。此时，X为观测数据，Y为未观测数据。生成模型为P(X,Y)
			>EM算法的收敛性：P(Y|θ)是观测数据的似然函数，θ(i) 是 EM算法得到的 参数估计序列, P(Y|θ(i)) 是 对应的似然函数序列。这个似然函数序列 如果是 递增的，则是正确的。事实上确实是递增的。
				>Jesson不等式很好理解：就是  凸函数 上两点切割的曲线大于曲线两端的线段：tf(x1) + (1-t)f(x2) <= f(tx1 + (1-t)x2)
					>Jesson不等式的推广：即 不止2点，而是n个点，如果 点集{xi}, λi >=0 且 Σλi = 1, 则  Σλi*f(xi) <= f(Σλi*xi)  使用数学归纳法，可以用n个不等式左右分别相加而得出这个最终的不等式。 
						>等效表达：如果将λi 看作 是  取值xi时的概率，则 不等式 解释为： E(f(x)) <= f(E(x))  对于 连续性变量，则表达为：∫f(x)p(x)dx <= f(∫x*p(x)dx)
				>递增证明：理想可观测随机变量的最终分布值： P(Y|θ) = P(Y|Z,θ) = P(Y,Z|θ)/P(Z|θ)	 进行对数化：logP(Y|θ)  <= logP(Y,Z|θ) - log P(Z|θ),  已知：ΣP(Z|Y,θ(i))*log(P(Y,Z|θ)) = Q(θ,θ(i)), 令 ΣP(Y|Z,θ(i))*log(P(Y,Z|θ)) = H(θ,θ(i)), 则 
					>Q(θ,θ(i)) - H(θ,θ(i)) = Σ (logP(Y,Z|θ) - logP(Z|Y,θ))*P(Z|Y,θ(i)) =  Σ(logP(Y|θ))*P(Z|Y,θ(i)) = logP(Y|θ) , 此时令θ=θ(i),θ(i+1) 并 相减， logP(Y|θ(i+1)) - logP(Y|θ(i)) 则可可以证明是>=0的，Q相减一定，而 H相减 = Σ(logP(Y|Z,θ(i+1))/P(Z|Y,θ(i)))*P(Z|Y,θ(i) <= log(P(ΣZ|Y,θ(i) *P(Y|Z,θ(i+1))/P(Z|Y,θ(i)))=log(ΣP(Y|Z,θ(i+1)))=log(1)=0
			>EM算法在高斯混合模型学习中的应用：
				>高斯混合模型：n次取值的 线性组合：P(y|θ)=Σαk*φ(y|θk) 其中 φ 是 高斯分布密度，θk = (μk,σk^2),  所以 φ(y|θk)= 1/(sqrt(2π)σk) * exp(-(y-μk)^2/2σ^2) 一般混合模型则替换 高斯分布密度即可。
			>EM算法的推广：解释为 F函数的极大极大算法。GEM算法。
		>隐马尔可夫模型：用于标注问题的统计学习模型。	
			>描述由隐藏的马尔可夫链随机生成观测序列的过程：属于生成模型。
			>隐马尔可夫模型：
				>定义：关于时序的概率模型。
					>隐藏的马尔可夫链：可以生成 一个 状态 构成 的 序列。称为状态序列I。
						>每个状态生成一个观测：从而 状态 序列 产生  观测序列O。
							>序列的每个位置：看作一个时刻。
					>不可观测的状态随机序列： 
				>决定参数：初始概率分布、状态转移概率分布A、观测概率分布。
					>状态集合：Q = {q1,q2,...}
					>观测集合：V = {v1,v2,...}
					>状态序列：I = {i1,i2,...}
					>观测序列：O = {o1,o2,...}
					>状态转移概率矩阵：A = [aij] 代表 当 t 时状态为qi 的条件下 转移到 t+1 时刻 状态为 qj 的概率： aij = P( i(t+1)=qj |it = qi)
					>观测概率矩阵：B = [bj(k)] 代表 当 t 时 状态为qj 的条件下 生成观测 vk的概率： bj(k)=P(ot=vk| it=qj)
					>初始状态概率向量：π=π(i) 代表 当 初始时刻即 t=1 时 状态为 qi的概率：π(i) = P(i1=qi)  ..或者叫 初始 状态 概率分布。不难得出结论：π1*A^(n-1)=π^(n)=I 即t时刻的状态分布可以计算出来。π * B^T = O,  由此可见，π,A,B可以确定 最终的状态序列 和 输出序列。称为马尔可夫模型的三要素。
				>基本假设：齐次马尔可夫性假设---任意时刻的状态只依赖上一时刻的状态；观测独立性假设--任意时刻的观测只依赖该时刻的马尔可夫链的状态；	
				>三个基本问题：
					>概率计算算法：已知 λ=(A,B,π)， 和观测序列O,计算 O出现的概率：P(O|λ)
						>直接计算法：P(I|λ) = π(i1)*a(i1,i2)*a(i2,i3)... 而，P(O|λ,I) = b(i1)(o1)*b(i2)(o2)*..., 而 I,O同时出现的概率 P(O,I|λ) = P(O|λ,I)*P(I|λ) 是一个联合概率，而 P(O|λ) = Σ P(O|λ,I)*P(I|λ) 需要知道所有可能的I  这种计算量太大。
						>前向算法：
						>前向概率：t时刻 状态为 qi  且 到此为止的观测序列为o1,o2,...ot  这件事 发生的概率 。即 P(o1,o2...ot,it=qi|λ) = α(t)(i) , 则α(1)(i)= π(i)*b(i)(o1) ,  右边第一项 读作初始时刻 状态为i的概率，第二项 读作 状态为i时观测到o1的概率， 所以出现观测序列{o1}的概率 = Σα(1)(i)。。而t时刻出现{o1...ot}这整个事件且t时刻状态为i的概率 = α(t)(i),  因此 t+1时刻 看，假设此时状态为i, 则整个过程观测到{o1,...ot+1}即整个事件发生的概率 =  α(t+1)(i) = 所有可能从t时刻转移到现在的状态*转移概率*观测到o(t+1)的概率 = (Σα(t)(j)aji)*o(t+1), 那么停止的事后，即T时刻，状态为i而观测到o1...oT的概率也可以计算出来，则观测到o1...oT的总概率 = ΣαT(i) = P(O|λ) 
								>条件概率：正确理解。就是限制范围内发生的概率，即事件B在A条件下发生的概率，对应的，在A条件下不发生的概率就是 它的被1减的差。
								>条件概率：正确理解2：条件本身发生的概率 认为是1，或者不考虑。或者认为已经发生了。条件下发生的概率 和 条件本身发生的概率， 和 所有条件构成的事件发生的概率。
						>后向概率：t时刻 状态为 qi 且 从t之后的观测序列为 ot+1,ot+2,...oT , 这件联合事件/顺序且事件 发生的概率/同时发生的概率 = β(t)(i) = P(ot+1,ot+2,...oT|it=qi,λ) ，发现β(t)(i)其实一个条件概率，即在 t时刻 且 状态为qi的条件下，观测到 ot+1...oT的概率；；；而在同样的条件下，观测到 ot的概率 则=bi(ot); 则在t-1时刻 且状态为qi的条件下，观测到ot-1的概率 = bi(ot-1) ，状态转移到j的概率 = aij, 而在j状态下观测到ot的概率=bj(ot),观测到ot+1,...oT的概率=βt(i), 所以t-1时刻且状态为qi条件下，观测到 ot,...oT的概率 β(t-1)(i) = Σaij*bj(ot)*β(t)(j),   而 β(T-1)(i)= (Σa(T-1)j*bj(oT)),  和上一个公式结合，也可以间接得出β(T)(i)=1	。。则观测到o1,o2...oT的概率 = Σπ(j)*t=1时刻观测到o1的概率*t=1时刻状态为j的条件下观测到o2,o3...oT的概率 = Σπ(j)*bj(o1)*β1(j)
								>前向后向共同表示P(O|λ) = t时刻时且状态为i条件下观测到o1...ot的概率* t时刻且状态为i条件下 观测到ot+1...oT的概率 =  Σ αt(i) * βt(i) = ΣΣ αt(i) * aij*bj(ot)*βt(j)
						>后向算法：
					>学习算法： 已知O， 计算使得 P(O|λ) (即观测序列出现概率最大,即用λ表达的观测序列出现的概率的极大值的λ)最大时的 λ
					>预测算法：已知λ，O, 求 它们 能够确定的最大的 P(I|O) 是多大，此时I是什么。
				--应用：语音识别、自然语言处理、生物信息、模式识别。
			




									
>统计和概率：统计  由数据推 概率，概率则 由概率 推 取值。	
https://github.com/ShangtongZhang/reinforcement-learning-an-introduction	
https://www.jianshu.com/p/9c153d82ba2d  贝叶斯估计。
https://blog.csdn.net/guoyunlei/article/details/77427870 拉格朗日乘子法重要文献。
https://blog.csdn.net/weixin_28880179/article/details/112368473 希尔伯特空间