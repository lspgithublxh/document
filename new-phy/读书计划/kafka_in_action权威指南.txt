---------读书战略：自顶向下，从应用层到底层细节。先熟悉的常用的，后底层的不常用的。精读好书---《深入理解系列》
--------es的处理思想都对应有 相应的“某种举动 及其导致 的问题”
>需求背景-痛点陈述：现实开发中遇到了什么痛点导致XXX的开发？  
>那么痛点的原因、产生的条件：分析是什么环境、什么动作 什么效果什么目的 等精准形容词的事实描述 导致了痛点的必然发生，难以避免？。条件-痛点。原因-结果。
>则可以消除痛点的最简单的精准事实陈述是什么：修改“条件-痛点”陈述的哪一部分就可以导致没有痛点产生。 要做的根本事情是什么：
>那么新陈述表明的解决动作是什么：
>新情况的新痛点是什么：新举动、新环境会产生的新痛点描述出来， 继续上述4条操作。

>举动-痛点：管理和处理流式数据，帮助处理持续数据流的插件。(而不是消息系统或者日志聚合系统或者ETL工具，不是一个数据存储系统；是一个把数据看作持续变化和增长的流的数据系统)
	>解法/处理思想：流平台。在这个平台上可以发布数据流和订阅数据流。并把它们保存起来、进行处理。
	  >不同于消息系统如AcitiveMQ/RabbitMQ: 集群方式运行。可以存储数据，保存多久都可以。可复制、持久化。消息系统只是传递消息, kafka流式处理能力却会处理派生流和数据集。
	  >不同于Hadoop: 实时版的Hadoop, hadoop存储和定期处理大量的数据文件，而kafka可以存储和持续处理大型的数据流。Hadoop大数据用在数据分析上，而kafka低延迟的特性更适合核心业务。
	  >不同于ETL: 都擅长移动数据，但kafka是以数据流为中心的架构。面向实时数据流的平台。基于持续数据流构建应用，而不是基于请求-响应构建应用。
      >使用场景：用于事件驱动的微服务系统的消息总线、流式应用和大规模数据管道。
	  >kafka的设计原则和可靠性保证:
	  >kafka的复制协议、控制器和存储层：
	  >设计决策和权衡背后的缘由：
	  >kafka提供的特性：数据按照一定顺序持久化保存的。具备数据故障保护和性能收缩能力。
	   >分布式提交日志：提供所有事务的提交记录，通过重放这些日志可以重建系统的状态。
	   >分布式流平台：
	  >发布与订阅系统：通过broker解耦了信息的发布者和订阅者。但是消息类型固定，新的类型消息需要新的消息系统实例，规模随着业务扩展或许也有困难。如果有一个单一的通用消息处理的集中式系统，并且可扩展规模，则可以了。
	  >kafka基本概念：
	   >消息：kafka的数据单元，字节数组。消息的元数据--键--也是一个字节数组。消息以可控的方式写入分区，写到哪个分区需要用到键：键---散列值---分区数取模。
	   >批次：为提高效率，消息被分批次写入kafka, 一个批次一组消息，属于同一个主题和分区。批次的数据数量需要指定，且数据可以压缩。
	   >模式：消息的表现形式/格式：json/xml。但是缺乏强类型处理能力。Apache Avro是一个序列化框架，为Hadoop开发的。且版本之间的兼容性好，向前兼容和向后兼容。
	   >分区：一个主题分为若干个分区。一个分区就是一个提交日志。消息以追加的方式写入分区，然后以先入先出的方式读取；分区之间消息没有先后关系，分区内有。分区可以实现数据冗余和伸缩性。分区可以在不同的服务器上。
	   >主题：消息通过主题而分类。
	   >kafka客户端：生产者和消费者。另外：高级客户端API:数据集成的kafka connect api 和用于流式处理的kafka streams
	    >生产者：
		 >消息键-分区器：可以让消息分发到指定的分区上，实现有序消息。将消息键散列，得出一个值映射到指定的分区。
	    >消费者：订阅一个或者多个主题。
		 >消息的偏移量：消费者检查，从而区分已经读取过的消息。是一个不断递增的整数值，在消息里，kafka创建消息时添加的。消费者需要把读取某个分区的每个偏移量都覆盖式的放到zk上，从而消费者崩溃后可以获取到上次读取消息到的偏移位置---也可以保存在kafka上。
	     >消费者群组：多个消费者同时消费一个主题。群组保证一个分区只被一个消费者读取。一个消费者可以同时消费多个分区。消费者对分区的所有权关系。且一个消费者失效，群组里的其他消费者将接管分区继续消费。
	   >broker: 一个独立的kafka服务器。对于提交的消息：接收消息、设置消息偏移量、提交保存到磁盘。对于读取分区的请求：返回已经提交到磁盘的消息。一个broker可以处理数千个分区以及每秒百万级的消息量。
	   >集群：一个集群中有一个broker充当集群控制器角色---自动从集群的活跃节点中选举出来。控制器：分配分区给broker, 监控broker.一个分区从属于一个broker---被称为分区的首领。
	    >分区复制：一个分区从一个broker复制到另一个broker.。当然会产生其他联动，传导。
	   >保留消息：消息保留策略：保留一段时间、一定量。过期清除。精确到主题级别。
	    >主题当作紧凑型日志：只有最后一个带有特定键的消息会被保留下来。
	   >消息复制机制：只能在单个集群里进行。
	   >MirrorMaker工具:实现消息的集群之间的复制。就是从一个集群消费而为另一个集群生产。 
       >复制系数：关系到broker的宕机而消息可见、集群可用。
	   >性能：亚秒级
	   >使用场景：前端用户信息传递到消息队列，拉取更新到推荐系统。
	   >传递消息：不用直接依赖公共组件，而关注它要求的数据格式---而在各个系统上开发同样的功能。甚至发送批量消息构成的通知，然后公共组件接收这些消息来处理。
	   >度量指标和日志记录：比如cpu/mem使用量发送到消息系统，监控系统读取实时展示，或者统计分析。
	   >提交日志：数据库的更新发布到kafka上，来记录数据库数据的变化，或许比直接让数据库去预写日志好些/快些。紧凑型日志主题，每个键只有一个值，那么当作键值对 来看待。
	   >流处理：流式处理框架，操作kafka消息，计算度量指标，为其他应用程序有效地处理消息分区。
	   >kafka设计目标：推拉模型解耦消费者和生产者；数据持久化以支持多个消费者；系统优化如Avro序列化框架实现高吞吐量；系统可以随着数据流的增长而横向扩展。
	  
举动-痛点-目标：安装运行，使用，配置，优化
	>解法/处理思想：kafka是java开发的。
	 >broker和主题的元数据的保存/消费者元数据消息偏移量的保存：在zookeeper上；。自带，也可以独立安装。telnet localhost 2181 验证安装；srvr也可以。
	  >zk集群/群组：zk使用一致性协议，建议包含奇数个节点。大多数可用，zk才能处理外部请求。不建议节点个数超过7个，这会因为一致性协议而降低zk性能。节点通信tcp端口2888和leader选举tcp端口不一样3888。
	 >创建主题和发送消息：可以使用sh脚本执行。kafka-topic.sh 创建主题， kafka-console-producer.sh发布消息。 kafka-console-consumer.sh接受消息。 
	 >基本配置：ip，port, zookeeper.connect:可以配置群组；zk路径chroot;log.dirs存放日志片段的目录-------kafka消息保存在磁盘上；可以多个目录，这样同一个分区下的日志片段放在一个目录下；
	  >日志片段的处理：多线程。如服务器启动、关闭、重启；num.recovery.threads.per.data.dir
	  >自动创建主题：auto.create.topic.enable 生产者开始往主题写消息，消费者开始读，客户端开始发送元数据。。num.partitions新建主题的分区数(默认)--自然的这个参数的选择：消费者读取速度和生产者吞吐量，可用磁盘和带宽。单个broker的分区数限制：否则首领的选举需要的时间越长。数据保留时间log.retention.ms/hours/minutes	检查每个日志片段文件的最后修改时间来看是否过期的。log.retention.bytes设置的是一个分区的最大容量；超过则日志片段文件多余部分删除。
	  >日志片段文件：log.segment.bytes指定一个日志片段文件的大小；超过则关闭日志文件而新开一个日志记录文件；一个分区下有若干个。
	  >消息压缩后的大小：message.max.bytes 
	 >硬件选择：因为生产者要等待服务端确认消息已经提交成功。磁盘吞吐量：串联存储技术/磁盘阵列等会影响。磁盘容量：决定消息量。(每天的消息量*存储时间)。。内存：页面缓存---消费者读取时如果跟在生产者后面则直接从内存中读取。网络：主要是出口流量，如果在集群分区复制的时候饱和，则延迟在所难免。
	   >cpu的使用：在批量解压消息，设置偏移量，再批量压缩，后保存到磁盘上。
	 >kafka集群：一个主题的分区不能在一个broker上，也不能在一台机器/节点上；实际上一个broker管理一个分区；复制系数决定一个分区还有多少个复制分区。
	 >操作系统调优：来提高kafka性能。虚拟内存、网络子系统、存储日志片段的磁盘挂载点方面。
	  >避免内存交换：内存交换可以避免因为内存不足而突然终止进程；所以要设置交换分区，但设置得小一点。vm.swappiness=1(0表示任何情况下都不发生数据交换)
	 >内核进程将脏页刷新到磁盘：减少脏页的数量：可以配置vm.dirty_background_ratio系统内存百分比；vm.dirty_ratio增加脏页的数量；60-80避免因系统崩溃而造成数据丢失。
	   >查看当前脏页的数量：cat /proc/vmstat | egrep 'dirty|writeback'。。避免频繁的页面刷新。
	 >文件系统：XFS。。配置时禁用掉文件最后访问时间atime.即配置noatime但不禁止创建时间和最后修改时间。
	 >linux网络栈调优：以实现对大流量的支持。配置读写socket缓冲区：net.core.wmem_default 和 net.core.rmem_default  128KB最大是2MB. tcp socket读写缓冲区也可以配置：net.ipv4.tcp_wmem和net.ipv4.tcp_rmem.... net.ipv4.tcp_window_scaling=1   net.ipv4.tcp_max_syn_backlog>1024  net.core.netdev_max_backlog>1000
	 >G1垃圾回收器的关键特征是：不会对整个堆空间回收，而是对region垃圾量排序而回收多的那一部分， 多少由回收频率和回收量而导致的暂停时间有多少。第二是，启动新一轮垃圾回收的条件：堆内存使用率达到设定值InitialingHeapOccupancyPercent:45----包含新生代和老年代。
	  >kafka: 设定最大暂停时间20ms, 35%启动新一轮；5G堆内存。通过KAFKA_JVM_PERFORMANCE_OPTS环境变量来修改。同时设定-XX:+DisableExplicitGC  -Djava.awt.headless=true
	 >kafka在数据中心的物理位置：与kafka集群的复制功能。 
	  >同一个机架上的服务器使用相同的电源和网络连接：使得如果出故障/或者定期维护(移动机器/重新连接电源)则都不可用，所以一个分区的副本不应该在同一个机架上，即集群的broker在不同的机架上。	
     >zookeeper的作用：kafka使用来保存broker\topic\partition的元数据。多个Kafka共享一个zk,但使用不同的chroot路径。最新的kafka， 消费者将分区消息的偏移量提交到kafka而不是zk。但是zk不建议给其他应用共享使用。

举动-痛点-目标：生产者和消费者
	>解法/处理思想：
	  >使用场景：一个程序提交事务到kafka, 另一个程序消费事务并且响应而发送消息到kafka, 第三个程序接收消息而更新事务状态。
	  >kafka二进制连接协议：第三方语言客户端连接到kafka.
	  >不丢失、低延迟、高吞吐量的场景要求：
	  >生产者：序列化器为字节数组 发送到网络上。分区器：生产者指定的则不变，否则根据对象的键选择一个。这条记录被添加到某个主题某个分区的一个批次里，有一个独立的线程负责将这个批次里的所有消息发送给对应的broker。服务器返回主题/分区/偏移量，失败则返回错误。生成者可以重复几次。
	   >配置：bootstrap.servers: 至少两个kafka broker的地址。。。key.serializer序列化器如ByteArraySerilizer但是要实现指定的接口。value.serializer类似。
	   >发送方式：发送就遗忘，同步发送，异步-回调发送；发送消息之前可能发生：序列化异常、批次消息缓冲区满、发送线程被中断；
	   >acks: 指定有多少个分区副本都受到消息了才算成功写入。acks=1则首领节点收到则返回成功。无首领则返回生产者错误而生产者重试。acks=all延迟更高，但更可靠。
	   >buffer.memory: 生产者内存缓冲区大小。
	   >compression.type: 消息发送给broker之前的压缩方式：snappy, gzip, lz4..使用压缩可以减少网络传输开销和存储开销。
	   >retries: 生产者重试发送消息的次数；重试等待时间默认100ms, 设置retry.backoff.ms  。。需要处理不可重试的错误类型和重试次数过多的情形，重试本身交给kafka客户端。
	   >batch.size ： 一个批次的内存大小；
	   >linger.ms: 一个消息后一个批次的等待时间；到了直接就发送批次内的消息；
	   >client.id: 标志使用。
	   >max.in.flight.requests.per.connenction: 一个消息的响应之前允许再次发送的消息数；可以保证顺序发送。
	   >各种等待时间：生产者等待broker响应，broker等待同步副本返回消息；如生产者获取目标分区的首领----这个元数据。
	   >max.request.size: 生产者请求的大小
	   >received/send.buffer.size:  tcp socket发送接收数据包的缓冲区大小；跨数据中心的网络高延迟和低带宽；。。重试批次对批次消息的顺序有影响。
	  >自定义序列化器：Avro, thrift, protobuf, json .。可以仅仅只是输入obj,输出byte[]
	   >Avro: schema 是一个json数组---像mysql建表语句一样指定各个字段类型/名称；。schema保存在注册表里，如confluent平台；schema从而就有一个注册和拉取的过程，来完成序列化和反序列化。
		>使用：只需要生产者配置参数里schema.registry.url配置。。也可以直接提供一个scheme字符串。
	  >分区器：不使用键，则轮询算法发送给各个分区。键散列值，对主题的总的分区数进行取模得值，
		>自定义分区器：实现Partitioner
	  >消费者： 一个消费者群组：订阅一个主题；每个消费者接收主题的一部分分布，互相不同；。增加消费者则分出流量给新的消费者，直到一个分区一个消费者，再增加消费者只会闲置。
	    >同一个主题：多个消费者群组，互相之间不影响，都能读取到主题的所有分区的所有消息。
		>再分配: 分区所有权从一个消费者转移到另一个消费者。添加或者删除消费者时发生。
		>群组协调器：指定的broer。保存了消费者和群组的从属关系，消费者和分区的所有权关系。消费者有一个提交偏移量的动作，和轮询消息(读取分区消息)的动作，此时消费者还会发送心跳给协调器。协调器才会认为消费者还活着。如果心跳时间间隔太长就会被认为死亡而对分区重新分配--再均衡。
		>活锁：消费者没死只是没有正常运行而被认为死亡而触发相关动作如再均衡；所以在消费者离开群组并触发再均衡之前可以有多少时间不进行消息轮询。
		>群主：第一个消费者。负责从群组协调器获取消费者列表，并且使用分配策略类将分区和消费者重新 映射，然后返回给群组协调器，协调器再将各个消费者的分配信息专门单独发送给相应的消费者。
		>配置：bootstrap.servers , key.deserializer , value.deserializer定义类似，仅仅是反序列化。订阅主题：.subscrible(Collections.singletonList("test.*"))正则表达式匹配多个主题--比如新增一个主题也可以---只要匹配到就可以接受到消息。
		 >group.id: 属于哪个消费者群组。
		 >消费者轮询工作：群组协调、分区再均衡、发送心跳和获取数据:都是在.poll(100)方法期间完成的/阻塞100ms最多。轮询方式获取数据：即Pull拉模式。一次pull拉取若干个结果--结果里自然包含主题、分区、偏移量、键、值。
		 >fetch.min.size: 消费者从服务器获取记录的最小字节数。服务器在收到请求时如果可用数据量小于这个值，就会等到足够时才返回给消费者。
		 >fetch.max.wait.ms: broker最大等待时间，如果时间到了可用数据仍然不足也会发送给消费者。
		 >max.partition.fetch.size: 每个分区发送给消费者的最大字节数，默认1MB。消费者就需要接收，因此缓冲区需要>接收的分区数*1MB, 同时避免再分配时接收的分区数增加还需要有缓冲空间。往往是一个消息的大小；所以说要比max.message.size大，否则接收不了消息。
		 >session.timeout.ms: 1s 默认，即消费者被认为死亡之前与群组协调器服务器断开连接的时间，即这个时间内必须要发送心跳，没有就会被认为死亡，而再均衡。检测节点崩溃的时间。
		 >hearbeat.interval.ms: 1/3s默认，即消费者向服务器发送心跳的频率。一般为上述时间的三分之一。
		 >auto.offset.reset: 消费者重启之后读取没有偏移量的分区时，默认从读取启动时间后生成的消息开始，latest, 也可以earliest则从起始位置读取。
		 >enable.auto.commit: 是否自动提交偏移量，当然false, 用户自己处理完毕再提交,保证避免重复数据和数据丢失。
		 >partition.assignment.strategy: 分区分配给消费者的策略。Range: 多个主题下，每个主题的分区A都先分给第一个消费者，然后往后所有主题的分区B都给第二个消费者；第二种策略是：RoundRoubin: 是将第一个主题的前n个分配给第一个消费者，然后再看后面的分区给其他消费者。
		 >client.id: 消费者客户端id
		 >max.poll.records: 返回的记录数量--单次。
		 >recived/send.buffer.size: 读写时的TCP缓冲区，跨数据中心时要设置大些。
		>因为每个消费者群组有群组的主题的多个分区偏移量记录：这个记录由消费者更新，更新的动作被称为提交。提交就是在_consumer_offset主题上发送消息；消息里就包含各个分区的偏移量。消费者崩溃或者重新加入而需要读取相应分区的偏移量，如果偏移量比上次的小，就会重复消息；如果比上次的大，就会丢失消息。
		 >auto.commit.ms: 自动提交，默认5s一次；将poll()方法收到的最大的偏移量提交上去，也是在轮询的时候完成的。手动提交：调用commitSync()完成，提交的是poll()之后最新的偏移量。异步提交commitAsync()。。混合提交。和在处理记录过程中提交也可以。
		>消费者再均衡监听器：ConsumerRebalanceListener。两个时间：停止读取消息，失去分区所有权即再均衡之前； 均衡匹配关系做好之后，但还没开始允许读取消息；两个时间事件期间可以做些工作。比如提交。
	     >seek()方法，将偏移量提交到数据库，和记录一起提交；在poll一次加入群组，再seek()获取到分区的偏移量，后面再poll()拉取记录。
		 >安全退出：wakeup()方法调用。则抛出一个WakeupException, 同时关闭close()消费者，提交该提交的。
		>独立消费者：无群组的消费者。如只需读取一个主题的所有分区或者固定某些分区。消费者同样也读取消息并且提交偏移量。此时不能订阅主题---意味着加入群组，而只是为自己分配分区。consumer.partitionsFor("topic")返回一个主题的所有分区列表。consumer.assign(设置后的分区列表)
	  
举动-痛点-目标：针对性深入调优 处理复制：处理来自消费者、生产者的请求：存储细节：
	>解法/处理思想：kafka内部工作原理
	 >集群成员关系：broker启动时在zk上创建临时节点，将自己的节点id注册到zk。订阅了zk的/brokers/ids/路径。。一个broker一个唯一的id, 但是它崩溃后可以被其他新broker使用来启动。
	 >控制器：集群第一个启动的broker在zk创建临时节点/controller让自己成为控制器，复制首领的选举。(每个broker启动时都会尝试创建，显然只有第一个会成功；但是没获取到的它们会watch节点，等待第一个失效后自己再次尝试创建；这是非公平方式；公平方式就是有序临时节点) 
	  >控制器发现broker离开集群：则这个broker管理的分区，即这些分区需要一个新的首领，控制器遍历这些分区的副本列表，并选择第一个副本从属的broker为分区新的首领/(新的主分区)，其他副本分区还是跟随者。并通知这些broker, 使得新首领开始处理消费者和生产者的请求，而跟随者开始从新首领那里复制消息。新加入的broker，控制器会查看该broker id 下是否有现有分区的副本，有则开始从首领复制消息，同时把变更通知发送给其他broker
	 >复制： 每个broker可以保存成百上千个不同主题和分区的副本。分区：首领分区和跟随者分区。即主分区和副本分区。读写请求只到首领分区，跟随者分区从首领分区复制消息----这种请求和消费者为读取消息而发送的请求是一样的---即会返回消息的偏移量。首领可以查看每个跟随者的消息复制偏移量。如果10s没有请求消息，则认为是不同步的，则首领崩溃后不能被选举为新首领。相反，持续请求最新消息副本被称为同步的副本，则它broker可以被选举为新的首领。  
	  >首选首领：创建分区时的首领选举有用。副本清单里的第一个副本broker就是首选首领。  
	 >请求处理：先到先处理。基于tcp协议。标准消息头：Request type 请求类型ApI key, request version请求版本, correlation id标志请求消息，Client ID标志发送请求的客户端。
	  >broker在每个服务端口上：创建Accepter线程，在服务端口监听请求(NIO方式)，来一个请求则交给Proccesser线程，此线程将请求封装发送到请求队列，IO线程组会读取请求队列的请求而处理，后将结果封装发送到响应队列；Proccesser线程会从相应队列里读取结果数据返回给客户端。
	   >请求类型：生产者请求：生产者发送的请求，包括客户端要发送给broker的消息。
					>acks： 消息发送成功的标准；安全级别：发送就成功/首领写入成功/全部副本分区都写入成功(炼狱缓冲区) 才返回客户端成功(当然处理前，有验证：如topic是否有写入权限)。只有0,1,all取值。但是linux上写入，仅仅写入到文件系统的缓存里，不保证何时写入磁盘；kafka依靠复制来保证消息的持久化和可靠。
				  读取请求：消费者和跟随者副本需要读取消息时发送过来的请求。且都必须发送给首领分区来处理，非首领分区不会处理生产者请求或者读取请求---且直接返回错误：：即要求分区首领在这个broker上。
					>请求验证：也会先验证消息偏移量是否在分区有效。
					>broker发送数据给消费者: 零复制技术，直接从linux文件缓存发送到网络通道。。一个用户和内核都可以访问的区域，来避免字节复制，和用户态的切换。
					>broker累积数据后发送：
					>同步副本：复制消息之后消费者才可以读。没有足够多副本的消息是不安全的。
				  元数据请求：消费者发送给任意broker的关于分区信息的请求，刷新频率：metadata.max.age.ms
	 >单个分区的大小：受挂载点可用空间的限制：一个挂载点由一个磁盘或者多个磁盘组成。JBOD单个磁盘，RAID多个磁盘。
	   >存储分区的目录：log.dirs:会包含多个挂载点的目录。
	   >数据分配和管理保留：一个主题多个分区，每个分区的首领分区和副本分区需要在不同的broker上。分区系数决定了一个分区的首副总数。broker有不同的机架，则副本应该放到不同的机架上的broker上。一个broker可以有多个分区；按照约定，则这些分区都是不同的分区，没有同一个分区的多个副本---即一个分区的一个。
		 >分配分区的轮询方法：broker都在一个机架上。则先随机选择一个broker0, 然后以这个broker为起点，开始顺序依次分配各个分区的首领分区给各个broker---分区多则会循环到broker。首领分区分配完毕，开始第一个副本分区，则从broker1开始依次分配给各个broker各个分区副本，同理第2个副本分区，第3个，第n个。
							  broker在多个机架上，则先构造一个将broker排序成环的数据结构：规则是：依次从各个机架上取一个broker,取过的不取，取完一轮接着一轮，直到取完，然后再用同样的方式分配给各个broker各个分区。总结：先生成broker环，然后按照纵向顺序分配分区矩阵。
	     >为分区分配目录：分配规则：总是分配在目录下分区数最小的哪个目录下。基本上：一个磁盘一个目录，新加一个磁盘就新增一个目录，所以分区一般会分配到新增的目录下。
		 >文件管理：保留数据：为每个主题设定了保存期限和保存容量两个限制，其一满足就会清除过期的数据。
		  >一个分区的数据目录下：多个片段数据文件，写满一个则关闭而打开一个新的片段文件。一个片段1GB或者一周的数据。当前正在写的片段叫做活跃片段，不会被删除。保留7天，每天生产一个新片段文件，那么每天都会删除7天之前的数据。
		    >broker会为每个分区里的每个片段文件打开一个文件句柄：
	   >文件和索引格式：kafka对生产者发送过来的消息里包含键、值，设置偏移量，其他格式不变，保存在磁盘里，发送给消费者也是同样的格式。生产者可以发送一个批次的消息--包装了多个消息的一个包装消息。	可以使用DumplogSegment工具查看片段文件的内容：展示消息的偏移量、校验和、消息大小、压缩算法等。
	    >一个分区一个索引：索引建立了 从分区偏移量 映射到片段文件+文件中的位置的关系。索引也被分成片段，所以删除文件时可能会连带删除索引片段。索引可以全部删除，kafka会自动读取文件片段而生成一个索引。
	    >清理：log.cleaner.enabled=true每个broker会启动一个清理管理器线程和多个清理线程，选择污浊率较高的分区来清理。
	     >清理的内容：如果一个键有多个值：那么是污浊的。一个键只有一个值，是干净的。污浊部分，就是旧片段，等待被删除的片段---所以需要保留新片段里没有的键，新片段里有的自然就需要删除了。
		 >删除过程：将分区里污浊部分的消息重新映射到内存里的一个map里，key是消息的键的hash, val是消息中的偏移量；重复的键则可以重用key。。这部分形成的map是一个filter map ，然后从片段干净处读取消息，从最旧的开始，不在这个map里的消息才会保留，否则忽略，保留的放到一个替换片段中，完毕后替换片段代替原始片段。
	    >删除一个键的所有消息：发送一个key-null的消息，那么在清理过程中只保留了这个key的这个消息，消费者看到后可以做相关处理。最后也会被kafka从分区里清理掉。
	   >日志压缩工作原理：kafka长时间保存数据。了解内部原理，直接看源码。

举动-痛点-目标：可靠数据传输
	>解法/处理思想：
	  >可靠性保证：ACID:关系型数据库支持的标准可靠性保证。
       >kafka的保证：分区消息的顺序：先写入先读取。已提交的保证：当消息写入分区的所有同步副本才成功。消息不丢失保证：只要还有一个副本活跃，那么已经提交的消息就不会丢失。读取保证：消费者只能读取到已经提交的数据/消息。
		>权衡：消息存储的可靠性、一致性，可用性、高吞吐量、低延迟、硬件成本。
		>同步副本：跟随者副本中的一部分是，首领副本是。跟随者副本需要在10s内从首领获取过消息等条件才是同步副本。垃圾回收停顿可能造成broker和zk之间断开连接。
		>复制系数：replication.factor =N 当有N-1个broker失效的情况下，仍然可以从主题读取消息或向主题写入数据---即主题是可用的。N大可靠性、可用性更大；但空间占用更大、消息提交保存更耗时。3个更安全，5个常用。
		 >配置broker的机架：broker.rack 获取更高的可用性。(长时间可用，不可用时间短)
		>完全首领选举：当首领副本不可用时，一个同步副本会被选举为首领节点，如果选举过程中消息都被同步副本收到了，则这个选举就是完全的。
		>不完全首领选举：分区中会出现非同步副本的情形：因为网络原因复制落后延迟，或者连接故障，一旦变得不同步，而首领副本突然崩溃，那么非同步副本就是分区唯一的存活副本。此时两难选择：
		 >如果非同步副本不被提升为首领副本：那么在首领副本恢复之前，分区是不可用的。如果允许提升为首领副本，那么则丢失了延迟的那部分发送到旧首领的数据，导致数据不一致---之前用户读到了现在的用户则读不到，数据丢失了。
		  >unclean.leader.election.enable=false就等待旧副本恢复。如果对数据一致性要求较高的，则禁用，如银行。可以一段时间不可用，但是不能冒险处理错误的数据。
		  >min.insync.replicas: 最小同步副本数。主题的同步副本数：向一个主题写数据，会先写到同步副本，才开始往分区写数据。如果最小同步副本数的副本都不可用了，那么会拒绝生产者写入数据，变为只读的，以避免不完全选举，会向生产者返回NotEnoughReplicasException
		 >可靠性级别：配置合理。因为判断副本同步不同步也需要时间。所以acks设置all则可靠性高。同时生产者要正确处理首领正在选举中的消息重试发送的问题。acks=1有首领收到数据返回成功后立刻崩溃的消息丢失问题。
		>可重试错误：首领正在选举中leader_not_available。而invalid_config就是不可重试错误。
		 >消息重复：kafka不保证一个消息只被保存一次。因为网络故障，响应的消息确认可能没被收到。
		>消费者：提交一个已经处理好的分区消息偏移量位置。
	    >消费者的可靠性配置：首先读取的消息都是被写入了所有的同步副本中了的。
		 >group.id：所在群组。
		 >auto.offset.reset: 消费者没有偏移量可提交，比如首次启动或者偏移量不存在。那么earliest或者latest配置可以允许从分区最开始或者最近读取消息。
		 >enable.auto.commit: 是否自动提交。zuto.commit.interval.ms自动提交间隔。
		 ---收到后提交：还是处理完再提交，还是边处理边提交。
		 ---释放旧分区之前提交偏移量，同时新分区消息接收前清理掉状态。消费者自己专门再发送消息到另一个主题上---补偿处理。客户端一般不暂停，否则心跳可能终止。--被释放分区而重新分配。
	     >幂等性写入：新同键覆盖旧同键的值。则化 至少一次 为刚好一次。
	   >验证系统可靠性：
		>配置验证：VerifiableProducer 和 VerifiableConsumer
		>应用程序验证：
		>生产环境的应用程序监控：error-rate, retry-rate, consumer-lag//Confluent
	  
举动-痛点-目标：构建数据管道
	>解法/处理思想：大型缓冲区，解耦生产者和消费者。聚合区。
		>几近实时的数据管道：基于小时的批处理。消费者可以即时获取到，也可以定期获取积压的数据。
	    >至少一次传递：kafka本身就支持至少一次传递。基于此可以实现只传输一次。
		>高吞吐量和突发吞吐量的处理：轻松每秒几百兆。
	    >多言的数据格式的支持：schema支持变动。
	   >数据管道的构建：ETL/ELT 。。kafka可以替代。是数据系统的摄入工具。
	    >ETL: 提取-转换-加载：
	    >ELT: 提取-加载-转换：高保真。数据源---->数据管道/数据湖--->数据池。数据管道/数据湖架构。
	   >安全性：数据管道支持认证SASL和授权，加密传输数据。未加密的不会存到数据池里。
	   >故障处理能力：把缺损的数据挡在管道之外。
	   >耦合性和灵活性：
	    >临时数据管道：固定了端点的工具。比如Logstach只能向es导入数据，Flume只能向HDFS导入数据。。对端点固定要求就提高了耦合性。
	    >无scheme或者不允许变更：则也提高耦合性：需要额外的信息来解析数据。且双方开发人员需要同步升级才能完成shema变更的数据传递。如从Oracle--->HDFS
	  >Connect API和连接器：非生产者-消费者都是用户开发的模式。而Connect直接连接到数据库。如Oracle。也可以从数据库获取数据。
	    >配置连接器：配置管理、偏移量存储、并行处理、错误处理。多种数据类型和标准REST管理API
		 >连接器插件：Connect可以运行插件。插件负责移动数据。
	    >以worker进程集群方式运行：worker安装连接器插件，移动数据。数据池的连接器，数据源的连接器。
	   >运行：部分机器上启动broker,其他启动connect . connect-distributed.sh
		>connect进程配置参数：bootstrap.servers: 集群中的3个至少broker .. group.id 相同的worker属于同一个connect集群。key.converter  value.converter消息的键和值所使用的转换器。默认JSONConverter, 或者AvroConverter  消息可以包含schema
		 >集群有连接器和任务：运行在任意的worker上。
	     >REST API查看 8083端口的连接器信息。
		 >文件连接器：
		 >数据库连接器：将mysql中的表数据--->kafka主题--->加载到es上。然后对内容进行索引。
		  >需要两个：kafka-connect-elasticsearch和 kafka-connenct-jdbc 打包成两个jar放到相应的Connect目录下。。然后启动查看安装的两个连接器插件。额外的，还需要mysql的jdbc实现jar，放到同样的libs下。
		   >curl方式配置jdbc连接器。如数据源url之类。table.whitelist 和  topic.prefix=mysql. 之类。那么会直接将mysql数据库下的表映射为kafka上的一个主题，只要向mysql插入数据，那么数据就会立即出现在kafka的同名mysql.同表名的主题上。那么就可以读取了。
		   >启动es连接器：配置参数里指定topics, 每个kafka主题对应一个es索引， 主题名字和索引名字同。指定索引的type.name类别名。。。之后mysql里插入的数据会自动被读取到kafka主题里和es索引里。
	     >连接器和转换器的实现：转换器比如将Mysql数据转换为JSON数据给es连接器。
	       >jdbc连接器连接到mysql: 会先分割数据复制任务，生成一个个配置任务，woker进程则执行任务。轮询的方式获取mysql里的记录，并保存到相应的主题上。
	       >开发从kafka读取数据插入数据库只需要一两天：但是处理好配置、异常、RESTAPI 、监控、部署、伸缩、失效等问题，可能需要好几个月。所以要使用连接器。
	     >连接器就像是：插件方式存在的生产者和消费者。
	
>举动-痛点-目标：跨集群数据镜像
	>解法/处理思想：kafka失效备援	
	 >镜像：跨集群的数据复制 的概括称呼。
		>使用场景：跨数据中心访问数据。如各个城市的数据中心数据发送到总部数据中心。集群冗余。
		>特点：高延迟、有限的带宽。
		>架构：Hub, Spoke 中央集群模式。首领-跟随模式。
		>双活架构：读写都允许。一个失活，则直接网络重定向就可以。最具伸缩性、弹性、灵活性和成本优势的解决方案---amazon使用。
		>主备架构：为了冗余。达到灾备的目的。但资源浪费，毕竟仅仅用集群来等待灾难发生。SRE网站可靠性工程。主备之间允许有几千个消息延迟。kafka不支持事务，所以可能灾难发生时丢失数据。	补救措施：偏移量重置。但主集群和备集群无法保留偏移量。
	    >延展集群：来自不同的数据中心的broker构成一个集群。
		>MirrorMaker: 是一个内部多个消费者-一个生产者的模式，从一个集群读取数据通过生产者发送给另一个集群。nohup --deamon运行。输出重定向到一个日志文件。运行在Docker容器里。
		>自动化的部署和配置管理：Ansible, Puppet, Chef, Salt
		>linux网络调优：增加tcp缓冲区大小，启用时间窗口伸缩，减少tcp慢启动时间
		>Confluent 将Replicator实现为源连接器的形式：
		
>举动-痛点-目标：管理kafka
	>解法/处理思想：
	   >管理集群的变更：提供的命令行工具，如主题变更操作。
	    >主题操作：kafka-topics.sh 查看、创建、删除集群里的主题。主题创建时的命名不适合下划线开头--因为内部主题使用，也不适合有.号。
		>增加分区：对于基于键的主题来说，不适合；会改变键到分区的映射。
		>查看不同步的副本分区、无首领的副本分区：under-replicated-partitions    unavailable-partitions
	   >动态配置变更：
	   >启动副本选举：
	   >分区重分配：kafka-reassign-partitions.sh  手动指定某些主题迁移到某些broker上。
	   >副本验证：kafka-replica-verfication.sh
	   >消费和生产：kafka-console-consumer.sh kafka-console-producer.sh像生产者或者消费者一样使用。
	   >控制器：一个kafka集群有一个，是一个运行在某个broker上的进程。负责管理集群的操作。注册在zk的/controller上。控制器选举的手动触发：删除zk上的这个节点。
	    >分区重分配：发起重分配，控制器将分区添加到broker上，新的broker开始复制分区到各个跟随着上，直到副本个数达到同步副本个数为止。分区副本清单里，记录了分区和broker的从属关系。
		 >可以移除一个进行中的分区重分配任务：删除/admin/reassign-partitions节点，重新选举控制器。

>举动-痛点-目标：监控kafka
	>解法/处理思想：监控和告警的最佳实践
	   >关键度量指标：
        >获取：Nagios XI check_jmx插件连接JMX接口，或者MX4J代理--而通过HTTP接口来访问度量指标。zk上的/brokers/ids/id包含了broker信息。
		>指标：
		 >非同步分区数量度量指标：最不可缺少：broker崩溃或者资源的过度消耗/java线程中断。
		 >容量瓶颈：CPU, 磁盘IO，网络吞吐量，。磁盘容量不在，因为此时broker会直接崩溃。智能管理平台接口IPMI,。查看内核缓冲区日志：dmesg: 内存坏点、磁盘故障。SMART监控工具监控和测试磁盘。
		  >配置管理工具：Chef Puppet。
		  >broker本身不能移动，但是可以移动里面的分区。分区重分配的时候。
		 >主题流入字节数：对每个broker都有一个这样指标。决定是否要对分区进行再均衡。
		 >broker上的首领分区数量：如果不均衡，则需要副本首领选举。来均衡首领分布。
		 >离线分区数：百分位指标：样本里x%的数据都小于阈值m, 那么m就是x%的百分位指标。
		 >系统负载：即cpu使用情况。100%负载就表示所有cpu内核都在使用中。
		 >内存：kafka并不需要多大，只是用来缓存。真正大量需要的是：磁盘空间。磁盘每秒读写速度。读写平均队列大小，平均等待时间和 磁盘的使用百分比。
		 >BURROW:
		
举动-痛点-目标：流式处理
	>解法/处理思想：
		>流式处理系统：Storm,Spark Streaming, Flink, Samza。都将kafka作为唯一可靠的数据来源。还提供了强大的流式处理类库---作为客户端类库的一部分。
		>数据流：无边界数据数据集。事件流。数据有序。数据库里的数据实际无序，但数据库的事务是可以被看作事务流的。
		 >有序数据：
		 >不可变：不会消失。数据库里的redo.log重做日志会记录每一个成功的事务，增加记录或者删除记录都会。
		 >可重播的：经过套接字的tcp数据包不可重播的。重播几个月之前的事件流很有价值，常见需求。
		>数据流中事件包含的数据和每秒钟的事件数：实时的处理一个或者多个事件流就是流式处理。
		>流式处理：是编程范式，像请求-响应、批处理范式一样。
		  >请求响应：OLAP,POS系统。阻塞。响应时间毫秒、稳定。
		  >批处理：高延迟和高吞吐量。定时启动，处理数据，输出结果。数据仓库DWH，商业智能系统BI， 处理的都是旧数据。
		  >流式处理：对于有持续性、非阻塞的业务。网络告警、跟踪包裹。从一个无边界数据集里读取数据，处理并生成结果，就是在流式处理。
		>流式处理：
		  >事件时间：事件发生时间、记录创建时间。
		  >日志追加时间：保存到kafka的时间，即kafka给收到的记录追加的时间---添加的时间戳..
		  >处理时间:应用程序收到事件后对它处理的时间.
		 >状态:事件与事件之间的信息.
		  >本地状态:只能被本地访问.
		  >外部状态:使用外部系统来存储状态.
		 >流和表的二元性:流包含了变更,因为每个事件就是一个变更.表是静态的---包含了多个变更的结果.----------所以变更路径很多...取决关心变更结果还是变更事件本身.
		  >表转换为流:只需要捕捉表上发生的所有变更inset, update, delete.CDC解决方案. kafka连接器将这些变更发送到kafka, 用于后续的流式处理
		  >流转换为表: 只需要建立一张表,做完一个流程/遍历流里的所有事件,不断改变状态,得到的表状态就是某个时间点的状态.
		 >时间窗口:合并在相同时间片段上的事件.窗口大小--移动间隔/移动频率(间隔不同,分滚动窗口,滑动窗口)--窗口可更新时长(窗口时间内计算后有新事件来)
		 >设计模式:
		  >单个事件的处理:map/filter.无需维护事件状态.比如读取流,将其中ERROR和 INFO级别的信息分别读取放到另外的两个流里.
		  >基于时间窗口的事件聚合处理: 维护本地状态,计算min,avg等.
		   >内存使用:保存本地状态.
		   >持久化:Streams , RocksDB可以保存在内存并持久化到磁盘. 同时本地状态变更发送到kafka主题-----使用了压缩日志---确保不会无限增长.
		  >多阶段处理和重分区:
		  >使用外部查找来实现数据填充: 但延迟大. 流式处理每秒10-50w个事件
		  >捕捉数据库的变更事件并形成事件流:CDC change data capture 用来跟踪变更而更新本地缓存----使得上述外部查找的数据可以即时更新..称流与表的连接.
		>流与流的连接:基于时间窗口的连接. 每个流上分别取一个时间窗口,相差几秒钟,然后对内容进行关联.典型的比如"搜索"-"点击"两个流.
		>在一定时间段内重排序乱序事件:(压缩日志即保留最新键的值)
		>Streams DSL: 另一个是底层的Processor API
		  >拓扑:是一个有向图. 拓扑应用到流的事件上.
		 >使用过程:创建一个指向某个主题的KStream, 利用它的map-reduce流处理api计算出结果并写回另一个主题.KafkaStreams
		 >伸缩模型: 伸缩应用\处理故障.
		 --------关键区别于请求-响应模型:应该是---并非对每个请求都开辟一个线程来处理,而是批量处理\流式处理,在一个线程内.----通过消息解耦之后.,,或者甚至根本不需要即时回复,甚至不需要回复的场景.
		  ::消息通知\实时数据分析\日志处理\滚动统计\机器学习-风险预见\模式识别(大规模事件流里识别模式的问题)..
		 
		 
		 
		 
---开发更抽象的东西/接口性质的东西/流程式的东西 + 各个具体的实现插件/协议实现/数据格式实现/通讯实现/内容实现/策略实现。。来实现任何的解耦。		
参考资料：技术挑战是唯一原因。

学习，要有一种认真和小心翼翼。









