(严格高标准-动作规范-才能平静，否则会浮躁)(严格就是确保、证明、验证)(画谋并证明，分析并证明，推理并证明，计算并证明， 猜想并证明， 等价类比并证明， 模拟并证明)(一步一步有步骤，每个观点事实都要表达完整清楚不片面中断含糊不完整--即便感觉是错的-也不能半路停止表达)
(顺不骄，逆不馁，安不懈怠，危不惊惧)(不慌乱，不草率，不虎)(谨慎-慢呼吸-深呼吸， 宁可惊惧不可慌乱，镇定自若是最基本要求， 排除干扰与分散而聚焦注意力优质重要的内容上--并持续长时间聚焦积累--不断挑战自己和超越自己，讲究利弊权衡和行动策略)(阅读-书写-谈吐 都不随便 都珍惜 都尊重 都仔细 都理性 都证明 都验证 都确认， 对于画谋分析推理极端重要)(职业化道路是唯一的出路)(独立深入思考，得不容易得到的结论；谈吐非凡，见识深远)(看得懂技术爆发发展点 如区块链)
(叙述介绍，从分类入手；自顶向下进行，)(有趣只能自己，有价值的是要能解决问题-实际问题；满足需求)(越慢越独立思考越独立深远见识发现)(技术咨询方案公司)(永远理性，永远优化思考更好)
并发相关全知识：https://cloud.tencent.com/developer/news/313447 java并发编程的艺术
jvm技术规范：https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html#jvms-6.5.monitorenter
next work ,  sumerize every night!!new technology  广义相对论/量子力学 + AlphaGo/AlphaZero + 飞行器控制/机器鱼控制(自动化和其他学科的不同在于使用的层次不同)(数学是无限的) 
源码可以上github上去看。业余爱好：看看算法题和算法代码。
			 第负一阶段：数学奥赛题(包含丰富数学简便计算技巧，促使提出问题和分析问题) / 各种网络实际算法(路由算法/分布式-区块链算法)  逻辑思维题--抽象出方法论-通用的处理方法-通用的简便方法
			第零阶段：spring源码深度解析 + tcp/ip/leatcode/acm算法见识 + 微服务/docker私有云搭建
#阅读书籍：第一阶段：高可用mysql + redis设计与实现 + 深入理解Java虚拟机
		   第二阶段：kafka + hbase + elasticsearch
		   第三阶段：阅读分析一个开源项目代码。//阿里github上开源项目很多。
		   第四阶段：分析一个市场的一个需求的一个企业的商业设计和组织设计。分析一个从0到1的主将 而不是人数越来越多的管理者。
		   第五阶段： <<精实创业>>(The Lean Startup) 业务和项目。描述当前各个产品，分析各个产品的补集(天然难以到达)，那就是创业的方向。(产品的痛点，缺点，缺乏)
		   第六阶段：理论研究：阅读经典论文和前沿论文-计算机方面、人工智能、数学方面-《比特币：一种点对点的电子现金系统》

#开发：数据库、监控
#将痛点意识应用到生活之中。生活事务、人际事务中。//痛点是事务的痛点。事务普遍具有的特征。
	提供什么服务的项目。项目的核心技术。项目的团队。项目的周期。
#座右铭：不断挑战自己和超越自己。多提问多持久思考。聚焦注意力。定义概念辅助思考。永远理性，讲究科学方法。追求通用方法和简便方法。事情逻辑化、条理化、层次化
		  先思后行，永不着急做。分析对象，建立模型(最简模型)，用推理计算得出观点并证明。(清晰的认识，精准的描述)
		  积极准备，永不懒惰，永不偷工减料。饮食作息好习惯，情绪稳定，保持好状态。
		  怒和急躁会导致莽撞和不理性。
		  不要自大，不要硬碰硬，不要意气用事，因为会损失很大代价。
		  
#介绍一种技术是一种技能：是组织能力的体现，是描述能力的体现，是清晰洞察掌控能力的体现。描述技巧丰富度的体现。//而不是混乱的描述，东拉西扯的描述，只言片语说不清楚的描述。//不只是给面试官，而且给同事，给客户。
	>介绍一下线程池？
	>介绍一下NIO连接池？
	>介绍一下数据库连接池？
	>介绍一下spring?
		>自顶向下、索引树形展开搜索式的讲解：比如讲解spring: 则产生spring的原因--开发痛点陈述？、痛点的原因&影响&希望达到的治疗效果指标目标、根据达到目标设计必然的方案可行的方案(spring)、新问题和能力边界。
															>直接从容器中去取-而避免手动创建层层属性都要依赖注入的bean。spring根据依赖注入的逻辑自动注入自动装配
															>有注解的类如何创建-
#新的开发：
1.日志文件的三级索引：来时间层次统计内容。															
可以归结出几大技术点：(能力边界，消耗成本)(逻辑边界)
应用业务层次：---：

工具层次：------：数据流线角度来收拢聚集
#调用式编程：不要直接开始写代码---调用式，自顶向下编程；就是抽象通用方法编程，就是写思路，写算法。
#一切框架设计，工具设计，都要从最简单简洁高效的实现开始(而不是一来就是各种条条框框、固定思路)(开发思路设计，从核心简洁设计开始)(分析是否最优设计，没有冗余)。
--谨慎的认为自己已经掌握了，却说不清楚，仍然一片混乱！！比如spring
--没有彻底弄通就会只言片语、似懂非懂、朦朦胧胧、含糊不清。觉得什么都重要，关系没有弄清。
--
#web后端开发相关技术：
 >web容器启动时，会扫描实现了ServletContainerInitializer的类，执行其中onStartUp()方法，方法里面执行注册filter<pattern路径匹配, 类.class>, listener<类.class>, servlet</path路径，类.class>到入参对象servlet容器里。。该方法另一个入参是一个Set<Class> 是类上的@HandlesType注解中的各个类
  >重要的不是某个具体功能的详细调用过程：重要的有两种：一是模块调用过程。二是从这种业务相关的功能中分解出而抽象出存在的一类又一类的通用功能/一般功能/普适功能的实现过程/一类纯技术功能的实现过程(比如AOP,类加载,DI),专题功能的实现过程。
 >SpringServletContainerInitializer实现了ServletContainerInitializer类，

 >Junit4.class也只是一个类，反射的方式运行一个方法。
底层技术-------：(动机范围)(基础引用<用什么牌>,运用策略(<该牌怎么打>) (普通方案，最优方案)
数据结构层次：
	#字符hash：hashmap的hash函数(h ^ (h >>>16) )\fst\nginx一致性hash(圈)(nginx上/redis缓存上/)
	  >普通hash算法：服务器ip的hash值 - 请求资源的hash值一一对应。比如取模算法，这导致扩充机器时 请求资源会从各个已有机器转移到新的分配机器上(可以想象为从最左边开始的机器开始依次取些流量到新的依次的机器上---无论新增的机器有多少，旧机器取完了就循环)，对已有机器影响比较大；即分多台机器的流量。但是也有好处，机器挂了之后上面的请求会转移到最左边开始的机器上(流量线模型)(一台上，这是问题所在)，挂的越多分布的越广。
	  >原始一致性hash算法：服务器的ip的hash值作为分割点分割整数区域(0-2^32)为各个区间 - 请求资源的hash值在整数区域上的位置的顺时针最近的分割点对应的服务器 -- 一一对应。 取模算法也可以作为hash算法。从流量转移角度看，当扩充机器时，可以做到只影响一个区段/一台服务器的流量，全从一台机器上分； 当有一台机器宕机时，则上游的到自己的流量会转移到下游的那一台机器上。  
	   >现在一致性hash算法：原理类似上，但是新增一台机器，会新增n个虚节点。从而宕机一台的时候本机上的流量会均匀重新分配到其他n+1台机器上, 增加一台时也会从已经有的n个节点中获取流量。
		 >满足三个功能：1：没有挂时：流量均匀打到各台机器上； 2.增加机器时，流量从各台机器分部分流量到新机器上；3.减少机器时，流量从这台机器均匀转移分布到各台机器上。
	#索引结构的增删查：	
	#hashmap数据结构的增删查：Node链表数组(hash-key-value-next), key的hashcode的hash()函数结果 : h >>> 16 ^ h (作为entry[]的索引)。 h是正数，所以最高位一定是0，逻辑右移，所以最高为仍然是0。。初始容量时16
								>当链表节点数 >= 8则会把链表先转为双向链表(且反序)，再转为红黑树！！(每插入一个元素就进行一次红黑平衡算法平衡红黑树)(LL型平衡旋转(最长路径一直往左下)--右短分支加进来而本节点放在左边；RR型平衡旋转(最长路径一直往右下)--左短分支加进来而本节点放在右边)；；当最长路径存在左转右转时，先层层向上转为单向。最终一步都是使最长分支的顶点向下旋转----从而最长分支的长度减1。
								>删除：左右子节点不为空--则使用左子树最大节点为新顶节点，并递归删除该节点。
								>resize(): 当node entry个数 >= 长度 * 负载因子loadFactor   则先初始化一个2倍长的table，在逐个赋值；在新table的位置---如果只有一个元素-则该元素的hash & (newlength - 1)， 如果有多个链表型元素---则将元素的hash & oldtablesize  == 0(或者oldtablesize) 而分解为高位list和0低位list分别加到扩2倍的新node array的两个位置j, j + oldCap   ....因为线程不安全，所以多线程扩容的时候：：或许会导致产生大量的新2倍数组产生----从而导致内存和cpu都巨大消耗。hashtable线程安全----但是对entry数组整体加锁---从而性能不好。如果是树节点TreeNode，同链表一样会根据hash & oldtableSize == 0而分解为 高1 低0共2个TreeNode单向链表，如果单向链表的大小超过了新的load_factor*newCap, 那么就需要再次化为树，如果小于，则转为Node单向链表；放的位置依然是j或者j+oldcap
	#concurrentHashmap数据结构的增删查：Node链表数组，hash方法 h ^ h>>> 16 & (n - 1)来确保得到的数自然会是 < n-1的数(比如n是16,因为和高位0相与结果为0)，树也有next方法-所以统一，加节点时，如果数组i位置是null--则cas原子操作数组方法替换i位置为新的值，如果hash是-1会重新操作数组--transfer之类比较复杂，获取数组i位置的节点也是volatile方式缓存失效方式获取元素引用；最后在正常的有next的元素的put时---用这个新构造的Node进行synchronied同步----如果key存在则覆盖--key不存在next==null的位置加，如果是treeBin则相应的查找添加--甚至会在重构树时需要获取写锁--而使用到park(this)停车。//直接对Node加锁---而且用synchronized,且有用cas减少锁使用。扩容用transfer	
								>put元素：读取i位置元素要用cas可见读方式读取数组的index位置元素(hash & (n - 1))，元素为null时，也要cas方式存一个新Node元素，防止并发问题。i位置不是null而是f普通节点Node, 那么用f来sychronized同步，同key的val根据ifabsent被覆盖，不同key则直接简单的next赋值为一个新的val值的Node节点即可。是TreeNode节点，就加新节点到树里，								
									>treeifyBin():Node链表转变为TreeNode红黑树：当链表size大于构造树的大小后，synchronized(元素) 同步地 先将Node链表转变为TreeNode链表，然后用首元素构造TreeBin而转变为TreeBin红黑树, 再可见性地更新到index位置。
								>resize: 实际并没有这种方法，实际上是transfer()在做实际的数据转移工作，在复制的时候读元素也是cas,而开始复制的时候也sychronized同步数组的节点f。
									>transfer(): 是对ForwardingNode类型节点进行元素分解分类为2类，存放在这个节点的nextTable里---2倍老table。分类标准：如果是链表类型，看元素的hash & n 之后 == 0 归为低位链表，为1归为高位链表，两个链表构造好之后就volatile可见性地方式更新到newTable的i位置和i+n位置， 而也会new一个ForwardingNode注入newTable后而同样可见性地更新到老table的i位置---老table每个i都是同一个ForwardingNode实例(TreeNode/Node都是)---也是同一个nextTable：因为遍历老table之前就已经有newTable和ForwardingNode实例了。如果是树节点TreeBin，那么也按照hash & n == 0 或1而归类出低位链表--反序， 和高位链表-反序， 如果任一个链表TreeNode的大小不超过构造树的最小值，则退化为Node链表，如果超过，如果另一位是Null那么就把原来的根节点原子更新到相应位，另一位更新为Null;如果另一位不是null,那么低位和高位链表首元素为参数分别构造出两个TreeBin红黑树分别更新到i位置和i+n位置，ForwardingNode也同样的更新；更新方式也都是volatile可见性地更新。
								>get: 也是cas方式得table的i元素，剩下的则直接查，没有同步；
								>put方法类似hashmap。只是加了并发控制synchronized
	#hashtable的增删查：get/put/remove方法都是synchronized实例同步的。key得出index的方法：h & 0x7fffffff % length 即很简单的去除最高位再取模运算。。Entry[]数组类型(hash-key-value-next)，单项链表；所以冲突的时候依然是增加元素。。而且只在next增加元素，没有treeNode。不存在转变为treenodo的过程。
	#红黑树：新加节点都是红色；不满足“根黑 红-黑 黑数同” 则需要处理-重新平衡：变色(对一个节点变色检查：该节点定是红色，1:父叔同红-变父叔为黑、祖父红 ，当前为祖，继续 ；2：父红叔不红(黑或空)&当前为右-令当前为父-新当前再左旋，3：父红叔不红(黑或空)&当前为左-变父黑祖红-令当前为祖-新当前右旋； 根节点结束)、左旋(当前节点向左下，而右子上位)、右旋(当前节点向右下，而左子上位)。。黑下可以为黑。父红祖不红，则父旋(父子-祖父不同向)或祖旋(父子-祖父同向)；父红叔红，则父叔变黑祖变红，当前为祖，继续。
		>删除：无子节点则直接删除、有单一子节点则删除而用新子节点替换、有2子节点则删除而用右子树最左节点替换。https://www.jianshu.com/p/0eaea4cc5619  https://blog.csdn.net/eson_15/article/details/51144079 https://www.jianshu.com/p/e136ec79235c(最佳)
			>是红色：且最多1个节点，直接用子树替换。
			  >2个子节点：
				>替代元素是红色：直接移除并替代
				>替代元素是黑色：调整顺序：兄红父左旋(数差不变)、兄左儿红兄右旋、兄右儿红父左旋、兄黑兄双儿黑
			>是黑色：(考虑黑数和高差改变)
				>替代元素是红色：直接移除并替代元素为黑色
				>替代元素是黑色：
		>增加：原则：红之黑父，互换色而反红旋。即如果父叔叔子节点有单一红，则这样操作几次。如果父叔叔子节点都没有红，则直接变父叔为红而祖为黑；继续祖。
			>
	#treeMap数据结构的增删查：底层数据结构为Entry为节点的红黑树。线程不安全，没有可见性和没有同步。
	#BlockingArrayQueue(netty)数据结构的增删查:使用ReentrantLock进行同步、阻塞。扩容是也是create一个新的更长的数组。
	#ArrayBlockingQueue数据结构的增删查：使用ReentrantLock进行同步、阻塞。
	#CyclicBarrier类似CountDownLatch, 它功能为让并发的线程运行到相同位置。--先到的线程则阻塞。--直到num==0而开始都运行。
	#Semaphore信号量，类似synchronied、lock---但是不同之处是，可以多个线程同时获取到锁--即一个信号量，达到限额后获取才会阻塞。
	#Exchanger两个线程交换数据，使用Unsafe.park(false, time)方法进行先到线程等待后到线程，后线程Unsafe.unpark()来唤醒先到线程，Node节点里:item/match/parked/hash属性。其中item就是先到设置的，match是后到线程设置的，parked就是先到的停车线程。
	
	
	
线程方面：(多上下文方面)(数据+指令代码的临时环境内存环境)
 ・・・#线程不安全的原因：(每个线程的)工作内存和(物理机唯一的)主内存数据发生了不一致导致的。(多核，每个核有自己的L1L2缓存)
       >底层原因：编译器和处理器为提高执行性能常常对指令进行重排序。相似指令一起执行。
	     >重排序标准：不改变单线程程序语义--则编译器重新安排语句顺序; 多条指令不存在数据依赖性---则处理器将多条指令重叠并行执行; 处理器读主存时由于局域性原理会读一个缓存行-64B到高速缓存，从而看上去剩下的数据加载指令并行执行了。
		    >重排序的禁止：编译器重排序自己有规则--对于会改变程序语义的则不重排序；处理器重排序的禁止：编译器生成指令序列时会插入内存屏障指令 来禁止某些特殊的处理器重排序。
			>重排序的保证：as-if-serial语义：单线程执行结果不变。对编译器和处理器都是。
			>JMM向程序员保证的6条关于两条要求同步的操作执行先后顺序的语义(无论操作是否在同一个线程)(也即JMM的术语)(JMM也要求编译器(和处理器)严格实现这6语义)：同步操作的happens-before关系：先后顺序即 a操作对b操作可见，使得结果始终是a先b后的结果。。happens-before关系的6条具体表现：1.如Thread.start()操作先于程序内部的操作，Thread.join()的返回后于线程内部的操作。	
			>查看字节码：javap -v A.class  加了sychronized之后会出现两条指令
			  >monitorenter指令: 获取对象监视器monitor , 是互斥操作，未获取到的等待
			  >monitorexit指令：退出对象锁。
	   >线程之间交换信息的方法：共享内存和消息传递。只这两种机制。jvm内存模型是共享内存的并发模型。
	   >
	#线程之间同步的底层机制：
	   >内存屏障：使得一个内核写完主存 其他内核 才能读取主存。(相当于有一个锁主存的功能。StoreLoad)
	#同步：
	   >本地多线程同步(代码块)：同步代码块synchronized/Lock(可重入锁ReentrantLock/读写锁ReentrantReadWriteLock)/自旋锁(mcLock, clhLock)<利用了原子操作>/带Unsafe 利用了OS 的CAS原子操作来实现的同步 AtomicInteger, AtomicBoolean   (但实际上synchronized更简单---1.6以后它的jvm实现也是cas)
		  >CLH锁：节点对象：有一个next对象 以及 一个属性引用原子获取更新getAndSet()类对象(Unsafe类获得)， 取锁方法：首先n个线程原子获取更新next属性为自己的node对象，同时获得上一个线程设置的对象，由于每个都会串行执行，所以每个获取的都不同，并把这个上一个node.next指向自己。自己在这个node上自旋---退出条件是这个node被属于的那个线程设置它的lock属性为false-----即表明上一个线程释放了锁而自己得到了锁。
	     --使用锁性能问题的原因：线程上下文切换--用户态内核态切换(自旋则没有上下文切换)  ； 磁盘IO不同扇区引起盘片转动和读写磁头转动
		 --多线程缓存不一致的原因：A线程读取主存中的一个值，不是立即给cpu，而是给高速缓存，然后进行运算；且这两步都没有锁住总线---从而其他cpu可以访问主存中的该值；。。注：线程读取数据，具体到os执行级别，都是先看cpu缓存里有无，再定是否读主存；(每次因计算而更新了cpu缓存中的值则会保存到主存)
									>即某一个线程修改的变量仅仅更新到了对应内核的L1,L2高速缓存里而没有更新到主存，而此时其他内核之前读主存时读了含此变量的缓存行进来，从而它读的时候也只读了它对应的内核的L1/L2高速缓存，从而不一致。总之就是更新没有使得高速缓存失效。
		   多线程缓存不一致的解法：缓存锁定(总线锁定上述)：MESI协议：一个cpu在写主存中的一个值时，会发送RFO指令信号给其他cpu让内部缓存里的同一个主存地址的值失效(迫使其他cpu进行重读)(而其他cpu也会监听缓存状态-如果是M则要等待它刷到主存为止)。(一个cpu里多内核，各个内核也有自己的高速缓存)
																
		 --os提供的原子性操作：读写一个字节、Compare And Swap (利用了上述--总线锁定/缓存锁定) (乐观锁方式读取比较写入;相等则写入返回true-否则返回false) ---这串指令操作被封装到了Unsafe类的多个方法中，如Unsafe.compareAndSwapInt(实例，属性，期望值，更新值);更新失败可以重新读重写compareAndSwapInt();(在Unsafe.getAndAddInt(实例，属性，递增值)就是这样的重试机制)
	   >volatile: 只是保证在对变量更新后，立即从内核高速缓存回写到内存，而一般情况下没有volatile修饰的变量被修改不会立即被回写--而且很久一直都是从高速缓存读。称为变量有可见性。可见型变量，，无脏读变量。(非常重要，CopyOnwriteArrayList实现里就使用了--这个是先copy原数组并长度+1添加末尾元素后将数组指针赋值给原数组指针)
				>避免指令重排--所以volatile变量的读写 会加内存屏蔽(LoadLoad, StoreStore LoadStore StoreLoad) 指令。(重排序，在一个线程中执行的顺序指令，但是可能不是顺序的，导致另一个线程看到的值变化不是顺序变化的)(cpu汇编级别指令会重排-同类指令在一块执行-提高效率-减少来回切换)
	   >final: 也实现了可见性。毕竟不能被重新赋值---但可以第一次赋值---这个第一次赋值只能顺序执行中被赋值。
	   >happens-before:是操作之间的关系，用来辅助描述各种规则中的各操作的先后顺序。比如volatile变量的写happens-before该变量的读(多线程中)
	   >四种锁状态：以及升级关系：无锁->偏向锁(获取锁自己用，不存在释放锁,没有CAS也没有自旋，竞争时膨胀为轻量级锁)->轻量级锁(竞争失败时自旋或者CAS)->重量级锁(竞争失败时使用互斥量mutex阻塞当前线程-从而线程切换)     (锁对象：Mark word里会存获取本锁的线程信息、锁级别，线程cas方式获取和赋值)
					>一定是因为四种锁状态下“加锁和解锁”的底层操作指令不同、代价不同，所以在一种锁状态等待极限内仍然没有获取到锁--则会升级锁。----意义-应用：在竞争强烈的场景下，直接先关闭偏向锁功能；第二：设置轻量级锁自旋次数：-XX:PreBlockSpin；第三：升级轻量级锁为重量级锁可以避免浪费cpu资源(尽管线程切换、阻塞需要os内核态操作所以需要用户态-内核态切换又耗时)。
				    >轻量级锁的获取：CAS更新Mark WOrd中的锁状态和锁类型。或者Atomic::cmpxchg_ptr方式来更新Mark WOrd里的prt指针--来指向当前栈帧。
			>线程与锁：锁的区别用第二个线程去获取锁的成败表现来 描述：
				>重量级锁：第二个线程获取失败 则阻塞，加入 该锁的等待池中(锁对象的监视器的同步队列<AQS?>)。
				>轻量级锁：第二个线程获取失败 则直接返回false, 。往往后面我们会进行自旋。
				>偏向锁：第二个线程获取都会成功，直接把第一个线程挂起。
				>无锁：第二个线程无视第一个线程。直接获取资源。
	  >sychronized: 1.8以后性能持平或者更优。且concurrentHashmap用synchronized而不用ReentrantLock	。。synchronized先偏向锁--->再轻量锁--->再重量级锁
	      >底层原理：java对象头的Mark Word存放的值：包括了锁状态(是否加了锁)和锁标记位(什么类型的锁)。对象监视器的引用。hashcode, 分代年龄。(锁state、对象监视器-有ptr当前线程指针_owner)
		    >
		    >字节码方面：先获取对象的监视器，没有获取到就会进入监视器的同步队列等待。对象监视器的os实现是mutex lock。
			  >对象监视器和Mark Word的关系：从Mark Word里可以获取到对象监视器的引用/指针。重量级锁才有对象监视器。
			  >锁膨胀过程: 
			    >monitor竞争：分配一个对象监视器，将Mark Word里的锁状态cas尝试更新为inflating, 如果失败则自旋等待(实际是park有的占用不了太多cpu)，成功则设置对象监视器monitor的各个字段，返回这个对象监视器monitor.
				>monitor等待：尝试cas更新monitor的owner占用线程字段为当前线程，成功则该线程获得锁而返回，失败则自旋 park()下次竞争失败时继续park()等待被唤醒，唤醒后继续自旋竞争----唤醒就是得锁的线程调用unpark()激活park()的线程。
		  >重入性：对象有计数器，获取了对象锁则计数器加1.
	   >AQS:同步组件实现的关键部分,是基于CLH锁的FIFO同步器，本身提供“(取锁--即cas方式加到队列尾部tail)排队-循环阻塞-状态取赋更(可以导致退出阻塞)(释锁-激活下一个元素)”基本功能，比如使用Sync， CountDownLatch(利用sync实现自己的同步语义)就用了Sync。比如ReentrantLock(利用sync实现自己的同步语义)(sync的AQS的ConditionObject里的await()方法---根本是用了LockSupport的park()方法--而它则用了本地方法UNSAFE.park(false, 0L);方法来阻塞当前线程---其他线程则用UNSAFE.unpark(threadObject)来释放阻塞的线程)类里就用到。(AtomicInteger是用Unsafe, LockSupport也用Unsafe, concurrentHashMap 用lockSupport)
				>CLH锁中队列元素对应先后阻塞的线程，int整数代表同步状态(>0排队阻塞)(AQS自身不改变state的值)，AQS提供取、赋、CAS更新三个方法操作该状态。
				>实际继承AQS：常用来实现tryaquire  tryrelease 两个方法，实现中利用set/get state方法(有时会使用get/set ExclusiveOwnerThread来判断是否重入之类)(更新成功则得锁，没有自然就返回---从而可以不用阻塞)，而使用AQS自己实现的acquire release方法。
				>lock()里的accquire()里的其中acquireQueued()方法中：就是一个：先获取上一个节点是否是头节点，不是则park()，是则尝试cas更新状态和拥有线程，未成功也park()等待，被其他线程unpark()激活之后再次尝试获取waitStatus
				>unlock()里的release()方法：先cas更新状态，成功后unpark()唤醒头head节点的下一个节点(或者尾节点向前的最后一个waitStatus<=0的节点)的线程。
				---因此AQS是一种用了双向链表来同步线程同时又用park()/unpark()阻塞唤醒线程。
	   >cas操作：底层是CMPXCHG指令，封装为非阻塞的先比较后更新属性值--返回成功或失败。即本质是一种方法类型；先比较后更两个操作 底层实现保证了是顺序的和隔离的；或者说这两个操作是一个操作一同完成的。如果值当作得锁成败，或者得锁次数，就可以得到构造一个乐观锁。
	     >可见性都是通知机制：MESI机制的失效通知。volataile和cas都是。
	   >park()方法：每个线程都有Parker对象---这个要从JNI环境里获取，。参考https://blog.csdn.net/weixin_39687783/article/details/85058686
	     >POSIX编程接口：各种系统调用的接口(系统调用自然面向应用开发者，比如创建线程的接口：posix_fork() 而linux和windows都实现了这接口)，linux和windows都实现了的统一的系统调用接口(虽然各自内部实现不一样)
		  >park()的c++实现：
		   >平台实现的一组关键POSIX接口：pthread_cond_wait, pthread_mutex_lock, pthread_mutex_unlock
		     >linux实现进程阻塞的数据结构：等待队列(自旋锁spinLock一个 + 存放等待进程的队列)。。进程在等待队列中阻塞等待，直到条件为真被激活。
			   >操作系统挂起进程：直到条件满足才唤醒。
			   >阻塞与唤醒的原理猜测：操作系统里有一个 “等待队列”--每个元素是“进程-等待条件”， 加入到这个队列中的进程对应的程序代码就不在继续执行，相当于系统调用不返回--还没有执行系统调用返回指令，入队时保存当前进程的上下文，当条件满足时，则取出这个进程-继续程序计数器偏移量下一条指令开始执行，直至执行系统调用返回而切换为用户态上下文，而继续执行，出队恢复进程的上下文。
				 >从而系统层次/内核态 执行 出队 和 入队 (等待队列)就可以实现 对一个进程的 阻塞和唤醒；比如 磁盘写就绪 事件 信号 促使os/内核/cpu执行一个出队操作--根据条件/事件信号，从而唤醒在此条件/事件下等待的对应的进程。又比如网络协议引擎可写/可读的事件信号。中断信号; block/pending/handler三大表。参考https://blog.csdn.net/tong646591/article/details/8485454  https://blog.csdn.net/double_happiness/article/details/72897592
				 ----系统调用执行中条件不满足主动入队(保存上下文，释放cpu, 暂停了执行，相当于冻结了上下文--没了cpu一条指令也会不再执行)， 事件信号触发出队(获得cpu，恢复上下文，继续执行)。硬件设备发送中断信号给cpu。
				 ----另外一种情形是：当前线程新建一个上下文推入 等待队列，这个上下文就是回调函数---而等待条件/出队条件是 事件信号出发 或者 程序条件发生程序主动调用出队。从而主线程仍然可以在创建回调函数为内容的上下文并入队 后 系统调用返回继续用户态上下文执行。
			 >内核实现的事件通知机制：
			   >锁、信号量、条件变量：
			   >无差别轮询有限制(1024 文件描述符的个数 定时轮询，select)、无差别轮询无限制(事件发生轮询poll)、内核IO事件异步唤醒的相关返回(只返回有事件的流epoll)(系统注册回调函数，回调函数将唤醒的描述符/就绪fd拷贝到readyList里---用户获取事件就只看readyList里的--获取方式水平触发/边缘触发，有mmp内存映射少拷贝)：本质上都是同步IO。。 参考https://www.jianshu.com/p/397449cadc9a
				 --epoll是内核以事件回调方式实现的为用户提供的获取就绪文件描述符集合readyList的一种机制。
			 >内存空间(寻址空间/虚拟空间)的划分：内核空间和用户空间。内核具有特殊指令和内存空间的访问权限。操作系统的核心部分程序为内核。
			   >进程切换：内核保存一个进程上下文/到等待队列(挂起进程)，取出一个进程上下文继续执行/恢复执行。
			   >进程阻塞：进程自己执行了Block阻塞原语 而将自己的状态设置为阻塞状态而放弃cpu而被内核放到等待队列。
			   >文件描述符：用户态进程看到的内核返回的一个文件的引用，是一个非负整数。
			   >缓存I/O: 即I/O的数据都会先缓存在 内核的缓冲区(内核空间中)， 才进一步拷贝到用户空间(读)/硬件|网络协议引擎(写)(硬件或还包括缓冲区)
			 >指令级别的等待：
		   >内存屏障方法：OrderAccess::fence();赋值一个变量后加内存屏障，使屏障指令之后的线程读取到的变量的值就是内存屏障指令前的指令赋的值。。参考https://www.cnblogs.com/cynchanpin/p/6927045.html
		     >linux实现：__asm__ volatile ("lock; addl $0,0(%%rsp)" : : : "cc" , "memory");等。
			 >cpu在内存系统上提交Store操作和Load操作：如果这些Load,Store指令没有数据依赖关系，那么实际执行顺序就是不确定的-随机的。干预方法就是内存屏障指令， 它保证内存操作部分有序--即在内存屏障一边的指令相对于另一边的指令是顺序的--局部顺序。
			   >写屏障：写屏障之前的写指令优先于之后的写指令。
			   >数据依赖屏障：两个Load操作，数据依赖屏障保证第二个Load操作执行之前-对应的目标地址内容已经更新(cpu1即便加了写屏障，但是两个数据如果存在不同的奇偶号缓存行里，那么更新到主存里的顺序也不是确定的；所以cpu2读取的时候也可能先读取到cpu1执行的写屏障之后的指令的写内容-----因此cpu2还要加一个数据依赖屏障来保证等待cpu1写屏障之前的指令执行完毕才开始读)。当然因为两个Load存在依赖关系，不加读屏障已经就是有序了的。常和写屏障一起使用。本屏障指令前后是写地址操作和读地址的内容操作，本屏障就保证写地址操作中写的“地址”定位到的内容已经在其他cpu中更新中则先更新。
			   >读屏障：读屏障之前的写指令优先于之后的读指令。
			   >通用内存屏障：读写屏障之前的写指令优先于之后的读写指令
			   >Lock操作/UnLock操作：单向渗透屏障，保证单向的本指令后的操作后执行 和 本指令之前的先执行。
		   >进入安全点，改线程状态为阻塞状态：tbivm()
		   >修改java线程拥有的操作系统线程的状态：osts()如修改为等待条件发生--CONDVAR_WAIT
		   >当前线程放到等待条件的线程列表里，再对互斥变量解锁：原子操作 pthread_cond_wait()  会导致线程等待，直到再次获得互斥变量的锁
		   >释放互斥量的锁：pthread_mutex_unlock()
		 >许可：原子变量_count
		 >
	   >远程调用/IO调用/read&write调用方式：同步阻塞/异步阻塞/事件回调
	   >分布式同步：
	   
	#线程模型：从任务特征上分类。
		>Runnalbe模型：无输入值无返回值直接异步执行任务 ,
		>Future模型：无输入值返回future并异步执行任务-父线程future.get()同步等待结果
		>Fork-join模型：有输入值有返回值(任务特征)，任务本身根据入参或者直接计算返回值或者分解出2个子任务异步执行并取它们的结果；父线程task1.fork(), task2.fork()异步执行,父线程task1.join(), task2.join()同步等待结果。
		>Actor模型：用信号signal异步发送woker请求request实例和response回调函数类实例给actor，无返回值。请求实例也会被异步执行---即如果在发送的时候做为循环发送n个req, 那么这个n个req会并行执行。(如果在回调里再发送，而外层只发送一个，则自然变成了顺序执行了。) 参考资料：http://www.agilewiki.org/projects/JActor2/1.0.0/JActor2RevisitedByExample.pdf

	#网络模型：nio的多线程selector为例
		>普通socket:阻塞模型，流数据模型。到则必须读，无则阻塞。写则直接写--不管端口状态。
		>NIO:端口四大事件通知模式--(linux上是epoll)，每个端口对应的通道四大事件有通知-无阻塞(相当于一个方法有值返回无值等待阻塞)，通道可以读出可以写入--根据实践类型而定----而这个通道读与写是同步的。Selector-->SocketChannel-->ByteBuffer
		>AIO:读与写是异步进行的Future线程模型
	#存储模型：副本为例
	
	
		
#内存方面：
	>内存管理：
	  >jvm内存管理：
		>各代划分和默认比例：
			>NewRadio: 新生代=1 年老代=n   默认为2
			>SurvivorRadio: Surivior0=1 Eden=m 默认是8  
			
		>回收器的标记回收过程与促进它的参数调整：基本都三个阶段：“标记-复制-清除”
			>垃圾回收器：Serial  Parellel CMS G1
			 >回收方法1：stop the world 标记所有根节点(静态属性，常量)-可达对象-清理不可达-复制可达到survivor/old。。。Serial Parellel 
			 >回收方法2：stop the world 标记所有根节点 --> 并行标记 可达对象(trace算法记录在bitmap) ---> STW 重新标记 动态用户线程产生的新可达对象 --> 并行清理不可达对象(对可达对象不复制)。CMS优先保证stw的时间短。而G1好处在内存空间管理更精细，空间碎片更少，影响的线程更少。
			>回收器组合1：+UseConcMarkSweepGC +ParNewGC 来对年老代和新生代进行垃圾回收
			>回收器组合2：+UseG1GC +MaxGCPauseMills=200
			>G1回收器：物理分区：Region(1M-32M)(Xmx/2048个) 每个Region内存分配最小单位Card(512B)   参考https://www.cnblogs.com/oldtrafford/p/6883796.html
				       被引用集合: points-into; RSET 一个RSET记录了一个Region中那些被其他对象(可在其他Region)引用的对象构成的一个个引用关系。在垃圾回收时查看可达对象就查看RSET而不是全表扫描
					   引用集合：points-out; Card Table。。CSET。
					   回收策略：先收集垃圾对象最多的分区
					   回收过程：可以认为同CMS的4阶段--但更多-如根分区扫描。仅仅是具体做法因为物理分区的不同而不同。
					 --优势：精细管理，随时间发展内存碎片更少利用率更高；区块为单位并行回收提高效率；内存压缩。https://www.cnblogs.com/oldtrafford/p/6883796.html
				>Young代的回收过程：STW方式，并发多线程地，年轻代存活的对象(多个年轻代Region包括S0区)拷贝到S1区 和O区。重新计算E区和S区的大小。
				>Old代的回收过程：STW根标记存活区-->并发扫描S区对O区的引用集合和被引用集合-->并发标记全堆可达对象()-->STW重新标记(会清除全部对象都失效的Region， STAB算法)--->STW拷贝/清理(先复制活跃度低的N区和O区的可达对象到没使用的区域，再清理被复制的区域)。(暂停时间决定要回收的region数量)。复制之后有内存压缩，释放内存。
		>各代使用查看、回收查看、实例查看、线程状态查看(进程的线程数、进程的线程资源占用)：
			>关注：参考：《深入理解Java虚拟机》
			 >内存泄露？(不使用而被根引用到/长周期对象引用到的对象占用的空间无法回收;短周期对象即时关闭和置为null)内存垃圾old代回收次数太多、均次耗时大？(STW)(jmap -histo:live pid | sort -n -r -k 3 | head -10)(jstat -gcutil pid) 泄露查看工具leakcanary
				>jcmd:
				>pmap:
				>系统层面:gperftools/strace/gdp/dump/strings/btrace
				>
			 >线程泄露？线程死锁？(ps -hH pid | wc -l) (jstack -l pid)系统线程数(cat /proc/pid/status)
			  >各方法执行次数\执行时间排序：
		>堆大小限制：32G以上，指针不能做压缩。
	  >零拷贝：使用DMA将磁盘数据放到内核态数据区后直接copy放到网络设备上，而不是先copy到用户态的数据区、切换到用户态、什么一般指令也没做就切换到内核态继续执行特权指令--将数据从用户态数据区cp到网络设备上。
		>以将磁盘上的一个文件发送到网络上(协议缓冲区)为例：普通的read()函数会用DMA技术把数据从磁盘读到内核态数据区再复制到用户态数据区，socket的write()函数也是如此：先写到内核态socket buffer 再从这个buffer复制到protocol engine 。mmap read()时则只切换到用户态--但数据仍然在内核态数据区kernel buffer --但是是共享的--用户态也可以访问。而sendFile()函数则直接DMA方式将数据从磁盘读取到内核态之后顺带立刻CPU copy方式将数据复制到socket buffer ，再DMA copy方式将socket buffer里的数据复制到protocol engine。。sendFile 2.4以后优化了，直接DMA将文件从磁盘复制到内核态数据区，之后复制一些标记位、描述符到socket buffer(从这个os复制层次上看，就几乎没有复制，所以说是零复制),再DMA copy将数据从socket buffer 复制到 protocol engine 
			>普通先读后写：4次复制、4次上下文(内核态/用户态)切换。 DMA copy 和cpu copy两种，只有。
			>mmap内存映射：3次复制、4次上下文切换 。。rockerMQ。。小数据量读写适合。
			>sendFile2.1： 3次复制、2次上下文切换 。。kafka。。大文件传输适合。。Channel的transferFrom(), transferTo()入参有socket，即底层是调用的senfFile函数。
			>sendFile2.4:  2次复制、2次上下文切换
		---参考：https://www.jianshu.com/p/275602182f39
	  >伪共享：cpu1更新了X但是没有再读X, 导致X所在的缓存行失效---而又因没读导致cpu1所在L3中的X所在的缓存行是失效的，---如果该缓存行里有Y变量被其他线程所在cpu2操作，那么cpu2就只能从主内存中取---导致效率不高。---伪共享问题。，和缓存行有关---cpu中L1L2缓存没有失效--没有因为其他cpu的更新而失效。	
		>问题产生：常常在多个long,int基础变量使用时，它们被放入了同一个缓存行。相反，如果每个变量单独放在一个缓存行，那么就互相不影响，因为只是一个缓存行的失效，不会影响其他缓存行失效--而重新去主存里拿。(disruptor这个内存消息队列就避免了伪共享--避免了重新去主存加载)
		>基本概念：缓存行(cpu缓存的基本单位，L1/L2/L3级中)(四种状态：修改-无效-专有-共享；最开始：专有E-->后多个核都有了变为共享S-->一个核修改了该核的该缓存行变为M修改其他核上的该缓存行变为I失效)(64Byte)
			>cpu加载数据：按缓存行加载，加载到L1/L2/L3。
			>缓存行共享导致：两个核轮番争抢所有权来更新缓存行 带来大量的RFO消息--使对方的缓存行失效，使得从新从L2/L3/主存上读取，竞争冲突带来低效率。此时的糟糕的共享就是伪共享，解决方法就是不冲突-即让一个缓存行只存一个变量-剩下的空间为构造的假数据--填充假数据。。。。例子：某个8byte变量a我不希望它被放入其他缓存行--而让它单独为一个缓存行---那么就定义的时候多定义7个同类型的无用变量，则cpu加载的时候就加载这8个数据---实际有效的就我那个a,jdk1.8用@Content注解在一个类上---上里面的所有属性都是独占一个缓存行的。--缺点是消耗缓存行。适合并发写频繁的场景。如ConcurrentHashMap中每个Node下的键值对数目。
		--参考：https://www.jianshu.com/p/a4358d39adac
#网络方面：
	#https协议：http协议 + ssl/tls协议： 最佳总结https://www.jianshu.com/p/14cd2c9d2cd2
		>https请求过程：前奏：客户端把独立生成的对称加密密钥client key  加密传输给服务端，服务端解密后得到后，服务端客户端就可以用这个对称密钥来加密解密了。  加密：是从服务端获取的公钥，解密，是服务端自己的私钥。
				客户端： 请求服务端                     公钥合法(证书有效),则独立生成一个client key即密钥，并用公钥来加密这个client key , 将结果发送给服务端																																																														客户端用client key解密得到内容即可。										
				服务端：              返回公钥(证书) 																											使用私钥解密该结果，得到client key 。这个client key就是双方进行通信的对称加密密钥(保证通过网络也只有客户端和服务端知道) ::从而双方可以用这个client key进行加密通信了。即服务端就当作对称密钥来对称加密要发送回客户端的内容，将结果发送给客户端。

		>http通信模型：请求-响应：一个请求：用 url + 报头 + 报文 表达，  一个响应：用 报头 + 报文   。 报头说明请求类型、报文类型、请求参数、参数长度、cookie信息
		>http/2: 多个相同域名的http请求并用一个tcp连接。2015年后。
		>证明B收到的信息肯定来自于A(保证信息没有被修改)：则只需要A用自己的私钥加密信息传给B，而B用A的公钥解密。可以解开得出合理信息则说明肯定来自于A（因为中间人不可能知道A的私钥，所以解开后无法再加密）
			>但直接加密“信息”本身则耗时，一般是加密“信息”的hash值，得到的就叫做数字签名。同理，将“数字签名”解密之后，得到的就是hash值---和将收到的信息进行hash后的结果比较。则可以确定来自于A。(因为中间人无法生成正确的数字签名)(即保证信息没有被修改)。(但无法保证信息没有泄露)
		>证明B收到的信息没有泄露(没有被中间人看到)：只需要A传输时用B的公钥对内容进行加密。(中间人没有私钥，所以无法解开)
			>因此传输过程：1.用B的公钥加密(文件+数字证书)发送给B(则中间人不能解开-无B的私钥，也不能调包-无A的私钥)(但条件是AB双方事先知道对方的公钥)。。就可以保证信息未被修改未泄露。
			>公钥获取正确的解决办法：因为公钥允许泄露，保证确实是B的即可；B的公钥作为信息，用另一个私钥X加密，传递给A；A用X对应的公钥解开---来获得B的公钥。。。而A要事先知道的X对应的公钥---就是CA中心的公钥。仍然两个问题：数字证书如何被服务端获取到；CA中心的公钥如何被客户端获取到(客户端存CA中心的根证书，则可以验证所有分中心下级中心颁发的证书)(根证书可靠：windows也预置了50多个根证书)(各个具体软件.exe属性可以查看数字证书：即该软件所在公司从CA中心申请的证书)。
			--传自己私钥加密的内容：保证来源。(传第三方私钥加密的内容：也可以保证来源。比如传CA中心的私钥加密后的内容：数字证书)(自己私钥加密的内容：只叫数字签名)
			--传对方公钥加密的内容：保证不泄露。
			
			
	#tcp/ip三次连接而不是2次的原因：(提问式反问式 找设计的理由找设计的原因式理解，而不是认识式定义式理解；；设计的原因才能让人真正理解，，而不是只是知道是什么；更要知道为什么要这样设计为什么不这样设计---因为这才是真正最透彻深刻的理解；否则是肤浅的)(这么设计目的：一是为了实现功能、二是为了避免带来新问题新不确定、三是一种解决不可避免的恶劣情况导致的功能故障的方案(悲观考虑必须、意外情况必须--不能只考虑正常情况的处理----意外情况往往就能改变设计思路局部甚至整体---导致设计的结果直观上不太容易理解--毕竟离正常情况已经有点远了-不纯粹只是为了处理正常情况而设计--这个毕竟也是不强大的))
		>服务端不能收到信息就确认建立新连接，而开始接收处理实际真正要传的数据。因为客户端可能会多次请求--没有回应就会重试。所以需要客户端确认才可以。即：客户端发起连接请求(标记为X: SYN=1 Seq=X)->服务端响应(标记为X+Y: SYN=1 ACK=X+1 Seq=Y)返回-->客户端确认(Y+Z: ACK=Y+1 Seq=Z)发送-->此时服务端才确认建立连接(ACK确认的是上一次的Seq)(说明：1.当服务端返回时--客户端就可以确认网络通了，而服务端收到客户端的确认时--就可以确认客户端也收到“我的确认”了；过程非常类似https请求--加密传递密钥)(客户端：经历“发-收”；服务端：经历“发-收”。双端都经历“发-收”(说明双端确认网络“可发可收”)连接建立)。(3次的目的：双端确认网络可以“可发可收”；服务端可以忽略建立后释放前的重试请求---免得浪费资源；客户端也可以忽略建立后释放前的反复确认--同样避免了浪费资源)
		>另一种原因：我说出的话对方能收到：证明的办法就是：对方回应。默认就是+1，来避免弄错回应顺序。
	#tcp/ip四次释放连接而不是2次的原因：首先释放连接的原因：--本方已经接收数据完毕--不用再连接了；这个消息要让对方知道-所以要发送结束报文(FIN=1 ACK=Z Seq=X)给对方，当然也要得到对方的收到确认(ACK=X+1 Seq=Z)(否则认为对方没有收到而继续发-此为需要接收确认的重要性)；同理，对方接收数据完毕，而需要告知对方(否则对方可能认为连接中断而重试重发)(发送FIN=1 ACK=X Seq=Y) , 也要接受到对方的确认为止(对方发送ACK=Y Seq=X)(否则认为对方没收到而继续发送)。
		>原因再次：接收完毕--响应确认(不再重复发送)。双方的数据发送是异步的，没有谁先谁后，结束也是。对方接收完毕，不代表我方接收完毕。
	#socket通信的底层实现：(表层socket,底层tcp/ip指令)先创建文件描述符FileDescriptor再创建Socket，创建SocketInputStream，SocketInputStream.socketRead0(FileDescriptor fd, byte[] b, int offset, int len, int timeout)这个native本地方法。根据文件描述符读取若干个字节到byte[]里面，有超时设置。
		>BSD Socket: 不同的系统的socket都是对这个规范的实现。
		  >区分两个Socket连接：五元组。2组端口-ip 和协议。
		  >本地端口绑定：如果是192.168.0.1:21那么同一个端口还可以绑定其他本地ip比如10.0.0.1:21 但是如果一个socket绑定了0.0.0.0:21那么这个端口就不能再绑定其他ip，会报错；即被独占了。
		>文件描述符的创建：文件描述符相关的还有它的使用次数--这个也会统计进来。	
		  >标准输入输出异常流也有文件描述符：handle=0/1/2
		>Nagle缓冲算法：发送方的缓存和接收方的缓存。
		>type-of-service类型：
		>SO_KEEPALIVE: 一个socket在不活跃2h后，会发送一个检测报文给对方，根据返回的ACK正常/RST重启复位/无响应 来判断连接是否正常而需要继续维持。正常则重置计数器，2h后再发送。复位则关闭自己的连接。无响应则75后重发9次；长时间后才关闭。不推荐使用。因为只能判断连接是否存活，不能判断上面的服务是否可用，因此用户要加自己的心跳检测。
		>SO_SNDBUF: 发送缓冲区大小
		>SO_RCVBUF: 接收缓存区大小
		>SO_LINGER延时关闭：close()执行但是send buffer没有发送完毕还可以继续发送，直到达到SO_LINGER的时间，然后发送一个TCP RST报文段终止连接、复位连接。
		>SO_REUSEADDR: 第一种：同一个服务器的不同实例允许在同一个端口上启动 ，只要每个实例绑定不同本地ip地址---多网卡即可。第二种：一个SOCKET处于TIME_WAIT状态，另一个SOCKET可以绑定它的IP端口而启动，这个最常用--仅仅为了避免服务器重启时端口未释放而无法绑定的问题。(udp连接甚至允许多个套接字绑定相同ip-port)。参考https://www.cnblogs.com/qiaoconglovelife/p/5416715.html
		>tcpdump:抓包工具，比如查看tcp rst报文段。终止连接报文段。
		>四次挥手中server-client状态变化图：主动结束端最后又一个TIME_WAIT时间等待---以便如果被动结束端未收到最后一次ACK而重新发起不再接收报文能够被接收，最后才是CLOSE。否则没有TIME_WAIT就会直接返回RST报文了。
			>服务端一个SOCKET在TIME_WAIT状态时：可以启动另一个SOCKT复用这个SOCKET的端口和ip, 只要配置了SO_REUSEADDR选项。
			 >主动关闭方在发送最后一个ACK之后：处于TIME_WAIT状态，时长30s/2min。。目的：如果被动关闭方没有收到ACK(路由器发生了迷途)而重新发送FIN时好再次重新发送ACK。提高tcp可靠性。TCP不复用处于这个状态的TIME_WAIT的socket.
	#java安全体系：SecurityManager， AccessController.doPrivileged()独立跑一个方法
	 >语言安全：编译时验证、加载时验证、
	 >密码体系：数字签名和信息摘要的API(对称加密和非对称加密算法、密钥交换协议DH-----利用(g^a mod p)^b mod p = (g^b mod p)^a mod p 实现--其中a,b双方私有；使用对方传递过来的第一次取模值用自己的私有值作底再次指数求值后取模)
	 >安全通信：认证机制：SSL\CAS协议\kerberos
	  >kerberos: client---KDS: client请求传给KDC， KDC生成会话密钥，用客户端的主密钥加密，另一份会话密钥用自己的密钥加密，一同传递给客户端；客户端解密得到会话密钥，用会话密钥加密server请求得结果+刚从KDC得到的另一份数据一同传递给KDC, KDC用自己主密钥解开得会话密钥，再用会话密钥解开得server信息，再生成一个会话密钥，用client和server主密钥加密，传递给用户。用户解开得到会话密钥，用来加密。。
	   >本质：通过第三方而安全地传递会话密钥的过程。--不用知道对方主密钥。只需要：第三方知道通信双方的主密钥并负责生成会话密钥即可。属于对称加密第三方生成的会话密钥方式的安全通信。https则时属于非对称加密客户端生成的会话密钥方式的安全通信。
	 >数字证书：数字证书对象
	#DNS过程：从host到本地DNS服务器到代理DNS服务器到询问13台根服务器
	#ip地址：IP地址是标识网络上每一台主机的id---点分十进制-4字节构成。由网络号+主机号构成。
		>地址类别：按网络号字节数1-3 和 1字节网络号的多播地址, 或者按网络号高到低首0位 而划分4类：A类：1B netnumber + 3Bhost number  网络号：0NNNNNNN 首位确定为0。同理B类：10NNNNNN + 2Bhost numbers C类：110NNNNN + 1B host numbers D类1110MMMM   组播地址  E类 11110 用于实验和将来使用。C类最大取223也就是做大的非组播的首位地址。D类地址的最大值是239也没有到最大值。E类地址是240-255
			>某类网络的网络数：2^可变网络位数  
			>某个网络的主机数：2^主机号位数 - 2
			>某个网络的子网掩码：只有一个作用：对一个IP地址划分出网络地址和主机地址--确定网络号的具体取值。也是一个点分十进制数---网络号.0...0 即网络号不变而主机号每位都是0。。子网掩码 & ip地址 = 网络号  
			  >更好的划分方法：CIDR 记录一个IP地址的子网掩码位数(主机号位数) 192.168.111.124/23 则有23位网络号，网段/网络号就是192.168.110.0/23
			>某类网络的私网地址：每类网络都选择了一个网址区间作为私网地址空间：比如A类：10.x.x.x  B类172.16.x.x - 172.31.x.x  C类192.168.x.x  
			>公共IP地址：公网IP地址,世界唯一，即ABC三类地址中除了私网地址之外的地址。
			>不清楚的主机和目的网络：0.0.0.0
			>本网段内的所有主机：255.255.255.255 即只在本网段内广播，而不会广播到其他网段中去。限制广播地址。
			>本机回环地址：127.0.0.1 - 127.255.255.254 主要用于测试，在传输介质上永远不会出现的地址。
			>组播地址：用于特定的程序和多媒体程序 D类地址。224.x.x.x - 239.x.x.x  224.0.0.1所有主机，224.0.0.2所有路由器 224.0.0.5是所有OSPF路由器。
			>DHCP配置而又无法从DHCP获取地址，则会分配这样的地址。169.254.x.x
		    >.0和.255结束的地址都是保留地址。
			--一个ip地址：前4位定网络类型，前-'A'+1字节为网络号，剩下的是主机号。但是实际上，还会从主机号里抠出前k位出来作为子网号，剩下的才是真正的主机号。子网掩码和ip地址相与 就得到 子网地址。
			-- 五类地址之间：没有空隙。
		>子网划分：一个IP地址有网络号，扩充这个网络号位数，占用主机号n位，构成的新的2^n个网络号 就是 子网络号。
		    >物理上的划分表现：
		>路由算法：
			>一个路由设备：处在多个网络中，一个端口处于一个网络，一个端口有一个ip地址，所以有多个ip地址。某个端口处于internet互联网中，ip地址就是公网地址。一个网络 用网络号 + 网络中的所有主机ip地址来完全表达。
			>超网：将多个不同网段的子网络ip地址映射位一个囊括不同网段进来的子网掩码更短的相同的ip地址。即 有一个端口 时一个不同网段的ip地址集合。//子网络组成一个超网络。//忽视小创新就是懒。
			>路由通信：
			 >主机路由：目的ip地址是一个具体主机IP号
			 >网络路由：目的IP地址是一个网段而不是一个具体的主机iP地址。//比如要发送到某个网段//根据网络号路由(不根据子网号和主机号)
			 >默认路由：主机直接连接的路由器对主机的路由。
			 >转发的根据-路由表：目的地址(网络号ID)-子网掩码-接口(发送的接口)-网关(目标地址不在本路由器所在的各个网络中，本接口连接到的下一个路由器的接口的ip)
			   >一个路由器连接的是不同的网络：不同的子网。即路由器的各个端口处在的子网网络号是不一样的。
		>DHCP过程：
		
	#网络读写、磁盘读写、内存读写：
	#线程管理、线程同步、线程异步：
#网络设备单点问题：(单点问题，不可用)
	>集群：
	>主备：
	>主从：(从节点宕机不可用；检测，下线)(主节点宕机：检测，重选)
	  >主的选举：一个选举周期内:  对于选的非自己---收：增、同  ； 发：广、定   。。对于选的是自己---收：增、定 ； 发： 广、同。
		>zk的类似fast-paxos算法：关键在第二阶段的时候，如果同时有n个节点进入了第二阶段的主发，那么收到消息的accepter节点原子更新自己的select和自增zid, 从而只有一个主发节点成功，其他主发节点则失败，收到超过一般的二阶段返回则更新自己为Leading开始广播---没有超过一般的则重新开始--显然二阶段结束后只有一个主发超过一半，其他则又从第一阶段开始；显然这个时候成功节点已经广播leading消息过来了---收到者全部更新自己的leader为它。(一阶段同时成功者可以小一半，但二阶段成功者却只有一个)
		 >简略表达：选举开始：都是looking状态，选举期间，
		     第一阶段：做两件事情：1.广播自己的选票；2.回复别人广播过来的选票，回复方式两种：1vl单点回复(比自己的优先级小，回复自己的选举)、1vn更新广播(比自己的优先级大，更新自己的选举后广播)
			 第二阶段：做一件事情：1.更新自己的状态：收到全部服务器的选票，直接更新自己的状态为leading或者following; 退出选举。没有收到全部服务器的选票，则再等200ms,没有数据到，且投票的leader超过半数投票，则更新自己的状态为leading 或者following，退出选举。
			-----优先级比较先后：时钟--->事务id--->服务器id
			-----fastleader算法
		>paxos算法：分布式系统一致性问题的共识算法。
			>目标：在一个集群内快速地对某个变量的值达成一致。使得集群中各个节点上的某个变量的值一致的 “读写节点动作”构成的一个流程算法。
				>需求场景：多个进程独立并行的向一个集群(全部节点)更新某个变量的值/提交某个变量的值，(如果什么也不做，直接让写进程更新，最后这个集群里各个节点的值就几乎绝对不一致，一些节点是这种值一些是那种值)如何保证尽快的更新结束，且集群中节点的该变量的值都一定是某个提交值即值都一致(只要每个节点网络都正常无延迟)(而不是一个节点这个值一个节点那个值, 出现了不一致的情形)。每次更新前，这个值肯定是失效的或者是null:尤其对选举的leader变量而言。
				>读取集群的各节点的某变量值：只要读取成功超过一半，那么就能读取到最新的更新值(因为至少能读出一个最大编号的值--最大编号就是最后一次成功超半数更新的组合)。
				>写入集群的各节点的某变量值：只要写入成功超过一半，那么此时读集群成功超过一半则一定可以读出至少一个成功更新的节点的值，所以可以认为写入成功了。否则假设是糟糕的场景，每个接收者都更新为不同的值--集合为V1，那么下一轮 开始，从第一轮读取最新值时，那么某次读成功超过一半得到最新值，这个最新值只有接收节点数/2即V1/2种可能，即大大减少了，那么再次更新回集群中，即便还是没有成功超过一半，那么现在集群中的值可能也只有V1/2种；然后开始第三轮 读取-更新，同理，之后即便没有成功，集群中的值范围 也会进一步缩小一半，...如此，V1/2^k < 2 时就够了，所以总共： k次，V1 < 2^(k+1), logV1 < k+1, logV1-1 < k,则k=logV1即可。 当V1/2^k=2时，则需要k+1次，即logV1=k+1, 因此无论哪种，所需更新次数(第二阶段执行轮数)都是：logV1向下取整--实际按照滑动窗口数看：是logV1向上取整。条件是每个节点每次都被更新。
					>读取成功超过一半，其实可以 实际 操作为：只读 超过一般的节点即可。实际上都是全部广播
					>利用大多数机制保证了2F+1个节点的系统有最多容忍F个节点故障的能力。实际实现都是全部广播
				>最重要的特征：当某个proposal A写入成功超过一半的那个瞬间：(即便proposal还没有收到这个消息)(把时间冻结来看这个画面)所有处于第二阶段的其他proposal的k值  一定是小于这个proposal A的--(证明：同时处于第二阶段的proposal的k值肯定互相不一样，且后进入第二阶段的要大；而进入第二阶段，肯定已经更新了超过半数的Acceptor的minProposal/maxN值, 而第二阶段提交能超过一半成功，则k值必须大于超过一半的minProposal/maxN---而这超过一半的Acceptor的maxN集合必然就包含至少一个其他proposal的k值，因此A的k值要大于proposal的k值), 既然小于，那么这一瞬间/时刻之后，这些处于第二阶段的其他的Proposal这次的更新都会失败--因为k比超过一半的Acceptor的maxN都小--从而都拒绝更新(尽管可能更新掉剩下的几个Acceptor)，再看这一瞬间之前：显然不存在有proposal处于读取了超过半数但还没有进入第二阶段的----因为读取成功一定会修改maxN的值--从而A根本无法更新成功--和条件矛盾，所以这一瞬间之前，所有的处于第一阶段的proposal都没有实质上的读成功(修改超过半数的maxN),要么读了个别，要么还在准备发起请求，请求还在网络上、刚好到达Acceptor,最快也一定在Acceptor同步更新maxN/acceptV之前;那么即便考虑这种最快，使得这一瞬间之后，maxN立刻被更新为了更大的值，那么这样的proposal读取到的大多数中最大的acceptN也一定是A更新后的值，从而即便它们再次进入第二阶段进行更新，也是把Acceptor的值更新为A的提案值。
					>所以只要更新超过一半：那么这个Proposal A可以不慌不忙的通知其他Proposal更新成功了，通知Learner 该变量的值一致化成功了。
				>为什么不单次提交：1pc?因为单次提交，会使得后面的提交直接覆盖前面的提交-且值不一样。	
			>缺陷：可能形成活锁，永远不会更新超过一半---所以需要加个时限。就是因为两个proposal依次循环更新某个Acceptor的minProposal值/maxN值，使得它对每次来的第二次提交都拒绝--而一直没有接受哪个提议值，导致了两个proposal一直循环提交没有大多数更新而终止。
				--所以basic paxos算法不能应用在实际工程中。
			>内容：2pc两阶段提交。K; maxN  , accpectN acceptV ;  一：向超过半数发送K(不必全部), 超半数则区结果中最大N的V ,二：构造K-V发送，收到 超半数，成功选举。接受方改AcceptN=K, AcceptV=V
			>本质2点规则(看作是一个更新值的博弈竞争游戏)：1.超半数成功才算成功(所以一轮成功最多只有一个)；2.连续2次成功则胜利(一次读一次写)。前面是判胜规则，还有条件信息：竞争方proposer多个，处理方acceptor多个，竞争方发起读或者写给处理方，处理方回应竞争方。
		>multi-paxos: proposal集合选举一个proposal为leader, 使用basic paxos即可，然后leader进行prepare-proposal-proposal....即因为没有竞争，所以只需要一次prepare,后面就可以直接一直proposal提交提案了，对于连续的更新，效率就会高很多。如果leader挂了则重新选举即可。
			
	  >有效读：zk可以从follower读取，但kafka只能从leader读取。
	  >有效写：同步数据时，从节点至少一半返回成功。
	>分布式系统理论：
		>一致性问题：
			>CAP理论：分布式系统的数据一致性、可用性、分区容忍性；分区性肯定保留，一致性要求节点之间要通过网络进行同步保持一致-显然阻塞耗时-，可用性要求系统快速响应-可用性不关心节点数据是否一致，从而一致性和可用性是矛盾的、反相关关系的。
				>更新成功X后立刻 并行的同时从分布式系统读取某个变量X：每个线程 读取到的值都是一致的，则满足一致性。(没有更新时)如果每个线程是马上能读取到 则满足可用性。分布系统不能保证马上都能读取到 且 都一致 这两个特征同时满足。
				>P:容忍进行分区。参考：https://www.cnblogs.com/mingorun/p/11025538.html
				>重新定义：
					>C: 某个节点成功更新了数据，其他节点也能读出数据，强一致。
					>A: 合理的时间返回合理的响应。而不是被阻塞和拒绝请求。
					>P: 分区容错性。
					--浓缩为一句话：只要进行了分区：为了保证一致性，必须拒绝请求，这样就影响了可用性。
						>实际选择：CP架构或者AP架构。
							>一般互联网项目：AP +BASE补偿一致性。BASE的补偿：基本可用：函数降级；软状态：中间状态不一致状态，最终一致性：经过一段时间之后数据一致。
							>金融系统项目：CP + ACID加强。不仅等待一个分布式系统的数据一致化完成，还使用分布式事务让对多个分布式系统进行操作的操作序列有事务的ACID特征：如可以回滚等来加强多个分布式系统作为一个整体的数据之间的一致性。
					---参考资料：https://developer.51cto.com/art/201808/581174.htm
			>BASE方案：而不是ACID方案。是承认上述矛盾而采取的分布式数据方案，来满足需求--这种需求就是更重可用性和分区性。基本可用，软状态。是一种对分布式系统CAP特性现实的一种权衡方案，是在承认CAP之间的制约矛盾下，系统基本可用、可分区而牺牲节点数据短暂不一致但确保最终一致的方案， 即存在不一致的状态，但时间极端。
				>基本可用：出了问题服务降级。(ACID要求出了问题则不能用且回滚)
				>软状态：允许异步。异步更新。最终一致。(ACID要求同步更新 且全部更新)
			>数据一致性类型：参考https://coolshell.cn/articles/10910.html
			 >弱一致性：写入新值后，读操作在副本上可能读出来或者读不出来。/异步执行成功或者抛弃。
			 >最终一致性：写入新值后，立即读可能读不出来，但在一个时间窗口后，保证能读出来。如电子邮件(写A发/B收读)。/带有异步特征，一定会做。
			 >强一致性：写入新值后，立即能够从任意副本读出来。如RDBMS这种系统。/带有同步特征。(返回成功则说明连副本都写入成功了)
			 ----例子：
			 >Master-Slave结构：主从结构，主同步给从， 可以异步/同步， 可以slave拉或者主push；这种异步则提供的是最终一致性。。写失败的标准：如果是异步，则没有。因此master挂了则写数据客户端必须认为失败了；如果要强一致，那么master先写自己log，成功后写slave,也成功了则返回成功，失败了则回滚自己，返回失败。
			 >Master-Master结构：不佳
			 >2PC: 协调者询问可提交？                        协调者返回提交                       协调者如果收到失败，发送回滚。
						             节点锁定资源，返回确认                  (如果未收到提交，则不知所措)节点提交，返回成功
			 >3PC: 协调者询问可提交？                           协调者返回预提交   						 协调者返回提交
									 节点直接返回可以，不锁资源                   节点锁定资源，返回确认                 (如果超时或失败没有收到提交，则自己提交)节点提交并且返回已经提交
			 >Paxos算法：集群节点存有一个键值，现在想更新这个键的值，
			   >最好的原理理解：一个proposer只要连续两轮竞争获得超半数票同意，则会成功修改超过半数的acceptor的值为新的值。从而紧接在该proposer成功后有proposor也连续两轮获得超半数票而成功修改---修改为的值也是第一个propersor修改为的值----因为第二个proposor在第一阶段获取的最大acceptN的acceptV就是第一个proposor更新的值。所以第二个proposor只是能够让更多acceptor的值更新为第一个proposor修改的值而已。同理第n个proposor，直到整个选举过程结束。
								 成功之后，proposor可以通知其他proposor不必再次发起投票了， 也可以通知所有leaner选举结束了，新的值就是xxx。
					>每轮竞争特征：即每轮发起提案的特征：成功的proposor都是时间顺序有间隔出现的--绝对不会同时出现，最多只有一个proposor获得超过半数票。如果一个proposor成功了，那么其他proposor失败了，下一轮则失败proposor要增加K值，而成功的那个proposor不用，继续一同竞争，如果第一次成功的那个proposor再次成功了--说明它已经修改了超过半数的acceptor，那么继续竞争修改就会都是这个值，所以选举可以结束了，更新可以结束了。
					>最后修改为的值：不定。//错误观点：就是第二轮成功之前最近一次第二轮失败时更新为的值。
					>编号的设计和高编号可以覆盖底编号的设计：最重要的效果是区分数据版本，以在第二阶段多次失败后都能够一致的认为/约定再次在第一阶段成功后读到的值那个值是最合理的最新的。
					>两阶段的设计原因：因为有个计算acceptor的最大acceptN的值而选择确定 这个获取集群最新值 然后再拿这个值去更新集群的两个步骤的不可少的过程。
					>编号设计导致的对acceptor判断的自然要求：第一阶段：K > MaxN 从而失败者必须要自增K才能被接收；第二阶段：K>= MaxN 因为第一阶段成功的proposor发送来的K还是上一次的K---而此时超半数的几点的MaxN已经是K。所以要有等于。
			  >NWR模型：W + R > N (可以保证读取的是最新的值)
			   >更新策略：同步更新(更新全部)、异步更新(无需等待更新，直接返回)、更新部分节点N+W>R(强一致性)(最终一致性：N+W<R)	
				>后两种的问题：脑裂。
			   >矢量钟： 
			  >分布式事务：一个事务里的操作不在一个节点上，而是多个节点上，每个节点都会操作数据库，这导致操作数据库的会话不是一个，而是多个，而传统的事务只能在一个会话里创建和提交/回滚。典型的微服务调用过程就是。
			   >需求背景：每个微服务都是一个分布式系统，而某个上级的数据聚合微服务则 会调用多个微服务--即调用多个分布式系统，都有进行读写操作。而一个功能就是一串调用这些微服务的操作序列，显然是要求这个序列具有事务的ACID特征，而这些操作实际又是在不同的分布式系统的不同节点上执行的，所以不是用一个数据库的本地事务可以包裹的操作序列集合，所以叫分布式事务。这是最常见的场景。分布式事务出现的最常见场景。
					>本质特征：一个操作序列里的操作不止操作一个节点上的数据。
    		   >例子：主服务调用了A服务和B服务，两个服务各自处理自己的数据，甚至数据库不一样；要求对A和B的调用在一个事务里--即满足“四大特性”的先后调用A和B构成的两个操作。
 			   >简略表达：事务模型：事务调度器内部调度多个事务，外部提供一个整体事务。对外打开一个事务，内部打开多个事务(且每个事务在不同的节点上打开的)。
				 >2pc: 本质是 上述事务调度器的实现整体事务的策略/协议(XA分布式事务协议的2pc实现)：询问节点打开事务prepare--->节点回禀--->命令提交global_commit--->节点提交并回禀。。。异常流程，则在第一次节点回禀时发现不是所有节点都准备好，则发送回滚global_rollback--->节点回滚并回禀--->协调器返回用户事务失败。。。可见特征是：各个节点开启事务，等待协调器提交或者回滚。
				  >问题：节点开启事务到提交事务长时间占用数据库资源；协调者单点故障导致节点事务未释放问题；局部网络问题导致协调者提交请求有的节点没收到。 ---自然的，需要引入超时机制--节点没收到请求自动回滚，协调者没收到回禀发送回滚。--2pc的协调器也有超时机制。
				 >3pc: 本质上 多了节点等待命令超时取消事务 两个功能。同时在询问节点打开事务之前多做了一件事--查询节点健康状态，协调器--->询问节点是否有能力处理事务(节点健康状态检查，事务尝试，未开启事务)--->都有能力则发送预提交否则abort命令，后面的过程类似2pc
				  >遗留问题：数据不一致的问题。
				 -------所以从2pc的过程上看，类似zk的fastselection算法：1.广播自己的选票，处理收到的选票后不更新单点返回或者更新后广播。2.更新自己的状态。(两个阶段3件事)
										    也类似paxos算法：1.读最新值(超半数读取成功而返回算成功) 2. 更新最新值(超半数更新成功并返回)
				 >TCC补偿事务：关键就是没有了协调器。try - confirm - cancel  尝试(操作)--确认(操作)--补偿(取消)
				  >问题：应用侵入性强。
				  >补偿：就是加1之后如果失败或者其他调用失败而调用回滚，那么
				 >本地消息表：是BASE实现，应用在一致性要求不高的场景。
					>服务A将消息和修改的数据先分别写到同一个数据库：用本地事务保证原子性。消息表和数据表。
					>再定时任务读取消息发送到消息队列：
					>另一个服务B读取消息：写本地事务表，后修改相关数据，修改成功后更新事务表中数据状态；
					>服务B将消息发送给服务A让A去更新消息表，或者直接更新消息表数据状态。
				 >RocketMQ来异步解耦：实际上是对本地消息表的一个封装--将本地消息表转移到了MQ内部。更好的实现数据的一致性。事务消息。
				  >最简略的表达：时序图。Broker注册到NameServ--对应topic, ------> Producer请求NameServ某个topic的Broker：-----> 然后直接和Broker通信：发送消息给它 ---->  Broker主从同步、数据落盘--->consumer建立与broker的连接，拉消息或者broker推送消息。
				   >部署：NameServ 3台； Broker多主多从。
				 >Saga事务：反向补充。
				>定义：如果一个分布式存储系统提供了类似单节点存储系统上提供的事务的功能，就称为分布式事务，而不是会所普通的事务；因为和普通的事务不一样：最大的不一样就是性质上不能实现CAP，采取的妥协就是牺牲强一致性；但是通过paxos等分布式一致性算法可以实现最终一致性(一致的节点数目越来越多)；且通过R+W>N方式来实现各个客户端读取数据是最新且一致。单节点存储系统的事务。
				>定义2：在分布式系统中保证读存的事务性。
				>mvcc: 一种单节点上控制并发读写的办法：条件1：每行数据都有多个版本，先后提交顺序形成版本链；规定2：多个并发来的修改操作按照先后顺序CLH锁方式形成一个修改链--每个链节点有一个事务ID；规定3: 当前事务修改成功后得到一个新的版本数据提交到版本链，下一个事务开始基于这个新的版本做。
			   >重新表达：
				>两阶段提交：
					>概念：
						>协调者：
						>参与者：
					>行为：
						>协调者询问参与者：准备好？事务执行成功？提交/回滚事务？
						>参与者返回执行结果：准备好/没有，执行事务成功/失败， 提交事务/回滚事务
					>规则：
						>首先：协调者询问参与者是否准备好，然后参与者返回好或者没有，如果好了则协调者发送事务执行命令，参与者返回执行成功或者失败，协调者再看如果全部成功则发送提交，否则发送回滚指令，然后参与者执行提交事务或者回滚事务。
					-问题： 
						>同步阻塞：所有参与者等待其他参与者时，不能进行其他操作。
						>协调者单点问题：引起参与者一直阻塞
						>数据不一致：协调者发送的commit有丢失导致。
						>太过保守：任意一个节点失败就会导致整个事务失败，没有完善的容错机制。
				>补偿事务：TCC。对每个操作都要注册一个对应的确认和补偿 操作。
					>规则：三个阶段：冻结-操作-解冻/回滚复原后解冻
						>try: 对业务系统做检测和资源预留。
						>confirm: 对业务系统做确认提交。只有try成功后才会执行。
						>cancel: 在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。
					>优点：流程简单，但数据的一致性要差。
					>缺点：
						>编码角度：应用层补偿。代码更多。
				>本地消息表：异步确保。 
					>规则： 
						>本地消息表和业务数据表处于同一个数据库中: 这样用本地事务可以实现对两个表的操作 满足事务特性。
						>使用消息队列保证最终一致性：
						>过程： 写业务写消息-查消息到消息队列-
							>分布式事务操作的一方写 业务数据 的操作之后，向本地消息表写入 消息：本地事务自然保证 两个写 是原子性的。
							>将本地消息表里的新消息转移到消息队列：转发成功则从消息表中删除。转发失败则重新转发。
							>分布式事务操作的另一方 从消息队列读取到消息：执行消息中的操作，写 业务数据 自己的数据库，成功之后修改 消息表里的消息状态。
					>优点：无分布式事务，实现了最终一致性。
					>缺点：消息表会耦合到业务系统中。
					>场景：某条数据要求在各个节点上都要被更新，但是没有强一致性的要求。
				>可靠消息一致性方案：MQ事务消息 
					>规则： 
						>不用本地消息表：记录到消息里。
						>过程：
							>A系统先发送prepared消息到mq, 成功了然后才执行本地事务，如果也成功了则发送确认消息到mq, 则mq将消息持久化并发送到B, B收到后则执行本地事务，然后成功了则发送消息到mq，确认成功，否则发送回滚消息，则通知给A系统的回调接口，A系统会进行回滚的操作。确认消费就是其实就是确定要不要重新消费。
					>优点：实现了最终一致性。
				>最大努力通知方案：
					>规则： 
						>A系统将消息发送到mq, 而专门有个最大努力通知服务会消费消息，先将消息入库，然后调用服务B,成功了则事务成功了，失败了则重试调用系统B。？？
				>阿里GTS：
					>
			  >分布式会话：
				>将session保存在redis即可：tomcat里甚至可以直接配置。但显然是不太好的。
				>使用spring-session: 在filter阶段就将会话读取或者保存到redis。引入配置类：RedisHttpSessionConfiguration
				>不使用session:使用JWT来存储用户身份，那么每个服务器都可以从redis读取到user的id信息。Java web Token
			  >分布式协调服务：协调是什么：就是让几个对象先后自然而然高效顺序排好队。(协调它们参与的各种活动--在其中有序)
			    >主-备服务器：其实也是一种排队执行的思想，所以可以用zk来协调主备；协调为排队执行。所以协调客户端、协调事务、协调主备服务器。
				>zk可以公平竞争：创建顺序临时节点
				>zk可以非公平竞争：每次竞争创建一个节点：其他客户端轮询检查这个节点znode
				>zk可以更加公平的竞争方式：选举。
				>冲突问题：就是协调的问题
			  >分布式系统的存储：存储形式：都是分片+副本：leader-follower
				>分片：使得自然对请求进行了负载均衡，还能函数降级/服务降级 
				>副本：保证了可用性：一个挂了可以其他副本继续使用。kafka/redis/es/hadoop/hbase/zk
				>同步机制：分片及其副本通过 三种写策略 同步：redis-写成功1个，hadoop/zk写成功超半数 (数据最终一致)
				>高可用机制：分片和副本存放在不同机房不同机架上；主分片挂了 通过选举服务 从副本中选出一个 主分片。kafka: 低版本通过zk, zk: 自带选举(paxos选举, multi-paxos提交更新)
			  >为什么和如何设计一个高并发系统：https://www.cnblogs.com/lezon1995/p/11220955.html
				>原因：数据库只能支撑每秒2000-3000的访问量。而推算知道，淘宝正常每天每秒大概：1w-5w的调用量。双11时每秒10w-60w的调用量/订单提交量。
				>高并发架构：从数据库开始拆分：分表分库分片分节点分机架机房地域
					>数据库：按不同业务而分表分库分节点，同一个业务同一个表可以分片分节点分机架机房地域。同一个表的数据还做预加载、服务里加布隆过滤器辅助判断。如果接受了变更消息那么还可以在写数据成功后写缓存！！！redis轻松单机几万并发---高并发读一定用缓存。高并发写--用消息。
						>分库分表：对不同业务。对同一个业务的同一张表，也可以拆分到不同的表里，减少sql压力。
						>读写分离：mysql主从架构部署。主库写，从库读。提高并发量。从库的多少取决读流量的多少。
						-分布式数据库：数据量如果在分库分表后还是不够用，那么就分布式数据库来处理。
					>业务：拆分为多个服务，每个服务做集群，对其他服务做微服务调用。对微服务做保护(限流/缓存/熔断降级)和治理(注册发现/变更下发/监控统计/容灾备份)。
						>高并发读：redis来缓存承载。单机几万承载。
						>高并发写：如果用redis:则不能保证事务，而且数据容易丢失，而且LRU可能生效。数据格式也简单不方便使用。所以用MQ,单机几万可以承载。业务异步写，先写成功到MQ即可，然后用并发多个job/其他服务系统接消息消费后写到数据库(还可以分库分表写)
						-高并发而结果简单的查询统计：es来承载。因为es本身分布式部署，随便扩容。
						-海量数据的承载：hbase
			  >传统的高可用分布式系统：主备：master-slave 。数据是一块。	
			  >继续融合：分布式事务和分布式锁，cap	
				>分布式事务：(一个分布式系统内不同分片的分布式事务，和跨多个分布式系统的每个系统单个分片的分布式事务)事务的所有操作不只在一个节点上,在这个条件下保证事务的ACID性质。start tx; cp1;cp2;cp3;... commit tx;  这个分布式事务操作序列需要分解提交到不同的节点，所以必然要先询问各个节点是否都可用。
					>目的: 为了实现强一致性系统(金融系统)：(内容之一：实现事务的ACID)
					>柔性事务三种架构：
					>实现方案：执行过程。(执行上面那一串操作不同节点上的数据的命令)
						>2pc: 借助协调者： 询问-提交。节点：锁资源-执行提交。。执行事务是计算值的过程，而提交才是覆盖到资源。
							>问题：
								>协调者单点问题，
								>节点同步阻塞的问题---所有节点在最后一个节点提交之前处于阻塞状态。
								>部分提交：而网络异常，使得部分提交而数据不一致。
								>任何一个节点失败：整个事务失败；即事务容错机制不完善。
						>3pc: 询问-预提交-提交 。节点：直接返回-锁资源-执行提交。
						>补偿事务tcc: 应用层补偿。先冻结资源(try)-提交修改(Confirm)-如果提交修改失败则回滚(Cancel)/之后解冻释放资源
							>问题：
								>补偿代码可能比较多，因为提交可能失败，而回滚本身也可能失败。(回滚失败后继续锁资源，而需要手动的回滚到之前状态？默认的事务失效时间？)
						>本地消息表：异步确保。利用各个单节点上的事务，而通知各个单节点通过消息队列，而各个单节点收到消息后 通过修改 初始事务消息状态来告知消息收到(收到消息 则补偿交给自己完成)。
							>目的：避免分布式事务，实现最终一致性。
						>MQ事务消息：RocketMQ支持。
							>事务发起方 增加了接收确认消息的逻辑：确认到了则认为事务执行完毕。形成一个：发消息-收消息处理再发确认消息-收消息处理再发消息...-收消息的 一环一环处理下去最后完成的过程。
				>本地事务：事务的所有操作都在一个节点上。
			  >分布式系统分两大类：
				>CP系统：实现了分布式事务，事务有ACID。(出了问题服务不可用，更新失败数据回滚)
				>AP系统：基本可用，软状态，最终一致。
				
			--参考：https://blog.csdn.net/kevin_loving/article/details/80655345	
		>保护：
			>缓存、降级、限流
	>zk：
		>使用场景：
			>分布式协调：协调两个互相不通信不了解的服务配合完成一件事请。就是A完成后，B开始做一件事，B做完成后A开始做一件事。而要保证AB按照这个顺序来配合做事情，就需要额外的第三方服务，就是监听-通知服务，A完成了，第三方监听到而通知B,B完成了而第三方监听到而通知A, 从而AB之间互相不用通信也能配合完成一件事请。zk节点的watch机制。
			>分布式锁：创建有序临时节点。类似于CLH队列。节点级别分布的线程的在zk上的竞争形成的CLH队列。节点分散而集中到zk的AQS。
			>分布式配置：配置信息管理：监听配置节点。
			>高可用性：HA高可用机制的实现。利用监听节点机制：主节点挂了那么感知到而切换到备节点--类似分布式锁：主备节点先排起队，而第三方只认为第一个节点是当前可用的。
	>分布式锁： 
		>redis单机实现：setNx
			>因为 尝试设置一个已经存在的key是不能成功的：即如果n个进程并发地设置同一个key,那么只有一个会成功：SET my:lock 随机值 NX PX 30000  。这就相当于CAS exp=null一样。
			>释放锁：因为可能中途自动释放锁了：所以不能直接删，需要带 设置时的 val即一个随机值 来用lua脚本删除：if redis.call("get",KEYS[1]) == ARGV[1] then return redis.call("del",KEYS[1]) else return 0 end 
		>redis分布式实现：RedLock算法---不推荐。
			>
#文件方面：
	#软连接：相当于快捷方式：并不复制内容，只是保持一个连接。ln -s 源文件名 软连接文件名   。。。只要源文件名是完整路径，就可以移动。
	#硬连接：相当于cp -p 文件 + 同步更新，；连同属性都复制，然后源文件更改硬连接文件会同步被修改，硬连接文件被更改源文件也会被更改。ln 源文件名 硬连接文件名。
		>源文件删除：快捷方式自然不可用。而硬连接文件可用。因为硬连接文件和源文件共用一个i节点号(i节点就是文件信息结构体)。linux删除文件只是删除i节点号与源文件这条映射、源文件(磁盘上的一个数据块)。而i节点号-硬连接文件依然存在。而展现上：目录下有：文件名-i节点信息
		>tomcat下的catalina.out存在着同步文件。
#中间件
 >nginx:

数据库方面：
	>数据库一致性问题(渐进)和对应的隔离性级别解法
	 事务隔离级别		数据库一致性问题
				脏读	不可重复读	幻读
	 读未提交    x                         (值为是否解决，x未解决， v为解决)
	 读已提交    v         x
	 可重复读    v         v    	x		(锁不了新加列)
	  序列化     v		   v		v
	 >隔离级别：描述的是一个事务对另一个事务的可见程度、影响感知程度(是事务并发时的可能情形)。。。解决并行带来的问题。(序列化能彻底解决，MVCC也能彻底解决)  
	  >未做任何隔离：A事务可以读未提交的B事务正在处理的数据。脏读。
	  >设置未提交的数据不能被读到这种隔离限制：写期间锁住资源限制(B锁住)。不会脏读，但连续两次读：事务B前和后A事务两次读，结果不一样。不可重复读。
	  >设置读取事务执行期间相关行不能被写限制：读期间也锁住资源(A锁住)。可以并行读。不会出现不可重复读。因为读事务不会并行于更新事务。但是可以并行于插入事务，即读事务执行时，不仅可以并行读，还可以执行并行插入数据的事务---这就会导致查询有多的数据，就是幻读。
	  >设置任何两个事务都序列化串行执行限制：什么问题也就都没有了，没有隔离不足的问题。幻读也不会有，因为一切并行的问题都没有。读不加锁的MVCC方式也可以实现：读取的时候只读某个版本以下的数据---另一个事务新插入的新版本数据就忽略了。
	-------读写 / 写读 并发执行往往产生上述4种问题，而mvcc的引入使得 读写/写读可以并发进行，提高了性能。
	 >mvcc: 多版本并发控制 。。mysql用来实现/提供 并发 读写/写读 中的四种隔离级别 效果。好处：在写的时候同样可以读，读老版本即可。新版本数据写成功后，将最新版本指针指向这个新版本即可。CLH,MC锁公平锁一样连接起来各个版本。
	  >版本链：聚簇索引的叶子节点上，对每行记录都有两个隐藏列：trx_id事务id , roll_pointer该行记录的上一个版本的undo日志中的地址(更新一行数据的时候，会先把老版本数据(包含tx_id和roll_pointer)写入undo日志，再更新数据和tx_id和roll_pointer)。
	  >读的过程：ReadView中有一个当前活跃/正在执行中的事务列表，也即begin了没有提交的事务列表(修改的新版本数据也在版本链上)；访问记录的时候，如果要访问的记录版本的事务id在事务列表最大事务id和最小事务Id之间，则遍历判断是否在列表里，在则事务未提交而不能读，不在则事务已经提交可以读；如果记录的版本<列表最小事务id则可以读，>列表最大事务id则不可读。
	             select读取记录的时候就要先生成ReadView列表，再读版本链，每读一个版本都要和ReadView列表进行比较，逐条过滤直到判定了某个版本可以读再返回这条记录。
				 可重复读和读已提交的区别在于：一个事务连续两次读，另一个事务在第一个事务两次读之间更新同一数据且提交，可重复读隔离级别下第二次select用的是第一个select生成的ReadView列表，而读已提交隔离级别下第二次select时读取的是重新生成的ReadView列表。
	  >回滚的过程：自然的，就是将回滚事务id对应的在版本链中的数据清除。那么即便在读未提交隔离级别下也不能再访问到回滚了的事务的数据了。
	  >公平锁和非公平锁的区别：公平锁：获取锁而建立一个按先后顺序排队的线程等待队列--MC锁/CLH锁；排队依次使用。非公平锁：每次获取锁都是竞争：
		>ReentrantLock: 可定时、可轮询、可中断的锁，可以是公平锁。
		>synchronized: 非公平锁 。可能产生线程饥饿。
	>事务的ACID: (实现事务的要求，要求实现的事务服务要有这些特性)(数据库提供的事务服务)
		>A:事务中的所有操作要么都成功、要么失败回滚使得数据库回到操作前状态。没有中间态。
		 >其中之一失败则回滚机制：实现原子性。
		>C:事务修改使数据库从一个一致状态转变到另一个一致性状态，用户感知不到数据库的中间变化。
		 >用锁来同步：锁住资源，阻塞访问，实现一致性。如先后修改两个数据过程中，两个数据都不能被读和写。
		 >读-计算-更新：必须是CAS类似方式进行的才能一致性变化。即在实际更新的时候该变量的值还是我的计算所基于的值-之前我读的值。否则就是乱覆盖，各个变量的值会不一致：不再满足某个关系。
		>I:事务都有自己的数据空间，互相不影响。即时操作同一个数据也有自己的数据空间。更新也是从事务数据空间/临时表空间到数据库存储空间的更新。
		 >事务并发时：系统保证Ti事务和Tj事务看起来是串行的，即事务在执行时感觉不到有其他事务在并行。
		 >自己的实现机制：事务更新需要提供一个数据版本。
		>D:事务一旦成功，对数据库数据的修改就是永久性的，被永久性的保存了下来，即使崩溃重启也是修改后的数据。
		 >修改同步到磁盘上：实现持久化。
	>mysql提供的事务接口和内部对事务的实现：mysql事务特征，Mysql事务控制语句。(是mysql事务，mysql提供的事务，而不是hbase事务，是描述mysql事务的特征、提供的事务控制语句，内部的事务实现)
		>用户接口：设置事务隔离级别(读已提交、可重复读、序列化)、打开事务、执行操作序列、提交保存点、提交、回滚到保存点。//spring事务管理时，打开事务后执行操作序列时，操作序列或在不同的方法里，因此有事务传播级别这个概念---是否打开新事务--是则设置保存点，
		>对事务的实现：
			>开启事务-执行事务-提交事务：先将这些数据备份到Undo.log里，再对数据进行修改，如果出现错误或者用户执行了Rollback回滚，那么将备份的那个数据复制到原来的位置，以恢复到事务开始之前的状态；如果正常，那么修改后的数据将会备份到Redo log。从而实现事务的原子性和一致性。
				>提交事务前：需要将内存中的新数据的备份写到Redo log , 即新数据需要备份，而内存中的全部数据本身不用持久化。这样系统可以根据Redo log 而恢复数据到 新数据状态--尽管内存中的实际数据没有持久化。 这样实现了事务的持久性。
			>事务之间：可能互相竞争一个资源 且都是修改它：用数据库锁来锁住资源执行事务-提交之后-释放锁，实现事务的隔离性。
	>mysql:
	  >索引的存储数据结构与增删查：B+树
		>B树：平衡多叉树，关键字分布在非叶子节点上，数据在叶子节点上，可以在非叶子节点上命中。非叶子容器K个元素K个儿子；叶子容器K2个元素。从添加、删除过程看，构造出整颗树的过程看，K>=M/2 且K<M(<M/2 >=M会重新平衡). K2>=L/2 且K2<L同样在这个范围之外会重新平衡。
		 >搜索性能：按最少算：=查找次数= 每层的查找次数(都一样) * 层数(跟总数、单容器节点数有关) = log2(M/2) *  logM/2(N/(M/2)) 其中N / (M/2)自然就是容器个数，而层数与容器个数X的关系就是：M/2^(h) - 1 = X 所以认为h = logM/2(X)...化简后近似=log2(N)
		 
		>B+树：平衡多叉树。不能在非叶子节点命中，叶子节点形成链表-前后引用-自然从左到右是有序的。
		 >缺点：叶子节点的分裂，使得逻辑上连续的数据块在磁盘上并不顺序存储，使得读数据需要大量的随机读(磁头频繁移动，磁盘频繁旋转)---从而性能低下。
		>B*树
		：平衡多叉树。B+树变种。同层非叶子非根节点也前后互相引用。
			>单容器数据结构：从树的形成过程中可以看出，同层最左容器才可能有左子节点，其他元素都是只有右子节点---即子节点中元素最小值>=自己的值
			>增加一个元素：后，叶子容器个数<L不动，>=L 自底向上分裂，即叶子容器分裂为两个，父容器增加一个节点-值为分裂的右半容器的最小值；然后判断父容器是否要分裂。如此下去，直到根节点。
			>删除一个元素：后，叶子容器个数>=L/2不动，否则 看右容器元素有多少---如果>L/2则借一个补充；<=L/2则合并右容器，并把右容器的父节点的值改为合并容器的最小节点值，同时删除左容器。然后看父容器节点个数是否足够--进行相应借或者合并直到根节点。
	  >innodb引擎： 默认施加聚集索引的顺序：主键-->无则UNIQUE列-->生成“包含行ID在内的合成列”  。主键索引和数据存在同一个文件里。
		>主键为聚集索引: 叶子节点的引用指向一个页面，这个页面包括了一行所有的数据。(而不像非聚集索引只指向 单列数据)
		>事务： 
		>行锁：
	  >数据item的存储结构和增删查：
	    >事务下的修改(innodb)：事务开始-->原数据放到undo.log-->修改数据在内存-->修改后的记录放到redo.log--->事务提交，返回给用户成功。条件满足，触发checkpoint, 将内存中的数据page合并写道磁盘()
	  >mysql执行DML语句的过程：插入更新删除。代码层次。
		>mysql_execute_command(): 根据语句类型调用专门的方法来处理---> 如果执行中有错，执行回滚事务方法trans_rollback_stmt(), 否则执行提交事务方法trans_commit_stmt()
		>
	  >单点问题的解决方案：主备切换、主从同步
		>主备同步：
		 >binLog: 日志格式mixed: 如果不会引起不一致，则用statement格式语句写到binlog, 如果会引起不一致，则用row格式(直接变化的记录)。delete:记录删除的每一行。update:记录前后的数据。insert记录本身。所以row格式方便恢复数据。
		  >通过binlog恢复数据库：即只需要mysql事先开启了binlog, 则输入binlog起始位置和删除数据之前的位置重新执行一遍即可(更新数据自然也可以binlog来恢复)：mysqlbinlog --start-position=4183 --stop-position=4592 /opt/lampp/var/mysql/mysql-bin.000001 | mysql -u root -p
		 >备库动作：将主库的操作同步过来 本地执行，来保持数据一致。
		 >过程：用户update语句-->主库undolog-->redolog准备-->binlog--->redolog commit提交--->返回客户端ok。。另外，写到binlog后，还会另一条线：和备库保持连接的dump_thread将按照备库传过来的binlog偏移量 读取binlog相应位置开始的内容发送给B，B本身启动了io_thread	和 sql_thread, io_thread接收到A发来的binlog日志后写到relay log中转文件里，sql_thread线程则从relay log里读取出来执行其中的sql命令。
		 >双M结构：不用修改主备关系，两个都是主都是备；而发送binlog的日志时日志在接收用户写入时已经加了serverid, 发送给备，备又会原样返回，此时serverid是自己，会忽略这个消息。从而结束。
		 >主备延迟：seconds_behind_master 主库执行完事务写binlog开始到备库执行完事务结束这个时间差，大半部分是备库从relay log里读数据构成的。
		 >主备切换策略：参考https://blog.csdn.net/hanpeiyu1995/article/details/89499550
		  >可靠性优先：当备库的seconds_behind_master 足够小比如<5s时，说明基本要同步完了，可以切换了，于是将主库的readonly设置为true, 等seconds_behind_master 等于0的时候，把备库的readonly设置为false, 再把业务请求切到备库上即可。这样，仍然有时长为seconds_behind_master的时间不可写只能读。
		  >可用性优先：切换时直接切换到备库，备库的readonly立即设置为false,同时因为binlog_format=row 从而会binlog传递的是记录，那么主库未完成的binlog传递的条数就是冲突的最多个数----此时共有此数*2条数据未插入。所以丢失还是严重。
		  >个人方案：也是立即切换，但是备库在写的时候仅仅写到undolog里，暂时不实际的执行。等到主库binlog写完之后备库也执行完毕之后，才开始实际执行undolog--------此时undolog里有T3-T1这个落后时长的积累的数据，但是没有遗漏也没有丢失数据-----只是将会暂时不可用---卡住--否则暂时不可读。
		 >+keepalived:
	  >集群cluster的读写过程、节点加入移除和故障的检测恢复：redis为例 参考https://my.oschina.net/u/2600078/blog/1923696
	    >redis cluster基本架构：三主三从 + 若干sentinal构成的集合。
	    >hash slot:供16384个(2^18)，每个master分得若干个slot。从key得到对应的slot从而找到存储此key的master的算法：hash slot = crc16(key) mod 16384 。。这样，集群中，不同master存储的就是不同的key。16就是除数的位数，即现在crc编码就是15位
		>crc16:循环效验码 参考https://blog.csdn.net/xinyuan510214/article/details/80104356
			 >发送k位数据M: 先生成n位冗余码，然后k+n发送出去。其中n+1就是crc-16中的16这个值。
			 >冗余码生成规则：  M<<n 异或除 P(len=n+1)(收发端事先商定固定的除数) 得到一个余数FCS , 从而FCS + M<<n就是最终要发送的数据。异或除：每轮的被除数共n+1位，如果首位是0，则后面的位落下直至构成首位非0的n+1位数，继续和P异或运算，直至最后得出一个n位的余数。  
			 >冗余码接收端差错校验：直接将接收的数据 L  异或除 P 看结果如果是0则正确！！！
		>redis cluster读写过程：参考https://redis.io/topics/replication 避免master自动重启 http://redisdoc.com/topic/replication.html
			>读：slave转给master
			>写：master写之后，直接返回，非阻塞的，而异步的同步命令给slave。。master上有backlog， 同时接收slave的offset来发送剩下的指令。
			   >同步过程：slave发送psync命令给master, master验证runid未通过则发送RDB文件给slave, 验证通过则根据offset同步部分数据给slave---即发送缓存区的所有命令.
		>故障转移：master down之后集群会将master的一个slave晋升为新master。。自动化实现机制：sentinel架构。哨兵模式高可用架构。
		 >sentinel节点：互相心跳检测，也检测主从节点。也是一个redis实例。
		 >工作过程：当一个sentinel检测到某个master在指定时间内无心跳返回，则开始询问其他sentinel节点是否真的故障，其他sentinel则也去访问该master并且返回给询问sentinel结果，如果询问sentinel收到超过半数的确认认为故障，则这个询问sentinel开始故障转移工作：即从master所在节点组选择一个slaveA 发送slaveof on one 让它升级称为新的master，然后向其他slave节点发送 slaveof slaveA的ip:port 命令，从而其他slave开始设置新的master节点。另外，sentinel会继续监控老master,如果恢复了--则发送slaveof xxx同样的给它。
		 	
		>水平拆分：
		>水平扩展：增加一组master-slave实例，或者物理上的水平扩展(master slave交叉部署在多台物理机器上--更高可用)。
		>分片迁移：slot的迁出迁入
		>拓扑结构(集群结构)：按负责的slot分为多个 节点组 --->  每个节点组:master1 + slave多个 (master读写服务，slave只读服务)
	  >高并发的使用策略：读写分离、分库分表、sql优化
	>hbase: 自顶向下、职责划分
	  >设计理念：存储海量，通过rowkey把相关数据存储在一起---第一可以避免join-第二可以按rowkey分割存储在不同服务器--可扩展性强, 提供基于行的原子性操作。
	   >cell: 每一个cell存储了：rowkey --> colomn family --> column --> version/timestamp --> value  这也是每个cell值的完整索引。版本个数默认3， 超过则删除老的。
	   >Meta table: 保存集群region信息(rowkey范围-->region id-->region server)  通过B树组织。
	  >物理上：三大服务器集群：zk(主从)(服务器状态监控通知) , HMaster(主从--通过zk创建临时顺序节点时间), HRegionServer(集群)(和hdfs datanode同台部署)
	   >zk:
	    >一致性算法：
	  >读：客户端从本地或zk上读取所在MetaTable信息--->找到所在RegionServer--->发送给RegionServer读取请求--->RegionServer定位到HRegion-HStore, 依次Block Cache--> MemStore--->磁盘上HFile的索引/bloomFilter找到相应的cell读出
	   >读放大效应：memStore多次刷数据导致多个Hfile,导致需要读取多个HFile来
	    >措施：minor compaction: 自动将一些小HFile合并到几个大HFile中。
			   major compaction: 定时将一个cf下的所有HFile整理已经删除的或过期的cell并合并为一个HFile。因为会读大量HFile, 所以有写放大效应。且如果是在HRegion分裂后一个Region被转移到另一台服务器上，那么还会远程读取HFile到本地。
	  >写：客户端同上找到所在RegionServer--->发送给RegionServer---->RegionServer先append顺序添加写入WAL，再写入memFile, 然后返回写入成功；而再异步的判断memFile是否足够大而需要刷入磁盘形成一个新的Hfile/StoreFile(每条数据顺序刷，减少寻址时间所以高效)
	  >删除：给cell打上删除标签---使得不可读。
	  >备份：WAL和HFile都会在HDFS上有备份。
	  >HRegionServer宕机：zk通知HMaster，HMaster重新分配宕机机器上的Region给其他机器，同时将宕机机器上的WAL分配给对应其他机器，其他机器就可以顺序读取执行WAL中的数据操作产生memStore---当满了则flush到HFile
	  >HMaster: 多实例部署，zk负责进行leader选举.
		>对Region管理: Region 分裂合并后重新分配，调整Region分布，HRegionServer挂了之后迁移上面的Region到其他HRegionServer
		>对HRegionServer: 负载均衡，状态监听--通过zk通知
		>对用户：接收增删改查，数据库的创建删除
	  >HRegionServer: 多实例部署, HMaster负责对多实例负载均衡。
	   >对客户端写入：进行IO，存储数据到HDFS；读取也可以。
	   >对Region: 存储1000 个Region(可以来自不同表格，可以来自同一个CF的不同rowkey段)
	   >本地文件：
	     >WAL--日志文件-故障后恢复数据用，写数据到来时先写到WAL日志里。
		 >Block Cache 读缓存---访问次数最高的n个数据的缓存
		 >MemStore 写缓存---累积n个、排序后再刷到磁盘，形成一个Hfile/StoreFile
	  >HRegion: 默认大小：1GB
	   >对HStore: 存储多个HStore
	   >和family关系：一一对应
	  >HStore:
	   >对storeFile: 存储多个StoreFile, 组织方式：LSM树
	   >对memFile/memstore: 一个。
	    >LSM树：日志结构合并树。读弱写强。大多数NoSQL数据库采用。日志结构合并树，优化措施：bloomfilter判断小树有无，小树合并为大树提高查询效率。参考https://www.cnblogs.com/bonelee/p/6244810.html
		 >写强原因：日志顺序写磁盘，实际数据先在内存中用多颗B+树有序缓存--数据量达到一定时刷入磁盘(磁盘中是有序数据)。移动磁头、旋转磁盘、读取头标和数据；即由柱面号 移动臂移动到指定柱面，根据盘面号确定盘面，把块号指定的磁道段移动到磁头下，磁头读取数据传到内存。(移动臂移动自己--查找时间，移动磁道段--等待时间，磁头读写--传输时间))。(磁盘一般2-3个盘片)每一个盘片的两个盘面上都有一个读写磁头， 每个盘面1024个同心圆称为磁道(最外为0)，一个磁道上分多个圆弧称为扇区；所有盘面上同一个磁道构成一个柱面(每个柱面最上的磁头编号为0)(写数据按照同一个柱面最上磁道开始，一直写到最下磁道---电子切换；然后才移动磁头到里面的柱面---机械移动), 一个磁道多个扇区/块，一个块里分扇区头标(盘柱扇号+CRC)和数据(数据512B+ECC)两部分
		 >读弱原因：内存中小树读时间=N/m *log2(m) (使用bloomfilter会更少)， 没有则读磁盘已经写入的树文件(定期树文件合并merge为大树，减少磁盘读另外还减少了读时间---磁盘文件的m变大)，。(?合并的最大文件量是一个磁盘块--即一页8k?)
		 >内存小树写到磁盘后：为减少磁盘随机读(多文件)，多个storeFile 会合并为一颗大树(因为有序，所以合并简单)文件。这个叫小树文件compact。因为查询时间= N/m * log2(m) , m为每棵小树的数据量，从而m增大，查询时间会减少，当m=N时，达到log2N
		 >读取一个数据的过程：先从一棵小树上二分查找，没有则下一颗小树中找....
		  >ECC: 纠正一个bit错误和检测2个bit错误。参考https://blog.csdn.net/liaoyaonline/article/details/80166133
		   >q元码：(n,k,d) n为码字长度，k为码字数量，d为码字之间的最小汉明距离。
		   >汉明距离: v=(v1,v2,...vi) u = (u1,u2,...ui) v的汉明重量=w(v)=1的个数， v,u的汉明距离=d(v,u)=w(v-u)其中-号是异或。。。根据空间三角形，有d(u,v)<=d(u,w) + d(w,v)
		   >一个信号x的ECC编码：奇偶校验编码：x后面加上一位--x的1的个数(偶校验码), 如果x后面加上一位--x的1的个数的非(奇校验码)..总之，整个编码1的个数是偶数--偶效验码，个数是奇数-几效验码
						海明码：数据位N, 校验位r ， 则r位校验位可以组合出的最大的数2^r - 1， 也是能够检测的最大位，更高的位则不能检测；而在最大位以内又有r位是校验码，所以只剩2^r - 1 - r位来表示数据, 所有数据位N<= 2^r - 1 - r。
								r位校验位的位置：2^k, k=0,...r-1
								第i位校验码新增校验的全部位置：2^i * 奇数  (全体整数={2^n *奇数}其中n=0,...n)。所以全部位置都会被校验到。而且第i位刚好未知其他位都知道---从而确定第i位的奇偶校验值。比如对于偶校验，0位上的校验码值=所有奇数位的1数
								第i位校验码值确定：取i+1位隔i+1位再取i+1位...得到的数作为奇偶校验的输入信息。参考https://blog.csdn.net/lycb_gz/article/details/8214961
		  >局域性原理：一个数据被使用时，附近的数据也会被马上使用。如程序运行中，需要的数据不在主存，则触发缺页异常，系统向磁盘发读取信号，磁盘会找到数据起始位置并连续读一页或几页载入主存，异常返回程序继续执行。
		  >操作系统管理存储器的逻辑块：称为页。8k字节(16个扇区)。
		  >磁盘碎片产生的原因：文件在磁盘上连续存储，而文件新增则在其他地方，文件删除从而就会留下许多空隙，文件读取就会多次寻道而耗时更多。参考https://www.cnblogs.com/cxzdy/p/5379570.html
		 >跳表：多层链表。搜索时自顶向下搜索。每层链表有尾指针Nil 。
		  >搜索：查找元素为C, 在第i层链表中某个节点N处发现N<C，如果下一个节点是Nil, 则找N的下一层链表的元素M,继续比较。直到最底层，如果没有找到则没有。
		  >添加：同查找，直到最底层，D1<C<D2 也没有发现C，则在D1,D2之间插入一个节点值为C, 然后随机决定要不要在上一层垂直上方添加一个元素--值也为C。
		 >bloom filter: 初始化集合到 bloom内部数据结构：输入信息m, 用k个hash函数处理m得出ki, i=1,...m 标记bit[ki] = 1 ；形成过滤器需要的数据结构。查询时，只要input的k个hash值对应bit[]上的值有一个是0，则input一定不在集合里。但如果都是1，则大概率在集合里--也可能不在。
		 >bitmap: 本质上是长度为n的bit数组bit[n]来表示元素最大不超过n的k个自然数构成的集合。集合中有的自然数m则在bit[m]=1, 没有的自然数c则bit[c]=0。。。bit[n]的初始化，只需要遍历一遍集合就可以得出来；而bit[n]是有序的，再遍历一遍bit[n]就得出了排序后的自然数集合---海量数据排序。如果用byte[]数组来表示bit[],则 一个整数M在byte[]中的位置= byte[M/8]中的M%8下标位设置为1 byte[M/8] = byte[M/8] | 0x01<<(M%8) 
		 >Level DB: 顺序写磁盘日志Append Only; 应用到内存有序表SkipList(满了之后打包成key有序文件sst(顺序写到磁盘，追加而不是重写旧数据))(读则利用table cache, block cache, bloomfilter)
	   >对memFile: 存储一个memFile
	  >StoreFile：即刷入磁盘的一颗小树(B+树)
	   >数据结构：一个多层索引系统 + 一个个64kB数据块顺序构成，每个数据块可以当作B+树的叶子容器：包括--叶子容器的所有cell数据升序排列作为节点、叶索引、bloomfilter.。而多层索引系统就是B+树的根索引 + 中间索引+叶子索引；这个多层索引系统读到HStore中就是block cache
	  >memFile:	即LSM中的一颗小树(B+树)
	   >保存的每一行就是一个rowkey的一个列簇的所有cell信息(rowkey-cf-co-version-value),同时保存了最后一次写操作的序号---这样保证HFiles之间是通过序号有序组织的。
	   >保存新加入和更新的行：
	  >BlockCache: 最近读取过的n行
	  >Region: 分裂合并
	   >Region大小大于设定值时，平分分裂为两个，HMaster负责分配新产生的HRegion的分配。分配到其他HRegionServer上的Region会在它下一次的major compaction时将HFile文件下载到本地。
	>elasticsearch: 不用关系型数据库来实现全文检索。高扩展性、高可用性的实时数据分析。扩展至上百台，PB级数据。
	  >分词：将输入的搜索词 分解为 多个 关键词/字。。经过分词器Tokenizer得到Token词元。
	  >倒排索引：浏览数据而形成的 “关键字---文档id集合” 称为倒排索引，词典Term Directory(词与词频) --> 文档列表Postings List；但量太大而不会装载都jvm heap里， 所以对Term词典做了一个前缀索引Term Index, 用FST实现占用空间小--从而全量加载到内存heap； 从而先查内存中的前缀索引Term Index 再查磁盘上的Term Directory。es为每个column都使用索引--倒排索引(而不是一般关系型数据库的B树索引)。。参考http://www.360doc.com/content/16/0106/17/17572791_525951877.shtml
	   >词典前缀索引数据结构：FST树 参考http://www.shenyanchao.cn/blog/2018/12/04/lucene-fst/
	    >FST树：有限状态转移机(输入相应的信息--状态进行相应的转移：转移的起止路径对应输入的信息集合)：开始节点：单词最大前缀匹配， 后不够再增加边，直到 结束节点。形成一个有向无环图。
	     >作为key-value数据结构：压缩率有3倍-20倍，省内存---相比于hashMap/treemap
		>Trie树：存储字符串的链表，一个节点一个字符。子节点则最多26个(可以在父节点中用hashmap,array[26], bitmap表达)。达到其中一个节点必然只有一种路径---所以其中一个节点有标记--是否是单词(尾字符)。
		 >add: 从头开始查找，没有的字符则新增该字符节点，然后继续。
		 >query: 从头开始查找，没有的字符则在Trie树中不存在，直到最后Trie树的结尾/或者字符串的结尾，为判断的终止。
		 >delete: isWord=false或者删除整个都是false的分支---且只从下往上删到中间节点有分支为止。
		 ---排序：先序遍历。
		 ---词频统计：单词尾记录出现次数, 最后遍历一遍单词。
		 ---字符串存在检索/去重：相当于查询一次Trie树---效果比hash之类好是因为避免了和不必要的字符串比较---因为每个字符都是精准匹配的。参考https://segmentfault.com/a/1190000008877595
		>bit-wise Trie: 不是存字符，而是存0\1的 前缀树。/地址分配/路由管理.。。参考https://www.cnblogs.com/justinh/p/7716421.html
	  >全文检索：搜索词 分词 后去查 倒排索引 得到的 文档id集合 再去 查相应的文档 并返回，就是一个全文检索的过程。
	  >Lucene: 单机模式, java开发；索引、检索功能。ES的存储引擎，ES在之上增加了分布式接口而已--依赖Luncene；即建立一个分布式框架，而节点底层依赖Luncene进行实质操作；查询时并行查节点，结果合并；索引时，各节点独立索引自己节点上的数据。索引树：FST树---每个单词对应着的值就是docid集合。
	    >es喂给/传输给lucene的：已经是非常规范化的数据；一个Lucene实例也不用关心其他Lucene实例。
		>Lucene倒排索引生成过程：先在内存生成Inverted Index , 再定期以段文件segment file形式刷到磁盘上的，而且不再修改，更新会写到新的文件。所以一个段segment就是一个完整的倒排索引。而segment memory就是倒排索引词典的前缀索引。
		>开箱即用：不用配置。
		>jvm参数配置：jvm堆大小不超过os内存一半--以便os缓存磁盘数据。
	  >Solr: 数据格式更强：不止json,html,excel也可以。但实时性差。
	  >数据组织层次：索引--类型--文档--字段
	   >物理上：集群-->多个es实例节点-->
	            索引-->切分为多个分片--->每个分片多个 副本分片 --> 每个分片多个类型--->每个类型有一个mapping: 字段类型说明 --->每个类型下多个文档:row---->每个row多个field：column
				-----:分配：分片分配给节点。
				      gateway: es索引快照存储方式：一般是本地磁盘，也可以是hdfs,云。存储发生的时间：es先将索引存在内存，达到一定大小就调用gateway持久化到本地/hdfs/云。
	  >对用户提供：Transport交互方式:默认是tcp,但也提供了http: RESTFUL API ：curl 和java接口
	  >索引过程：全量/增量
	   >增量：/修改：修改也会把文档视作新文档而添加到新的segment里，分钟级别可以被重新索引(因为刷磁盘的fsync操作)。
	  >搜索过程：将Query转换为Lucene Query , 在所有的segment中计算
	    >具体过程：词法分析找出关键词；语法分析形成语法树 --> 关键词语言处理器处理规范化---> 从倒排索引中找出关键词的文档列表来交集/差集，---> 对文档排序(根据词对文档的重要性---词权重/词频数/词在的文档数，得到文档的词频向量，而查询语句也是一个词频向量，从而计算两个向量余弦值--作为相关性的打分，分值大的就排在前面: VSM向量空间模型算法)
		>极好参考：https://blog.csdn.net/guoyuguang0/article/details/76769184/
	  >添加数据：新的segment会先缓存到内核中，然后才flush到磁盘。来提高性能。而本身在文档修改,写入es节点时，会首先append写到translog里持久化，方便内存中的segment崩溃后从最近的commit point的数据恢复。
		>segment合并：同hbase HFile的合并，segment也会合并为新的更大的segment，同时整理了segment里的被标记为删除的数据。
		>segment内容：Inverted Index、Stored Fields 、Document Values
		>具体过程：数据生成索引存入内存Buffer, 同时写入translog, 内存中的数据每隔一秒以segment格式存入系统缓存，系统缓存中的segment定期刷入磁盘/同时清除translog中的记录。
		 >数据生成索引：分词得到词元-->语言处理组件将词规范化还原-->词传给索引组建 创建字典、排序、合并相同的词、形成文档倒排 链表
	  >删除数据：segment中的数据删除--仅仅是一个标记；下次会被索引到--但是返回给用户时过滤了。每一个提交点有一个.del文件--记录哪个segment的哪个docid被删除了。
	             文档删除---也仅仅是标记为删除.
	  >分片重新分配：当es节点宕机后，上面的分片会被重新分配到其他节点。
	  >p2p系统：广播和多播
	  >ES节点自动发现：Discovery.zen 相当于每台节点排序广播得到的网络中的serverid， 字典序第一个的作为leader，如果一个节点获得了超过半数票--则只会有一个--从而它就是新的leader
		>参考：https://blog.csdn.net/qq_17864929/article/details/54923720
缓存方面：(三大问题：key总是没有(穿透了)，突然没有-一个热key或者大量key(击穿了，雪崩了))
	>缓存穿透：key数据库不存在，缓存也不存在。导致对数据库不断的查询--给数据库带来压力。(解法：也存到缓存，但是值为特殊的)
		>解决办法：数据库数据预加载到缓存里，监听mysql.binlog从而有变动也更新到缓存里。主键缓存到 布隆过滤器 辅助判定。
	>缓存雪崩：缓存的同时缓存了大量的数据，失效时间也同，集中过期，(或者缓存服务器节点宕机、断网)，导致突然给数据库很大的访问压力--而且是周期性的。(解法：冷门访问缓存时间少，热门长；缓存时间加随机因子。)
	>缓存击穿：热点数据太热，在失效瞬间给数据库很大压力。

#微服务方面：
	>分布式系统的保护：缓存、降级、限流。容灾、容错。
		>降级：服务舍弃一些突发不可用功能确保整体可用，子服务不可用、超时，整体服务还是要能用，即时因而是有损的。(N次请求m次超时失败, 称为可用率m/N * 100%)
			>解释2：当服务器压力剧增的时候为了保障核心服务的运行而释放服务器资源而对一些服务有策略的降级。
			>先重试：后降级。失败重试和超时重试,几次之后还不行，则启动降级。降级就是服务不可用之后给备用的逻辑结果返回。
		>限流：一个服务限制调用方对它的调用速率。限速。达到速率后拒绝服务、排队等待、降级等。
			>限流算法：
				>计数器算法: 窗口切分，分别统计，滑动窗口。计数周期和判定周期。
					>实现：分布式环境下：redis的incr原子自增性质。
					>用途：QPS限流。
					>问题：临界问题。
				>令牌桶算法：一定速率放令牌，请求则从桶里取，取到才能继续。令牌数有最小和最大限制。
				>漏桶算法：请求入桶，按一定速率放行。漏桶满了则忽略。显然Ratelimiter就是漏桶算法。CLH队列入队等待和独立线程按一定速率激活(每次激活n个)。队列长度就是容量。也可以非阻塞，都统一为非阻塞接口。入参仅仅为：算法类型,算法参数json.
		>缓存：提升系统访问速度，增大系统处理容量。
			>缓存雪崩：即redis不存在而数据库也不存在：则预加载到某个地方。而预加载的方法可以是监听binlog而加到某个缓存里。---如果是重新从mysql里读取 如果不存在则向某个缓存里放--那么就太迟了--而且不准。用布隆过滤器来查看。预加载是架构的一部分。
		-容灾：
		-容错：
	>分布式系统的治理：
		>注册订阅：服务的注册和订阅。
		>变更下发：服务节点的移除和新增。	
		>服务管理：基于版本号。用于灰度发布(部署到某些ip上，让调用方调用)。
		>流量回放：请求的复制回放，用于模拟真实的流量进行压测。
		>负载均衡和路由策略：更灵活。
			>负载均衡策略：
				>轮询：round robin
				>随机：random
				>最少活跃：感知后端服务性能情况。如响应时间。
				>一致性hash算法：相同请求相同后端、没有则下一个节点。
			>集群容错策略：
				>fail over: 失败自动切换，自动重试其他机器。读服务。
				>fail fast: 快速失败。写操作。一次调用失败就立即失败。
				>fail safe: 出现异常时忽略。用于不重要的接口调用，比如日志。
				>fail back: 失败了记录日志，定时重发，适合写消息队列。
				>forking: 并行调用，成功一个则返回。
				>broadcast: 逐个调用所有的节点。provider
			>动态代理策略：
				>javassist: 动态字节码生成，创建代理类。
				>spi扩展机制配置自己的动态代理策略。servie provider interface: 服务提供接口，而实现类可以指定。配置在文件里。
		>在线压测：给请求打标签。
		>内置熔断机制：避免分布式系统产生雪崩效应。
		-服务监控统计：入口出口/调用方/依赖方的调用量：平均调用时间。
			>统计上报、鉴权：
		>调用链路自动生成：一图展示服务之间是如何调用的，明确调用链路。
		>服务访问压力以及时长统计：每个服务的每个接口每天被调用多少次，TP50/TP90/TP99访问延时分别是多少。第二，从源头开始，一个完整的请求链路 经过几十个服务之后完成一次请求，这种全链路层次的请求量和请求延迟TP50/TP90/TP99分别是多少。
			>目的：看当前系统的压力主要在哪里，如何来扩容和优化。
		>服务可用性：
			>服务分层: 避免循环依赖
			>调用链路失败监控和报警：
			>服务鉴权：
			>每个服务的可用性的监控：99.9% 99.99%, 还是其他。
		>服务降级：熔断降级，重试次数确定。
	>分布式系统的设计：系统分析和设计(3年后会真正从事的事情：技术方案的讨论和确定)
		>服务接口的幂等性：就像增加缓存一样--增加一层幂等。
			>每个请求有唯一的标识：
			>每个处理请求之后有一个记录标识这个请求处理过了：
			>每次接收请求需要判断是否之前已经处理：
		>rpc框架的设计：
			>注册中心：zk/redis, 服务端利用中心的client而和注册中心之间保持心跳。注册中心就实时动态记录了服务的ip:port列表。
			>客户端动态代理：服务的client需要被动态代理，动态代理则从注册中心获取服务的ip:port并且向服务端发起长连接，来写入响应的接口/方法/方法参数；并序列化后发送，同步的等待返回的数据，反序列化。
			>数据序列化的格式/通信协议：json/serialize/hessian/protobuf
			>客户端负载均衡：选择一个服务端进行本次的请求处理。
	
消息队列方面：kafka为例子(独特数据结构+算法方面)
	>主题-队列组织结构：一个主题下多个分区，每个分区主从组织,每个分区的消息量。producer和主题的每个分区的leader保持socket通信，并且hash方式均匀发送到各个分区leader
	>acks级别：收到、收到且刷入本地队列、刷到从节点、从节点写入成功
	>多个consumer读取一个主题：如果在同一个group，则读取的消息唯一，所以此时comsumer也需要一个均衡算法
	>数据的存储和获取：本地磁盘的顺序批量操作(先buffer再flush到磁盘)，较少磁盘IO和磁盘旋转磁头运动，  
	>zero-copy机制:
	>消费模型：pull模型，comsumer控制消费速率，手动确认是否重新消费。
	>网络模型：nio的多线程selector
	>存储模型：副本机制
	>partion日志分段：每个消息都会append log, 每个段都有index和log两个文件；一个partition日志分段为了方便二分查找日志--先找到端再找到内容，从而方便随机读；而顺序读，利用os预加载page cache页缓存，所以快。
	>leader所在broker故障而消息不丢失的原因机制是什么：HW高水位  
	>消息发送成功三种语义保证：at least once , at most once , exactly once 。最后一种的实现：可以不丢失消息还保证消息顺序，一个producer一个broker,broker对消息进行对齐；消息格式为"序号+消息"，序号小认为重复序号大1以上认为消息有丢失
	>事务：producer对消息队列一次操作的集合。  
	>系统架构/组织架构：节点-broker-topic-partition(主从，不在一个节点上；且有ISQ量统计)。 zk集群，broker多节点，
	>最主要的数据结构：
	>读：
	>写：
	>高可靠机制-实现而做的事情：(本机制定义：消息不丢失，不乱序，节点宕机自动发现并故障转移重新恢复--broker/partition，)请求到partition先顺序写日志，副本机制，分布式系统处理语义：
		>日志目录下多个日志段LogSegment: 每个日志段分为.index索引文件和.log数据文件
		  >稀疏索引/二次索引/辅助索引：叶子节点保存的是数据行引用--主键--物理地址
		  >稠密索引/聚集索引：只有一个，叶子节点保存的是整个数据行
		>发送方处理语义：至少一次，至多一次，刚好一次(一个producer一个topic下对应着一个专门的pid 。 pid + 序列号；；实现partition下的消息队列顺序)
		>事务保证处理语义: 节点重启后从断点处继续执行
	>高性能机制-实现而做的事情：客户端selector读写都是单线程处理，服务端selector监听，读线程池处理读请求，发送请求到message queue, 写线程池读取队列的消息处理并返回客户端。
		>零拷贝：
		>异步：
	>高可用机制-实现而做的事情：副本机制
		>
	>高并发机制-实现而做的事情：高可扩展机制---分布式部署-多节点-节点增加
	
	>高吞吐量机制-实现而做的事情：零拷贝
		>零拷贝：存在几个数据区：内核态数据区、用户态数据区、socket发送缓冲区、协议引擎。
		 >无用户操作的普通方式的数据从一个磁盘文件拷贝到另一个磁盘文件的调用过程：用户态read()调用-->切换到内核态--->读取/拷贝磁盘数据到内核态数据区-->拷贝数据到用户态数据区,--->系统调用返回，切换到用户态-->用户处理后write()调用--->切换到内核态，将用户态数据缓存区数据拷贝到内核态数据区--->系统调用返回，切回用户态。-->内核将内核缓冲区数据写到磁盘。从而4次切换4次拷贝
		 >无用户操作的零拷贝方式的拷贝过程: 用户态切换到内核态-->读取/DMA拷贝数据到内核态数据缓存区---> CPU再拷贝内核态数据到与socket相关的缓冲区，系统调用返回--->通过DMA引擎将缓冲区数据拷贝到协议引擎
		              零拷贝用gather copy: 用户态切换到内核态-->DMA拷贝数据到内核态数据区-->再拷贝内核态数据描述: 缓冲区地址+数据偏移量 拷贝到socket相关缓冲区 ,系统调用返回---> DMA引擎 通过 gather copy方式只需要根据 缓冲区地址+ 偏移量就将数据直接从数据区拷贝到协议引擎。 java nio transferTo()
					--linux mmap()系统调用的零拷贝实现(java mappedByteBuffer)：关键在DMA拷贝数据到内核态数据区--可以被用户直接操作，从而不用拷贝内核态数据区到用户态数据区；此时数据拷贝到与socket相关的缓冲区仍然是cpu拷贝。所以叫内存映射。
					--DirectByteBuffer:堆外内存、页面对齐、自己管理释放分配。
			>DMA: 外设直接内存读写，不需要CPU参与---即CPU不用亲自读取数据到内存再到寄存器，再从寄存器回写到外设，而直接DMA控制器就可以。
		 >应用：文件复制、消息读取发送到网络：			  
		
日志采集方面：flume为例子，agent(source/channel/sink)到agent的结构(一般部署在日志采集节点，中心再部署agent
)(selector->channel->sink->kafka/hdfs/agent)
	>日志+处理模式：比单纯的日志有更多的处理：一是拦截器链(使得不一定存到channel/memorychannel/filechnnel/jdbchannel)，二是写到了channel里(channnel看作一个队列，但有很多接口方便sink调用)，三是有sink异步的独立的调用channel而获取里面的数据，进一步的处理可以为rpc发送到另一个agent；这个agent再将event数据调用source写入channel，而另一方面sink调用channel读取event写入jdbc或者文件等。
	>数据单位：event是一个字节数组，有header头信息。
	>多级流：nginx, tomcat, log4j, syslog日志混合在一起的日志流，可以在在下一级Source时分开发送到不同的channel里，再sink到不同的地点。
	>负载均衡：一个channel可以配置多个sink,并且均衡的从channel里取得event(或者说channel均衡发送event到不同的sink---当然是取好--速率由接收方决定)
    >source: spoolsource监控spool目录，有新日志文件就会读取数据(利用log4j配置为精确到分钟级别)，完毕后文件后缀变为.COMPLETED  而 Exec Source可以实时采集日志
	>本地日志log4j2:
		>distrupor: 高效低延时的消息组件-读写数组(队列)。高性能有界内存队列。(CAS和缓存行补齐方法性能提升)
			>数据结构：数组(环形)，长度：2^n ， index只需要递增，下一个元素位置就是 index & (2^n - 1)按位与就可以了。
			>读写一致性保证：CAS保证写的同步从而一致。即会先申请位置，然后CAS方式写入。(实际上：可以先读取index, 再计算出下一个位置，再CAS写入，失败重读index)
			>缓存行补齐：避免伪共享问题。一个缓存行有64字节，一个缓存行就是cpu L1L2缓存失效的基本单位,而缓存失效是其他cpu发出的失效命令导致的---即改cpu要更新数据写到主存(同时加内存屏障)。
			>avalibleBuffer: 写入后更新多个位置读avalible，读出后更新那些位置写avalible。从而下一次写之前读位置时就有位置可以读了。	
			
	
大数据统计分析方面：Hive为例子
	>
大数据数据导出方面：sqoop为例子
	>

架构设计与技术选型：系统设计/方案设计评审	
	>系统设计的几个主题：参考：https://blog.csdn.net/u013007900/article/details/79049961

软件工程方面：
	>系统建模：数据流图、架构图、时序图、组件图。。。。参考：https://www.jianshu.com/p/4c9f795da7ea
	>
	
#开发方面：
	>能提前评审发现隐藏的潜在的问题：也发现该技术的能力边界。 
	
	
Spring方面：
   >IOC和AOP的实现：
	>IOC：bean交给spring来实例化，只需要配置该bean的属性，对于对象类型的属性-甚至可以不必配置--直接默认也交给spring注入，。(控制反转，依赖注入)
    >AOP: 方法的代理 ，代理方法的内容就是切面编程(方法的前后两个切面)。
	>spring做的事：
		>1.浏览包，对每一个带注解的类/接口 对应生成一个继承/实现它的子类/实现类(字节码)(代理类、动态代理类)--这个类对父类方法全部重写-内容都是调用一个回调类的方法(如果动态代理是cglib，就是实现了MethodInterceptor的类的intercept()方法;javax.tools包则根据类字符串而动态字节码生成)(这个回调类方法里，spring的处理是先获取方法的所有注解，先执行在被代理方法invoke之前要执行的注解的实现类的before方法(以及显示匹配本方法的切面bean的before方法)，再invoke父类方法，再执行之后注解实现类要执行的after方法(和显式匹配到本方法的切面bean的after方法))，将此代理类反射实例化，放入容器(map)。(常规的@Controller @Service @POST @RequestParam; @Aespect)。。属性的注入体现了spring做的依赖注入IOC，方法的代理体现了spring做的切面编程AOP(编写一个切面)。
		>2.执行bean初始化方法：init-method
   >WF实现的IOC和AOP:
  >事务的产生：在并发请求下确保数据库数据的一致性变化(要保证一系列操作具有ACID四种特性。为分别这些操作和其他无4特征的操作，称有4特征的一系列操作为一个事务，为分离4特征的一系列操作和其他操作的额外动作就是事务的动作---如打开事务、提交事务、关闭/回滚事务；从操作数据流上看，是这样子)。数据库使用事务的具体做法：打开事务、提交事务、关闭事务/回滚。(非分布式所以没有预提交的过程)
	>数据库对事务的实现：加锁(打开事务)--> 新临时空间执行一系列操作 --> 提交修改(两份，一份老一份新，乐观锁)--> 释放锁--->返回成功。paxos的两个阶段也类似于获取锁而顺序化、提交修改而成功而释放锁。
	>行级锁：必然要记录哪些行没有加锁，加了哪些锁
  >分布式事务的产生：如上，就是在分布式环境下确保数据库数据的一致性变化。数据库要提供一种机制，确保 可以让一系列操作 具有ACID四种特性。
  
  >事务的实现：依赖数据库的事务：打开连接-打开事务-执行操作CURD-提交事务(故障则回滚事务)-关闭连接。。可见执行操作CURD可以被切面编程，而前后的动作都是固定的，所以spring只需要直接写一个事务切面就可以了--用户则只需要使用这个事务切面的注解即可。       参考资料：https://www.jianshu.com/p/2449cd914e3c
	>事务的隔离级别：5种，上述。事务之间互相影响的结果/程度/大小。
	>事务的传播行为：决定新建一个事务，或者加入一个已经有的事务(使用同一个连接)(已有的事务是什么？---即保存在事务ThreadLocal里的当前线程的值)(此外，还有连接ConnectionHolder， 也是当前线程保存的连接)。
		>产生此概念的原因：A方法上有事务，B方法上有事务，而A方法又调用了B方法。
		>行为1：PROPAGATION_REQUIRED 保持一个事务。
		>行为2：PROPAGATION_SUPPORTS 支持当前事务，没有就也不新建。
		>行为3：PROPAGATION_MANDATORY 支持当前事务，但没有要抛出异常。
		>行为4：PROPAGATION_REQUIRES_NEW 当前有事务，则挂起。建立另一个事务执行完再说。
		>行为5：PROPAGATION_NOT_SUPPORTED 不使用事务，当前有也不使用--把它挂起。
		>行为6：PROPAGATION_NEVER 不使用事务，但没有要抛出异常。
		>行为7：PROPAGATION_NESTED 当前有事务，则嵌套一个事务；当前没有事务，则新建一个事务执行。
	>事务的保存点：回滚到保存点。保存点之前的不用回滚，确认已经ok。
	>mysql 命令控制事务：
		>设置数据库、会话的事务隔离级别： set session transaction isolation level read committed; set global transaction isolation level repeatable read;
		>设置不自动提交：set autocommit = 0;
		>开启事务：start transaction; begin;
		>回滚事务：rollback;
		>提交事务：commit;
		>在一个事务中设置多个保存点：savepoint tx1;   则xxx之后，可以 rollback to tx1;//回滚到保存点
		>开启只读：在作用结束前本会话也读不到其他会话/事务插入的内容：SET TRANSACTION READ ONLY;  结束只读：commit;

>设计模式：很大一块是在处理/trade-off权衡：集中与分散的问题。将分散集中的办法：如工厂模式、建造者模式。节俭节省的问题；代价和收益问题。编码中。
 >		
		
>业务模型：	业务高峰：如中午/深夜。
 >业务建模：
	
>源码计划：
 >集合:
 >线程：
 >锁：
 >NIO： 

 
>工具开发标准：
 >需求和功能和设计特性：没有这样特性的成熟系统，则自己开发。
 >实现方案：(数据视图层次)数据结构、算法、底层技术、(编码层次/代码组织层次)机制框架模式。
 >未来一定会出现的各方面的情况和场景和问题的罗列：预先考虑并处理。
 >性能指标测试：
 
 
>监控开发完毕之后：
 >源码看+性能测试实验编写：以便对性能指标能说得出来。会测调比较---从而真正的会技术选型。
 >基于Nginx...进行二次开发：开发模块。
 >开源项目快速架构梳理-模块梳理：对它的每个模块都能评头论足，如网络通信模块、线程模块、锁模块、数据结构和算法模块。
 
----------------------------2020-7-9----------------
最好的架构讨论-确定过程。最好的思路，最科学的画图过程。(正确的思路过程而不是一个隐藏了思路过程的最终结果图)
>个人理解的架构：用这套逻辑结构(模板)来分析(按框填写，按分支匹配走)各个公司的软件架构。最好的架构思想：就是从最核心功能的实现开始，逐步的升级发展进化自动化，而把更多的问题就带出来了(稳定性、可靠性、可用性、扩展性、维护性问题)。
	>业务架构：
		>方案1-微服务架构:
			>落地问题1-: 设计方面：
				>分布式服务调用框架：发展进化阶段：实现服务部署和远程调用--->服务实例的注册、状态感知和通知订阅者(注册中心/配置服务器的引入,地址服务器的引入--来知道注册中心/Diamond服务器的ip列表,而调用者通过域名来查域名服务器来知道地址服务器的ip列表)--->服务调用者的实例选择、调用超时实现-失败容错(重试/熔断)等--->服务权限配置和实现、最大QPS配置和限流实现、客户端对各个实例的调用权重设置(Diamond服务器引入)--->服务治理：服务节点水位(cpu/mem/io)监控和实例节点扩缩--->服务治理：调用链跟踪的支持、每个调用都记录，服务实时监控平台(qps,请求失败量,超时量,平均耗时分布)---->服务运维：业务完善后的技术优化：通讯框架netty,序列化协议hession(数据量较小时好)
					>服务提供者的定义：提供远程调用的服务，部署在web容器，容器运行在docker资源容器。
						>需要解决的各方面问题：开发的服务接口实现代码部署在哪里-来供远程调用？远程调用协议支持什么？如何保证服务的高可用？(集群部署、缓存)如何保证服务的稳定性？(限流、降级)
							>服务实例的新增和退出的监控和通知：
							--限流隔离、灰度监控
					>服务调用者：调用远程服务。调用端。
						>需要解决的各方面问题：服务在哪里？服务怎么调用？对服务容错的措施？对服务保护的措施？
							>服务实例的选择算法：
							>服务实例调用失败的后续策略：
							>服务调用超时机制的实现：
								>准备连接时：将Thread.currentThread()当前线程推入一个专门负责中断的Utils工具类，这个工具类有独立线程，专门负责定时中断传入的线程；传入的线程+时间封装到一个对象而加入队列，独立线程负责查看队首的到期时间--如果到了则中断，否则sleep那么长的时间
								>超时之后的重试：相同机器的重试，不行则摘除/熔断降级; 或者换不同的节点再次重试。
							--超时重试、降级熔断	
					--架构过程时序：
						>rpc的实现-->注册中心的监控和通知-->调用端负载均衡和超时重试-->服务端鉴权和限流-->服务实时治理中心的服务节点流量监控统计(超时/耗时/异常统计)和水位监控(告警和扩缩)-->服务线下治理中心的服务调用链路展示和各环节平均耗时统计、异常调用的各个环节的传递的参数
				>分布式服务的治理：治理原因--乱-崩溃-高负荷。目的：提供稳定及时可靠可扩展的服务。
					---对于一个好的系统：快速响应且总在服务-->不容易挂(检测自身bug)-->挂了快速恢复(不可抗外力)
					---高可用：本质上就是 部件冗余+故障检测+快速切换；故障检测：心跳机制(检测网络)+资源报告(cpu/mem/disk/thread/升级/进程); 快速切换：部件管理+切换策略(最少连接/轮询/随机)；部件冗余(主备主从多主)+分散存放(机器机架机房)
					---高稳定：本质上就是 检测并消除不稳定因素：检测：压测、引流、灰度、监控；不稳定因素：内存泄露、线程死锁、资源利用率太高；消除：扩容-限流、降级-熔断、缓存-池化-并发-异步；用户级/os级简化操作步骤
					>高可用：(含义、架构、策略(冗余检测切换恢复))处理挂了怎么办的问题？(单点挂了怎么办)(挂了怎么快速恢复服务) https://www.jianshu.com/p/35e4c2db6fde (度量一个系统一年之内持续提供服务的能力:99.999%则5min停止)
						>单点挂了：切到备节点(主备架构)、从节点(主从架构)；选举新主节点；哨兵；
						>同城节点都挂了：异地灾备恢复，切换到异地；
						--计算高可用：
							>架构方式：主备(备不提供服务)、主从(主-任务A,从-任务B)、对称集群(节点相同服务)、非对称集群(节点不同任务：master-任务1，slave1-任务2,slave2-任务3)
								>多机热备：一主多备；主异常则备监听到而接管主。
									>软件方式实现：
										>Keepalived: 基于VRRP协议。主备机器上都需要部署。
											>原理：让一组路由器(nginx)构造一台虚拟路由器(形成一个备份组)。按照优先级从备份组选举主控路由器，作为master,其他为slave,master负责数据包转发，master故障时按照动态的故障转移机制切换到备用路由器---优先级最高的slave抢占为master,接管虚拟Ip，继续提供服务。
										>其他原理：
											>动态ip: isp分配给连入internet电脑的一个ip地址。同一个电脑，不同时候连入可能被分配的ip地址不同；
											>静态IP：isp分配的固定的ip地址，一般是给某个公司的某个网站、邮件服务器的。人们用固定ip构成的url访问网站比较麻烦，因此引入域名来映射固定ip;那么url就变成了域名等来描述了。
											>浮动ip: 项目管理者手动分配给 实体设备的；实体设备被分配静态ip和浮动ip，静态ip用来内网实体设备之间通讯，浮动ip用于被外网设备识别和访问。
											>浮动ip的获取和分配：用户获取了浮动ip, 他可以分配这个浮动ip给某个计算实体，分配之后也可以移除而分配给其他计算实体。浮动ip就像一种资源。
											>多个平行的浮动ip给同一个服务：来保证其中一个浮动ip不可用时 有其他浮动ip可以接管。
										>浮动ip应用场景：failover 和switchover。故障恢复和手动维护切换。后者比如：网站升级、应用升级(一个节点开始升级，则它的流量被其他节点接管/分配给其他节点；切换完成，则重新导入流量)。
										>浮动ip应用场景2：对外提供统一的IP，而不是实际的服务器ip。https://www.cnblogs.com/wangwangever/p/8125464.html
						--存储高可用：	
							>架构方式：主备(备不读写)、主从(主读写，从读)、主备/主从切换、主主(双主-都读写，互相同步)、集群(数据集中集群--一主多备、数据分散集群--HDFS架构-数据分片及其副本分散在多个机架)、分区(洲际、国家、城市、同城分区)
								>主备：同步方法：可以按照用户Id来逐个同步，因为可能落后几秒；而在这几秒内，所有没有在线的用户都可以标记为同步完成，而可以切换到备数据库，来实现热切换；对于新增的用户，自然是在备；
									>对于灾备，如果是主直接挂了，那么自然需要等待几秒，将binlog数据同步完成，才切换到备；如果主已经无法访问，网络中断，则几秒的数据，需要等待？还是就允许不一致。
						>基本解决策略：冗余检测切换恢复
							>消除基础架构中的单点故障：某个功能部件没有冗余，就认为是单一故障点；
								>案例1：负载均衡器 + 冗余web服务器 架构；此架构中，当一个web服务器故障，则nginx自动摘掉而将流量重定向到剩下的在线服务器(即nginx接收到调用失败则重新选择节点)。
									>web服务器层不是单点故障：因为 有 冗余、有检测、有切换恢复。
									>nginx负载均衡器是单点故障：因为没有冗余、没有检测、没有切换恢复。
										>解法：浮动IP. 即ip地址重映射，ip地址在不同的服务器之间移动(不同的服务器上就是不同的nginx)，而DNS-ip则保持固定不变；当ip映射的静态ip地址不可访问时，切换到其他静态ip地址，即让ip地址浮动到另一个服务器；(ip地址在内网机器上重分配？)
						>负载平衡器的选择：
							>HAProxy: 多个层负载平衡，不同类型服务器的负载平衡；
							>浮动IP后创建负载均衡器集群：Corosync, Pacemaker两种软件实现；
						>负载均衡器的介绍：
							>DNS轮询：本地DNS服务器-->DNS根服务器-->.com域服务器-->163.com域服务器 
								>缺点：无法感知服务器宕机；无法身份验证。
							>全局负载均衡系统：
							>内存缓存系统：CDN
							>服务器负载均衡系统：https://www.cnblogs.com/and/p/3366400.html
								>调度算法：轮询、加权轮询、最少连接、加权最少连接。
								>接入方式：串联路由模式、单臂模式、DSR模式
								>健康性检查算法：探针机制，检查服务器真实的健康状况，避免客户端的请求分发给故障的服务器。提高HA能力。
									>Ping: ICMP报文。
									>TCP: 
									>HTTP:
									>FTP:
								>系统加速：
									>SSL加速：
									>HTTP压缩：对html/css/js采用http压缩技术。降低网络传输量。
									>连接复用：服务端用同一个连接处理多个客户端的连接。
									>TCP缓存：中间层负责TCP三次握手和四次挥手；数据交互则中间层透传。
										>三次握手：像篮球投递：发SYN, 收ACK+SYN,发ACK; 
										>四次挥手：像通知回复机制：发FIN，收ACK; 收FIN,发ACK；。。
											>含义：只要不发送FIN,维持长连接，那么显然服务端可以推送消息给客户端的，这样，聊天工具就形成了。	
								>会话保持：
									>IP会话保持：同一ip请求发送给同一个处理服务器。在会话期限内。
										>NAT技术：内网一个机器发送的请求到网关的NAT模块，则NAT模块会替换包里的私有IP地址为公有IP,或者私有ip地址前面添加公有IP地址，再发送到互联网；同理，收到消息后，也是寻找包里的私有地址而转发到内网中对应的那台机器。
									>Cookie会话保持：同一个http请求中相同的Cookie发送给同一个处理服务器。
									>URL哈希会话保持：无需会话保持表，同url计算的结果相同则转发到同一个处理服务器。后端服务器：Cache服务器为常见。
								>设备选型：
									>硬件：F5,citrix,Redware,Aio
									>软件：LVS,Nginx,HAProxy,zen loadbalance
					>高稳定：(含义、机制、策略)处理怎样才不容易挂的问题？(单点不容易挂)
					>高并发：(含义、机制、策略)海量请求怎么接住且正确处理的问题？(多手接快速返回)
					>高性能：处理响应怎样更快的问题？(单点请求更快)。减少锁同步，减少线程创建销毁切换开销，减少IO阻塞影响。
					>治理目标：稳定性、可靠性、扩展性(集群方式天然保证)。稳定、及时、可靠。(不能停、不能错、不能等)。(高性能: 不能等)(不能久等结果、不能等到拒绝服务结果、不能等到错误/失败的结果)
						>高可用：稳定及时。减少停工时间，不能服务时间。99.999%可用。只有很少的不能提供服务的时间。(减少串联，增加并联)
							>读且无状态：多主/多实例即可。
							>读写：(分库分表)多主多从，一主多从。
							>高可用策略：做更多的并联和隔离。分流(集群：多主-主从; 目的就是隔离)和灾备(同城热备-异地灾备)。硬件上：双网卡、双电源、多机房。
								>软件：
									>集群：主备，主从
									>容灾： 同城热备和异地灾备。两地多机房，同城两活。两地双活三中心。
									>冗余：
									>隔离： 故障隔离
									>单元化：
									>灰度：蓝绿发布。
									>正确的灰度(无缝重启)：先摘除，等先前流量处理完了/跌0后开始重启容器；启动后，逐步引流，正确性验证--有异常及时摘除。
										>命名服务：Name Service , 自动摘除故障节点。
									--异地双活：的部署架构。数据库“集群-分库-双活”
									--热点消除、写冲突避免：软硬件优化。
								>硬件： 
									>双电源：
									>双网卡：
									-多机房：灾备
							--节点主备：一主一备，一主多备；主备切换--keepalived实现
						>高可靠：一致容灾
						>高性能：成本与效率。缓存，和优化。
							>cpu/mem扩容：
						>高并发：对系统能并发处理多少请求的度量。qps, 响应时间。并发用户数。
							>高并发策略：
								>缓存：
								>池化：连接池、线程池 、资源池 
								>并发：多线程
								>异步：消息队列/线程异步；
						>高稳定：各种突发和大流量下,升级版本，cpu/mem应该平稳，服务平稳，没有故障；
							>稳定性机制：
								>服务本身被调用：限流、集群、隔离
									>隔离(措施)：业务隔离、分pc/app端隔离、机房隔离、物理机隔离、区域隔离
									>集群：主备、主从(从提供部分服务)、多主(都完整服务)(负载均衡集群)
								>调用其他服务要：超时、重试、降级。缓存。
									>降级：不直接查服务/数据库，而直接查备用的服务，查缓存，甚至固定的数据。
							>稳定性策略：codereview-->压测-->灰度-->监控
								>codereview
								>压测：高并发压测单机，负载达到60%；全链路压测
								>灰度：分地区、分用户、分端 来分组；以做到隔离。
								>监控：系统和服务的监控，指标数据收集；巡检平台。
							>服务运行不稳定表现：进程异常退出或卡死。
								>内存泄漏：
								>线程死锁：
								>资源占用未释放：文件句柄、数据库连接、网络资源
								
					>治理手段：缓存、限流、降级。业务一致性平台。(缓存-避免久等(及时性/可用性)，降级-避免因为个别子服务的网络原因等异常而完全拒绝提供其他子服务(可靠性)，限流-避免资源不足而计算失败/错误/传输丢失结果(可靠性)、同时避免崩溃(稳定性))。业务一致性平台：对比对账--可靠性。
						>限流算法: 
							>漏桶算法: 可以保持比较稳定的输出速率;相当于有个缓冲的过程.超过缓冲区的直接抛弃;强行限制最大传输速率(限制瞬时并发数);削峰填谷;
								>漏桶已满: 新的流量是抛弃还是等待?
								>问题: 有固定延迟问题. 比如突然来了大流量,然后后面的流速和出口流速一致,那么漏桶始终满的,始终有延迟,每个流量都有延迟.
								>简单实现: 先漏水(和上次时间间隔内), 漏了之后还是满的,则拒绝,否则添加;-成功 。即：间隔小则加到桶里，间隔大则成功；直接成功或等待或失败；
							>令牌桶算法: RateLimiter 恒定速率往桶里放令牌 , 限制平均传输速率
								>应对场景:突发流量,如巨型流量 uΔt = V + vΔt,  尽管u很大,但是总量不大则以然可以缓冲.(对于漏桶,则巨型流量会导致漏桶里的阻塞量越来越多)
								>没有令牌,是等待还是抛弃?
								>简单实现: 先加令牌(和上次时间间隔内), 加了之后还不够1个,则拒绝,否则成功；；。。即：补充这段时间的令牌 后 扣令牌；直接成功或失败；
							--共享和互斥的全新理解:
								>共享就是重入次数无限的锁: 且对不同的线程都重入次数不限; Semaphore 属于, 锁能一个线程获取,然后另一个线程释放;---可以限制资源同时被访问的线程个数
									>死锁恢复:
									>限制总并发数:Semaphore更加适合. 比如连接数据库的连接池,线程池. 另外,AtomicLong也可以,毕竟每次incrementAndGet()的结果每个线程必然都是不相同的;
										>只需使用一个api: 不用手动释放: guava.LoadingCache, 来定期的失效而更新为一个新的AtomicLong(0)
								>互斥就是对一个线程的重入次数不限而对其他线程重入次数限制为0: 锁只能一个线程获取,同时该线程释放;
							>计数器控制:AtomicLong 非阻塞的返回 限流/不限流标记.. 
							>消息队列MQ: 流量削峰
							--nginx lua各维度限流
							>滑动窗口限流算法：来个请求，根据时间看落在哪个单元里，有必要则计算增加新的单元，然后统计得量是否过限
						>分布式限流: redis + lua 
							>发送lua脚本: jedis.eval()可以实现; 还是使用 key过期 + 总量机制; 类似guava.LoadingCache (类似计数器算法)
							>(模糊的/但比较好的)节点自己算：分到自己头上，或者权重配置到自己头上限流多少；
				>分布式服务的运维：问题排查、调用链跟踪、每个调用过程的定位。
			>落地问题2-：开发方面：
				>session共享：
				>分布式事务：添加、删除、更新。
					>2pc-事务协调器:
					>try-commit-cancel: 
					>分布式事务框架: 开源的. 主-子 
					>事务消息: 
			>逻辑问题3-：运维方面：推进方面-试点方面
				>服务边界的划分标准：服务重用、数据统一
				>服务接口的定义：
		>方案2:-SOA架构：
	>数据库架构：
		>方案1-主备架构：性能问题：读写分离-主从架构。数据量问题：分库分表(数据分区)。分布式事务问题。
			>采用这方案的原因(对症下药)：基于实际情况的需要、当前的指标要求和未来的考虑打算扩展。
				>实际情况：并发量、数据量、日增长量；
				>实际情况的需要-目标-要解决的问题： 稳定性、可靠性、可用性、高性能、扩展性。
				>可用的资源：根据实力-是否无限制。
				>上述条件圈定的实际技术方案的长相、画像和模样：开源产品/方案中是否有和这个画像非常相近的相似物，如果没有则自己开发(进一步的问题：需要多少资源、多长时间、多大把握、多长远的价值)。
			>落地问题：
				>设计方面： 
				>开发方面：分表之后 Join,group,排序,事务的问题。
					>分库分表： 在建立索引时，需要异构索引。对同一条数据对不同字段先后hash而分库分表，存2份乃至多份。
					>读写分离：产生副本。需要同步。
					--CAP：最好的介绍思路：数据不分区(C)(单机数据库)-->数据分区(CA)(分库分表)--->分区有副本(AP,CP)(读写分离,分布式数据库)
						--自然引申介绍BASE：可用性和一致性之间的取舍。根据实际情况，对于大部分web系统(核心诉求是高可用)，允许很短时间的不一致(出现软状态)，但不允许不可用(基本可用-函数降级)，同时要保证最终是一致的；提供这样的服务的分布式系统，就是一种符合BASE的服务。
					>分布式事务问题： 事务日志记录和补偿(消息投递)、数据版本和乐观锁、事务消息(消息有状态)(预减流水，即辅助明细表)
				>测试方面：
				>运维方面： 
		>方案2-分布式系统： 
			>采用这方案的原因(对症下药)：基于实际情况的需要、当前的指标要求和未来的考虑打算扩展。
				>实际情况：数据海量、日增巨量
			>落地问题：
				>设计方面：
					>分片: 划分和存储、扩展(产生新分片)。
					>分片副本：多地-多机房-多机架分发存储和失效崩溃转移、副本选举出主副本fast-paxos、 
					>请求路由到分片： 读请求路由到副本、写请求路由到主分片
					>分片主-副数据同步：分布式数据一致性算法paxos
				>
	>缓存架构：
		>方案1:-主从架构：
	>分布式消息系统架构：
		>方案1：
	>分布式配置架构：
		>方案1： 
	>分布式监控统计系统架构：
		>方案1：
	>分布式搜索系统架构：
		>方案1：
	>私有云平台架构：
		>方案1：
	>大数据平台架构：
		>方案1：
	>CI-DI自动化平台：
		>方案1；
 
-------------------------------------------------2020-7-13-------------------------------------
讨论原理：原理连成一片，有条理有逻辑--知识点组织方法，都按照升级过程和业务模型/概念模型来描述。按先后想知道的问题逐步画出来。从知识点结构来看，关于这个知识点最重要的部分逐步展开。
>面向对象来描述：
	>java优化：
		>系统层面：缓存、限流、降级
		>业务层面：调用链中微服务性能瓶颈找到并替换(codereview,单机压测，全链路压测)
		>代码层面：字符串拼接优化、锁优化(逃逸分析/锁粗化:jvm和编译器会做)、事务合并、多线程-有些动作是否可以并发执行的、异步-有些数据是否可以直接先返回、调整没必要的对象创建、消除告警。减少锁同步，减少线程创建销毁切换开销，减少IO阻塞影响。
		>虚拟机层面：新生代老生代垃圾回收算法选择、堆区最大最小初始化及eden-s0s1-old比例
		>编译器优化+JIT优化开启：或用第三方进行；热代码编译执行。
	>锁： (使用互斥量更新，使用CAS更新,直接更新)
		>锁对象ObjectMonitor: 是重量级锁类型时，MarkWord中30位指针指向的对象，另外2位就是锁标记位--取值10。
			>重入次数：count
			>阻塞池：EntryList
			>等待池：WaitSet
			>当前持有此锁的线程：owner
		>锁对象Lock Record: 是轻量级锁类型时，MarkWord中30位指针指向的对象，另外2位就是锁标记位--取值00。这个锁对象在虚拟机栈的栈帧当中。
			>MarkWord的拷贝：即标记位为01时的32位内容(已偏向或者未偏向)(解锁时替换回对象头里)
			>元数据： 
			>owner:
			--没有EntryList,WaitSet,所以当第一个线程更新Mark word成功为轻量级锁之后，运行中，第二个线程来只尝试1次CAS更新lockRecord指针为它的lockRecord指针,如果成功则获取到锁，失败了则更新锁标记位为10即为重量级锁，同时自己进入EntryList阻塞;等待第一个线程退出时尝试更新MarkWord而发现已经膨胀而唤醒EntryList中的一个线程。
			>遇到竞争时：其他线程则CAS失败，然后尝试自旋，自旋失败最后再尝试升级锁，然后自己在entryList里阻塞。
			--获取锁和解锁：分别成功和失败。解锁失败的原因---都是因为锁标记位改变了--升级了。
		>偏向锁：没有新建对象，Mark word中，除了锁标记位位01，另外1位标识是偏向锁-即值是1，剩下4位对象分代年龄，25位的ThreadID+Epoch时间戳 指向偏向的线程；精确到秒，时间戳是10位；
			--没有EntryList,WaitSet,也没有额外的锁对象，直接更新Mark Word的锁标记位位01和偏向位为1和CAS更新ThreadID25位为当前线程ID,成功则偏向成功。。是CAS比较，则和哪个比较？即比较值和更新值是什么？是标记位、偏向位 还是 其他位置？：：可能的情况是：比较 偏向位，而更新ThreadID位。所以如果已经偏向，则第二个线程更新失败，从而更新锁标记位为偏向锁，而第一个线程还在执行，那么第二个线程则自旋？
			>引入偏向锁的原因：偏向锁应用的场景：实际场景中有 不仅不存在锁竞争，而且还经常是一个线程反复获得多次锁；而为了让这个线程获取锁的代价更低，就引进了偏向锁。因为轻量级锁也会CAS更新Record lock和复制它，有代价。
				>偏向锁：则只CAS更新ThreadID和epoch两项；//单个线程只更新epoch//适合于交替访问。
			>遇到竞争时：偏向的线程先到全局安全点暂停，解锁，撤销偏向；(竞争者则先自旋一定次数，还没获取到则将锁升级为重量级)。如何判断遇到竞争？其他线程发送的撤销偏向锁信号?
			--获取锁和解锁：分别成功和失败。解锁失败的原因---都是因为锁标记位改变了--升级了。
		>无锁： 没有新建对象，Mark word中，除了锁标记位位01，另外1位标识是偏向锁-即值是0，剩下4位对象分代年龄，25位的对象hashcode
		>GC标记： 锁标记位是11，剩余部分都是空。参考：https://www.jianshu.com/p/36eedeb3f912
		>锁的内存语义：
			>锁释放时该线程本地内存中的共享变量刷新到主内存： 同时MESI机制通知其他高速缓存相应的缓存行失效
			>获取到锁的线程：计算时发现已经有失效的，所以重新从主存加载；
		--无超时机制，非公平锁；不可线程中断地获取锁/不能非阻塞获取/不能超时获取；锁升级/不降级；结构决定多个锁不能交叉只能内包(不支持交叉就不支持降级)；异常则退出代码块时就退出--一般的锁必须手动退出-否则节点不会消除
		--编译器可以对锁包围代码进行逃逸分析而锁消除-不用同步(无锁)(synchronized代码块是有monitorenter-monitorexit; 但方法上则是方法标记位中一部分)	
		--synchronized作为修饰符比放到方法体内生成的字节码会更少，效率更高13%；
		--升级而不能降级：目的是提高获得锁和释放锁的效率；因为环境已经变了。
		>volatile: 
			>标记的变量，每次访问时，都会对主内存进行一致化；那么意味着写则没有？
		>synchronized:
			>有序性：屏障指令(读写的时候前后加，四种)，避免指令重排；
			>原子性：一次一个线程完整执行完毕。
			>可见性：JMM多核之间的可见。
			>可重入性：
			>不可中断性：
			>不可超时性：
			>非公平性：
			>升级过程:
			>锁优化：锁消除核锁粗化
	>java object header:对象头
		>32位的Mark Word: 
		>32位的Class Metadata Address：类型指针，指向类元数据Class对象,来确定这个实例是哪个类的实例。Class对象存储在堆。
		--如果对象是数组，则还有32位的数组长度
	>jvm虚拟机：
		>运行时数据区：(不包含直接内存区)
			>堆：
				>空间划分：
				>对象分配过程：new的过程：先heap分配内存，布局如下；实例数据字段先默认缺省值；调用构造函数，先只有局部变量表填充形参，后调用父类构造方法,后本类的实际构造函数里的代码；
				>对象内存布局：对象头+实例数据+对齐填充(保证对象起止空间差是8字节的倍数)
				>对象引用类型：按回收时机早到晚：虚引用-为了回收时收到通知(引用队列;get()返回始终是null,而不是引用;)，弱引用-下次回收(不能阻止被回收，即便可视可见Gc Roots引用着)，软引用-堆区将溢出(缓存数据可以封装为这个类型)，强引用-永不回收
				>确定对象存活算法：
					>根搜索算法。GC roots对象来源：三个区引用的对象。即：方法区静态变量和常量引用的对象+虚拟机栈中局部变量(表)引用的对象+本地方法栈中变量引用的对象。说明正在运行需要，所以直接可以判定不可以被回收。
				>垃圾收集算法：
				>垃圾收集器：
				>对象确定要回收的回收过程：先确定 没有与Gc Roots开始的可达树 相连的对象，对每一个，判定它也不是强软引用类型，判定要执行它的finalize()方法，则统统加入F-Queue队列，专门的Fianlizer线程会执行这些对象的finalize()方法,如果执行后又和GcRoots树相关联，则不被回收，否则被清除。
					>方法区回收：运行时常量池中无引用字符串+无引用类。
			>虚拟机栈：
				>分配规则： 一个线程一个。
				>栈元素：栈帧：
					>局部变量表：本地变量表。其中的引用类型变量是直接指针，指向堆区对象。
					>操作数栈：字节码指令将变量/常量加载到操作数栈。也能将数从操作数zhan存储到局部变量表。
					--栈上分配和标量替换。
					--下一个栈帧A的操作数栈和上一个栈帧B的局部变量表有部分重叠原因：A->B关系，A传递的入参即在A的操作数栈，而B的栈帧中则是形参在局部变量表；
					>动态连接：栈帧指向运行时常量池中该栈帧所属方法的引用(取代码来执行)。
					>方法返回地址：栈帧出栈，将返回值放到下一个栈帧的操作数栈；所以这个方法返回地址 ，在出栈前必须知道，然后才能赋值入栈到操作数栈。
			>方法区：
				>运行时常量池：即Class文件中常量池部分的内容 + 运行期间加入的内容(String.intern())。
				>类代码：用户写的+cglib等动态代理产生的。
			>程序计数器：一个变量。
				>分配规则：一个线程一个，记录解释器执行到的字节码位置。
			>本地方法栈：
		>类加载机制：
			>加载：有连接过程;
				>双亲委派模型：是由java.lang.ClassLoader.loadClass(String name , boolean resolve)实现的。
					>其中继承ClassLoader的类需要实现：@findClass(String name)方法
					>用户一般不会直接实现这个，用户自定义的ClassLoader会使用：@Class<?> defineClass(String name, byte[] b, int off, int len) 这个核心方法，将从文件等地方获取的byte[]数组转换为Class对象--内部还是一个native方法实现：private native Class<?> defineClass1(String name, byte[] b, int off, int len,ProtectionDomain pd, String source);
						>在这个defineClass1()本地方法，一定对当前this即类加载器实例的类有感知，并且保存在了方法区。
				>查看一个类的加载器：A.class.getClassLoader(); 如果是null则是启动类加载器；如果是应用类加载器，则sun.misc.Launcher$AppClassLoader
				>自定义类加载器来将字节数组作为类内容而加载为一个Class: 只需要三步：第一，继承class NetworkClassLoader extends ClassLoader；第二，重写findClass(String name), 第三，重写内容为获取byte[]后调defineClass(name,b,0,b.length)即可；。名字最好有结构："java.lang.String"之类。
			>验证：常量索引值和字节码语义正确性分析。
			>准备：
			>解析：将字段表方法表等中的符号引用(对常量池中第几个变量的引用) 转换 为地址引用(直接地址)
			>初始化：类加载，但不一定执行初始化。初始化:new-读取静态, main所在类,反射调用,初始化类的父类,。新建类数组不会(jvm生成新的类)，引用父类的静态字段则子类不会，final静态字符串字段引用不会--已经编译替换。
			>使用：
			>卸载：
		>对象生命周期：
			>创建：分配空间(头-实例属性零值-对齐)->构造方法层层调用->指针引用赋值局部变量
			>使用：调用方法、访问属性之类；
			>不可视：程序运行到一个位置，上面的局部变量在局部作用域内已经无法访问；
			>不可达：已经被标记，从Gc roots开始不可达；
			>可收集、终结和释放：finalize()的执行和对象空间的释放；
	>Class文件：
		>内容结构: 无符号数、表
			>无符号数：
				>编译器版本：
			>表：
				>字段描述信息：
				>方法描述信息： 
				>接口描述信息：
				>常量池：
					>内容：字面量和符号引用。
					>内容类型：表
		>内容顺序：
			>魔数：cafebabe  4字节--实际u4
			>版本：xx.xx 4byte--实际u2+u2.  52代表1.8
			>常量池： 
				>计数器：多少个常量。u2
				>第一个常量项：u1常量类型标记位 + 该常量类型规定的占用字节及其用途。如果要引用其他常量，规定的字节位置代表的索引值 就是 常量的序号。
					>占用字节常见结构：先u2标识内容字节长度 + 再紧跟着这么多长度个字节的内容(如UTF-8缩略编码字符串,具体值如：org/apache/Unsafe)。这种常量类型往往用来描述 字段、方法。
				>第二个常量项：u1常量类型标记位 + 该常量类型规定的占用字节及其用途。
				...
			>访问标记：2byte 标记：类/接口/枚举.., 标记2：public/abstract/final是否有
			>类索引：u2 索引值指向常量池中CONSTANT_Class_info类型的常量。这种类型的常量，内部又会指向其他常量-如字符串类型常量。
			>父类索引：u2 索引值指向常量池。
			>接口索引集合：u2集合：先u2接口数 + 再 若干个u2接口索引。
			>字段表：
				>u2容量计数器
				>字段1描述：u2修饰符 + u2简单名称索引(字段名) + u2描述符索引(字段类型，方法参数列表/返回值，构成的字符串)
				>字段2表述：...
			>方法表：
				>u2容量计数器
				>方法1描述：u2修饰符 + u2简单名称索引(字段名) + u2描述符索引(方法参数列表/返回值，构成的字符串)+ u2属性项数计数器 + u2属性名称索引(如索引到的值为Code， 名称决定属性值的结构) + u2操作数栈最大深度(max_stacks) + u2本地变量表容量(max_locals) + 2u2字节码区域空间长度 + 这么多个字节的代码指令 + 4u2显式异常处理表(a->b行发生异常x跳转到c行) + ...其他属性	
				>方法2描述：...
	>线程池：设计：架构过程。运行：提交任务和执行任务。
		--->依次的执行者：new worker-->加入队列(如果入队了线程池立刻停止则rejected执行,如果入队了worker全退出了则新增无任务worker)-->new worker-->rejectedHandler
		>数据结构：任务队列
		>算法：
			>默认任务：new Worker()时就注入了Worker.firstTask。条件为：当worker数<maxPoolSize时
			>从队列取任务：取fistTask, 否则无限时间 或者 限时poll--看worker数。worker数>corePoolSize且小于maxPoolSize 则限时拉取。
			>提交任务：给默认Worker, 否则给任务队列, 否则调拒绝执行处理器。当worker数>maxPoolsize时，或者添加失败，或者线程池已经不在running
		>api: 
			>shutdown(): 
				>一是cas+自旋方式将状态更新为shutdown(running->shutdown->stop->tidying->terminated),
				>二是在mainLock同步下interrupt()所有worker---则影响等待的worker和正在执行用户(join/wait/sleep)逻辑但处于等待中的worker
				>三是尝试更新为terminate状态：但当前为running、shutdown&队列不为空、worker数>0、已经为tidying 这四种情况下直接放弃；否则在mainLock同步下先cas为tidying，然后强更为terminated
			>shutdownNow():
				>一是cas+自旋方式将状态更新为stop
				>二是在mainLock同步下interrupt()所有worker---则影响等待的worker和正在执行用户(join/wait/sleep)逻辑但处于等待中的worker
				>三是转移任务队列里的任务到列表里返回：
				>四是尝试更新为terminate状态：同shutdown()
			>addWorker():
				>哪些情况下不再使用worker来执行：state>shutdown(即执行了shutdownNow()), state==shutdown但firstTask!=null或者firstTask空且任务队列已空也不执行；直接返回false
					>第二大类：worker数大于了 maximumPoolSize, 当然并发情况下可能会超，因为不是同步的判定。
				>在mainLock同步下新的worker添加到workers：添加条件是：state<shutdown, state==shutdown且firstTask==null 
				--说明了执行了shutdown()之后提交新的任务不会被addWorker(), 从execute()看，只有reject()拒绝执行handler处理这一种结果。同理,shutdownNow()也是。
			>execute():
				>哪些情况下才会尝试新增worker: worker数<corePoolSize;此时并发下，新增的worker就可能超过corePoolSize，甚至maxpoolSize;  第二是：状态为运行，添加任务成功，但是没有worker了，则主动新增。
	>线程：
		>线程状态：new->runnable->blocked->waiting->time_waiting->terminated
			>wait for monitor entry: 被阻塞了等待被唤醒
			>wait for condition: 主动释放了锁而等待被条件唤醒
			>sleep: 休眠等待(单纯主动阻塞,无锁)
			>park: 停车等待(单纯主动阻塞,无锁)
			>running:
		>线程之间数据的可见性：
			>内存屏障指令：
			>volatile:
			>锁： 
	>IO模型： 
		>同步：
			>阻塞：BIO :  阻塞等待连接就绪，就绪了需要手动调用API读取数据；
			>非阻塞：NIO： 用户不用阻塞等待，只需要内核轮询；就绪了还是需要用户手动调用API读取数据；
		>异步： 
			>非阻塞：AIO： 用户不用阻塞等待连接就绪，同时连接调用时就注入了回调处理类----当连接就绪时自动回调。
-------------------------------------------2020-7-13----------------------------------
跨语言描述：彻底弄懂的原因：从核心开始，层层递进封装，不断提问，各种提问；都能解释和给出结果。(知其然知其所以然)
>面向业务模型来描述：
	>mysql: 事务、锁、索引
		>锁： 行锁、表锁。锁实现了事务的隔离性要求。
			>行锁： 共享锁、排他锁。
			--一致性非锁定读(隔离级别：读已提交|可重复读)：读取的是历史版本数据(快照数据,一行记录的最近快照数据)，无需加锁和等待锁释放--没有事务会修改历史版本数据。
			--一致性锁定读：select ..for update 加X锁(阻塞S/X锁); select ... in share mode  加S锁(只阻塞X锁)
			>latch: 线程锁。轻量级锁：排序对象-线程。保证并发线程操作临界资源的正确性，没有死锁检测机制。读写锁、互斥量。
			>lock: 事务锁。排序对象-事务。用来锁定数据库对象：表、页、行；在事务提交/回滚后释放。死锁检测。行锁、表锁、意向锁。
				>行级锁：共享锁S、排他锁X。行的共享锁，事务T1获取后，未释放时事务T2也可以获取。加行锁前，需要对表加意向锁-意向共享锁IS(想获得表的某几行的共享锁),意向排他锁IX(想获得表的某几行的排他锁)；
					>行加X锁后的读：一致性非锁定读，则不等待X锁的释放的直接读取快照数据，快照数据在undo日志里。提高数据库的并发性。行多版本，对此的并发控制就是MVCC。
					>锁的释放：
					>锁的算法：
						>Record Lock: 对索引页记录加锁。
						>Gap Lock: 对辅助索引范围加锁，但不对范围内已有记录加锁。阻止多个事务将记录插入同一个范围内，因此对这个范围加间隙锁，避免幻读问题。
						>Next-Key Lock: 锁住一个范围，且对范围内的已有记录加锁。
				--查看事务对表加的锁：查表。也可以查看哪个事务阻塞了哪个事务。
				--查看事务隔离级别：select @@tx_isolation ...开启事务不意味着就给行加锁了。select ... for update 加X锁(在可重复读下，是加的next-key lock, 在读已提交下，是加的record lock)，select ...lock in share mode 加S锁(范围锁,next-key lock算法的 S锁)。两个share-insert 操作事务，可能有个事务就会报死锁错误。
			>隔离性问题： 参考https://my.oschina.net/woter/blog/1822293
				>脏读：读取未提交的数据。A更新，尚未提交，B读到了，而后来A又回滚了，则B脏读。
				>不可重复读：没有范围加锁，所以产生幻读；幻读问题就是不可重复读问题。
				>丢失更新：第二类：AB都读取数据后更新，第一个的丢失了，没有实现真正的乐观锁方式。第一类：A读，B读，B读后更新提交，A读后后于B而执行了回滚，则B的提交丢失了。
				>幻读：尽管是可重复读，但如果不是select ... col > 100 for update之类的带范围和X锁的查询，或者更新，则也不会加间隙锁；即只有在这两种情况下才会加间隙锁；其他的sql语句还是不会加间隙锁，因此会出现幻读问题；例如仅仅select ** from x而没有用for update 带X锁，不带独占锁和范围锁/间隙锁都会出现幻读的问题；
			>死锁问题：
				>超时机制：
				>wait-for graph: 等待图。死锁检测,innodb采用的方式。深度优先算法,非递归方式；检测到了则直接回滚undo量最小的。
			>表锁：没有索引的情况下，查询Innodb引擎表使用的是表锁。	
		>隔离级别： 设置：在 会话事务级别： set session transaction isolation level repeatable read;
			>读未提交：
			>读已提交： 
			>可重复读：读取的始终是事务开始时的数据版本；即便第二次读之前其他事务修改了这一行的值并且提交了，它再次读的时候也是读取上次读的那个版本。
				>在此隔离级别下：开启事务 带X锁的对索引字段范围查询如select... col > 100 for update 会加间隙锁Next-key lock, 尚未提交事务，此时执行另一条事务语句，如插入105记录，则会阻塞，直到第一个事务提交了或者回滚了。
				>在此隔离级别下：开启事务  带X锁的对索引字段定值查询-走索引(才会使用行锁-独占行锁)，那么使用行锁；行锁是对索引确定的行加锁，而不是直接对记录加锁。只要确定的行已经加了锁，即便用其他索引确定同一个行，也会被阻塞；
				>在此隔离级别下：开启事务  带X锁的对非索引字段定值查询，使用表锁;(因为是对索引项加锁)
		>事务： 
			>事务类型：扁平事务、带保存点的扁平事务(无需全部回滚;但无持久性，系统崩溃需要重头开始做事务;且不释放锁)、链事务(一个事务完了释放锁)、嵌套事务、分布式事务
			>事务的原子性和持久性：redo log来保证。恢复提交事务修改的页操作(记录对页的修改)(事务对页的物理修改);物理日志。顺序写；512字节为块单位进行保存，和一个扇区大小一样，保证原子性，不用doublewrite。
				>log block: 512,其中12开头8结尾，中间是连续的事务记录。
			>事务的一致性：undo log来保证--通过回滚行记录到到某个特定的版本；逻辑日志。随机读写；记录的是历史版本。
			>事务的隔离性：锁来保证。
			>显式开启和提交/回滚：begin;start transaction;rollback ;savepoint;rollback to savepoint ;commit;。不显式，那么insert/delete/update就是默认提交。
				>不能回滚：ddl。truncate table t;
			>分布式事务时：innodb隔离性级别需要设置为 序列化。
				>XA事务：资源管理器 + 事务管理器 + 应用程序 来完成。
					>JTA的支持：MysqlXADataSource 为基本的API接口。
				>内部XA事务：写binlog和InnoDB引擎写redo log  采用XA事务来保持一致，如果写binlog后宕机了，那么重启后会重新写redo log提交。
		>日志：
			>undo: 事务提交之前的数据。即将要修改的数据，先copy出来到undo log, 回滚的时候就会使用到(前向回滚)。记录的是历史版本，所以一个用来回滚，一个作用用来读。
				>作用1： 回滚时读取出来覆盖。
				>作用2： 提供多版本并发控制MVCC下的读。即读某行记录，发现有X锁，则当前事务读取该行的历史版本，实现非锁定读。
				>存储位置：数据库内部的一个特殊段内，段位于共享表空间中。文件在数据文件目录下。
				>记录内容：
					>回滚操作：insert->delete, delete->insert, update->update。insert 的undo就是delete, delete的undo就是insert;update的undo就是update
					>内容形式：回滚段、日志段、undo页、undo log。
						>undo log日志格式：delete/update/insert都不相同。查看表：innodb_trx_undo 可以看到对undo文件的页号和offset下增加的size内容。
				>undo log会产生redo log: 因为undo log需要持久性保护。
				
			>redo: 事务的内容，失败的成功的,未提交的回滚了的都会记录。即事务修改的数据，修改好了，先copy出来到redo log,多处修改则多处都写入redo log, 最后持久化redo log,  然后再提交事务--就写到了binlog。参考：https://www.jianshu.com/p/5d7eda0205ea
				>作用1：确保事务的持久性。
				>位置：存储引擎的数据目录下，至少两个:ib_logfile0, ib_logfile1; 文件<512G,不能太大也不能太小；太小则来回切换性能抖动async checkpoint。
				>记录的是每个页page的更改的物理情况：51种重做日志类型：redo_log_type, space表空间id, page_no页偏移量,redo_log_body每个重做日志的数据部分,
				>redo logfile 内容表面形式/存储形式：Log File Header + CP1 + CP2 + log block +log block.... 。其中 log block 里的内容就是真正的T1事务的内容，长度不同，但无间隔顺序连续存储。
					>重做日志的内容格式：51种重做日志类型。
						>头部格式：redo_log_type + space + page_no 后面的内容：根据不同的重做事务类型和事务内容会有不同的格式。
					>LSN: 重做日志的字节总量。重做日志中有，每个页中也有-每个页的头部保存了。
					>恢复过程：innodb引擎启动时，都会例常 检测页的LSN和重做日志中该页的LSN，如果后者更大，且该事务已经提交，那么数据库就需要进行恢复操作；将重做日志应用到P1页中。小于则不需要。
						>查看LSN：show engine innodb status\G; 可以看到页的LSN,重做日志的LSN，刷新到磁盘的LSN(checkpoint的LSN)
						>恢复动作：顺序读取重做日志后并行应用,即应用对页的修改-聚集索引页和辅助索引页的修改都有记录。
				>写入方式：先写入重做日志缓存，再按一定条件顺序写入日志文件。从缓存到日志文件：按照512字节即一个扇区的大小，逐个写入，因为扇区是写入的最小单位，所以保证写入是成功的，因此无需双写。
					>缓存到磁盘的条件：master thread每秒会将缓存写入磁盘--无论事务是否提交。第二是配置参数值指定通过事务提交时主动刷一次磁盘,这个参数值是1,来保证事务的持久性。
			>binlog: 事务最终提交之前的事务内容；记录所有对数据库执行成功的更改操作。server层，上层一点。用于主从复制，按时间点恢复、审计-是否有对数据库进行注入的攻击。
				>可能因为缓存而丢失数据、可能因commit未执行而保存了需要回滚的事务：这需要正确设置某些配置参数来避免。
				>row模式更消耗存储：因为要记录每行的更改。
				>不止Innodb引擎的，Heap, Myisam也记录。
				>记录的事务的具体操作内容：是逻辑日志。
			--errorlog(启动运行关闭过程的记录-错误-警告-正常的信息), 
			--slow query log：慢查询日志，定位存在问题的sql, 设置long_query_time阈值，超过的都记录下来。	 
			--relay log： 从节点的IO thread收到主的binglog增量后写到relay log; 后Sql thread读取relay log应用。
			--query log: 查询日志。所有的请求信息，即便连接失败也记录。
		>数据-日志文件：
			>datafile: 对应有缓存，data buffer ;当不显式加事务时，只是刷到缓存，就成功了，不会等待持久化到磁盘datafile, 加事务-因为ACID保证，尽管也会出现没有持久化到datafile的已成功事务，但是redo 日志已经保存了，可以恢复(数据库世界里，日志最重要，数据文件不是最重要的)。
			>logfile: 对应有缓存, log buffer 。undo log 和 redo log日志都是先写到 log buffer ，后刷新落盘到log file的, 为立即落盘到log file:设置innodb_flush_log_at_trx_commit=1来保证数据不会丢失。这种先持久化日志的策略叫WAL：Write Ahead log。
				>在做data buffer丢失的数据恢复时：从 logfile的最后一个checkpoint检查点开始，逐个成功的事务的执行即可 恢复出数据。
				>当data buffer写入磁盘后，对应会将checkpoint写入redo log, 这样redolog最后一个checkpoint之前的数据就已经在data log中持久化了。
		>索引：
			>B+树: 每个中间节点为一页大小16kB(32个磁盘扇区,4个文件系统块);对于主键为bigint的索引，每个中间节点可存储的指针数=16kB/(bigint 8byte + pointer 6 byte)=1170个。实际因为指针比数据多一个，实际更多。每个叶子节点可存储的数据:如果每条数据1kB,则可存16条。从而2层，可以存上万的数据。那么3层：1000*1000=100w,4层：10亿指针，百亿行数据。
				>好处： 一个中间节点可以存一千多个bigint-指针 + 范围查询 + 天然排序。数据按主键顺序存储。
				>数据页：双向链表互相指向，首位也互相指向。每个数据页内的记录之间也是链表指向。
				>根节点：和内部节点一样：存放 key + 指针。
				>非聚集索引：可能只有1个页，对于其中每个节点：其中key是该列值，pointer是主键值。
				>联合索引：也是辅助索引。(a,b,c) 那么 where a=xx and b=xx order by c 也是可以使用到的。
				>覆盖索引：当返回的列就是辅助索引的key或者不需要数据行，那么也会使用联合索引--即便不满足最左匹配-即(a,b)联合索引--但查询满足b列覆盖索引即可--那么这个联合索引也会被优化器使用。优化器选哪个索引，一看筛选排序列，二看返回数据列，三看用户是否指定强制索引force index。
			>MRR优化：优化器先将查询条件拆分，再进行数据查询；optimizer_switch参数控制启用MRR。
			>ICP优化：在存储引擎层进行where条件来过滤记录, 从而返回数据库层的数据会大大减少。index condition pushdown
			>缓存页的索引：hash表。缓存池中有大量的页，比如10M缓存池则大概640页，则查询索引采用hashtable,且key=spaceid<<20+spaceid+offset
		>应用问题： 
			>为什么要建立联合索引：节省空间
			>explain sql的结果中的type列和ref列：
				>type: const--使用了索引(主键索引);index--排序列使用了索引; ref--索引列后面部分没有使用到。range--有的列使用了in来过滤/或者like查询%不出现在开头/或者出现范围查询<,因为可以使用到叶子节点互相指向-所以也不用filesort。all---没有使用到索引:非最左匹配/函数列不会使用到/表达式列不会使用到。
				>ref: const,const,const 三列过滤都使用了索引
				>key: PRIMARY ：实际使用了的索引：这里是主键索引
				>key_len: 4;59; 使用了索引的多少前缀;4个前缀还是59个前缀。	
				>extra: Using where ---说明sql中有的列没有使用到任何索引 而 走了筛选where的办法。Useing index--可能因为满足覆盖索引条件而使用了联合索引。using filesort---说明使用了额外的排序-即没有使用索引天然的排序。using interact(b,a)则使用了a,b两个索引的结果求交得的结果。Using MRR 开启Multi-Range Read 将访问数据转化为顺序后查询，性能得到提高。Using index condition;
				>possible_keys: 可以/可能使用的所有索引
			>建立索引的条件：表记录数大，超过2000; 列选择性高：distinct / count >0.5
				>对某列值的前几个字符建立索引、参与联合索引：
			>show profiles: 显示查询语句的耗时。
			>前缀索引不能应用于：order by , group by 操作??；也不能用于覆盖索引。参考：https://blog.csdn.net/weixin_30531261/article/details/79329722
			>mysql优化：开启查询缓存; explain看使用索引;sql优化最少的搜索如limit 1;垂直分表;事务合并(避免大量事务而大量写重做日志)
				>sql优化、事务合并；
				>开启查询缓存;
				>索引优化：建立索引、辅助索引、联合索引；开启MRR优化、ICP优化；页缓冲池预热。
				>锁优化：共享锁;
			>幻读是如何避免的：next-key locking
			>mysql优化：sql优化、索引优化(不妨联合索引,不用索引)、MRR优化、ICP优化、缓冲池预热。
		>存储引擎：	表级别定义
			>innodb: 面向OLTP应用。支持事务、聚集索引、行锁设计。支持一致性非锁定读(MVCC-不同隔离级别下读取不同版本的数据)。1TB数据,1000w性能有所下降，但非线性。最大64TB
				>高性能高可用的支持手段：
					>插入缓存(辅助索引更新批量到页,提升性能)：因为辅助索引的索引页的访问和插入往往是随机的，性能就有下降；对于这个问题，则先将索引插入缓存中Insert Buffer--B+树，当用bigmap跟踪到buffer的某页没有无空间可用时则开始和原页合并,或者页读取到缓冲池时也会触发合并。
					>双写(提升数据页的可靠性,先到double buffer, 先写临时表空间--这样数据表空间不变,然后才开始写数据表空间;假设写临时表空间时挂了,那么数据表空间不变,如果写数据表空间挂了，则用临时表空间恢复即可;接下来用redo重做日志即可恢复)。
					>自适应hash索引: Innodb存储引擎为热点页建立AHI 索引：即自适应hash 索引。建立条件：单模式访问100次-则建立该页的AHI。读写速度提高。
					>刷新临近页: 检查脏页的临近页，是脏页则一起刷新。 
					>异步IO: 磁盘操作性能的提高。读取页 时，非顺序发-收-发-收，而是先顺序发，后等待所有IO完成，甚至连续的页可以merge合并返回。实际：read ahead(预读), 脏页的刷新都是AIO。iostat命令查看rrqm/s,wrqm/s
				>缺点：存储消耗大-同一张表存储空间占用更大。批量插入慢。
			>myisam: 面向OLAP应用。默认支持256TB单表数据。不支持数据缓存支持索引缓存;支持全文索引。支持查询缓存。
			>ndb: 集群存储引擎。Join开销大，因为在数据库层完成。
		>Mysql整体组成：连接池组件-->查询解析器-->优化器--->缓存组件-->插件式存储引擎
	>redis: 数据类型、持久化策略、缓存过期策略、内存使用策略(虚拟内存)
		>数据存储位置：内存。有持久化。
			>持久化方案：//往往master不开启而slave开启。
				>aof(操作日志缓存后append,每秒/每操作)(appendonly yes配置开启)：实时、体积大、对性能影响大、兼容性好。不丢失数据，恢复慢。
				>rdb(数据定时dump,到临时文件后替换)(fork一个子进程执行)(默认开启)：非实时、体积小、对性能影响小、兼容性差。丢失数据问题，恢复快。
			>避免单点故障：主从复制。
		>值的类型：string,list, set, sorted set(zset)、hash。
			>string: 字符串。setnx k v 当k不存在时设置。底层：c的动态字符串。
				>位串操作：最大长度是512MB即2^32约40亿位。高效在于：setbit/getbit 时间复杂度为O(1)。bitcount/bitpos/bitop时间复杂度O(n) 
			>list: 链表。使得lpush代价更低。元素为字符串。40亿个元素可以。BLpop key timeout 移除第一个元素，没有阻塞timeout
			>hash: 哈希的值只能是字符串。H开头的命令。hsetnx key field value 当field不存在时设置field-value。40亿个field可以。
			>set: 集合。字符串无序唯一集合，hashtable实现。sadd key mem1 mem2...。sinter key1 key2 key3求交集。40亿个mem可以。
			>sorted set: 有序集合。字符串有序唯一集合。mem唯一而分值可以重复，重复后mem按照字母序排序。zadd key score mem score mem2 ...。
				>其他函数：
					>指定score范围返回：zcount key min max 。
					>指定元素索引区间返回：zrange key start stop ; zrevrange key start stop ;
					>指定首字母区间范围：zrangebylex key (b [p ; zrangebylex - +;
			-HyberLogLog: 基数统计，固定内存来存一个集合的基数。
		>并发控制：使用队列，并行访问变为串行访问。
		>key淘汰策略：不删除、全部lru、expire那部分Lru、全部随机、expire那部分随机、expire那部分ttl最短的
			>key超时：和key的值改变有关，即便改变了那么必须要重新设置expire过期，否则就是不过期。
		>IO模型：多路I/O复用模型，非阻塞IO。
		>底层模型：使用自己的VM机制,调用系统函数时间开销更小。
		---上述5个原因导致：redis单线程，但是快。内存(cpu不是瓶颈)、数据结构简单、串行访问无锁无线程下上文切换、NIO、自己的VM机制调用系统函数。
		>内存模型：数据、进程(代码、常量池)、缓存(客户端连接、复制积压缓存、AOF缓存区保存最近的写命令)、碎片。
		>主从复制：实现读写分离，高可用、热备。读负载均衡，但写仍然单点。
			>丢失数据问题：如果写成功而master就挂了，slave还来不及复制刚写成功的数据，则这个数据就会丢失。(即便可能aof机制已经存储到了master的日志里)
			>哨兵机制：failover，自动故障转移和恢复。
			>写高可用：双机热备,两个master。
		>数据分片：cluster模式
			>槽位算法：普通hash算法---当节点增加或删除时会有大量key都会缓存穿透。容错性差。一致性hash算法：根据hash环上删除一个节点测试，只会让一个节点上的部分key缓存穿透--不再从这个节点上读-而从新节点上读-从而穿透-但少；但有数据倾斜的问题。redis采用：crc16 % 2^18计算得出。
		>应用问题：
			>缓存击穿：缓存某个key到期后重新请求数据库，并发量高则压力大。解法：热点数据永不过期-binlog同步更新。
				>解法2：分布式锁来进行缓存重建？(并发量必须不大)(节点部署不多则用本地锁也可以)
					>其中：分布式锁可以建立多个：甚至每个key都建立一个锁，来达到不影响其他key的好的并发。分组key共用锁则其次
						>分布式锁： 
							>redis: set key uuid NX EX 100 这样原子的命令，来做设置值uuid且过期时间指定；释放锁，则需要 提交Lua脚本---判断UUID是否是自己的，是才释放；保证自己释放自己的，自己不释放别人的；----即同样支持过期机制，过期了再次释放--就不会释放别人的了！！
							>zk: 创建临时有序节点；共享锁和独占锁都可以实现了；
			>缓存穿透：不存在的key/id, 每次都从数据库读。一个是代码检验过滤，一个是“预加载+binlog同步”(数据量就这么多，不存在的数据库里也一定不存在)。
			>缓存雪崩： 大量key同时到期后重新请求数据库，并发量高则压力大。解法：热点数据永不过期+binlog同步更新；随机缓存时间。
				>解法3：备份缓存。A缓存设置过期时间；B缓存不设置过期时间。(备份缓存的更新，即走备份的时候启动一个逻辑去查数据库来更新)
			>何时使用缓存：缓存和数据库之间不要求强一致。
			>memcache区别比较：
				>支持的值数据结构：
				>是否使用虚拟内存：物理内存使用完后，redis可以交换到磁盘。
				>过期策略：mem set指定，redis 可以后续expire指定
				>数据存储安全与灾难恢复：mem无持久化；redis有aof,rdb 持久化和恢复。
				>数据一致性：mem使用cas, redis则弱。	
				>内存管理效率：mem更高。
				>100k以上大量数据：mem多核使用，性能更高。
			>分布式锁的实现：setnx k v 不存在k时设置k的值,同时设置超时时间--在jedis上一个函数set()即可， v=requestId即是谁加的锁；即方面后面用Lua脚本cas方式的解锁。(猜测加锁和解锁都是Lua脚本实现的，Lua脚本运行在redis保证了原子性)
			--总结：数据预热/预加载 + 随机失效时间 + 分布式锁更新/备份-定期更新。
		>高级工具：性能测试、Pipeline、事务、Lua自定义命令、Bitmaps、
		>常用管理命令：dbsize---key的总数；monitor---实时监听server接收到的所有请求信息。exists key是否存在key
		>应用场景：
			>分布式锁：setnx, 设置过期时间。
			>异步队列：list, rpush ,lpop
	>kafka: 一个主题若干个分区，一个分区若干个副本。
		>producer: 目标主题+发送的内容-->序列化-->分区器--->记录批次,
			>消息的键：辅助分区。
			>消息发送：重试。
			>消息可靠级别：acks	主分片持久化。acks=1则要首领副本收到消息,acks=all则需要所有的副本分区收到消息；acks=0则无需等待，发出即可。
		>consumer: 消费者群组下的消费者：消费的分区不重复。一个分区只被一个消费者群组下的一个消费者消费。一个主题下的多个分区。不同的消费群组，互相不影响。从某主题某分区某偏移量开始拉取-->反序列化
			>消息消费模式：pull/push
			>分区上的偏移量：消费者提交。自动提交，手动提交，异步提交。
		>broker: 保存成百上千个属于不同主题和分区的副本。处理客户端(生产者消费者)、分区副本、控制器发送给首领分区的请求。
			>首领副本：
			>跟随者副本：。和首领同步的跟随者副本。
			>线程模型： 监听端口都有一个Acceptor, 创建Processor线程处理，将请求放进请求队列，IO线程读取而处理后放到响应队列，Processor再从响应队列取出响应消息发送给客户端。
			>保存的额外信息：元数据：主题列表，每个主题的分区-每个分区的副本-哪个副本是首领。
			>发送分区消息给消费者：零拷贝技术；sendfile()
			>分区分配：n个分区,m个broker,k的复制系数；多个机架。
				>分配规则： 假设各个broker在哪个机架上已经确定了，那么对broker排序：broker1, broker2,.... 相邻的broker在不同的机架上;然后对每个分区-副本开始分配，此时轮流算法：分区0首领在broker1,则它的副本依次放到后面的broker上，分区1则首领在broker2,它的副本依次放到后面的broker上，如此下去。
			>文件管理：一个分区多个片段；	分区保存在磁盘。
		>控制器：全局一个，是某个broker,所有的broker通过zk创建临时有序节点确定是哪个--以及动态切换。
-----------------------------------------2020-7-13--------------------------------
中间件源码：知识点的总结：最好是树形---从左到右为：抽象到具体。一层一层，扼要而清晰。
			认识到知识的层次非常重要。不要把不同层次的知识混在一起讲、一起看、相提并论，就会没有条理就会乱，就会没有逻辑关系。
>面向业务模型描述：
	>spring: 
	
		>AOP:
			>jdk动态代理handler: JdkDynamicAopProxy
		>jdbc: 
			>事务传播级别：request, request_new, nested, madatory, support, not support ,never
---------------------------------------2020-7-13-------------------------------
常见问题：
>java api:
	>1.7hashmap resize()死循环问题：
		>原因：将老元素hash之后放到新的位置：头插法---三角形两边替换一边。
			>并行头插法：并行执行时，移动两个元素AB,假设线程1先执行：A->X, 线程2执行：B->A->X,结束；线程1继续：->A->X; next是B,开始第二轮,而线程2已经改过，所以第一步获取的next是A, ->B->A->X, 开始第三轮：,next=X，则形成A->B->A
			--简化为：两个线程并行的将A->B转移到head->下，发生的循环问题。
			--形象解释：将链表中一个元素取出放到头位置，但是不处理指向这个元素的元素指向下下个元素。
	>1.7hashmap resize()丢失数据问题：
			>并行头插法-模型类似：但此时是分别转移到两个head, 两个线程并行的将A->B转移到head1,head2下，线程1刚把A转了head1->A暂停，线程2则执行成功：head1->A,head2->B, 线程1继续，next是B，开始第二轮，head1->B->B产生了死循环。两种方式都会产生死循环。
	>1.8 hashmap resize() 还是会出现不安全：覆盖问题。对null元素的赋值，两个线程并发，就会覆盖前一个的。
		>对列表和红黑树节点： e.hash & oldCap == 0 来分割出低位和高位两个链表，而分别放到新数组前后两半的原因：e.hash是高低位亦或之后的结果,已经离散在某个8000的范围内，oldCap一般是2的指数次，相与==1说明这个位为1，说明e.hash至少是大于等于它的，因此有合理性，且这个位也比较随机。都分裂为2个单向链表。
	>1.8 hashmap: 确定索引：h ^ (h>>>16) & (n-1) 简单位运算就进行了分散；好处：entry[]每8000一个段，如果hashcode值在某个区间之内，那么它经过这个运算之后还是落在在这个区间，只是已经混肴了,分散了，区间分散了。前面是亦或的好处;相与的结果一定小于双方，可能发生碰撞(位运算更简单--取模更复杂)。
	>1.8 concurrentHashMap: 
		>确定Node数组的大小: 大于等于 initailSize / loadFactor 的最小的2的n次方的数;
		>size()的计算: 关键是put()方法 最后 的@addCount(1, binCount); 然后是这个方法调用的@fullAddCount(long x, boolean wasUncontended)
			>对于@fullAddCount()方法: 对当前线程,计算随机数存储到线程的属性h:threadLocalRandomProbe 里-->从大小为2开始创建CounterCell[]数组,后续2倍扩容-->定位当前线程put新增元素的计数 应该对哪个CounterCell新增:算法(n-1)&h定位到,cas创建值为1(即位置为null),或者cas更新CounterCell.value+1(如果更新失败,h改变计算新位置;再次初始或cas+1更新还是失败则扩容2倍;但超过cpu核数则只改变h重试而不再扩容)-->特殊情况,如果cas更cellBusy来尝试新建CounterCell时cas失败了,则直接cas对baseCount+1就结束了.
			--说明:CounterCell 这个类有@sun.misc.Contended 注解,避免伪共享;(即自己在value属性后补充6个字节达到64B--一个缓存行的大小)
		>1.8死循环的方法putIfAbsent(): 条件在：用嵌套的computeIfAbsent(key1, key ->{return map.computeIfAbsent(key2, keyx->23);})，且key1,key2的hashcode一样；
	>1.8 concurrentHashmap: 确定索引：h ^(h>>>16) & (0x7fffffff) &(n-1)
	>arraylist: object[]数组,
	>CopyOnwriteArrayList: Object[] 但是每次添加,都复制新数组,且使用锁同步
	>hashset: hashmap为实质,值为new Object()
	>TreeMap: 红黑树
	>ConcurrentLinkedQueue:  单向链表
		>offer(): 从tail开始,cas设置tail.next为新数据节点, 如果成功,则cas更新tail为新节点; 如果cas tail.next失败了,则循环内重试;
		>poll(): 技巧1: 标签来避免递归...
			>从head开始,cas更新Node.item为null, 
				>如果成功:更新head为p.next 
				>如果失败: 则可能被抢先了,先看p.next是否是Null,如果是,那么head更新为p; 如果不是,那么p=p.next开始下一轮
	>阻塞队列: offer-poll(返回特殊值), add-remove(抛出异常), put-take(一直阻塞) 
		>特征维度: 数组\有界\优先级
		>出入同步: 可重入锁 
		>条件等待: 可重入锁的条件对象: 等待两个条件-: empty时拉取阻塞而等待notEmtpy, full时添加阻塞而等待notFull
		--park()使用了互斥量
	>双端队列:LinkedBlockingDeque
	
------------------------------------------------------------------------------------
常见问题： 
>分布式id生成方案：
	>分发整数区间：
	>纳秒时间戳15位+10位节点id编号(查数据库分配一个/微服务分配一个)+7位内部自增数。共32位数字。
	
-------------------------------------------2020-7-20----------------------------------------	
创业方向：
>制定50个可能的方向和产品/服务，选择出5个成功概率最大的和最好的：
	>
------------------------------------------2020-8-15------------------------------------
找工作：真正看重的东西
>项目：不是简单业务逻辑开发，而是业务建模、数据分析到系统分析架构。
------------------------------------------2020-8-15---------------------------------------
平时修炼：真正看重的东西  
>心态上：不要把自己太当回事，坚决避免出现别人尊重我 多于 我尊重别人；甚至就要避免别人尊重我。同时也要避免或者提前预见别人即将不尊重我，什么言谈举止 会导致 别人会不尊重我，这样的言行举止需要避免----典型的：自大凌驾别人。
>能力上：可能不要把工程技术当作自己的强项，当然再差也要达到很强的水平；但最强的(同时也认为是最独特最有优势最有趣的)，一个是数学物理推理建模能力；二个是用户需求细述(商业分析)；产品体验创新设计、工程技术(系统分析架构能力)。
	>精力投入：
		>数理推理建模：机器学习、深度学习、强化学习。自动化工具、效率工具。
		>用户需求细述：用户需求是什么？当前行业商家的满足情况？用户的满意度情况？满意度指标？
		>系统分析架构：抽象分析，层层细化，到技术选型；
		>产品体验创新：吸引人的东西是什么？这个产品上能否有？怎样的形式和内容加进来？
>职业上：行业、项目、技术、待遇、规模、氛围。

-------------------------------------------------------------------
	
>项目中的技术难点、有技术挑战的项目：(不能对技术难题没有概念，而问的时候又不知道回答不上来----问的就是技术难题：必须要事先准备---技术准备是对面试官最好的态度和尊重---否则被哄出去：出洋相出丑)
 >就是面试官问的关于项目的刁难的问题：缓存、限流、降级、瞬间峰值、
 >讲法：课题明确抽出定义，思路分析推理证明，设计评审(自我评审,要考虑哪些问题哪些情况哪些方面一定会发送的情况全部需求全部场景一定要知道；对一件事情-你觉得要考虑哪些问题-罗列一下-可以看出差距:绝对不是无章可循，然后思考方案；方案系于方面)编码
  >P7级别了：见识和观点。有自己的语言概念体系，有自己的规律总结。根本原因和直接原因按某个维度分类。框架和逻辑分离。
  >墨菲定律：如果有可能出问题，就一定会出问题。
  >不是考虑了主要情况就可以，根本不合格。要考虑所有情况，才是工具开发的基本。描述必分方面,方面必有主次.
 >项目中遇到的难题：必须直接要解决的难题，以及需要优化的痛点-技术痛点，如周边工具开发；技术难题：真正想问的---其实是技术方案(比如权限系统)，真正想看的----其实是方案提交评审之前你做的工作---你整理的框架以及如何将信息放到你的框架里、范式里(比如整理明确为：问题-方案 结构)(大多数人缺的，不是对一个已经具体描述表达出来的问题去思考它的方案，大多数人来说都养成了这种习惯了，但是却没有意识去具体描述表达出一个问题)。其实只要把问题表述出来，大多数人都会思考这个问题且直接下意识被问题吓到！！（就像问他一个广义相对论/量子力学/统计力学的问题一样）
  >即：接手业务需求之时遇到的难题、措手不及、毫无思路的场景(条件-约束-目标)：然后解决的办法：即陈述业务需求，然后给出必然的最好的技术方案。注意：业务需求的陈述完全使用业务语言，没有一丝跟技术有关。
   >问题1：已知有一个知识库-问题库，求怎么给用户推荐问题？
    >第一步-问题分析：要解决哪几个问题。制定设计目标。明确目标关键参数。
    >第二步-方案调研和设计：比较各种技术方案的特点特征，优势劣势。如私有云--机器私有安全--可以充分配置优化；公有云--外部机器充分多--可以快速大量扩展。云使得业务工作负载(节点)在业务之间漂移。
	>第三步-方案论证：方案对所有的数据情况、用例情况、环境情况是否都能良好处理：优雅处理，一一讨论、论证；调整，改进，优化。
	>第四步-方案提交评审：
	>第五步-编码实现：
   >问题2：已知一门考试下多个科目，一个科目下多轮模考，	请设计一种权限体系，给购买了科目/模考的用户赋权：以后科目可能会增加或者减少、模考也会增加或者减少。
   >问题3：已知要售卖几门课程，要求商品单卖有优惠、组合购买有优惠、活动时间内购买也有优惠，优惠券也有优惠，请设计一种产品展示页面，页面中同步展示选购课程总价格：
    >子问题1：设计合适的订单表、订单状态的变化：
	>子问题2：购买商品后给用户赋权：
   >问题4：验证知识点更新算法的正确性：给定一个算法：如何判断它的优劣：  可视化
   >问题5：知识点新增或删除后的对用户的同步：
   >问题6：知识点重复推送原因是什么：
   >问题7：推荐时跨库、分库分表查询的问题：以后数据新增所要做的事情，都要拎出来，及早暴露，讨论。。而不是憋着，捏着藏着掖着，等到故障的时候才发现问题--注意“发现”两个字。
   >问题8：线程虚拟机栈监控。
  >所谓项目难点：不是叫实现一个TCP/IP协议(当作方案，则需求是什么)，或者实现一个微服务框架(把微服务当作方案，那么需求是什么)，RPC框架，或者实现一个聊天程序(把聊天程序当作方案，那么需求是什么)，尽管这对我来说是有技术含量的。干技术，一定要明确它的需求，就像作答一定要明确问题，听懂问题一样。
   >一时想不到、一时想不完的，都可以称作是项目技术难点。技术细节。各种情况的处理。
   >为什么感觉不复杂---仅仅考虑了方案而没有去想且论证最佳的方案：因为根本没有独立去考虑，去收集整理，去总结做事分哪些步，每步遇到了哪些问题，方案好不好。写得不够多？因为写的不够细！！！
   >给这种方案打多少分？思维方式的转变：
    >常见方案的不足和问题：
	>情况复述-重新表达，问题重新复述-重新表达，得出理想方案所要具备的特征和满足的关系/约束若干条：从表象到本质，从各种变化具体而异的东西到不变的东西，阶段划分，不同阶段不同理论，框架化，体系化，生长化进化。
	>关联导出理想方案：
   >讲：要像讲博士论文一样，一章一章各方面一个问题一个问题有框架架构地递进。而不是说一点点肤浅的。还可以应对各方面各种可能的质疑和提问。
  >所谓独当一面：就是可以做第一步到第五步的人，而不是只是做第五步的人。仅仅是想到方案，却不是最佳的方案也没有论证自己的方案。
   >清晰的知道整个大事有哪些步骤有哪些环节有哪些并行方面：各方面时空上如何配合，各环节各步骤上的最优方案是什么，齐心协力之下能否推出达到目标的结论。权谋、见识、指挥。知事，侦查反侦查(面试过程中，尤其到了高层业务负责人程度--更多)。


	---介绍的哲学：无论介绍项目、介绍系统、介绍技术、介绍机制原理：都要自顶向下介绍；先概括一句话说，后分开讲；(先用一句话讲，后用两句话讲，后用4句话讲，后更详细的8句话....)
		>解决问题的办法就是提出问题；
		
		
		-----不要骄傲自满--觉得自己学得差不多了。懂得差不多了。永远不要有这种想法。否则只会在高手面前出洋相。
  ---问面试官问题：不要问什么业务是什么、技术是什么，甚至“你觉得我的缺点是什么”这种求救一般的问题，非常迂腐愚蠢可笑。
					而是问现在遇到了哪些问题(技术难题，业务难题)，哪些问题需要解决？---对方需要，我也需要(当面解决他的技术难题)。问题导向。--希望我去解决什么问题！！哪些需求，哪些问题。
					问：业务现在有多少个项目已经启动，多少个在准备启动中，已经启动的项目进度到哪个阶段了，项目类型-存储结构，有多少个产品在跟进需求
					您希望您招的人能够为公司做出哪些贡献？
					问：数据量、并发量多大；采用的支撑技术是什么？
					贵公司对跳槽多是怎么定义和怎么看的呢？(是否一定不好？)(对公司设置的实习期怎么定义怎么看的呢)(不是所有的公司都会想得很好，说的跟做的一样)
					>工作制度、技术氛围技术水平、同事历史、领导靠谱；不当忙碌的螺丝钉
					--精准不废话---把事情说清楚/精细环节/不拖泥带水；
					>发现问题和找事情，并知道找的事情之间的关系；和现在的事情的关系；
					>团队有没有异性，多少人，直属上级是谁，之前在哪里上班；技术氛围怎么样；
					>不说不会，说还没有总结。
					
  --回答离职原因：不要回答什么没有活儿、涨工资问题。直接个人发展，没有技术挑战。。甚至可以说工作重复、乏味。
				 以前的公司都不是最终的归宿。(缺少技术挑战，不甘平凡)
				 想去大公司，职业背景，学以致用，运用一下自己的理论。接触和解决更复杂的问题(以前的处理好了吗？)。
				 工作重复简单没有技术含量没有挑战。
				 58--没有项目做。
				 >现在不紧张，30岁以后才紧张；所以选好公司很重要，对彼此都好；偏重技术，。
				 >水平太次：无法融入同流合污；
				 
  --回答能给公司带来什么：
				1.开发写代码(了解规范，看过源码，熟悉工具包---如guava),熟悉java,mysql,redis,spring,spring-boot,spring-cloud,linux下的devops, ci/di(swagger/jenkins/ansible/zipkin/maven/git/docker/k8s/nginx)(自动开发编译测试部署)
				2.制定技术方案和评估(微服务架构，推荐系统架构, 分布式数据库)，熟悉kafka/hbase/es/zk/rocksDB/oceanbase/spanner。各种微服务问题，微服务专家。全部数据库问题，数据库专家。(网络问题网络专家，操作系统问题操作系统专家)
				  >经常演练业务---技术方案的推导和映射。
				  >开源技术：各个赛道领域，建立了多维度的特征的排名。并同样有充沛的理由。并且每天在刷新。
				3.凭空构建一个业务领域内的优秀产品具有的所有业务和业务特征和功能(用起来轻松容易简单爽、安心、满意、值得推荐)：(调研)分析出而构建出一个逻辑体系。发现产品中存在的痛点----并有论证证明(数据和理由)----可以提高留存、访问和消费意愿。 
				 >经常追踪分析当前形势蕴藏的会爆发的业务形态：旧的或者新的。以及该业务最好的内容应该是什么样子了然。然后看当前产品具不具备---基本的让不让人满意--给打几分，不具备则有机会----决定做与不做。
				 >市场追踪：当前大家(男女老幼)喜欢的东西是什么？需求是什么？稳定需求硬需求是什么？
	
  --回答缺点优点：
	    缺点：超高高并发全路径的大项目经验不足；不太爱表现自己。不喜欢无效加班。每天熬到11-12点，没意义。不喜欢奉承跪舔。不喜欢无挑战工作。不喜欢不尊重人的氛围。不喜欢重复无意义的工作，无并发/无技术挑战内网项目。
			>爱睡懒觉，不太擅长应酬、交际活动不太活跃、比较宅。没什么形式，自然不装。平时看看新闻.看书.做笔记(写我的感受和认识).
		优点：务实，责任心强，爱学习，爱思考, 担当，抗压/心理承受力强，不断提高自己的水平(技术水平和业务水平; 因为技术其实是一种业务,业务也要有技术的严密态度;技术支撑-业务带动)。
				一定的压力可以让注意力和精力都集中。
  --跳槽的看法：不做简单业务逻辑开发；不敢再跳了。只要预判认为公司不再发展了，就走了；主动请命而不是被动听命，才是真正的大将-主将；主动发现问题-发现痛点-发现需求，发现机会，发现新业务，社会新业务，社会新需求，社会新问题；
  --自我介绍：1分基本+3分经历+4分技能兴趣+2分计划
  
  -- 有什么想问的：项目并发量多大、技术栈、有没有中台、团队工作内容-项目发展。
 -- 来公司的目的: 个人成长; 个人发展; 来也是,走也是.	对我没有提升就走了;或者发现自己没有提升就走了;。。没事干就走了。公司发展太慢，而个人发展要求较高。
 --怨气重不能要不能留，怕做出奇葩和阴谋怪事。
 -- 为什么要团队：正如为什么要集群？分布式。一个脑袋总有出问题的时候，多个脑袋保证高可用；	
 -- 不在于在公司待得久--来提高技术？而是阅读思考，和从头到尾深度参与多个高质量的项目，并进行深刻思考其中遇到的各个问题的发现和总结经验。	
	
  --谦虚谨慎，狐狸尾巴不能露出来，危机意识，绝对把握，不断前进。好勇斗狠，卑躬屈膝 都是下策；
  --不尴尬的夸奖：就是既夸他，又有独特措辞概念而体现自己思想水平深度(你知我也知道的)。
  --各种技术的原理机制：每天都去理解一遍。
  --全局性的。业务架构到技术架构的转变：
	>高可用性的条件：限流、安全、降级、容灾。分片--来实现负载均衡，副本--实现分片级别的高可用。
	>	
  >难题列表：
   >Nginx reload带来的服务抖动：
   >Web Container的服务发现：
   >复杂业务系统的抽象设计能力。//注意是抽象设计，而不是具体开发。抽象中就能设计
   >redis底层怎么实现的：
   >虚拟机规范和虚拟机源码：
   >方案集合：架构+问题确定过程 树形结构.
   >创业：聚集的人少，就会使得权力在某些部下身上集中。
   
  ---------------算法-源码-微服务(分布式系统)-数据库--------------业务模型|架构
  
>做非常困难的事：什么困难复杂做什么---比如源码理解，模型建立，预测分析计算，并且8月能力修炼完毕而预备以后去大公司。  
  >重复很无聊，但是重复可以很深刻。所以很少人 成功。
参考资料：
1.https://max.book118.com/html/2018/0827/7146011140001144.shtm(fastleader算法)
2.https://blog.csdn.net/qq_16525279/article/details/82424422(隔离性的较好描述)
3.https://www.jianshu.com/p/18fe36708dfe(可重复读和读已提交的最好描述)
4.http://coding-geek.com/how-databases-work/
5.http://qcsdn.com/index/order.html?id=cb54a138-208d-4400-a366-b6af195a06fc(下载地址)
6.http://www.yunpz.net/document/f-1c31f6dea1a046968798ae7bd4363676.html(下载地址2)