(严肃才能平静，否则会浮躁)
(叙述介绍，从分类入手；自顶向下进行，)
并发相关全知识：https://cloud.tencent.com/developer/news/313447 java并发编程的艺术
jvm技术规范：https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html#jvms-6.5.monitorenter
next work ,  sumerize every night!!new technology 
可以归结出几大技术点：(能力边界，消耗成本)(逻辑边界)
应用业务层次：---：

工具层次：------：数据流线角度来收拢聚集
#调用式编程：不要直接开始写代码---调用式，自顶向下编程；就是抽象通用方法编程，就是写思路，写算法。

底层技术-------：(动机范围)(基础引用<用什么牌>,运用策略(<该牌怎么打>) (普通方案，最优方案)
数据结构层次：
	#字符hash：hashmap的hash函数(h ^ (h >>>16) )\fst\nginx一致性hash(圈)(nginx上/redis缓存上/)
	  >普通hash算法：服务器ip的hash值 - 请求资源的hash值一一对应。比如取模算法，这导致扩充机器时 请求资源会从各个已有机器转移到新的分配机器上(可以想象为从最左边开始的机器开始依次取些流量到新的依次的机器上---无论新增的机器有多少，旧机器取完了就循环)，对已有机器影响比较大；即分多台机器的流量。但是也有好处，机器挂了之后上面的请求会转移到最左边开始的机器上(流量线模型)，挂的越多分布的越广。
	  >原始一致性hash算法：服务器的ip的hash值作为分割点分割整数区域(0-2^32)为各个区间 - 请求资源的hash值在整数区域上的位置的顺时针最近的分割点对应的服务器 -- 一一对应。 取模算法也可以作为hash算法。从流量转移角度看，当扩充机器时，可以做到只影响一个区段/一台服务器的流量，全从一台机器上分； 当有一台机器宕机时，则上游的到自己的流量会转移到下游的那一台机器上。  
	   >现在一致性hash算法：原理类似上，但是新增一台机器，会新增n个虚节点。从而宕机一台的时候本机上的流量会均匀重新分配到其他n+1台机器上, 增加一台时也会从已经有的n个节点中获取流量。
		 >满足三个功能：1：没有挂时：流量均匀打到各台机器上； 2.增加机器时，流量从各台机器分部分流量到新机器上
	#索引结构的增删查：	
	#hashmap数据结构的增删查：Node链表数组(hash-key-value-next), key的hashcode的hash()函数结果 : h >>> 16 ^ h (作为entry[]的索引)。 h是正数，所以最高位一定是0，逻辑右移，所以最高为仍然是0。。初始容量时16
								>当链表节点数 >= 8则会把链表先转为双向链表(且反序)，再转为红黑树！！(每插入一个元素就进行一次红黑平衡算法平衡红黑树)(LL型平衡旋转(最长路径一直往左下)--右短分支加进来而本节点放在左边；RR型平衡旋转(最长路径一直往右下)--左短分支加进来而本节点放在右边)；；当最长路径存在左转右转时，先层层向上转为单向。最终一步都是使最长分支的顶点向下旋转----从而最长分支的长度减1。
								>删除：左右子节点不为空--则使用左子树最大节点为新顶节点，并递归删除该节点。
								>resize(): 当node entry个数 >= 长度 * 负载因子loadFactor   则先初始化一个2倍长的table，在逐个赋值；在新table的位置---如果只有一个元素-则该元素的hash & (newlength - 1)， 如果有多个链表型元素---则将元素的hash & oldtablesize  而分解为1高位list和0低位list分别加到扩2倍的新node array的两个位置j, j + oldCap   ....因为线程不安全，所以多线程扩容的时候：：或许会导致产生大量的新2倍数组产生----从而导致内存和cpu都巨大消耗。hashtable线程安全----但是对entry数组整体加锁---从而性能不好。如果是树节点TreeNode，同链表一样会根据hash & oldtableSize == 0而分解为 高1 低0共2个TreeNode单向链表，如果单向链表的大小超过了新的load_factor*newCap, 那么就需要再次化为树，如果小于，则转为Node单向链表；放的位置依然是j或者j+oldcap
	#concurrentHashmap数据结构的增删查：Node链表数组，hash方法 h ^ h>>> 16 & (n - 1)来确保得到的数自然会是 < n-1的数(比如n是16,因为和高位0相与结果为0)，树也有next方法-所以统一，加节点时，如果数组i位置是null--则cas原子操作数组方法替换i位置为新的值，如果hash是-1会重新操作数组--transfer之类比较复杂，获取数组i位置的节点也是volatile方式缓存失效方式获取元素引用；最后在正常的有next的元素的put时---用这个新构造的Node进行synchronied同步----如果key存在则覆盖--key不存在next==null的位置加，如果是treeBin则相应的查找添加--甚至会在重构树时需要获取写锁--而使用到park(this)停车。//直接对Node加锁---而且用synchronized,且有用cas减少锁使用。扩容用transfer	
								>put元素：读取i位置元素要用cas可见读方式读取数组的index位置元素(hash & (n - 1))，元素为null时，也要cas方式存一个新Node元素，防止并发问题。i位置不是null而是f普通节点Node, 那么用f来sychronized同步，同key的val根据ifabsent被覆盖，不同key则直接简单的next赋值为一个新的val值的Node节点即可。是TreeNode节点，就加新节点到树里，								
									>treeifyBin():Node链表转变为TreeNode红黑树：当链表size大于构造树的大小后，synchronized(元素) 同步地 先将Node链表转变为TreeNode链表，然后用首元素构造TreeBin而转变为TreeBin红黑树, 再可见性地更新到index位置。
								>resize: 实际并没有这种方法，实际上是transfer()在做实际的数据转移工作，在复制的时候读元素也是cas,而开始复制的时候也sychronized同步数组的节点f。
									>transfer(): 是对ForwardingNode类型节点进行元素分解分类为2类，存放在这个节点的nextTable里---2倍老table。分类标准：如果是链表类型，看元素的hash & n 之后 == 0 归为低位链表，为1归为高位链表，两个链表构造好之后就volatile可见性地方式更新到newTable的i位置和i+n位置， 而也会new一个ForwardingNode注入newTable后而同样可见性地更新到老table的i位置---老table每个i都是同一个ForwardingNode实例(TreeNode/Node都是)---也是同一个nextTable：因为遍历老table之前就已经有newTable和ForwardingNode实例了。如果是树节点TreeBin，那么也按照hash & n == 0 或1而归类出低位链表--反序， 和高位链表-反序， 如果任一个链表TreeNode的大小不超过构造树的最小值，则退化为Node链表，如果超过，如果另一位是Null那么就把原来的根节点原子更新到相应位，另一位更新为Null;如果另一位不是null,那么低位和高位链表首元素为参数分别构造出两个TreeBin红黑树分别更新到i位置和i+n位置，ForwardingNode也同样的更新；更新方式也都是volatile可见性地更新。
								>get: 也是cas方式得table的i元素，剩下的则直接查，没有同步；
								>put方法类似hashmap。只是加了并发控制synchronized
	#hashtable的增删查：get/put/remove方法都是synchronized实例同步的。key得出index的方法：h & 0x7fffffff % length 即很简单的去除最高位再取模运算。。Entry[]数组类型(hash-key-value-next)，单项链表；所以冲突的时候依然是增加元素。。而且只在next增加元素，没有treeNode。不存在转变为treenodo的过程。
	#红黑树：新加节点都是红色；不满足“根黑 红-黑 黑数同” 则需要处理-重新平衡：变色(对一个节点变色检查：该节点定是红色，1:父叔同红-变父叔为黑、祖父红 ，当前为祖，继续 ；2：父红叔不红(黑或空)&当前为右-令当前为父-新当前再左旋，3：父红叔不红(黑或空)&当前为左-变父黑祖红-令当前为祖-新当前右旋； 根节点结束)、左旋(当前节点向左下，而右子上位)、右旋(当前节点向右下，而左子上位)。。黑下可以为黑。父红祖不红，则父旋(父子-祖父不同向)或祖旋(父子-祖父同向)；父红叔红，则父叔变黑祖变红，当前为祖，继续。
		>删除：无子节点则直接删除、有单一子节点则删除而用新子节点替换、有2子节点则删除而用右子树最左节点替换。https://www.jianshu.com/p/0eaea4cc5619  https://blog.csdn.net/eson_15/article/details/51144079 https://www.jianshu.com/p/e136ec79235c(最佳)
			>是红色：且最多1个节点，直接用子树替换。
			  >2个子节点：
				>替代元素是红色：直接移除并替代
				>替代元素是黑色：调整顺序：兄红父左旋(数差不变)、兄左儿红兄右旋、兄右儿红父左旋、兄黑兄双儿黑
			>是黑色：(考虑黑数和高差改变)
				>替代元素是红色：直接移除并替代元素为黑色
				>替代元素是黑色：
		>增加：原则：红之黑父，互换色而反红旋。即如果父叔叔子节点有单一红，则这样操作几次。如果父叔叔子节点都没有红，则直接变父叔为红而祖为黑；继续祖。
			>
	#treeMap数据结构的增删查：底层数据结构为Entry为节点的红黑树。线程不安全，没有可见性和没有同步。
	#BlockingArrayQueue(netty)数据结构的增删查:使用ReentrantLock进行同步、阻塞。扩容是也是create一个新的更长的数组。
	#ArrayBlockingQueue数据结构的增删查：使用ReentrantLock进行同步、阻塞。
	#CyclicBarrier类似CountDownLatch, 它功能为让并发的线程运行到相同位置。--先到的线程则阻塞。--直到num==0而开始都运行。
	#Semaphore信号量，类似synchronied、lock---但是不同之处是，可以多个线程同时获取到锁--即一个信号量，达到限额后获取才会阻塞。
	#Exchanger两个线程交换数据，使用Unsafe.park(false, time)方法进行先到线程等待后到线程，后线程Unsafe.unpark()来唤醒先到线程，Node节点里:item/match/parked/hash属性。其中item就是先到设置的，match是后到线程设置的，parked就是先到的停车线程。
	
	
	
线程方面：(多上下文方面)(数据+指令代码的临时环境内存环境)
 ・・・#线程不安全的原因：(每个线程的)工作内存和(物理机唯一的)主内存数据发生了不一致导致的。(多核，每个核有自己的L1L2缓存)
       >底层原因：编译器和处理器为提高执行性能常常对指令进行重排序。相似指令一起执行。
	     >重排序标准：不改变单线程程序语义--则编译器重新安排语句顺序; 多条指令不存在数据依赖性---则处理器将多条指令重叠并行执行; 处理器读主存时由于局域性原理会读一个缓存行-64B到高速缓存，从而看上去剩下的数据加载指令并行执行了。
		    >重排序的禁止：编译器重排序自己有规则--对于会改变程序语义的则不重排序；处理器重排序的禁止：编译器生成指令序列时会插入内存屏障指令 来禁止某些特殊的处理器重排序。
			>重排序的保证：as-if-serial语义：单线程执行结果不变。对编译器和处理器都是。
			>JMM向程序员保证的6条关于两条要求同步的操作执行先后顺序的语义(无论操作是否在同一个线程)(也即JMM的术语)(JMM也要求编译器(和处理器)严格实现这6语义)：同步操作的happens-before关系：先后顺序即 a操作对b操作可见，使得结果始终是a先b后的结果。。happens-before关系的6条具体表现：1.如Thread.start()操作先于程序内部的操作，Thread.join()的返回后于线程内部的操作。	
			>查看字节码：javap -v A.class  加了sychronized之后会出现两条指令
			  >monitorenter指令: 获取对象监视器monitor , 是互斥操作，未获取到的等待
			  >monitorexit指令：退出对象锁。
	   >线程之间交换信息的方法：共享内存和消息传递。只这两种机制。jvm内存模型是共享内存的并发模型。
	   >
	#线程之间同步的底层机制：
	   >内存屏障：使得一个内核写完主存 其他内核 才能读取主存。(相当于有一个锁主存的功能。StoreLoad)
	#同步：
	   >本地多线程同步(代码块)：同步代码块synchronized/Lock(可重入锁ReentrantLock/读写锁ReentrantReadWriteLock)/自旋锁(mcLock, clhLock)<利用了原子操作>/带Unsafe 利用了OS 的CAS原子操作来实现的同步 AtomicInteger, AtomicBoolean   (但实际上synchronized更简单---1.6以后它的jvm实现也是cas)
		  >CLH锁：节点对象：有一个next对象 以及 一个属性引用原子获取更新getAndSet()类对象(Unsafe类获得)， 取锁方法：首先n个线程原子获取更新next属性为自己的node对象，同时获得上一个线程设置的对象，由于每个都会串行执行，所以每个获取的都不同，并把这个上一个node.next指向自己。自己在这个node上自旋---退出条件是这个node被属于的那个线程设置它的lock属性为false-----即表明上一个线程释放了锁而自己得到了锁。
	     --使用锁性能问题的原因：线程上下文切换--用户态内核态切换(自旋则没有上下文切换)  ； 磁盘IO不同扇区引起盘片转动和读写磁头转动
		 --多线程缓存不一致的原因：A线程读取主存中的一个值，不是立即给cpu，而是给高速缓存，然后进行运算；且这两步都没有锁住总线---从而其他cpu可以访问主存中的该值；。。注：线程读取数据，具体到os执行级别，都是先看cpu缓存里有无，再定是否读主存；(每次因计算而更新了cpu缓存中的值则会保存到主存)
									>即某一个线程修改的变量仅仅更新到了对应内核的L1,L2高速缓存里而没有更新到主存，而此时其他内核之前读主存时读了含此变量的缓存行进来，从而它读的时候也只读了它对应的内核的L1/L2高速缓存，从而不一致。总之就是更新没有使得高速缓存失效。
		   多线程缓存不一致的解法：缓存锁定(总线锁定上述)：MESI协议：一个cpu在写主存中的一个值时，会发送RFO指令信号给其他cpu让内部缓存里的同一个主存地址的值失效(迫使其他cpu进行重读)(而其他cpu也会监听缓存状态-如果是M则要等待它刷到主存为止)。(一个cpu里多内核，各个内核也有自己的高速缓存)
																
		 --os提供的原子性操作：读写一个字节、Compare And Swap (利用了上述--总线锁定/缓存锁定) (乐观锁方式读取比较写入;相等则写入返回true-否则返回false) ---这串指令操作被封装到了Unsafe类的多个方法中，如Unsafe.compareAndSwapInt(实例，属性，期望值，更新值);更新失败可以重新读重写compareAndSwapInt();(在Unsafe.getAndAddInt(实例，属性，递增值)就是这样的重试机制)
	   >volatile: 只是保证在对变量更新后，立即从内核高速缓存回写到内存，而一般情况下没有volatile修饰的变量被修改不会立即被回写--而且很久一直都是从高速缓存读。称为变量有可见性。可见型变量，，无脏读变量。(非常重要，CopyOnwriteArrayList实现里就使用了--这个是先copy原数组并长度+1添加末尾元素后将数组指针赋值给原数组指针)
				>避免指令重排--所以volatile变量的读写 会加内存屏蔽(LoadLoad, StoreStore LoadStore StoreLoad) 指令。(重排序，在一个线程中执行的顺序指令，但是可能不是顺序的，导致另一个线程看到的值变化不是顺序变化的)(cpu汇编级别指令会重排-同类指令在一块执行-提高效率-减少来回切换)
	   >final: 也实现了可见性。毕竟不能被重新赋值---但可以第一次赋值---这个第一次赋值只能顺序执行中被赋值。
	   >happens-before:是操作之间的关系，用来辅助描述各种规则中的各操作的先后顺序。比如volatile变量的写happens-before该变量的读(多线程中)
	   >四种锁状态：以及升级关系：无锁->偏向锁(获取锁自己用，不存在释放锁,没有CAS也没有自旋，竞争时膨胀为轻量级锁)->轻量级锁(竞争失败时自旋或者CAS)->重量级锁(竞争失败时使用互斥量mutex阻塞当前线程-从而线程切换)     (锁对象：Mark word里会存获取本锁的线程信息、锁级别，线程cas方式获取和赋值)
					>一定是因为四种锁状态下“加锁和解锁”的底层操作指令不同、代价不同，所以在一种锁状态等待极限内仍然没有获取到锁--则会升级锁。----意义-应用：在竞争强烈的场景下，直接先关闭偏向锁功能；第二：设置轻量级锁自旋次数：-XX:PreBlockSpin；第三：升级轻量级锁为重量级锁可以避免浪费cpu资源(尽管线程切换、阻塞需要os内核态操作所以需要用户态-内核态切换又耗时)。
				    >轻量级锁的获取：CAS更新Mark WOrd中的锁状态和锁类型。或者Atomic::cmpxchg_ptr方式来更新Mark WOrd里的prt指针--来指向当前栈帧。
	  >sychronized: 1.8以后性能持平或者更优。且concurrentHashmap用synchronized而不用ReentrantLock	。。synchronized先偏向锁--->再轻量锁--->再重量级锁
	      >底层原理：java对象头的Mark Word存放的值：包括了锁状态(是否加了锁)和锁标记位(什么类型的锁)。hashcode, 分代年龄。(锁state、对象监视器-有ptr当前线程指针_owner)
		    >
		    >字节码方面：先获取对象的监视器，没有获取到就会进入监视器的同步队列等待。对象监视器的os实现是mutex lock。
			  >对象监视器和Mark Word的关系：从Mark Word里可以获取到对象监视器的引用/指针。重量级锁才有对象监视器。
			  >锁膨胀过程: 
			    >monitor竞争：分配一个对象监视器，将Mark Word里的锁状态cas尝试更新为inflating, 如果失败则自旋等待(实际是park有的占用不了太多cpu)，成功则设置对象监视器monitor的各个字段，返回这个对象监视器monitor.
				>monitor等待：尝试cas更新monitor的)_owner占用线程字段为当前线程，成功则该线程获得锁而返回，失败则自旋 park()下次竞争失败时继续park()等待被唤醒，唤醒后继续自旋竞争----唤醒就是得锁的线程调用unpark()激活park()的线程。
		  >重入性：对象有计数器，获取了对象锁则计数器加1.
	   >AQS:同步组件实现的关键部分,是基于CLH锁的FIFO同步器，本身提供“(取锁--即cas方式加到队列尾部tail)排队-循环阻塞-状态取赋更(可以导致退出阻塞)(释锁-激活下一个元素)”基本功能，比如使用Sync， CountDownLatch(利用sync实现自己的同步语义)就用了Sync。比如ReentrantLock(利用sync实现自己的同步语义)(sync的AQS的ConditionObject里的await()方法---根本是用了LockSupport的park()方法--而它则用了本地方法UNSAFE.park(false, 0L);方法来阻塞当前线程---其他线程则用UNSAFE.unpark(threadObject)来释放阻塞的线程)类里就用到。(AtomicInteger是用Unsafe, LockSupport也用Unsafe, concurrentHashMap 用lockSupport)
				>CLH锁中队列元素对应先后阻塞的线程，int整数代表同步状态(>0排队阻塞)(AQS自身不改变state的值)，AQS提供取、赋、CAS更新三个方法操作该状态。
				>实际继承AQS：常用来实现tryaquire  tryrelease 两个方法，实现中利用set/get state方法(有时会使用get/set ExclusiveOwnerThread来判断是否重入之类)(更新成功则得锁，没有自然就返回---从而可以不用阻塞)，而使用AQS自己实现的acquire release方法。
	   >cas操作：底层是CMPXCHG指令，封装为非阻塞的先比较后更新属性值--返回成功或失败。即本质是一种方法类型；先比较后更两个操作 底层实现保证了是顺序的和隔离的；或者说这两个操作是一个操作一同完成的。如果值当作得锁成败，或者得锁次数，就可以得到构造一个乐观锁。
	     >
	   >park()方法：每个线程都有Parker对象---这个要从JNI环境里获取，。参考https://blog.csdn.net/weixin_39687783/article/details/85058686
	     >POSIX编程接口：各种系统调用的接口(系统调用自然面向应用开发者，比如创建线程的接口：posix_fork() 而linux和windows都实现了这接口)，linux和windows都实现了的统一的系统调用接口(虽然各自内部实现不一样)
		  >park()的c++实现：
		   >平台实现的一组关键POSIX接口：pthread_cond_wait, pthread_mutex_lock, pthread_mutex_unlock
		     >linux实现进程阻塞的数据结构：等待队列(自旋锁spinLock一个 + 存放等待进程的队列)。。进程在等待队列中阻塞等待，直到条件为真被激活。
			   >操作系统挂起进程：直到条件满足才唤醒。
			   >阻塞与唤醒的原理猜测：操作系统里有一个 “等待队列”--每个元素是“进程-等待条件”， 加入到这个队列中的进程对应的程序代码就不在继续执行，相当于系统调用不返回--还没有执行系统调用返回指令，入队时保存当前进程的上下文，当条件满足时，则取出这个进程-继续程序计数器偏移量下一条指令开始执行，直至执行系统调用返回而切换为用户态上下文，而继续执行，出队恢复进程的上下文。
				 >从而系统层次/内核态 执行 出队 和 入队 (等待队列)就可以实现 对一个进程的 阻塞和唤醒；比如 磁盘写就绪 事件 信号 促使os/内核/cpu执行一个出队操作--根据条件/事件信号，从而唤醒在此条件/事件下等待的对应的进程。又比如网络协议引擎可写/可读的事件信号。中断信号; block/pending/handler三大表。参考https://blog.csdn.net/tong646591/article/details/8485454  https://blog.csdn.net/double_happiness/article/details/72897592
				 ----系统调用执行中条件不满足主动入队(保存上下文，释放cpu, 暂停了执行，相当于冻结了上下文--没了cpu一条指令也会不再执行)， 事件信号触发出队(获得cpu，恢复上下文，继续执行)。硬件设备发送中断信号给cpu。
				 ----另外一种情形是：当前线程新建一个上下文推入 等待队列，这个上下文就是回调函数---而等待条件/出队条件是 事件信号出发 或者 程序条件发生程序主动调用出队。从而主线程仍然可以在创建回调函数为内容的上下文并入队 后 系统调用返回继续用户态上下文执行。
			 >内核实现的事件通知机制：
			   >锁、信号量、条件变量：
			   >无差别轮询有限制(1024 文件描述符的个数 select)、无差别轮询无限制(事件发生轮询poll)、内核IO事件异步唤醒的相关返回(只返回有事件的流epoll)(系统注册回调函数，回调函数将唤醒的描述符/就绪fd拷贝到readyList里---用户获取事件就只看readyList里的--获取方式水平触发/边缘触发，有mmp内存映射少拷贝)：本质上都是同步IO。。 参考https://www.jianshu.com/p/397449cadc9a
				 --epoll是内核以事件回调方式实现的为用户提供的获取就绪文件描述符集合readyList的一种机制。
			 >内存空间(寻址空间/虚拟空间)的划分：内核空间和用户空间。内核具有特殊指令和内存空间的访问权限。操作系统的核心部分程序为内核。
			   >进程切换：内核保存一个进程上下文/到等待队列(挂起进程)，取出一个进程上下文继续执行/恢复执行。
			   >进程阻塞：进程自己执行了Block阻塞原语 而将自己的状态设置为阻塞状态而放弃cpu而被内核放到等待队列。
			   >文件描述符：用户态进程看到的内核返回的一个文件的引用，是一个非负整数。
			   >缓存I/O: 即I/O的数据都会先缓存在 内核的缓冲区(内核空间中)， 才进一步拷贝到用户空间(读)/硬件|网络协议引擎(写)(硬件或还包括缓冲区)
			 >指令级别的等待：
		   >内存屏障方法：OrderAccess::fence();赋值一个变量后加内存屏障，使屏障指令之后的线程读取到的变量的值就是内存屏障指令前的指令赋的值。。参考https://www.cnblogs.com/cynchanpin/p/6927045.html
		     >linux实现：__asm__ volatile ("lock; addl $0,0(%%rsp)" : : : "cc" , "memory");等。
			 >cpu在内存系统上提交Store操作和Load操作：如果这些Load,Store指令没有数据依赖关系，那么实际执行顺序就是不确定的-随机的。干预方法就是内存屏障指令， 它保证内存操作部分有序--即在内存屏障一边的指令相对于另一边的指令是顺序的--局部顺序。
			   >写屏障：写屏障之前的写指令优先于之后的写指令。
			   >数据依赖屏障：两个Load操作，数据依赖屏障保证第二个Load操作执行之前-对应的目标地址内容已经更新(cpu1即便加了写屏障，但是两个数据如果存在不同的奇偶号缓存行里，那么更新到主存里的顺序也不是确定的；所以cpu2读取的时候也可能先读取到cpu1执行的写屏障之后的指令的写内容-----因此cpu2还要加一个数据依赖屏障来保证等待cpu1写屏障之前的指令执行完毕才开始读)。当然因为两个Load存在依赖关系，不加读屏障已经就是有序了的。常和写屏障一起使用。本屏障指令前后是写地址操作和读地址的内容操作，本屏障就保证写地址操作中写的“地址”定位到的内容已经在其他cpu中更新中则先更新。
			   >读屏障：读屏障之前的写指令优先于之后的读指令。
			   >通用内存屏障：读写屏障之前的写指令优先于之后的读写指令
			   >Lock操作/UnLock操作：单向渗透屏障，保证单向的本指令后的操作后执行 和 本指令之前的先执行。
		   >进入安全点，改线程状态为阻塞状态：tbivm()
		   >修改java线程拥有的操作系统线程的状态：osts()如修改为等待条件发生--CONDVAR_WAIT
		   >当前线程放到等待条件的线程列表里，再对互斥变量解锁：原子操作 pthread_cond_wait()  会导致线程等待，直到再次获得互斥变量的锁
		   >释放互斥量的锁：pthread_mutex_unlock()
		 >许可：原子变量_count
		 >
	   >远程调用/IO调用/read&write调用方式：同步阻塞/异步阻塞/事件回调
	   >分布式同步：
	   
	#线程模型：从任务特征上分类。
		>Runnalbe模型：无输入值无返回值直接异步执行任务 ,
		>Future模型：无输入值返回future并异步执行任务-父线程future.get()同步等待结果
		>Fork-join模型：有输入值有返回值(任务特征)，任务本身根据入参或者直接计算返回值或者分解出2个子任务异步执行并取它们的结果；父线程task1.fork(), task2.fork()异步执行,父线程task1.join(), task2.join()同步等待结果。
		>Actor模型：用信号signal异步发送woker请求request实例和response回调函数类实例给actor，无返回值。请求实例也会被异步执行---即如果在发送的时候做为循环发送n个req, 那么这个n个req会并行执行。(如果在回调里再发送，而外层只发送一个，则自然变成了顺序执行了。) 参考资料：http://www.agilewiki.org/projects/JActor2/1.0.0/JActor2RevisitedByExample.pdf

	#网络模型：nio的多线程selector为例
		>普通socket:阻塞模型，流数据模型。到则必须读，无则阻塞。写则直接写--不管端口状态。
		>NIO:端口四大事件通知模式--(linux上是epoll)，每个端口对应的通道四大事件有通知-无阻塞(相当于一个方法有值返回无值等待阻塞)，通道可以读出可以写入--根据实践类型而定----而这个通道读与写是同步的。Selector-->SocketChannel-->ByteBuffer
		>AIO:读与写是异步进行的Future线程模型
	#存储模型：副本为例
	
	
		
#内存方面：
	>内存管理：
	  >jvm内存管理：
		>各代划分和默认比例：
			>NewRadio: 新生代=1 年老代=n   默认为2
			>SurvivorRadio: Surivior0=1 Eden=m 默认是8  
			
		>回收器的标记回收过程与促进它的参数调整：基本都三个阶段：“标记-复制-清除”
			>垃圾回收器：Serial  Parellel CMS G1
			 >回收方法1：stop the world 标记所有根节点(静态属性，常量)-可达对象-清理不可达-复制可达到survivor/old。。。Serial Parellel 
			 >回收方法2：stop the world 标记所有根节点 --> 并行标记 可达对象(trace算法记录在bitmap) ---> STW 重新标记 动态用户线程产生的新可达对象 --> 并行清理不可达对象(对可达对象不复制)。CMS优先保证stw的时间短。而G1好处在内存空间管理更精细，空间碎片更少，影响的线程更少。
			>回收器组合1：+UseConcMarkSweepGC +ParNewGC 来对年老代和新生代进行垃圾回收
			>回收器组合2：+UseG1GC +MaxGCPauseMills=200
			>G1回收器：物理分区：Region(1M-32M)(Xmx/2048个) 每个Region内存分配最小单位Card(512B)   参考https://www.cnblogs.com/oldtrafford/p/6883796.html
				       被引用集合: points-into; RSET 一个RSET记录了一个Region中那些被其他对象(可在其他Region)引用的对象构成的一个个引用关系。在垃圾回收时查看可达对象就查看RSET而不是全表扫描
					   引用集合：points-out; Card Table。。CSET。
					   回收策略：先收集垃圾对象最多的分区
					   回收过程：可以认为同CMS的4阶段--但更多-如根分区扫描。仅仅是具体做法因为物理分区的不同而不同。
					 --优势：精细管理，随时间发展内存碎片更少利用率更高；区块为单位并行回收提高效率；内存压缩。https://www.cnblogs.com/oldtrafford/p/6883796.html
				>Young代的回收过程：STW方式，并发多线程地，年轻代存活的对象(多个年轻代Region包括S0区)拷贝到S1区 和O区。重新计算E区和S区的大小。
				>Old代的回收过程：STW根标记存活区-->并发扫描S区对O区的引用集合和被引用集合-->并发标记全堆可达对象()-->STW重新标记(会清除全部对象都失效的Region， STAB算法)--->STW拷贝/清理(先复制活跃度低的N区和O区的可达对象到没使用的区域，再清理被复制的区域)。(暂停时间决定要回收的region数量)。复制之后有内存压缩，释放内存。
		>各代使用查看、回收查看、实例查看、线程状态查看(进程的线程数、进程的线程资源占用)：
			>关注：参考：《深入理解Java虚拟机》
			 >内存泄露？(不使用而被根引用到/长周期对象引用到的对象占用的空间无法回收;短周期对象即时关闭和置为null)内存垃圾old代回收次数太多、均次耗时大？(STW)(jmap -histo:live pid | sort -n -r -k 3 | head -10)(jstat -gcutil pid) 泄露查看工具leakcanary
			 >线程泄露？线程死锁？(ps -hH pid | wc -l) (jstack -l pid)系统线程数(cat /proc/pid/status)
			  >各方法执行次数\执行时间排序：
		>堆大小限制：32G以上，指针不能做压缩。
	  >零拷贝：使用DMA将磁盘数据放到内核态数据区后直接copy放到网络设备上，而不是先copy到用户态的数据区、切换到用户态、什么一般指令也没做就切换到内核态继续执行特权指令--将数据从用户态数据区cp到网络设备上。
	  >伪共享：cpu1更新了X但是没有再读X, 导致X所在的缓存行失效---而又因没读导致cpu1所在L3中的X所在的缓存行是失效的，---如果该缓存行里有Y变量被其他线程所在cpu2操作，那么cpu2就只能从主内存中取---导致效率不高。---伪共享问题。，和缓存行有关---cpu中L1L2缓存没有失效--没有因为其他cpu的更新而失效。	
		>问题产生：常常在多个long,int基础变量使用时，它们被放入了同一个缓存行。相反，如果每个变量单独放在一个缓存行，那么就互相不影响，因为只是一个缓存行的失效，不会影响其他缓存行失效--而重新去主存里拿。(disruptor这个内存消息队列就避免了伪共享--避免了重新去主存加载)
		
#网络方面：
	#https协议：http协议 + ssl/tls协议： 最佳总结https://www.jianshu.com/p/14cd2c9d2cd2
		>https请求过程：前奏：客户端把独立生成的对称加密密钥client key  加密传输给服务端，服务端解密后得到后，服务端客户端就可以用这个对称密钥来加密解密了。  加密：是从服务端获取的公钥，解密，是服务端自己的私钥。
				客户端： 请求服务端                     公钥合法(证书有效),则独立生成一个client key即密钥，并用公钥来加密这个client key , 将结果发送给服务端																																																														客户端用client key解密得到内容即可。										
				服务端：              返回公钥(证书) 																											使用私钥解密该结果，得到client key 。这个client key就是双方进行通信的对称加密密钥(保证通过网络也只有客户端和服务端知道) ::从而双方可以用这个client key进行加密通信了。即服务端就当作对称密钥来对称加密要发送回客户端的内容，将结果发送给客户端。

		>http通信模型：请求-响应：一个请求：用 url + 报头 + 报文 表达，  一个响应：用 报头 + 报文   。 报头说明请求类型、报文类型、请求参数、参数长度、cookie信息
		>http/2: 多流并用一个tcp连接。2015年后。
		>证明B收到的信息肯定来自于A(保证信息没有被修改)：则只需要A用自己的私钥加密信息传给B，而B用A的公钥解密。可以解开得出合理信息则说明肯定来自于A（因为中间人不可能知道A的私钥，所以解开后无法再加密）
			>但直接加密“信息”本身则耗时，一般是加密“信息”的hash值，得到的就叫做数字签名。同理，将“数字签名”解密之后，得到的就是hash值---和将收到的信息进行hash后的结果比较。则可以确定来自于A。(因为中间人无法生成正确的数字签名)(即保证信息没有被修改)。(但无法保证信息没有泄露)
		>证明B收到的信息没有泄露(没有被中间人看到)：只需要A传输时用B的公钥对内容进行加密。(中间人没有私钥，所以无法解开)
			>因此传输过程：1.用B的公钥加密(文件+数字证书)发送给B(则中间人不能解开-无B的私钥，也不能调包-无A的私钥)(但条件是AB双方事先知道对方的公钥)。。就可以保证信息未被修改未泄露。
			>公钥获取正确的解决办法：因为公钥允许泄露，保证确实是B的即可；B的公钥作为信息，用另一个私钥X加密，传递给A；A用X对应的公钥解开---来获得B的公钥。。。而A要事先知道的X对应的公钥---就是CA中心的公钥。仍然两个问题：数字证书如何被服务端获取到；CA中心的公钥如何被客户端获取到(客户端存CA中心的根证书，则可以验证所有分中心下级中心颁发的证书)(根证书可靠：windows也预置了50多个根证书)(各个具体软件.exe属性可以查看数字证书：即该软件所在公司从CA中心申请的证书)。
			--传自己私钥加密的内容：保证来源。(传第三方私钥加密的内容：也可以保证来源。比如传CA中心的私钥加密后的内容：数字证书)(自己私钥加密的内容：只叫数字签名)
			--传对方公钥加密的内容：保证不泄露。
			
			
	#tcp/ip三次连接而不是2次的原因：(提问式反问式 找设计的理由找设计的原因式理解，而不是认识式定义式理解；；设计的原因才能让人真正理解，，而不是只是知道是什么；更要知道为什么要这样设计为什么不这样设计---因为这才是真正最透彻深刻的理解；否则是肤浅的)(这么设计目的：一是为了实现功能、二是为了避免带来新问题新不确定、三是一种解决不可避免的恶劣情况导致的功能故障的方案(悲观考虑必须、意外情况必须--不能只考虑正常情况的处理----意外情况往往就能改变设计思路局部甚至整体---导致设计的结果直观上不太容易理解--毕竟离正常情况已经有点远了-不纯粹只是为了处理正常情况而设计--这个毕竟也是不强大的))
		>服务端不能收到信息就确认建立新连接，而开始接收处理实际真正要传的数据。因为客户端可能会多次请求--没有回应就会重试。所以需要客户端确认才可以。即：客户端发起连接请求(标记为X: SYN=1 Seq=X)->服务端响应(标记为X+Y: SYN=1 ACK=X+1 Seq=Y)返回-->客户端确认(Y+Z: ACK=Y+1 Seq=Z)发送-->此时服务端才确认建立连接(ACK确认的是上一次的Seq)(说明：1.当服务端返回时--客户端就可以确认网络通了，而服务端收到客户端的确认时--就可以确认客户端也收到“我的确认”了；过程非常类似https请求--加密传递密钥)(客户端：经历“发-收”；服务端：经历“发-收”。双端都经历“发-收”(说明双端确认网络“可发可收”)连接建立)。(3次的目的：双端确认网络可以“可发可收”；服务端可以忽略建立后释放前的重试请求---免得浪费资源；客户端也可以忽略建立后释放前的反复确认--同样避免了浪费资源)
		>
	#tcp/ip四次释放连接而不是2次的原因：首先释放连接的原因：--本方已经接收数据完毕--不用再连接了；这个消息要让对方知道-所以要发送结束报文(FIN=1 ACK=Z Seq=X)给对方，当然也要得到对方的收到确认(ACK=X+1 Seq=Z)(否则认为对方没有收到而继续发-此为需要接收确认的重要性)；同理，对方接收数据完毕，而需要告知对方(否则对方可能认为连接中断而重试重发)(发送FIN=1 ACK=X Seq=Y) , 也要接受到对方的确认为止(对方发送ACK=Y Seq=X)(否则认为对方没收到而继续发送)。
		>

	#DNS过程：从host到本地DNS服务器到代理DNS服务器到询问13台根服务器
	#ip地址：IP地址是标识网络上每一台主机的id---点分十进制-4字节构成。由网络号+主机号构成。
		>地址类别：按网络号字节数1-3 和 1字节网络号的多播地址, 或者按网络号高到低首0位 而划分4类：A类：1B netnumber + 3Bhost number  网络号：0NNNNNNN 首位确定为0。同理B类：10NNNNNN C类：110NNNNN D类1110MMMM  组播地址  。C类最大取223也就是做大的非组播的首位地址。D类地址的最大值是239也没有到最大值。E类地址是240-255
			>某类网络的网络数：2^可变网络位数  
			>某个网络的主机数：2^主机号位数 - 2
			>某个网络的子网掩码：只有一个作用：对一个IP地址划分出网络地址和主机地址。也是一个点分十进制数---网络号.0...0 即网络号不变而主机号每位都是0  
			  >更好的划分方法：CIDR 记录一个IP地址的子网掩码位数(主机号位数) 192.168.111.124/23 则有23位网络号，网段/网络号就是192.168.110.0/23
			>某类网络的私网地址：每类网络都选择了一个网址区间作为私网地址空间：比如A类：10.x.x.x  B类172.16.x.x - 172.31.x.x  C类192.168.x.x  
			>公共IP地址：公网IP地址,世界唯一，即ABC三类地址中除了私网地址之外的地址。
			>不清楚的主机和目的网络：0.0.0.0
			>本网段内的所有主机：255.255.255.255 即只在本网段内广播，而不会广播到其他网段中去。限制广播地址。
			>本机回环地址：127.0.0.1 - 127.255.255.254 主要用于测试，在传输介质上永远不会出现的地址。
			>组播地址：用于特定的程序和多媒体程序。224.x.x.x - 239.x.x.x  224.0.0.1所有主机，224.0.0.2所有路由器 224.0.0.5是所有OSPF路由器。
			>DHCP配置而又无法从DHCP获取地址，则会分配这样的地址。
		    >.0和.255结束的地址都是保留地址。
		>子网划分：一个IP地址有网络号，扩充这个网络号位数，占用主机号n位，构成的新的2^n个网络号 就是 子网络号。
		    >物理上的划分表现：
		>路由算法：
			>一个路由设备：处在多个网络中，一个端口处于一个网络，一个端口有一个ip地址，所以有多个ip地址。某个端口处于internet互联网中，ip地址就是公网地址。一个网络 用网络号 + 网络中的所有主机ip地址来完全表达。
			>超网：将多个不同网段的子网络ip地址映射位一个子网掩码更短的相同的ip地址。即 有一个端口 时一个不同网段的ip地址集合。
			>路由通信：
			 >主机路由：目的ip地址是一个具体主机IP号
			 >网络路由：目的IP地址是一个网段而不是一个具体的主机iP地址。//比如要发送到某个网段
			 >默认路由：主机直接连接的路由器对主机的路由。
			 >转发的根据-路由表：目的地址(网络号ID)-子网掩码-接口(发送的接口)-网关(本接口连接到的下一个路由器的接口的ip)
			   >一个路由器连接的是不同的网络：不同的子网。即路由器的各个端口处在的子网网络号是不一样的。
		>DHCP过程：
		
	#网络读写、磁盘读写、内存读写：
	#线程管理、线程同步、线程异步：
#网络设备单点问题：(单点问题，不可用)
	>集群：
	>主备：
	>主从：(从节点宕机不可用；检测，下线)(主节点宕机：检测，重选)
	  >主的选举：一个选举周期内:  对于选的非自己---收：增、同  ； 发：广、定   。。对于选的是自己---收：增、定 ； 发： 广、同。
		>zk的类似fast-paxos算法：关键在第二阶段的时候，如果同时有n个节点进入了第二阶段的主发，那么收到消息的accepter节点原子更新自己的select和自增zid, 从而只有一个主发节点成功，其他主发节点则失败，收到超过一般的二阶段返回则更新自己为Leading开始广播---没有超过一般的则重新开始--显然二阶段结束后只有一个主发超过一半，其他则又从第一阶段开始；显然这个时候成功节点已经广播leading消息过来了---收到者全部更新自己的leader为它。(一阶段同时成功者可以小一半，但二阶段成功者却只有一个)
		>paxos算法：分布式系统一致性问题的共识算法
			>内容：2pc两阶段提交。K; maxN  , accpectN acceptV ;  一：向超过半数发送K(不必全部), 超半数则区结果中最大N的V ,二：构造K-V发送，收到 超半数，成功选举。接受方改AcceptN=K, AcceptV=V
			
	  >有效读：zk可以从follower读取，但kafka只能从leader读取。
	  >有效写：同步数据时，从节点至少一半返回成功。
	>分布式系统理论：
		>一致性问题：
			>CAP理论：分布式系统的数据一致性、可用性、分区性；分区性肯定保留，一致性要求节点之间要通过网络进行同步保持一致-显然阻塞耗时-，可用性要求系统快速响应-可用性不关心节点数据是否一致，从而一致性和可用性是矛盾的、反相关关系的。
			>BASE方案：是承认上述矛盾而采取的分布式数据方案，来满足需求--这种需求就是更重可用性和分区性。基本可用，软状态。是一种对分布式系统CAP特性现实的一种权衡方案，是在承认CAP之间的制约矛盾下，系统基本可用、可分区而牺牲节点数据短暂不一致但确保最终一致的方案， 即存在不一致的状态，但时间极端。
			>数据一致性类型：参考https://coolshell.cn/articles/10910.html
			 >弱一致性：写入新值后，读操作在副本上可能读出来或者读不出来。/异步执行成功或者抛弃。
			 >最终一致性：写入新值后，立即读可能读不出来，但在一个时间窗口后，保证能读出来。如电子邮件(写A发/B收读)。/带有异步特征，一定会做。
			 >强一致性：写入新值后，立即能够从任意副本读出来。如RDBMS这种系统。/带有同步特征。(返回成功则说明连副本都写入成功了)
			 ----例子：
			 >Master-Slave结构：主从结构，主同步给从， 可以异步/同步， 可以slave拉或者主push；这种异步则提供的是最终一致性。。写失败的标准：如果是异步，则没有。因此master挂了则写数据客户端必须认为失败了；如果要强一致，那么master先写自己log，成功后写slave,也成功了则返回成功，失败了则回滚自己，返回失败。
			 >Master-Master结构：不佳
			 >2PC: 协调者询问可提交？                        协调者返回提交                       协调者如果收到失败，发送回滚。
						             节点锁定资源，返回确认                  (如果未收到提交，则不知所措)节点提交，返回成功
			 >3PC: 协调者询问可提交？                           协调者返回预提交   						 协调者返回提交
									 节点直接返回可以，不锁资源                   节点锁定资源，返回确认                 (如果超时或失败没有收到提交，则自己提交)节点提交并且返回已经提交
			 >Paxos算法：集群节点存有一个键值，现在想更新这个键的值，
			   >最好的原理理解：一个proposer只要连续两轮竞争获得超半数票同意，则会成功修改超过半数的acceptor的值为新的值。从而紧接在该proposer成功后有proposor也连续两轮获得超半数票而成功修改---修改为的值也是第一个propersor修改为的值----因为第二个proposor在第一阶段获取的最大acceptN的acceptV就是第一个proposor更新的值。所以第二个proposor只是能够让更多acceptor的值更新为第一个proposor修改的值而已。同理第n个proposor，直到整个选举过程结束。
								 成功之后，proposor可以通知其他proposor不必再次发起投票了， 也可以通知所有leaner选举结束了，新的值就是xxx。
					>每轮竞争特征：即每轮发起提案的特征：成功的proposor都是时间顺序有间隔出现的--绝对不会同时出现，最多只有一个proposor获得超过半数票。如果一个proposor成功了，那么其他proposor失败了，下一轮则失败proposor要增加K值，而成功的那个proposor不用，继续一同竞争，如果第一次成功的那个proposor再次成功了--说明它已经修改了超过半数的acceptor，那么继续竞争修改就会都是这个值，所以选举可以结束了，更新可以结束了。
					>最后修改为的值：就是第二轮成功之前最近一次第二轮失败时更新为的值。
					>编号的设计和高编号可以覆盖底编号的设计：最重要的效果是区分数据版本，以在第二阶段多次失败后都能够一致的认为/约定再次在第一阶段成功后读到的值那个值是最合理的最新的。
					>两阶段的设计原因：因为有个计算acceptor的最大acceptN的值而选择确定 这个获取集群最新值 然后再拿这个值去更新集群的两个步骤的不可少的过程。
					>编号设计导致的对acceptor判断的自然要求：第一阶段：K > MaxN 从而失败者必须要自增K才能被接收；第二阶段：K>= MaxN 因为第一阶段成功的proposor发送来的K还是上一次的K---而此时超半数的几点的MaxN已经是K。所以要有等于。
			  >NWR模型：W + R > N (可以保证读取的是最新的值)
			   >矢量钟：
		>保护：
			>缓存、降级、限流
	
#文件方面：
	#软连接：相当于快捷方式：并不复制内容，只是保持一个连接。ln -s 源文件名 软连接文件名   。。。只要源文件名是完整路径，就可以移动。
	#硬连接：相当于cp -p 文件 + 同步更新，；连同属性都复制，然后源文件更改硬连接文件会同步被修改，硬连接文件被更改源文件也会被更改。ln 源文件名 硬连接文件名。
		>源文件删除：快捷方式自然不可用。而硬连接文件可用。因为硬连接文件和源文件共用一个i节点号(i节点就是文件信息结构体)。linux删除文件只是删除i节点号与源文件这条映射、源文件(磁盘上的一个数据块)。而i节点号-硬连接文件依然存在。而展现上：目录下有：文件名-i节点信息
		>tomcat下的catalina.out存在着同步文件。
#中间件
 >nginx:

数据库方面：
	>数据库一致性问题(渐进)和对应的隔离性级别解法
	 事务隔离级别		数据库一致性问题
				脏读	不可重复读	幻读
	 读未提交    x                         (值为是否解决，x未解决， v为解决)
	 读已提交    v         x
	 可重复读    v         v    	x		(锁不了新加列)
	  序列化     v		   v		v
	  
	>事务的ACID:
		>A:事务中的所有操作要么都成功、要么失败回滚使得数据库回到操作前状态。没有中间态。
		>C:事务修改使数据库从一个一致状态转变到另一个一致性状态，用户感知不到数据库的中间变化。
		>I:事务都有自己的数据空间，互相不影响。即时操作同一个数据也有自己的数据空间。更新也是从事务数据空间/临时表空间到数据库存储空间的更新。
		>D:事务一旦成功，对数据库数据的修改就是永久性的，被永久性的保存了下来，即使崩溃重启也是修改后的数据。
	>mysql:
	  >索引的存储数据结构与增删查：B+树
		>B树：平衡多叉树，关键字分布在非叶子节点上，数据在叶子节点上，可以在非叶子节点上命中。非叶子容器K个元素K个儿子；叶子容器K2个元素。从添加、删除过程看，构造出整颗树的过程看，K>=M/2 且K<M(<M/2 >=M会重新平衡). K2>=L/2 且K2<L同样在这个范围之外会重新平衡。
		 >搜索性能：按最少算：=查找次数= 每层的查找次数(都一样) * 层数(跟总数、单容器节点数有关) = log2(M/2) *  logM/2(N/(M/2)) 其中N / (M/2)自然就是容器个数，而层数与容器个数X的关系就是：M/2^(h) - 1 = X 所以认为h = logM/2(X)...化简后近似=log2(N)
		 
		>B+树：平衡多叉树。不能在非叶子节点命中，叶子节点形成链表-前后引用-自然从左到右是有序的。
		 >缺点：叶子节点的分裂，使得逻辑上连续的数据块在磁盘上并不顺序存储，使得读数据需要大量的随机读(磁头频繁移动，磁盘频繁旋转)---从而性能低下。
		>B*树：平衡多叉树。B+树变种。同层非叶子非根节点也前后互相引用。
			>单容器数据结构：从树的形成过程中可以看出，同层最左容器才可能有左子节点，其他元素都是只有右子节点---即子节点中元素最小值>=自己的值
			>增加一个元素：后，叶子容器个数<L不动，>=L 自底向上分裂，即叶子容器分裂为两个，父容器增加一个节点-值为分裂的右半容器的最小值；然后判断父容器是否要分裂。如此下去，直到根节点。
			>删除一个元素：后，叶子容器个数>=L/2不动，否则 看右容器元素有多少---如果>L/2则借一个补充；<=L/2则合并右容器，并把右容器的父节点的值改为合并容器的最小节点值，同时删除左容器。然后看父容器节点个数是否足够--进行相应借或者合并直到根节点。
			
	  >数据item的存储结构和增删查：
	  >单点问题的解决方案：主备切换、主从同步
		>主备同步：
		 >binLog: 日志格式mixed: 如果不会引起不一致，则用statement格式语句写到binlog, 如果会引起不一致，则用row格式(直接变化的记录)。delete:记录删除的每一行。update:记录前后的数据。insert记录本身。所以row格式方便恢复数据。
		  >通过binlog恢复数据库：即只需要mysql事先开启了binlog, 则输入binlog起始位置和删除数据之前的位置重新执行一遍即可(更新数据自然也可以binlog来恢复)：mysqlbinlog --start-position=4183 --stop-position=4592 /opt/lampp/var/mysql/mysql-bin.000001 | mysql -u root -p
		 >备库动作：将主库的操作同步过来 本地执行，来保持数据一致。
		 >过程：用户update语句-->主库undolog-->redolog准备-->binlog--->redolog commit提交--->返回客户端ok。。另外，写到binlog后，还会另一条线：和备库保持连接的dump_thread将按照备库传过来的binlog偏移量 读取binlog相应位置开始的内容发送给B，B本身启动了io_thread	和 sql_thread, io_thread接收到A发来的binlog日志后写到relay log中转文件里，sql_thread线程则从relay log里读取出来执行其中的sql命令。
		 >双M结构：不用修改主备关系，两个都是主都是备；而发送binlog的日志时日志在接收用户写入时已经加了serverid, 发送给备，备又会原样返回，此时serverid是自己，会忽略这个消息。从而结束。
		 >主备延迟：seconds_behind_master 主库执行完事务写binlog开始到备库执行完事务结束这个时间差，大半部分是备库从relay log里读数据构成的。
		 >主备切换策略：参考https://blog.csdn.net/hanpeiyu1995/article/details/89499550
		  >可靠性优先：当备库的seconds_behind_master 足够小比如<5s时，说明基本要同步完了，可以切换了，于是将主库的readonly设置为true, 等seconds_behind_master 等于0的时候，把备库的readonly设置为false, 再把业务请求切到备库上即可。这样，仍然有时长为seconds_behind_master的时间不可写只能读。
		  >可用性优先：切换时直接切换到备库，备库的readonly立即设置为false,同时因为binlog_format=row 从而会binlog传递的是记录，那么主库未完成的binlog传递的条数就是冲突的最多个数----此时共有此数*2条数据未插入。所以丢失还是严重。
		  >个人方案：也是立即切换，但是备库在写的时候仅仅写到undolog里，暂时不实际的执行。等到主库binlog写完之后备库也执行完毕之后，才开始实际执行undolog--------此时undolog里有T3-T1这个落后时长的积累的数据，但是没有遗漏也没有丢失数据-----只是将会暂时不可用---卡住--否则暂时不可读。
		 >+keepalived:
	  >集群cluster的读写过程、节点加入移除和故障的检测恢复：redis为例 参考https://my.oschina.net/u/2600078/blog/1923696
	    >redis cluster基本架构：三主三从 + 若干sentinal构成的集合。
	    >hash slot:供16384个(2^18)，每个master分得若干个slot。从key得到对应的slot从而找到存储此key的master的算法：hash slot = crc16(key) mod 16384 。。这样，集群中，不同master存储的就是不同的key。16就是除数的位数，即现在crc编码就是15位
		>crc16:循环效验码 参考https://blog.csdn.net/xinyuan510214/article/details/80104356
			 >发送k位数据M: 先生成n位冗余码，然后k+n发送出去。其中n+1就是crc-16中的16这个值。
			 >冗余码生成规则：  M<<n 异或除 P(len=n+1)(收发端事先商定固定的除数) 得到一个余数FCS , 从而FCS + M<<n就是最终要发送的数据。异或除：每轮的被除数共n+1位，如果首位是0，则后面的位落下直至构成首位非0的n+1位数，继续和P异或运算，直至最后得出一个n位的余数。  
			 >冗余码接收端差错校验：直接将接收的数据 L  异或除 P 看结果如果是0则正确！！！
		>redis cluster读写过程：参考https://redis.io/topics/replication 避免master自动重启 http://redisdoc.com/topic/replication.html
			>读：slave转给master
			>写：master写之后，直接返回，非阻塞的，而异步的同步命令给slave。。master上有backlog， 同时接收slave的offset来发送剩下的指令。
			   >同步过程：slave发送psync命令给master, master验证runid未通过则发送RDB文件给slave, 验证通过则根据offset同步部分数据给slave---即发送缓存区的所有命令.
		>故障转移：master down之后集群会将master的一个slave晋升为新master。。自动化实现机制：sentinel架构。哨兵模式高可用架构。
		 >sentinel节点：互相心跳检测，也检测主从节点。也是一个redis实例。
		 >工作过程：当一个sentinel检测到某个master在指定时间内无心跳返回，则开始询问其他sentinel节点是否真的故障，其他sentinel则也去访问该master并且返回给询问sentinel结果，如果询问sentinel收到超过半数的确认认为故障，则这个询问sentinel开始故障转移工作：即从master所在节点组选择一个slaveA 发送slaveof on one 让它升级称为新的master，然后向其他slave节点发送 slaveof slaveA的ip:port 命令，从而其他slave开始设置新的master节点。另外，sentinel会继续监控老master,如果恢复了--则发送slaveof xxx同样的给它。
		 	
		>水平拆分：
		>水平扩展：增加一组master-slave实例，或者物理上的水平扩展(master slave交叉部署在多台物理机器上--更高可用)。
		>分片迁移：slot的迁出迁入
		>拓扑结构(集群结构)：按负责的slot分为多个 节点组 --->  每个节点组:master1 + slave多个 (master读写服务，slave只读服务)
	  >高并发的使用策略：读写分离、分库分表、sql优化
	>hbase: 自顶向下、职责划分
	  >设计理念：存储海量，通过rowkey把相关数据存储在一起---第一可以避免join-第二可以按rowkey分割存储在不同服务器--可扩展性强, 提供基于行的原子性操作。
	   >cell: 每一个cell存储了：rowkey --> colomn family --> column --> version/timestamp --> value  这也是每个cell值的完整索引。版本个数默认3， 超过则删除老的。
	   >Meta table: 保存集群region信息(rowkey范围-->region id-->region server)  通过B树组织。
	  >物理上：三大服务器集群：zk(主从)(服务器状态监控通知) , HMaster(主从--通过zk创建临时顺序节点时间), HRegionServer(集群)(和hdfs datanode同台部署)
	   >zk:
	    >一致性算法：
	  >读：客户端从本地或zk上读取所在MetaTable信息--->找到所在RegionServer--->发送给RegionServer读取请求--->RegionServer定位到HRegion-HStore, 依次Block Cache--> MemStore--->磁盘上HFile的索引/bloomFilter找到相应的cell读出
	   >读放大效应：memStore多次刷数据导致多个Hfile,导致需要读取多个HFile来
	    >措施：minor compaction: 自动将一些小HFile合并到几个大HFile中。
			   major compaction: 定时将一个cf下的所有HFile整理已经删除的或过期的cell并合并为一个HFile。因为会读大量HFile, 所以有写放大效应。且如果是在HRegion分裂后一个Region被转移到另一台服务器上，那么还会远程读取HFile到本地。
	  >写：客户端同上找到所在RegionServer--->发送给RegionServer---->RegionServer先append顺序添加写入WAL，再写入memFile, 然后返回写入成功；而再异步的判断memFile是否足够大而需要刷入磁盘形成一个新的Hfile/StoreFile(每条数据顺序刷，减少寻址时间所以高效)
	  >删除：给cell打上删除标签---使得不可读。
	  >备份：WAL和HFile都会在HDFS上有备份。
	  >HRegionServer宕机：zk通知HMaster，HMaster重新分配宕机机器上的Region给其他机器，同时将宕机机器上的WAL分配给对应其他机器，其他机器就可以顺序读取执行WAL中的数据操作产生memStore---当满了则flush到HFile
	  >HMaster: 多实例部署，zk负责进行leader选举.
		>对Region管理: Region 分裂合并后重新分配，调整Region分布，HRegionServer挂了之后迁移上面的Region到其他HRegionServer
		>对HRegionServer: 负载均衡，状态监听--通过zk通知
		>对用户：接收增删改查，数据库的创建删除
	  >HRegionServer: 多实例部署, HMaster负责对多实例负载均衡。
	   >对客户端写入：进行IO，存储数据到HDFS；读取也可以。
	   >对Region: 存储1000 个Region(可以来自不同表格，可以来自同一个CF的不同rowkey段)
	   >本地文件：
	     >WAL--日志文件-故障后恢复数据用，写数据到来时先写到WAL日志里。
		 >Block Cache 读缓存---访问次数最高的n个数据的缓存
		 >MemStore 写缓存---累积n个、排序后再刷到磁盘，形成一个Hfile/StoreFile
	  >HRegion: 默认大小：1GB
	   >对HStore: 存储多个HStore
	   >和family关系：一一对应
	  >HStore:
	   >对storeFile: 存储多个StoreFile, 组织方式：LSM树
	   >对memFile/memstore: 一个。
	    >LSM树：日志结构合并树。读弱写强。大多数NoSQL数据库采用。日志结构合并树，优化措施：bloomfilter判断小树有无，小树合并为大树提高查询效率。参考https://www.cnblogs.com/bonelee/p/6244810.html
		 >写强原因：日志顺序写磁盘，实际数据先在内存中用多颗B+树有序缓存--数据量达到一定时刷入磁盘(磁盘中是有序数据)。移动磁头、旋转磁盘、读取头标和数据；即由柱面号 移动臂移动到指定柱面，根据盘面号确定盘面，把块号指定的磁道段移动到磁头下，磁头读取数据传到内存。(移动臂移动自己--查找时间，移动磁道段--等待时间，磁头读写--传输时间))。(磁盘一般2-3个盘片)每一个盘片的两个盘面上都有一个读写磁头， 每个盘面1024个同心圆称为磁道(最外为0)，一个磁道上分多个圆弧称为扇区；所有盘面上同一个磁道构成一个柱面(每个柱面最上的磁头编号为0)(写数据按照同一个柱面最上磁道开始，一直写到最下磁道---电子切换；然后才移动磁头到里面的柱面---机械移动), 一个磁道多个扇区/块，一个块里分扇区头标(盘柱扇号+CRC)和数据(数据512B+ECC)两部分
		 >读弱原因：内存中小树读时间=N/m *log2(m) (使用bloomfilter会更少)， 没有则读磁盘已经写入的树文件(定期树文件合并merge为大树，减少磁盘读另外还减少了读时间---磁盘文件的m变大)，。(?合并的最大文件量是一个磁盘块--即一页8k?)
		 >内存小树写到磁盘后：为减少磁盘随机读(多文件)，多个storeFile 会合并为一颗大树(因为有序，所以合并简单)文件。这个叫小树文件compact。因为查询时间= N/m * log2(m) , m为每棵小树的数据量，从而m增大，查询时间会减少，当m=N时，达到log2N
		 >读取一个数据的过程：先从一棵小树上二分查找，没有则下一颗小树中找....
		  >ECC: 纠正一个bit错误和检测2个bit错误。参考https://blog.csdn.net/liaoyaonline/article/details/80166133
		   >q元码：(n,k,d) n为码字长度，k为码字数量，d为码字之间的最小汉明距离。
		   >汉明距离: v=(v1,v2,...vi) u = (u1,u2,...ui) v的汉明重量=w(v)=1的个数， v,u的汉明距离=d(v,u)=w(v-u)其中-号是异或。。。根据空间三角形，有d(u,v)<=d(u,w) + d(w,v)
		   >一个信号x的ECC编码：奇偶校验编码：x后面加上一位--x的1的个数(偶校验码), 如果x后面加上一位--x的1的个数的非(奇校验码)..总之，整个编码1的个数是偶数--偶效验码，个数是奇数-几效验码
						海明码：数据位N, 校验位r ， 则r位校验位可以组合出的最大的数2^r - 1， 也是能够检测的最大位，更高的位则不能检测；而在最大位以内又有r位是校验码，所以只剩2^r - 1 - r位来表示数据, 所有数据位N<= 2^r - 1 - r。
								r位校验位的位置：2^k, k=0,...r-1
								第i位校验码新增校验的全部位置：2^i * 奇数  (全体整数={2^n *奇数}其中n=0,...n)。所以全部位置都会被校验到。而且第i位刚好未知其他位都知道---从而确定第i位的奇偶校验值。比如对于偶校验，0位上的校验码值=所有奇数位的1数
								第i位校验码值确定：取i+1位隔i+1位再取i+1位...得到的数作为奇偶校验的输入信息。参考https://blog.csdn.net/lycb_gz/article/details/8214961
		  >局域性原理：一个数据被使用时，附近的数据也会被马上使用。如程序运行中，需要的数据不在主存，则触发缺页异常，系统向磁盘发读取信号，磁盘会找到数据起始位置并连续读一页或几页载入主存，异常返回程序继续执行。
		  >操作系统管理存储器的逻辑块：称为页。8k字节(16个扇区)。
		  >磁盘碎片产生的原因：文件在磁盘上连续存储，而文件新增则在其他地方，文件删除从而就会留下许多空隙，文件读取就会多次寻道而耗时更多。参考https://www.cnblogs.com/cxzdy/p/5379570.html
		 >跳表：多层链表。搜索时自顶向下搜索。每层链表有尾指针Nil 。
		  >搜索：查找元素为C, 在第i层链表中某个节点N处发现N<C，如果下一个节点是Nil, 则找N的下一层链表的元素M,继续比较。直到最底层，如果没有找到则没有。
		  >添加：同查找，直到最底层，D1<C<D2 也没有发现C，则在D1,D2之间插入一个节点值为C, 然后随机决定要不要在上一层垂直上方添加一个元素--值也为C。
		 >bloom filter: 初始化集合到 bloom内部数据结构：输入信息m, 用k个hash函数处理m得出ki, i=1,...m 标记bit[ki] = 1 ；形成过滤器需要的数据结构。查询时，只要input的k个hash值对应bit[]上的值有一个是0，则input一定不在集合里。但如果都是1，则大概率在集合里--也可能不在。
		 >bitmap: 本质上是长度为n的bit数组bit[n]来表示元素最大不超过n的k个自然数构成的集合。集合中有的自然数m则在bit[m]=1, 没有的自然数c则bit[c]=0。。。bit[n]的初始化，只需要遍历一遍集合就可以得出来；而bit[n]是有序的，再遍历一遍bit[n]就得出了排序后的自然数集合---海量数据排序。如果用byte[]数组来表示bit[],则 一个整数M在byte[]中的位置= byte[M/8]中的M%8下标位设置为1 byte[M/8] = byte[M/8] | 0x01<<(M%8) 
		 >Level DB: 顺序写磁盘日志Append Only; 应用到内存有序表SkipList(满了之后打包成key有序文件sst(顺序写到磁盘，追加而不是重写旧数据))(读则利用table cache, block cache, bloomfilter)
	   >对memFile: 存储一个memFile
	  >StoreFile：即刷入磁盘的一颗小树(B+树)
	   >数据结构：一个多层索引系统 + 一个个64kB数据块顺序构成，每个数据块可以当作B+树的叶子容器：包括--叶子容器的所有cell数据升序排列作为节点、叶索引、bloomfilter.。而多层索引系统就是B+树的根索引 + 中间索引+叶子索引；这个多层索引系统读到HStore中就是block cache
	  >memFile:	即LSM中的一颗小树(B+树)
	   >保存的每一行就是一个rowkey的一个列簇的所有cell信息(rowkey-cf-co-version-value),同时保存了最后一次写操作的序号---这样保证HFiles之间是通过序号有序组织的。
	   >保存新加入和更新的行：
	  >BlockCache: 最近读取过的n行
	  >Region: 分裂合并
	   >Region大小大于设定值时，平分分裂为两个，HMaster负责分配新产生的HRegion的分配。分配到其他HRegionServer上的Region会在它下一次的major compaction时将HFile文件下载到本地。
	>elasticsearch: 不用关系型数据库来实现全文检索。高扩展性、高可用性的实时数据分析。扩展至上百台，PB级数据。
	  >分词：将输入的搜索词 分解为 多个 关键词/字。。经过分词器Tokenizer得到Token词元。
	  >倒排索引：浏览数据而形成的 “关键字---文档id集合” 称为倒排索引，词典Term Directory(词与词频) --> 文档列表Postings List；但量太大而不会装载都jvm heap里， 所以对Term词典做了一个前缀索引Term Index, 用FST实现占用空间小--从而全量加载到内存heap； 从而先查内存中的前缀索引Term Index 再查磁盘上的Term Directory。es为每个column都使用索引--倒排索引(而不是一般关系型数据库的B树索引)。。参考http://www.360doc.com/content/16/0106/17/17572791_525951877.shtml
	   >词典前缀索引数据结构：FST树 参考http://www.shenyanchao.cn/blog/2018/12/04/lucene-fst/
	    >FST树：有限状态转移机(输入相应的信息--状态进行相应的转移：转移的起止路径对应输入的信息集合)：开始节点：单词最大前缀匹配， 后不够再增加边，直到 结束节点。形成一个有向无环图。
	     >作为key-value数据结构：压缩率有3倍-20倍，省内存---相比于hashMap/treemap
		>Trie树：存储字符串的链表，一个节点一个字符。子节点则最多26个(可以在父节点中用hashmap,array[26], bitmap表达)。达到其中一个节点必然只有一种路径---所以其中一个节点有标记--是否是单词(尾字符)。
		 >add: 从头开始查找，没有的字符则新增该字符节点，然后继续。
		 >query: 从头开始查找，没有的字符则在Trie树中不存在，直到最后Trie树的结尾/或者字符串的结尾，为判断的终止。
		 >delete: isWord=false或者删除整个都是false的分支---且只从下往上删到中间节点有分支为止。
		 ---排序：先序遍历。
		 ---词频统计：单词尾记录出现次数, 最后遍历一遍单词。
		 ---字符串存在检索/去重：相当于查询一次Trie树---效果比hash之类好是因为避免了和不必要的字符串比较---因为每个字符都是精准匹配的。参考https://segmentfault.com/a/1190000008877595
		>bit-wise Trie: 不是存字符，而是存0\1的 前缀树。/地址分配/路由管理.。。参考https://www.cnblogs.com/justinh/p/7716421.html
	  >全文检索：搜索词 分词 后去查 倒排索引 得到的 文档id集合 再去 查相应的文档 并返回，就是一个全文检索的过程。
	  >Lucene: 单机模式, java开发；索引、检索功能。ES的存储引擎，ES在之上增加了分布式接口而已--依赖Luncene；即建立一个分布式框架，而节点底层依赖Luncene进行实质操作；查询时并行查节点，结果合并；索引时，各节点独立索引自己节点上的数据。索引树：FST树---每个单词对应着的值就是docid集合。
	    >es喂给/传输给lucene的：已经是非常规范化的数据；一个Lucene实例也不用关心其他Lucene实例。
		>Lucene倒排索引生成过程：先在内存生成Inverted Index , 再定期以段文件segment file形式刷到磁盘上的，而且不再修改，更新会写到新的文件。所以一个段segment就是一个完整的倒排索引。而segment memory就是倒排索引词典的前缀索引。
		>开箱即用：不用配置。
		>jvm参数配置：jvm堆大小不超过os内存一半--以便os缓存磁盘数据。
	  >Solr: 数据格式更强：不止json,html,excel也可以。但实时性差。
	  >数据组织层次：索引--类型--文档--字段
	   >物理上：集群-->多个es实例节点-->
	            索引-->切分为多个分片--->每个分片多个 副本分片 --> 每个分片多个类型--->每个类型有一个mapping: 字段类型说明 --->每个类型下多个文档:row---->每个row多个field：column
				-----:分配：分片分配给节点。
				      gateway: es索引快照存储方式：一般是本地磁盘，也可以是hdfs,云。存储发生的时间：es先将索引存在内存，达到一定大小就调用gateway持久化到本地/hdfs/云。
	  >对用户提供：Transport交互方式:默认是tcp,但也提供了http: RESTFUL API ：curl 和java接口
	  >索引过程：全量/增量
	   >增量：/修改：修改也会把文档视作新文档而添加到新的segment里，分钟级别可以被重新索引(因为刷磁盘的fsync操作)。
	  >搜索过程：将Query转换为Lucene Query , 在所有的segment中计算
	    >具体过程：词法分析找出关键词；语法分析形成语法树 --> 关键词语言处理器处理规范化---> 从倒排索引中找出关键词的文档列表来交集/差集，---> 对文档排序(根据词对文档的重要性---词权重/词频数/词在的文档数，得到文档的词频向量，而查询语句也是一个词频向量，从而计算两个向量余弦值--作为相关性的打分，分值大的就排在前面: VSM向量空间模型算法)
		>极好参考：https://blog.csdn.net/guoyuguang0/article/details/76769184/
	  >添加数据：新的segment会先缓存到内核中，然后才flush到磁盘。来提高性能。而本身在文档修改,写入es节点时，会首先append写到translog里持久化，方便内存中的segment崩溃后从最近的commit point的数据恢复。
		>segment合并：同hbase HFile的合并，segment也会合并为新的更大的segment，同时整理了segment里的被标记为删除的数据。
		>segment内容：Inverted Index、Stored Fields 、Document Values
		>具体过程：数据生成索引存入内存Buffer, 同时写入translog, 内存中的数据每隔一秒以segment格式存入系统缓存，系统缓存中的segment定期刷入磁盘/同时清除translog中的记录。
		 >数据生成索引：分词得到词元-->语言处理组件将词规范化还原-->词传给索引组建 创建字典、排序、合并相同的词、形成文档倒排 链表
	  >删除数据：segment中的数据删除--仅仅是一个标记；下次会被索引到--但是返回给用户时过滤了。每一个提交点有一个.del文件--记录哪个segment的哪个docid被删除了。
	             文档删除---也仅仅是标记为删除.
	  >分片重新分配：当es节点宕机后，上面的分片会被重新分配到其他节点。
	  >p2p系统：广播和多播
	  >ES节点自动发现：Discovery.zen 相当于每台节点排序广播得到的网络中的serverid， 字典序第一个的作为leader，如果一个节点获得了超过半数票--则只会有一个--从而它就是新的leader
		>参考：https://blog.csdn.net/qq_17864929/article/details/54923720
缓存方面：(三大问题：key总是没有(穿透了)，突然没有-一个热key或者大量key(击穿了，雪崩了))
	>缓存穿透：key数据库不存在，缓存也不存在。导致对数据库不断的查询--给数据库带来压力。(解法：也存到缓存，但是值为特殊的)
	>缓存雪崩：缓存的同时缓存了大量的数据，失效时间也同，集中过期，(或者缓存服务器节点宕机、断网)，导致突然给数据库很大的访问压力--而且是周期性的。(解法：冷门访问缓存时间少，热门长；缓存时间加随机因子。)
	>缓存击穿：热点数据太热，在失效瞬间给数据库很大压力。

#微服务方面：
	>分布式系统的保护：缓存、降级、限流
		>降级：服务舍弃一些突发不可用功能确保整体可用，子服务不可用、超时，整体服务还是要能用，即时因而是有损的。(N次请求m次超时失败, 称为可用率m/N * 100%)
		>限流：一个服务限制调用方对它的调用速率。
	
消息队列方面：kafka为例子(独特数据结构+算法方面)
	>主题-队列组织结构：一个主题下多个分区，每个分区主从组织,每个分区的消息量。producer和主题的每个分区的leader保持socket通信，并且hash方式均匀发送到各个分区leader
	>acks级别：收到、收到且刷入本地队列、刷到从节点、从节点写入成功
	>多个consumer读取一个主题：如果在同一个group，则读取的消息唯一，所以此时comsumer也需要一个均衡算法
	>数据的存储和获取：本地磁盘的顺序批量操作(先buffer再flush到磁盘)，较少磁盘IO和磁盘旋转磁头运动，  
	>zero-copy机制:
	>消费模型：pull模型，comsumer控制消费速率，手动确认是否重新消费。
	>网络模型：nio的多线程selector
	>存储模型：副本机制
	>partion日志分段：每个消息都会append log, 每个段都有index和log两个文件；一个partition日志分段为了方便二分查找日志--先找到端再找到内容，从而方便随机读；而顺序读，利用os预加载page cache页缓存，所以快。
	>leader所在broker故障而消息不丢失的原因机制是什么：HW高水位  
	>消息发送成功三种语义保证：at least once , at most once , exactly once 。最后一种的实现：可以不丢失消息还保证消息顺序，一个producer一个broker,broker对消息进行对齐；消息格式为"序号+消息"，序号小认为重复序号大1以上认为消息有丢失
	>事务：producer对消息队列一次操作的集合。  
	>系统架构/组织架构：节点-broker-topic-partition(主从，不在一个节点上；且有ISQ量统计)。 zk集群，broker多节点，
	>最主要的数据结构：
	>读：
	>写：
	>高可靠机制-实现而做的事情：(本机制定义：消息不丢失，不乱序，节点宕机自动发现并故障转移重新恢复--broker/partition，)请求到partition先顺序写日志，副本机制，分布式系统处理语义：
		>日志目录下多个日志段LogSegment: 每个日志段分为.index索引文件和.log数据文件
		  >稀疏索引/二次索引/辅助索引：叶子节点保存的是数据行引用--主键--物理地址
		  >稠密索引/聚集索引：只有一个，叶子节点保存的是整个数据行
		>发送方处理语义：至少一次，至多一次，刚好一次(一个producer一个topic下对应着一个专门的pid 。 pid + 序列号；；实现partition下的消息队列顺序)
		>事务保证处理语义: 节点重启后从断点处继续执行
	>高性能机制-实现而做的事情：客户端selector读写都是单线程处理，服务端selector监听，读线程池处理读请求，发送请求到message queue, 写线程池读取队列的消息处理并返回客户端。
		>零拷贝：
		>异步：
	>高可用机制-实现而做的事情：副本机制
		>
	>高并发机制-实现而做的事情：高可扩展机制---分布式部署-多节点-节点增加
	
	>高吞吐量机制-实现而做的事情：零拷贝
		>零拷贝：存在几个数据区：内核态数据区、用户态数据区、socket发送缓冲区、协议引擎。
		 >无用户操作的普通方式的数据从一个磁盘文件拷贝到另一个磁盘文件的调用过程：用户态read()调用-->切换到内核态--->读取/拷贝磁盘数据到内核态数据区-->拷贝数据到用户态数据区,--->系统调用返回，切换到用户态-->用户处理后write()调用--->切换到内核态，将用户态数据缓存区数据拷贝到内核态数据区--->系统调用返回，切回用户态。-->内核将内核缓冲区数据写到磁盘。从而4次切换4次拷贝
		 >无用户操作的零拷贝方式的拷贝过程: 用户态切换到内核态-->读取/DMA拷贝数据到内核态数据缓存区---> CPU再拷贝内核态数据到与socket相关的缓冲区，系统调用返回--->通过DMA引擎将缓冲区数据拷贝到协议引擎
		              零拷贝用gather copy: 用户态切换到内核态-->DMA拷贝数据到内核态数据区-->再拷贝内核态数据描述: 缓冲区地址+数据偏移量 拷贝到socket相关缓冲区 ,系统调用返回---> DMA引擎 通过 gather copy方式只需要根据 缓冲区地址+ 偏移量就将数据直接从数据区拷贝到协议引擎。 java nio transferTo()
					--linux mmap()系统调用的零拷贝实现(java mappedByteBuffer)：关键在DMA拷贝数据到内核态数据区--可以被用户直接操作，从而不用拷贝内核态数据区到用户态数据区；此时数据拷贝到与socket相关的缓冲区仍然是cpu拷贝。所以叫内存映射。
					--DirectByteBuffer:堆外内存、页面对齐、自己管理释放分配。
			>DMA: 外设直接内存读写，不需要CPU参与---即CPU不用亲自读取数据到内存再到寄存器，再从寄存器回写到外设，而直接DMA控制器就可以。
		 >应用：文件复制、消息读取发送到网络：			  
		
日志采集方面：flume为例子，agent(source/channel/sink)到agent的结构(一般部署在日志采集节点，中心再部署agent
)(selector->channel->sink->kafka/hdfs/agent)
	>日志+处理模式：比单纯的日志有更多的处理：一是拦截器链(使得不一定存到channel/memorychannel/filechnnel/jdbchannel)，二是写到了channel里(channnel看作一个队列，但有很多接口方便sink调用)，三是有sink异步的独立的调用channel而获取里面的数据，进一步的处理可以为rpc发送到另一个agent；这个agent再将event数据调用source写入channel，而另一方面sink调用channel读取event写入jdbc或者文件等。
	>数据单位：event是一个字节数组，有header头信息。
	>多级流：nginx, tomcat, log4j, syslog日志混合在一起的日志流，可以在在下一级Source时分开发送到不同的channel里，再sink到不同的地点。
	>负载均衡：一个channel可以配置多个sink,并且均衡的从channel里取得event(或者说channel均衡发送event到不同的sink---当然是取好--速率由接收方决定)
    >source: spoolsource监控spool目录，有新日志文件就会读取数据(利用log4j配置为精确到分钟级别)，完毕后文件后缀变为.COMPLETED  而 Exec Source可以实时采集日志
	>本地日志log4j2:
		>distrupor: 高效低延时的消息组件-读写数组(队列)。高性能有界内存队列。(CAS和缓存行补齐方法性能提升)
			>数据结构：数组(环形)，长度：2^n ， index只需要递增，下一个元素位置就是 index & (2^n - 1)按位与就可以了。
			>读写一致性保证：CAS保证写的同步从而一致。即会先申请位置，然后CAS方式写入。(实际上：可以先读取index, 再计算出下一个位置，再CAS写入，失败重读index)
			>缓存行补齐：避免伪共享问题。一个缓存行有64字节，一个缓存行就是cpu L1L2缓存失效的基本单位,而缓存失效是其他cpu发出的失效命令导致的---即改cpu要更新数据写到主存(同时加内存屏障)。
			>avalibleBuffer: 写入后更新多个位置读avalible，读出后更新那些位置写avalible。从而下一次写之前读位置时就有位置可以读了。	
			
	
大数据统计分析方面：Hive为例子
	>
大数据数据导出方面：sqoop为例子
	>

架构设计与技术选型：系统设计/方案设计评审	
	>系统设计的几个主题：参考：https://blog.csdn.net/u013007900/article/details/79049961

软件工程方面：
	>系统建模：数据流图、架构图、时序图、组件图。。。。参考：https://www.jianshu.com/p/4c9f795da7ea
	>
	
#开发方面：
	>能提前评审发现隐藏的潜在的问题：也发现该技术的能力边界。 
	
	
Spring方面：
   >IOC和AOP的实现：
	>IOC：bean交给spring来实例化，只需要配置该bean的属性，对于对象类型的属性-甚至可以不必配置--直接默认也交给spring注入，。(控制反转，依赖注入)
    >AOP: 方法的代理 ，代理方法的内容就是切面编程(方法的前后两个切面)。
	>spring做的事：
		>1.浏览包，对每一个带注解的类/接口 对应生成一个继承/实现它的子类/实现类(字节码)(代理类、动态代理类)--这个类对父类方法全部重写-内容都是调用一个回调类的方法(如果动态代理是cglib，就是实现了MethodInterceptor的类的intercept()方法;javax.tools包则根据类字符串而动态字节码生成)(这个回调类方法里，spring的处理是先获取方法的所有注解，先执行在被代理方法invoke之前要执行的注解的实现类的before方法(以及显示匹配本方法的切面bean的before方法)，再invoke父类方法，再执行之后注解实现类要执行的after方法(和显式匹配到本方法的切面bean的after方法))，将此代理类反射实例化，放入容器(map)。(常规的@Controller @Service @POST @RequestParam; @Aespect)。。属性的注入体现了spring做的依赖注入IOC，方法的代理体现了spring做的切面编程AOP(编写一个切面)。
		>2.执行bean初始化方法：init-method
   >WF实现的IOC和AOP:
  >事务的产生：在并发请求下确保数据库数据的一致性变化(要保证一系列操作具有ACID四种特性。为分别这些操作和其他无4特征的操作，称有4特征的一系列操作为一个事务，为分离4特征的一系列操作和其他操作的额外动作就是事务的动作---如打开事务、提交事务、关闭/回滚事务；从操作数据流上看，是这样子)。数据库使用事务的具体做法：打开事务、提交事务、关闭事务/回滚。(非分布式所以没有预提交的过程)
	>数据库对事务的实现：加锁(打开事务)--> 新临时空间执行一系列操作 --> 提交修改(两份，一份老一份新，乐观锁)--> 释放锁--->返回成功。paxos的两个阶段也类似于获取锁而顺序化、提交修改而成功而释放锁。
	>行级锁：必然要记录哪些行没有加锁，加了哪些锁
  >分布式事务的产生：如上，就是在分布式环境下确保数据库数据的一致性变化。数据库要提供一种机制，确保 可以让一系列操作 具有ACID四种特性。
  
  >事务的实现：依赖数据库的事务：打开连接-打开事务-执行操作CURD-提交事务(故障则回滚事务)-关闭连接。。可见执行操作CURD可以被切面编程，而前后的动作都是固定的，所以spring只需要直接写一个事务切面就可以了--用户则只需要使用这个事务切面的注解即可。       参考资料：https://www.jianshu.com/p/2449cd914e3c
	>事务的隔离级别：5种，上述。事务之间互相影响的结果/程度/大小。
	>事务的传播行为：决定新建一个事务，或者加入一个已经有的事务(使用同一个连接)(已有的事务是什么？---即保存在事务ThreadLocal里的当前线程的值)(此外，还有连接ConnectionHolder， 也是当前线程保存的连接)。
		>产生此概念的原因：A方法上有事务，B方法上有事务，而A方法又调用了B方法。
		>行为1：PROPAGATION_REQUIRED 保持一个事务。
		>行为2：PROPAGATION_SUPPORTS 支持当前事务，没有就也不新建。
		>行为3：PROPAGATION_MANDATORY 支持当前事务，但没有要抛出异常。
		>行为4：PROPAGATION_REQUIRES_NEW 当前有事务，则挂起。建立另一个事务执行完再说。
		>行为5：PROPAGATION_NOT_SUPPORTED 不使用事务，当前有也不使用--把它挂起。
		>行为6：PROPAGATION_NEVER 不使用事务，但没有要抛出异常。
		>行为7：PROPAGATION_NESTED 当前有事务，则嵌套一个事务；当前没有事务，则新建一个事务执行。
	>事务的保存点：回滚到保存点。保存点之前的不用回滚，确认已经ok。
	>mysql 命令控制事务：
		>设置数据库、会话的事务隔离级别： set session transaction isolation level read committed; set global transaction isolation level repeatable read;
		>设置不自动提交：set autocommit = 0;
		>开启事务：start transaction; begin;
		>回滚事务：rollback;
		>提交事务：commit;
		>在一个事务中设置多个保存点：savepoint tx1;   则xxx之后，可以 rollback to tx1;//回滚到保存点
		>开启只读：在作用结束前本会话也读不到其他会话/事务插入的内容：SET TRANSACTION READ ONLY;  结束只读：commit;
