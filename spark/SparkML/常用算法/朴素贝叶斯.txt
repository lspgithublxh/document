1.问题描述
  >预知1：贝叶斯定理
	>类别项、待分类项、分类映射方法。其中分类映射方法不是一来就可以知道的，是根据样本数据--经验数据而构造的、学习、训练、调整出来的，与数据特征、训练方法有关。
	>条件概率：P(A|B)----即事件B发生后事件A发生的概率。
		   >条件概率就是在因事件发生的情况下，果事件发生的概率，即两个事件是相关的，前一个影响后一个。（必须是相关的，否则是无关事件--条件概率等于AB两个事件的概率的乘积）
			且因果事件，相当于事物的因果情况，事物的A种事情，可能会发生多种情况，而事物在每种情况下可以再发生B种事情的多种情况中的某种情况。
			澄清：即1）事物会发生A类事情和B类事情。
				2）A类事情和B类事情都有多种情况。
				3）A类事情发生后，B类事情才能发生。
				4）对于A类事情可能的每种情况，B类事情发生的每种情况的概率是不同的（即A类的每种情况对应的B类事情的情况分布是不同的）。
				5) A类事情的每种情况都有一定的发生概率（和为1），B类事情的每种情况也是。
			--------可以用从左到右为从A到B的树延展来表示因果情况和各个情况概率的对应关系---------------
			推论1：上述的树实际上就展示出了P(A),P（B|A）, P(B|A反)三个主要的概率，而且展示了B事件发生的各个情况的概率比----当然也包括B事件发生的概率P(B)（A的各种情况下B发生的概率求和的方式得出，自然得出A的各种情况(这里只有两种：发生或者不发生)在促使B发生所占的概率比例），从而自然推出计算出B事件发生后再发生A事件时的概率（即看B发生是多大程度上多大比例上由A发生所引起的，据前面的成分比例，一目了然。。和上述的树概率结合）:写成代数表达式就是：P(A|B) = (P(A) * P(B|A)) / P(B)
				（此推论的结果就是贝叶斯定理）。。
			等价表述：无论A发生或者不发生，B都有发生的概率，但发生的概率不一样。。即A发生时对B发生促使的作用力影响力，大小不同于A不发生时对促使B发生的作用力和影响力，或者一小一大或者一大一小。
				   反过来，发生了B，对于已经发生的事，它发生的概率是1（对于未发生的事情，它发生的概率是小数，即小于1），而B的发生受A的发生或者不发生的影响--且都占一定的比例（P(B|A),P(B|A反)），那么根据这个比例表达的是A发生所导致B发生的可能性和A没发生所导致B发生的可能性大小之比。
				 >新的推论：已经发生的事，发生的概率是1。对于构成它发生的各种可能原因的分量（权重）之比---可以分析出来，而这个分量比作为B发生后表明已经发生各个可能原因的概率，，对已经发生的可能事件有推测指导作用，有价值。
				 >新推论2：未来的事，发生的概率小于1（对于独立发生的，不受任何概率因素影响的事的概率，是该事的先验概率）。对于已经发生的事，但是对于它原因，有多种可能，即还表明发生了另一种事，但该种事有多种可能，到底是哪种，有一个概率可以计算-推测（这是向后推测，定义这种概率是条件概率）。
				 >不变性1：无论发生了多少B,都有：A发生的条件下发生B的概率是不变的（P(B|A)是不变的）。
					   发生一个B之后，可以知P(A|B),而这个概率就要当作在B发生后那个时刻时的人来看来认为A发生的概率,即把P(A|B)当作B时的人认为的P（A），如果接着又发生了B，而我们知道P（B|A）是不变的，那么那时的人可以计算这第二个B发生的概率及其成分（关键是计算它的成分） （ P(B|B)= P(A|B) * P(B|A) + P(A反|B) * P(B|A反) = P(B|A|B) + P(B|A反|B)）,从而推出第二个B发生后表明的A发生的概率：P(A|B,B) = P(B|A|B)/(P(B|A|B) + P(B|A反|B))。
						概率不好看规律，而概率比好看规律：
						发生第一次B之后，A发生与不发生的概率比为：P(A|B) / P(A反|B) 
						发生第二次B之后，A发生与不发生的概率比为：P(A|B) / P(A反|B) * (P(B|A) / P(B|A反)),这是交换第二第三项后的形式。
						同理，类推，发生n次B后，A发生与不发生的概率比为：P(A|B) / P(A反|B) * (P(B|A) / P(B|A反))<n - 1> 。 （根据P(A|B)的计算公式等，还有另一种形式：P(A) / P(A反) * (P(B|A) / P(B|A反))<n>,,但更通用）
						如果这个事情已经发生了n次，共发生了k次B，n - k 次B反（即未发生B）,那么同理，类推：A发生与不发生的概率比为：P(A) / P(A反) * (P(B|A) / P(B|A反))<k> * (P(B反|A) / P(B反|A反))<n - k>
					--------上述即是拓展后的贝叶斯定理（公式）。
		   >先验概率：
			
	>经典问题：已知袋子里n个白球m个黑球，求摸一次摸出黑球的概率。反过来，袋子里有若干个球，摸了100次，每次摸一个并放回去，统计每次摸到的颜色，最后统计各个颜色的球出现的概率，求袋子里各颜色球的概率。
   			（逆向概率问题:已知概率求情况。而概率问题是已知情况求概率。）
		  >两个独立的未来事件（即现在还没发生,可能将来一前一后发生，或者同时发生）都发生的概率：概率相乘
		  > 一个事件可能会变成多种事件中的某一种（现在还不知道），求是某两种独立事件中的任一种的概率：概率相加
		  >历史事件：事件的统计：已经发生的n个事件，对将发生的那个同类事件，是有影响的，即我们计算概率是以知道的信息为根据的，这信息包括现实事件本身所属于的事物情况本身也包括事物情况最近发生的各个事件（统计来量化说明的问题，说明的事实），但是如果事物情况本身已经在多次事件种暴露了更多的性质、更多的事实，从而我们对事物情况了解更深更精确，那么事物将发生什么事件，我们必然有更精确的计算更精确的概率肯定----即更加接近真实的概率:而不应该和之前对事物情况毫不了解时作的概率估计一样。
	





https://www.zhihu.com/question/19725590（推导贝叶斯定理）
http://blog.csdn.net/kesalin/article/details/40370325/（简要介绍贝叶斯定理）	  
http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html（贝叶斯定理数学推广）
http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html（贝叶斯分类器例子很好）
http://blog.csdn.net/tanhongguang1/article/details/45016421（贝叶斯分类器，简洁又代码）
http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html