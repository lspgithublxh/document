1.问题描述
  >预知1：贝叶斯定理
	>类别项、待分类项、分类映射方法。其中分类映射方法不是一来就可以知道的，是根据样本数据--经验数据而构造的、学习、训练、调整出来的，与数据特征、训练方法有关。
	>条件概率：P(A|B)----即事件B发生后事件A发生的概率。
		   >条件概率就是在因事件发生的情况下，果事件发生的概率，即两个事件是相关的，前一个影响后一个。（必须是相关的，否则是无关事件--条件概率等于AB两个事件的概率的乘积）
			且因果事件，相当于事物的因果情况，事物的A种事情，可能会发生多种情况，而事物在每种情况下可以再发生B种事情的多种情况中的某种情况。
			澄清：即1）事物会发生A类事情和B类事情。
				2）A类事情和B类事情都有多种情况。
				3）A类事情发生后，B类事情才能发生。
				4）对于A类事情可能的每种情况，B类事情发生的每种情况的概率是不同的（即A类的每种情况对应的B类事情的情况分布是不同的）。
				5) A类事情的每种情况都有一定的发生概率（和为1），B类事情的每种情况也是。
			--------可以用从左到右为从A到B的树延展来表示因果情况和各个情况概率的对应关系---------------
			推论1：上述的树实际上就展示出了P(A),P（B|A）, P(B|A反)三个主要的概率，而且展示了B事件发生的各个情况的概率比----当然也包括B事件发生的概率P(B)（A的各种情况下B发生的概率求和的方式得出，自然得出A的各种情况(这里只有两种：发生或者不发生)在促使B发生所占的概率比例），从而自然推出计算出B事件发生后再发生A事件时的概率（即看B发生是多大程度上多大比例上由A发生所引起的，据前面的成分比例，一目了然。。和上述的树概率结合）:写成代数表达式就是：P(A|B) = (P(A) * P(B|A)) / P(B)
				（此推论的结果就是贝叶斯定理）。。
			等价表述：无论A发生或者不发生，B都有发生的概率，但发生的概率不一样。。即A发生时对B发生促使的作用力影响力，大小不同于A不发生时对促使B发生的作用力和影响力，或者一小一大或者一大一小。
				   反过来，发生了B，对于已经发生的事，它发生的概率是1（对于未发生的事情，它发生的概率是小数，即小于1），而B的发生受A的发生或者不发生的影响--且都占一定的比例（P(B|A),P(B|A反)），那么根据这个比例表达的是A发生所导致B发生的可能性和A没发生所导致B发生的可能性大小之比。
				 >新的推论：已经发生的事，发生的概率是1。对于构成它发生的各种可能原因的分量（权重）之比---可以分析出来，而这个分量比作为B发生后表明已经发生各个可能原因的概率，，对已经发生的可能事件有推测指导作用，有价值。
				 >新推论2：未来的事，发生的概率小于1（对于独立发生的，不受任何概率因素影响的事的概率，是该事的先验概率）。对于已经发生的事，但是对于它原因，有多种可能，即还表明发生了另一种事，但该种事有多种可能，到底是哪种，有一个概率可以计算-推测（这是向后推测，定义这种概率是条件概率）。
				 >不变性1：无论发生了多少B,都有：A发生的条件下发生B的概率是不变的（P(B|A)是不变的）。
					   发生一个B之后，可以知P(A|B),而这个概率就要当作在B发生后那个时刻时的人来看来认为A发生的概率,即把P(A|B)当作B时的人认为的P（A），如果接着又发生了B，而我们知道P（B|A）是不变的，那么那时的人可以计算这第二个B发生的概率及其成分（关键是计算它的成分） （ P(B|B)= P(A|B) * P(B|A) + P(A反|B) * P(B|A反) = P(B|A|B) + P(B|A反|B)）,从而推出第二个B发生后表明的A发生的概率：P(A|B,B) = P(B|A|B)/(P(B|A|B) + P(B|A反|B))。
						概率不好看规律，而概率比好看规律：
						发生第一次B之后，A发生与不发生的概率比为：P(A|B) / P(A反|B) 
						发生第二次B之后，A发生与不发生的概率比为：P(A|B) / P(A反|B) * (P(B|A) / P(B|A反)),这是交换第二第三项后的形式。
						同理，类推，发生n次B后，A发生与不发生的概率比为：P(A|B) / P(A反|B) * (P(B|A) / P(B|A反))<n - 1> 。 （根据P(A|B)的计算公式等，还有另一种形式：P(A) / P(A反) * (P(B|A) / P(B|A反))<n>,虽然不直观,但更通用,但在数学形式上是直观的）
						如果这个事情已经发生了n次，共发生了k次B，n - k 次B反（即未发生B）,那么同理，类推：A发生与不发生的概率比为：P(A) / P(A反) * (P(B|A) / P(B|A反))<k> * (P(B反|A) / P(B反|A反))<n - k>
					--------上述即是拓展后的贝叶斯定理（公式）。
					且若干问题明确定义、指定、假定其中的A事件是什么、B事件是什么之后，使用上述公式，即可计算出n次结果后的条件概率。非常惊人。准确。
			事情分析：----分析一件事情（通过最普通的哲学），即便不知道任何理论和规律，也能把事情描述清楚，把事情表述清楚，也知道方案可能是什么，应该是什么。
				
				明确随机结果、明确随机事件，从而精确描述随机事件，从而定义关于它的各种概率（如条件概率）。
		   >多级贝叶斯：
				因有很多个（因情况是一种组合情况），而结果只有一个（具体取值不定，分别有一定的概率）。求某种组合因发生的条件下,结果的分布概率。
				---组合因作为条件，求结果是难以求的。但是，组合因本身发生的概率却又计算方法：即只要组合因中的每个因是独立的，那么组合因发生的概率就是各个因发生的概率之积。这个可以推广到很多的因素，n个因素一个结果的情形。同时，结果也可能是多种，则可以计算每种结果的概率--而不只是一种结果的概率。而显然，每种结果的概率是不一样的，如果这个结果是分类结果，而概率是某个分类的概率，那么这些概率中有个最大值，而这个最大值就对应的那个结果--分类应当就是对应该种条件的分类。---此即贝叶斯分类器。
				推论：贝叶斯分类器--如上所述。
				     > 方法条件：样本是一个个个体，每个个体有若干个特征（属性），每个特征有具体的取值，而不同的个体，取值是不同的。而每个个体，都有一个类别--对应一个类别值--对于样本，这个类别值是明确了的。而类别值是离散的，是有限的。
						 很多个样本，会对应到同一个类别值。把某一个类别值看做结果，对应的各个样本的各个特征值看作是条件，而各个特征是独立的--从而特征取什么值与其他特征无关，从而取值概率也无关。所以以一个类别的多个样本为样本集合，任意一个特征的每个特征值都有取值概率，这个是相对于这个样本集合--或者说这个类别的概率，所以是条件概率，而每个特征都可以得到这个类别的若干个条件概率，而类别本身相对于全部类别，也有一个取值概率。而条件的结合可以得到笛卡尔直积个联合概率（而且往往，很多种联合条件是样本所没有的，但是因为特征的无关独立而可能联合而可以联合，从而联合概率本身也就是一种预测,对尚未出现过的联合条件的预测，新型样本的分类的预测）----即上述算分类的等效概率。
						  而每一个类别都可以按上述处理，那么每个类别都可以得到若干个联合概率，而每个类别中，肯定有重复的联合条件，而取联合概率最大的联合条件对应的那个类别为这个联合条件的类别。----而上述已经表明，联合条件可以是样本中没有的取值联合，所以对于一个新类型取值联合的样本，其属于什么分类也可以预测。。更不用说已经在样本中出现过了的取值联合类型。
				    >  特征的分类：一是区间，二是按排序按个数，三是将特征值视为按照正态而分布---从而知道特征取各个值的概率（其实这也是本质，根本目的，计算概率，而正态可以计算概率，且特征现实属于正态分布，所以用正态分布求特征取值概率）。
					
				（上述才是贝叶斯定理惊叹的地方，独辟蹊径的地方，其他方法很难给出足够信服理由这样计算就是真实结果的地方、路径、途径）
		   >先验概率：
         -------在各种结果事实发生后，此时考察条件概率，会比先验概率更接近于真实概率，且下一次得到的条件概率比上一次的条件概率更接近于真实概率（而下一次的条件概率的计算基于发生的事实和上一次的条件概率。。如果从数学形式上看，则有一个阶段式的方法）。（结果反过来说明条件应该是什么样子）
			

	>经典问题：已知袋子里n个白球m个黑球，求摸一次摸出黑球的概率。反过来，袋子里有若干个球，摸了100次，每次摸一个并放回去，统计每次摸到的颜色，最后统计各个颜色的球出现的概率，求袋子里各颜色球的概率。
   			（逆向概率问题:已知概率求情况。而概率问题是已知情况求概率。）
		  >两个独立的未来事件（即现在还没发生,可能将来一前一后发生，或者同时发生）都发生的概率：概率相乘
		  > 一个事件可能会变成多种事件中的某一种（现在还不知道），求是某两种独立事件中的任一种的概率：概率相加
		  >历史事件：事件的统计：已经发生的n个事件，对将发生的那个同类事件，是有影响的，即我们计算概率是以知道的信息为根据的，这信息包括现实事件本身所属于的事物情况本身也包括事物情况最近发生的各个事件（统计来量化说明的问题，说明的事实），但是如果事物情况本身已经在多次事件种暴露了更多的性质、更多的事实，从而我们对事物情况了解更深更精确，那么事物将发生什么事件，我们必然有更精确的计算更精确的概率肯定----即更加接近真实的概率:而不应该和之前对事物情况毫不了解时作的概率估计一样。
	





https://www.zhihu.com/question/19725590（推导贝叶斯定理）
http://blog.csdn.net/kesalin/article/details/40370325/（简要介绍贝叶斯定理）	  
http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html（贝叶斯定理数学推广）
http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html（贝叶斯分类器例子很好）
http://blog.csdn.net/tanhongguang1/article/details/45016421（贝叶斯分类器，简洁又代码）
http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html