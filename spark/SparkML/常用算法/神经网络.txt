1.问题描述：
	>条件--问题：
		   对于样本，样本有多个属性，有些为特征属性，这些值都知道--容易采集，而有的为目标属性，这个一般只有样本有。而目标属性和特征属性之间有线性关系或者其他明确的关系，但是关系中的系数是不明确的，利用样本求这些系数。
		 >神经网络（两层）对于分类的处理：
					>对于超平面，直线的分类，可以直接形象看作，超平面的参数。
					>对于曲线的分类，可以用更高维度线性组合表达式来描述，此时各个线性参数已经不是同一层次的超平面/直线的参数（因为同一层次的超平面、直线的参数远少于此时目标的超平面参数的个数），而是同一层次二项式变量组合（这组合容易想到就是任意曲线）的系数,从而以线性的方式（更多参数而已）等效表达了低层次的超平面上的任意曲线。
		  			>输入节点个数代表变量个数（输入向量的维度），中间处理层节点个数代表超平面变量的个数（也代表了超平面的系数的个数），输出节点个数代表分类曲线的个数（也代表了类别的个数）。。
					>分类方法：通过调整中间处理层各个节点输出和输出的参数和节点个数（一般是增加），可以获得任意多个不同的分类曲线----从而对各种维度的数据进行分类。
		  			>关键在于：如何调整参数（使得对所有的样本输入，输出都是符合预期的）（粒度和方向）
						  >训练：因为对于一个样本，输入值知道，希望输出值也知道，而 实际输出值也知道。
							 计算损失：差的平方和。损失最小那么参数才最优，使用最小二乘法求最小损失时的各个参数值，可以一轮一轮的进行，每轮有参数个样本，第一轮的参数是随机的，第二轮的参数是使用第一轮的参数和输入值-实际输出值-希望输出值计算出来的，下一轮的参数基于上一轮的参数-输入值-实际输出值-希望输出值而根据最小损失使用最小二乘法计算出来的，同理下一轮，...已知到后面的很多轮。
							 直至结果误差非常小为止。
							 误差的平方和的梯度：
							 -----梯度下降法也可以（沿着梯度的反方向前进-步进，比如梯度为正那么向梯度不断减小的方向步进，梯度为负则向梯度增大至0的方向前进，总之就是向0靠近（各个分量都是））----这是一种纯粹的计算机方法（完全不使用数学公式，而最小二乘法非常依赖于公式，是根据公式直接计算出目标在哪里，梯度下降法是纯计算机方式一次次逼近出来的）。
							另外:对于神经网络，因为有三个参数矩阵，需要估计三次，所以从后往前进行估计：即输出层估计整体的系数，用实际输出值和应当输出值可以估计中间计算层的每个节点应当输出值和实际输出值，，因此这里又可以构造一个误差函数，求其最值就可以得到这一层的参数系数，同理再往前推。（反向传播算法）
						  >梯度：函数f在各个方向（维度）上的分导数。
		  			>神经网络结构的描述：各层的节点的个数。
		>多层神经网络：
				即在两层神经网路后面再增加神经网络。形象化理解，可以把输入层输入的当作基本元素，第一层对基本元素进行区分，第一层的输出就是基本元素的权重组合，产生第二层的区分（即组合出了这么多种组合体），第二层的输出再次进行线性组合，产生更高级的组合，，，，一直组合到最后的结果。
				（可以用原料---一桌菜来形容，或者笔画---全部汉字来形容，或者基本关键字、语法----全部代码结构来形容，或者基本材料--各种建筑大厦来形容）
				>正则化技术：
		>java实现一个两层神经网络：
					>即用数组（一维和二维）来实现（矩阵运算当然也可以）	
		>矩阵是一种并行表示和并行计算的方式。
		>卷积是一种历史各个时刻开始的响应演变到现在的值的线性叠加（响应是一个函数，线性叠加的系数就是另一个函数的逆序各个历史值）。
			>图片上的卷积--在空间上的响应的线性叠加（非时序上的响应的线性叠加）：
				      把图片当作原函数，空间上的每个值就是一个小方格从图片最左上方顶角位置一直运动到右上方顶角...一直到图片最右下方的顶角过程中每个瞬间的（每移动一个像素位置）的小方格框定的图片区域的值（而相差一两个像素位置的小方格对应的图片内容可以看做是一样的，就像函数在相邻的两个时刻对应的函数值看做是一样的，同一个意思），而这个运动的瞬间的个数就是图片的像素的个数，把每个像素位置放一个系数的小方格感受片当作激励函数，而图片的一个区域对感受片的输出就是图片在这个方格区域内的每个像素位置上的一个值（即颜色值）和方格在这个位置的系数的乘积的线性和。所以一个图片的一个方格小区域只输出一个值（这个值可以看作是这个空序时输入值和单位激励响应值的线性组合---只是说这个单位激励响应值一直不变，不因为小方格位置的不同而不同而已）。从定义可以看出，这个输出值受到一个小方格区域的数据的影响，而这个值代表的是图片在这个空序（即小方格位置）上的响应值，且有小方格在运动过程中同时进行这个响应，并且把响应值按照小方格运动的方向位置进行放置，那么运动完成后则形成了一张和原图片同样多像素的图片，
					就像我们想看输入函数输入一段时间后，那个时刻系统输出的值（响应值，实际想看组成成分，或者说计算组成成分）一样，我们想看小方格运动到某个位置时输出的组成成分---显然这是所有历史输出的组成--已经构出了部分图片了，而不断的运动，新产生的值就不断的组合进去，----这种历史响应的组合特性正和卷积的含义相似。所以叫做图片的卷积。
			>上述是图片对小方格的卷积，小方格相当于一个固定不变的单位激励响应---与位置无关，如果与位置有关，那么这是一个随运动位置变化而变化（方格内部系数变化）的小方格,显然，如果把系数当作颜色值，小方格运动变化也是运动一个像素位置基本不改变内部系数--或者只改变部分系数，那么小方格的运动变化的参数集合就构成一张同等原图片的图片。
			 ----这就是图片对图片的卷积。
			----意义在于：输出是原图片的特征。即可以利用这个卷积来提取原图片的特征。
					---看图片的那些位置有某种特征，以及这种特征的强度有多大。
					---所以说，如果用不同的小方格（内部系数不同）来卷积，那么就得到原图片哪些处具有新的特征及其强度。。这样，用很多个小方格进行卷积，那么就得到图片上哪些位置上有哪些特征特征的强度有多大的结果。
			----卷积的输出：可以看作是区域运动的每一瞬间（每移动一像素）的特征的喷出（从小方格最左上方位置）。
					运动中每一瞬间（每移动一个像素）所框住的区域中的特征依次喷出来。就像在大海里航行，船上所见得到的视野区域就是小方格，航行方向就是小方格运动方向，特征就是船上能看到的岛屿，最简单的情形--特征就是最大的岛屿，那么在运动的一段时间内视野内都只能看到某个大岛，显然这段时间的输出都是这个特征。所以卷积结果代表的是对应运动时间时刻视野区域内的特征。而多个卷积核，相当于其他卷积核--船上观察的特征不是最大的岛了而是看有多少海豚跳出水面，测量此时区域内海底最深的深度。
			----排除旋转图形的影响：
					只需要把图形变成像圆一样的处处对称的图形就可以了。
				>预知：
				     # 张量：三维张量比如一张图片的红绿蓝三个灰度矩阵，就是三维张量（可见，张量就是，形式是向量，值是矩阵的这么个怪咖）
					   本质：一个张量是一种线性变换,线性变换的表示需要基（坐标系），在不同的基（坐标系）下的表示形式是不同的，基之间有专门的转换规则。
					   预备：总是想找两个量之间的一般乘法的乘积关系：即自变量乘以另一个同类的量等于另一个同类的量即因变量，这叫变换，乘以的那个量就是变换本身（变换器）。
						 标量，两个量之间的变换是同类的量。
						 向量，两个量之间的变换也可以进行，如果变换器也是向量那么乘法是叉乘---不是一般的乘法，如果变换器是矩阵那么乘法可以是一般的乘法--并且很直观可以计算，且叉乘的本质也归结为过程是矩阵乘法，因此矩阵是变换器最好，矩阵就是线性变换,但是不同的矩阵可能达到同样的变换效果，这些矩阵是相似矩阵，因为它们的基（特征向量,坐标系,单位坐标轴向量）不一样,但它们因为同样的效果，都是代表了同一种变换（比如长度增加多少，角度旋转多少）。
						       矩阵是一种线性变换（的表示），它对不同的向量，变换效果是不同的，有的效果是伸缩---即对特征向量,有的效果是旋转（...）,如果是同一种变换效果，那么这些矩阵都有相互平行的坐标系（张成的超几何立体图形相似）（即对应的轴平行，两个坐标系可以无缝重合,仅仅是某些轴更长或者更短而已）（相互平行的特征向量组，向量组,伸缩变换向量组）。
							推论1：矩阵有一般的矩阵：每一列当作是一个向量来看，这些向量之间没有什么关系---即不全部互相垂直，有分向量互相垂直的矩阵，显然，这样的话一个和其中某一个向量平行的向量乘以这个矩阵，那么在其他向量上的分量都是0，而在这个平行的向量上的分量不为0，而这些分量值构成了新的结果向量，所以这个结果是和原来的向量平行的，而如果矩阵的各个分向量不垂直，那么之前相乘的结果就会在不垂直的向量上有投影分量，导致结果不和原来的向量平行，所以这个向量不是特征向量，反过来，如果一个向量是特征向量，如果矩阵是特征矩阵，那么特征向量除了在平行的特征向量方向的投影不为0，其他方向上的投影必须为0，所以矩阵一个分量的特征向量是矩阵除此之外的向量的法向量（法向量是肯定满足条件的）。
							推论2：垂直于n - 1个矩阵向量分量的向量肯定是特征向量。
							推论3：向量A乘以矩阵M可以这样看：先把矩阵M看作是n列向量E，而向量A乘以矩阵M就是乘以每列向量，就是向量A在每列向量上的投影乘以该列向量的模长，相当于该列向量的单位向量乘以一个值，这个值可以看作是这个单位向量表达的坐标轴上的值（有正负）。把矩阵的n列向量当作一个n维坐标系的n个轴上的向量，各列向量的模长构成的一个坐标当作这个坐标系上的一个点。一个向量乘以一个矩阵时，这个矩阵可以看作是一个坐标系加上这个坐标系上的一个点，一个坐标系和这个坐标系上的一个点（或者叫各轴初始放缩量，各轴矩阵放缩量，各轴放缩量）也可以确定一个矩阵被向量乘以时的主要特征，而乘以矩阵的向量和矩阵的各个分量在定义时所在的坐标系（参考系）是原始坐标系，矩阵的各个分向量所构成的新的坐标系是目标坐标系，求乘以矩阵的向量在矩阵的各个分向量所代表的坐标系中的坐标（向量）（在乘以初始放缩量后的坐标、向量）就是这个乘法的结果，就是向量乘以矩阵的含义。。
							推论4：向量乘以矩阵的结果的含义：一个在（用）原始坐标系中（来）表示的向量在 一个由原始坐标系表示各个分向量的矩阵  所代表的目标坐标系中的表示。所以，这可以看作是一种变换，看作向量的一种变换，看作是从旧坐标系到新坐标系的变换（新旧坐标系中的坐标变换）。
							推论5：向量的各个分量之间的关系，各个分量是对同一个事物的不同方面的刻画，度量，即是不同的量,是一种可以线性组合、线性组合有意义的量。
					   欧式空间：
					   空间：就是有一定性质和结构的集合，不是其中元素有的特征和性质，而是这个集合本身的性质和结构。
					   线性空间：全部向量的集合因为向量可以看作点，而点构成了空间，所以说全部向量的集合是空间。	
						     如何描述空间：用基本的向量，来表示其他所有的向量，这最少的有互相关系的向量就是基。因为空间中的向量是不可数的---即其中向量不能一一和实数对应。
						    ---------反过来，有基就有空间。一组基就可以表示出一个空间，这个空间中每个元素的坐标---就是这个元素用空间的基来表示时的系数。线性空间，那么基之间通过线性组合就可以表示出空间中其他所有的元素（点），系数就是元素的“坐标”。
							推论1：泰勒级数基---全部阶的系数为1的多项式可以作为一组基，通过线性组合可以表示所有的函数--任何的函数，这些系数就构成了函数用泰勒基来表示的坐标，所以全部一维变量函数构成了一个函数空间，这个函数空间的基就是泰勒基。同理：傅里叶基也是函数空间的基。当然维度可以扩展，二维函数，同样用全部组合阶多项式表示的方式，可以表示出全部的二维函数，从而形成二维函数空间，基就是多变量泰勒基。扩展到n维函数，也自然是可以的。
							推论2：某某空间，就是任意的某某，可以用一组基本的东西通过线性组合的方式来表达。这组东西就是空间的基。
					   内积空间：（可以考虑角度问题）
					   函数空间：	      
					   线性泛函：
					   最速降线问题：
					   变分法：
					   傅里叶变换：
					   对偶空间：若函数和向量一一映射，那么函数空间就是向量空间的对偶空间。
							函数和向量结合可以对应到一个值，是为函数值。函数可以和其他向量结合，向量可以和其他函数结合。结合是一种运算。函数是一种对象，向量自然是一种对象，对象即运算对象。
					   张量的本质2：多线性映射。
							什么东西能实现某某映射（谁到谁的映射）（实现方式就是和自己相乘），什么东西就是张量 V* V --> R		
					   张量空间：
						   >每一个线性映射f都是一个(0,1)张量。对偶空间中每一个元素是线性映射，对偶空间是一个(0,1)张量空间。
							（所谓矢量空间，最好的理解都是从其中任意一个矢量来看）
							  （映射当作一种运算）
						    映射f-->R是(1,0)张量，如向量，就能对求模长这种线性映射 映射到R，这里的f是线性映射。
						    线性映射V-->R是(0,1)张量，如进行求模长的运算
						    线性映射V-->V是(1,1)张量，如进行乘矩阵运算，这种表达了线性变换的线性映射。
						
						----同时，须知，标量、向量的具体值都是相对的，相对于具体的坐标系的，坐标系不同，值不同。比如在高速运动的坐标系和静止的坐标系中（相对于地球），看到的同一个物体的长度、速度是不一样的。
					  	（所以说度量都是相对于某一个坐标系的，哪个坐标系都可以，但不是都现实需要）
					   		推论#：两个向量的线性组合，就是两个向量的点乘，就是一个向量在另一个向量上的投影乘以另一个向量的模长。 
							推论  
					   张量的逆变：
					   张量的协变：	
			>卷积层：
				如果输入是图片的每个像素，而若干个像素（小方格那么多，且对应位置）进行线性组合--权值明确得到一个输出，那么类似上述，这种组合有输入像素那么多个，每种组合对应到一个中间节点，那么这些中间节点就构成的就是中间层----卷积层。
				（另外，卷积，为什么要用卷这个字,积是乘积的意思，卷就是历史的输出结果不丢弃，卷起来，叠加起来，作为输出的一部分。。就像卷积云，在末端，云不走-走不掉，不断来的云就叠加在一起，向上卷-挤开了，所以某个时刻来看，这个结果云就是一个历史结果积累的卷起来的结果）
			>卷积核：上述小方格。
				计算|训练卷积核：
			>池化：就是压缩，就是特殊的小方格（只有一个系数是1，其他为0），也特殊的运动---每次滑动不止一个像素距离而是比如小方格个距离。
			>均值化：类似上面，小方格是均值系数--0.25，也是特殊的运动---滑动小方格个距离。
			>多层卷积：类似多层神经网络，多层，每层都学习上层的特征，一层层学习，特征就会越来越高级。
			
		>深度学习：
		>卷积神经网络：
		>递归神经网络：
		>量子计算机：

 	>预知：
	     >神经元：树突为输入，轴突为输出。细胞质为计算。能计算（变换），也能存储（保持输出一段时间）















http://www.cnblogs.com/subconscious/p/5058741.html（由浅入深，由低到高）
http://blog.csdn.net/zzwu/article/details/574931/（科普，大量实例）
https://www.zhihu.com/question/22553761（知乎，神经网络对分类的作用）
http://blog.csdn.net/yunpiao123456/article/details/52437794（卷积神经网络，非常好）
http://blog.csdn.net/zouxy09/article/details/9993371（卷积神经网络的具体数学计算）
https://www.zhihu.com/question/39022858（知乎经典，资料充分）
https://www.zhihu.com/question/20695804（张量）			

