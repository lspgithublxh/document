1.问题描述：
	>条件--问题：
		   对于样本，样本有多个属性，有些为特征属性，这些值都知道--容易采集，而有的为目标属性，这个一般只有样本有。而目标属性和特征属性之间有线性关系或者其他明确的关系，但是关系中的系数是不明确的，利用样本求这些系数。
		  >神经网络（两层）对于分类的处理：
					>对于超平面，直线的分类，可以直接形象看作，超平面的参数。
					>对于曲线的分类，可以用更高维度线性组合表达式来描述，此时各个线性参数已经不是同一层次的超平面/直线的参数（因为同一层次的超平面、直线的参数远少于此时目标的超平面参数的个数），而是同一层次二项式变量组合（这组合容易想到就是任意曲线）的系数,从而以线性的方式（更多参数而已）等效表达了低层次的超平面上的任意曲线。
		  			>输入节点个数代表变量个数（输入向量的维度），中间处理层节点个数代表超平面变量的个数（也代表了超平面的系数的个数），输出节点个数代表分类曲线的个数（也代表了类别的个数）。。
					>分类方法：通过调整中间处理层各个节点输出和输出的参数和节点个数（一般是增加），可以获得任意多个不同的分类曲线----从而对各种维度的数据进行分类。
		  			>关键在于：如何调整参数（使得对所有的样本输入，输出都是符合预期的）（粒度和方向）
						  >训练：因为对于一个样本，输入值知道，希望输出值也知道，而 实际输出值也知道。
							 计算损失：差的平方和。损失最小那么参数才最优，使用最小二乘法求最小损失时的各个参数值，可以一轮一轮的进行，每轮有参数个样本，第一轮的参数是随机的，第二轮的参数是使用第一轮的参数和输入值-实际输出值-希望输出值计算出来的，下一轮的参数基于上一轮的参数-输入值-实际输出值-希望输出值而根据最小损失使用最小二乘法计算出来的，同理下一轮，...已知到后面的很多轮。
							 直至结果误差非常小为止。
							 误差的平方和的梯度：
							 -----梯度下降法也可以（沿着梯度的反方向前进-步进，比如梯度为正那么向梯度不断减小的方向步进，梯度为负则向梯度增大至0的方向前进，总之就是向0靠近（各个分量都是））----这是一种纯粹的计算机方法（完全不使用数学公式，而最小二乘法非常依赖于公式，是根据公式直接计算出目标在哪里，梯度下降法是纯计算机方式一次次逼近出来的）。
							另外:对于神经网络，因为有三个参数矩阵，需要估计三次，所以从后往前进行估计：即输出层估计整体的系数，用实际输出值和应当输出值可以估计中间计算层的每个节点应当输出值和实际输出值，，因此这里又可以构造一个误差函数，求其最值就可以得到这一层的参数系数，同理再往前推。（反向传播算法）
						  >梯度：函数f在各个方向（维度）上的分导数。
		  			>神经网络结构的描述：各层的节点的个数。
		>多层神经网络：
				即在两层神经网路后面再增加神经网络。形象化理解，可以把输入层输入的当作基本元素，第一层对基本元素进行区分，第一层的输出就是基本元素的权重组合，产生第二层的区分（即组合出了这么多种组合体），第二层的输出再次进行线性组合，产生更高级的组合，，，，一直组合到最后的结果。
				（可以用原料---一桌菜来形容，或者笔画---全部汉字来形容，或者基本关键字、语法----全部代码结构来形容，或者基本材料--各种建筑大厦来形容）
				>正则化技术：
		>java实现一个两层神经网络：
					>即用数组（一维和二维）来实现（矩阵运算当然也可以）	
		>矩阵是一种并行表示和并行计算的方式。
		>卷积是一种历史各个时刻开始的响应演变到现在的值的线性叠加（响应是一个函数，线性叠加的系数就是另一个函数的逆序各个历史值）。
			>图片上的卷积--在空间上的响应的线性叠加（非时序上的响应的线性叠加）：
				      把图片当作原函数，空间上的每个值就是一个小方格从图片最左上方顶角位置一直运动到右上方顶角...一直到图片最右下方的顶角过程中每个瞬间的（每移动一个像素位置）的小方格框定的图片区域的值（而相差一两个像素位置的小方格对应的图片内容可以看做是一样的，就像函数在相邻的两个时刻对应的函数值看做是一样的，同一个意思），而这个运动的瞬间的个数就是图片的像素的个数，把每个像素位置放一个系数的小方格感受片当作激励函数，而图片的一个区域对感受片的输出就是图片在这个方格区域内的每个像素位置上的一个值（即颜色值）和方格在这个位置的系数的乘积的线性和。所以一个图片的一个方格小区域只输出一个值（这个值可以看作是这个空序时输入值和单位激励响应值的线性组合---只是说这个单位激励响应值一直不变，不因为小方格位置的不同而不同而已）。从定义可以看出，这个输出值受到一个小方格区域的数据的影响，而这个值代表的是图片在这个空序（即小方格位置）上的响应值，且有小方格在运动过程中同时进行这个响应，并且把响应值按照小方格运动的方向位置进行放置，那么运动完成后则形成了一张和原图片同样多像素的图片，
					就像我们想看输入函数输入一段时间后，那个时刻系统输出的值（响应值，实际想看组成成分，或者说计算组成成分）一样，我们想看小方格运动到某个位置时输出的组成成分---显然这是所有历史输出的组成--已经构出了部分图片了，而不断的运动，新产生的值就不断的组合进去，----这种历史响应的组合特性正和卷积的含义相似。所以叫做图片的卷积。
			>上述是图片对小方格的卷积，小方格相当于一个固定不变的单位激励响应---与位置无关，如果与位置有关，那么这是一个随运动位置变化而变化（方格内部系数变化）的小方格,显然，如果把系数当作颜色值，小方格运动变化也是运动一个像素位置基本不改变内部系数--或者只改变部分系数，那么小方格的运动变化的参数集合就构成一张同等原图片的图片。
			 ----这就是图片对图片的卷积。
			----意义在于：输出是原图片的特征。即可以利用这个卷积来提取原图片的特征。
			>卷积层：
				如果输入是图片的每个像素，而若干个像素（小方格那么多，且对应位置）进行线性组合--权值明确得到一个输出，那么类似上述，这种组合有输入像素那么多个，每种组合对应到一个中间节点，那么这些中间节点就构成的就是中间层----卷积层。
				（另外，卷积，为什么要用卷这个字,积是乘积的意思，卷就是历史的输出结果不丢弃，卷起来，叠加起来，作为输出的一部分。。就像卷积云，在末端，云不走-走不掉，不断来的云就叠加在一起，向上卷-挤开了，所以某个时刻来看，这个结果云就是一个历史结果积累的卷起来的结果）
		>深度学习：
		>卷积神经网络：
		>递归神经网络：
		>量子计算机：

 	>预知：
	     >神经元：树突为输入，轴突为输出。细胞质为计算。能计算（变换），也能存储（保持输出一段时间）















http://www.cnblogs.com/subconscious/p/5058741.html（由浅入深，由低到高）
http://blog.csdn.net/zzwu/article/details/574931/（科普，大量实例）
https://www.zhihu.com/question/22553761（知乎，神经网络对分类的作用）

			

