1.问题描述：
	>条件--问题：
		   对于样本，样本有多个属性，有些为特征属性，这些值都知道--容易采集，而有的为目标属性，这个一般只有样本有。而目标属性和特征属性之间有线性关系或者其他明确的关系，但是关系中的系数是不明确的，利用样本求这些系数。
		 >神经网络（两层）对于分类的处理：
					>对于超平面，直线的分类，可以直接形象看作，超平面的参数。
					>对于曲线的分类，可以用更高维度线性组合表达式来描述，此时各个线性参数已经不是同一层次的超平面/直线的参数（因为同一层次的超平面、直线的参数远少于此时目标的超平面参数的个数），而是同一层次二项式变量组合（这组合容易想到就是任意曲线）的系数,从而以线性的方式（更多参数而已）等效表达了低层次的超平面上的任意曲线。
		  			>输入节点个数代表变量个数（输入向量的维度），中间处理层节点个数代表超平面变量的个数（也代表了超平面的系数的个数），输出节点个数代表分类曲线的个数（也代表了类别的个数）。。
					>分类方法：通过调整中间处理层各个节点输出和输出的参数和节点个数（一般是增加），可以获得任意多个不同的分类曲线----从而对各种维度的数据进行分类。
		  			>关键在于：如何调整参数（使得对所有的样本输入，输出都是符合预期的）（粒度和方向）
						  >训练：因为对于一个样本，输入值知道，希望输出值也知道，而 实际输出值也知道。
							 计算损失：差的平方和。损失最小那么参数才最优，使用最小二乘法求最小损失时的各个参数值，可以一轮一轮的进行，每轮有参数个样本，第一轮的参数是随机的，第二轮的参数是使用第一轮的参数和输入值-实际输出值-希望输出值计算出来的，下一轮的参数基于上一轮的参数-输入值-实际输出值-希望输出值而根据最小损失使用最小二乘法计算出来的，同理下一轮，...已知到后面的很多轮。
							 直至结果误差非常小为止。
							 误差的平方和的梯度：
							 -----梯度下降法也可以（沿着梯度的反方向前进-步进，比如梯度为正那么向梯度不断减小的方向步进，梯度为负则向梯度增大至0的方向前进，总之就是向0靠近（各个分量都是））----这是一种纯粹的计算机方法（完全不使用数学公式，而最小二乘法非常依赖于公式，是根据公式直接计算出目标在哪里，梯度下降法是纯计算机方式一次次逼近出来的）。
							另外:对于神经网络，因为有三个参数矩阵，需要估计三次，所以从后往前进行估计：即输出层估计整体的系数，用实际输出值和应当输出值可以估计中间计算层的每个节点应当输出值和实际输出值，，因此这里又可以构造一个误差函数，求其最值就可以得到这一层的参数系数，同理再往前推。（反向传播算法）
						  >梯度：函数f在各个方向（维度）上的分导数。
		  			>神经网络结构的描述：各层的节点的个数。
		>多层神经网络：
				即在两层神经网路后面再增加神经网络。形象化理解，可以把输入层输入的当作基本元素，第一层对基本元素进行区分，第一层的输出就是基本元素的权重组合，产生第二层的区分（即组合出了这么多种组合体），第二层的输出再次进行线性组合，产生更高级的组合，，，，一直组合到最后的结果。
				（可以用原料---一桌菜来形容，或者笔画---全部汉字来形容，或者基本关键字、语法----全部代码结构来形容，或者基本材料--各种建筑大厦来形容）
				>正则化技术：
		>java实现一个两层神经网络：
					>即用数组（一维和二维）来实现（矩阵运算当然也可以）	
		>矩阵是一种并行表示和并行计算的方式。
		>卷积是一种历史各个时刻开始的响应演变到现在的值的线性叠加（响应是一个函数，线性叠加的系数就是另一个函数的逆序各个历史值）。
			>图片上的卷积--在空间上的响应的线性叠加（非时序上的响应的线性叠加）：
				      把图片当作原函数，空间上的每个值就是一个小方格从图片最左上方顶角位置一直运动到右上方顶角...一直到图片最右下方的顶角过程中每个瞬间的（每移动一个像素位置）的小方格框定的图片区域的值（而相差一两个像素位置的小方格对应的图片内容可以看做是一样的，就像函数在相邻的两个时刻对应的函数值看做是一样的，同一个意思），而这个运动的瞬间的个数就是图片的像素的个数，把每个像素位置放一个系数的小方格感受片当作激励函数，而图片的一个区域对感受片的输出就是图片在这个方格区域内的每个像素位置上的一个值（即颜色值）和方格在这个位置的系数的乘积的线性和。所以一个图片的一个方格小区域只输出一个值（这个值可以看作是这个空序时输入值和单位激励响应值的线性组合---只是说这个单位激励响应值一直不变，不因为小方格位置的不同而不同而已）。从定义可以看出，这个输出值受到一个小方格区域的数据的影响，而这个值代表的是图片在这个空序（即小方格位置）上的响应值，且有小方格在运动过程中同时进行这个响应，并且把响应值按照小方格运动的方向位置进行放置，那么运动完成后则形成了一张和原图片同样多像素的图片，
					就像我们想看输入函数输入一段时间后，那个时刻系统输出的值（响应值，实际想看组成成分，或者说计算组成成分）一样，我们想看小方格运动到某个位置时输出的组成成分---显然这是所有历史输出的组成--已经构出了部分图片了，而不断的运动，新产生的值就不断的组合进去，----这种历史响应的组合特性正和卷积的含义相似。所以叫做图片的卷积。
			>上述是图片对小方格的卷积，小方格相当于一个固定不变的单位激励响应---与位置无关，如果与位置有关，那么这是一个随运动位置变化而变化（方格内部系数变化）的小方格,显然，如果把系数当作颜色值，小方格运动变化也是运动一个像素位置基本不改变内部系数--或者只改变部分系数，那么小方格的运动变化的参数集合就构成一张同等原图片的图片。
			 ----这就是图片对图片的卷积。
			----意义在于：输出是原图片的特征。即可以利用这个卷积来提取原图片的特征。
					---看图片的那些位置有某种特征，以及这种特征的强度有多大。
					---所以说，如果用不同的小方格（内部系数不同）来卷积，那么就得到原图片哪些处具有新的特征及其强度。。这样，用很多个小方格进行卷积，那么就得到图片上哪些位置上有哪些特征特征的强度有多大的结果。
			----卷积的输出：可以看作是区域运动的每一瞬间（每移动一像素）的特征的喷出（从小方格最左上方位置）。
					运动中每一瞬间（每移动一个像素）所框住的区域中的特征依次喷出来。就像在大海里航行，船上所见得到的视野区域就是小方格，航行方向就是小方格运动方向，特征就是船上能看到的岛屿，最简单的情形--特征就是最大的岛屿，那么在运动的一段时间内视野内都只能看到某个大岛，显然这段时间的输出都是这个特征。所以卷积结果代表的是对应运动时间时刻视野区域内的特征。而多个卷积核，相当于其他卷积核--船上观察的特征不是最大的岛了而是看有多少海豚跳出水面，测量此时区域内海底最深的深度。
			----排除旋转图形的影响：
					只需要把图形变成像圆一样的处处对称的图形就可以了。
				>预知：
				     # 张量：三维张量比如一张图片的红绿蓝三个灰度矩阵，就是三维张量（可见，张量就是，形式是向量，值是矩阵的这么个怪咖）
					   本质：一个张量是一种线性变换,线性变换的表示需要基（坐标系），在不同的基（坐标系）下的表示形式是不同的，基之间有专门的转换规则。
					   预备：总是想找两个量之间的一般乘法的乘积关系：即自变量乘以另一个同类的量等于另一个同类的量即因变量，这叫变换，乘以的那个量就是变换本身（变换器）。
						 标量，两个量之间的变换是同类的量。
						 向量，两个量之间的变换也可以进行，如果变换器也是向量那么乘法是叉乘---不是一般的乘法，如果变换器是矩阵那么乘法可以是一般的乘法--并且很直观可以计算，且叉乘的本质也归结为过程是矩阵乘法，因此矩阵是变换器最好，矩阵就是线性变换,但是不同的矩阵可能达到同样的变换效果，这些矩阵是相似矩阵，因为它们的基（特征向量,坐标系,单位坐标轴向量）不一样,但它们因为同样的效果，都是代表了同一种变换（比如长度增加多少，角度旋转多少）。
						       矩阵是一种线性变换（的表示），它对不同的向量，变换效果是不同的，有的效果是伸缩---即对特征向量,有的效果是旋转（...）,如果是同一种变换效果，那么这些矩阵都有相互平行的坐标系（张成的超几何立体图形相似）（即对应的轴平行，两个坐标系可以无缝重合,仅仅是某些轴更长或者更短而已）（相互平行的特征向量组，向量组,伸缩变换向量组）。
							推论1：矩阵有一般的矩阵：每一列当作是一个向量来看，这些向量之间没有什么关系---即不全部互相垂直，有分向量互相垂直的矩阵，显然，这样的话一个和其中某一个向量平行的向量乘以这个矩阵，那么在其他向量上的分量都是0，而在这个平行的向量上的分量不为0，而这些分量值构成了新的结果向量，所以这个结果是和原来的向量平行的，而如果矩阵的各个分向量不垂直，那么之前相乘的结果就会在不垂直的向量上有投影分量，导致结果不和原来的向量平行，所以这个向量不是特征向量，反过来，如果一个向量是特征向量，如果矩阵是特征矩阵，那么特征向量除了在平行的特征向量方向的投影不为0，其他方向上的投影必须为0，所以矩阵一个分量的特征向量是矩阵除此之外的向量的法向量（法向量是肯定满足条件的）。
							推论2：垂直于n - 1个矩阵向量分量的向量肯定是特征向量。
							推论3：向量A乘以矩阵M可以这样看：先把矩阵M看作是n列向量E，而向量A乘以矩阵M就是乘以每列向量，就是向量A在每列向量上的投影乘以该列向量的模长，相当于该列向量的单位向量乘以一个值，这个值可以看作是这个单位向量表达的坐标轴上的值（有正负）。把矩阵的n列向量当作一个n维坐标系的n个轴上的向量，各列向量的模长构成的一个坐标当作这个坐标系上的一个点。一个向量乘以一个矩阵时，这个矩阵可以看作是一个坐标系加上这个坐标系上的一个点，一个坐标系和这个坐标系上的一个点（或者叫各轴初始放缩量，各轴矩阵放缩量，各轴放缩量）也可以确定一个矩阵被向量乘以时的主要特征，而乘以矩阵的向量和矩阵的各个分量在定义时所在的坐标系（参考系）是原始坐标系，矩阵的各个分向量所构成的新的坐标系是目标坐标系，求乘以矩阵的向量在矩阵的各个分向量所代表的坐标系中的坐标（向量）（在乘以初始放缩量后的坐标、向量）就是这个乘法的结果，就是向量乘以矩阵的含义。。
							推论4：向量乘以矩阵的结果的含义：一个在（用）原始坐标系中（来）表示的向量在 一个由原始坐标系表示各个分向量的矩阵  所代表的目标坐标系中的表示。所以，这可以看作是一种变换，看作向量的一种变换，看作是从旧坐标系到新坐标系的变换（新旧坐标系中的坐标变换）。
							推论5：向量的各个分量之间的关系，各个分量是对同一个事物的不同方面的刻画，度量，即是不同的量,是一种可以线性组合、线性组合有意义的量。
					   欧式空间：
					   空间：就是有一定性质和结构的集合，不是其中元素有的特征和性质，而是这个集合本身的性质和结构。
					   线性空间：全部向量的集合因为向量可以看作点，而点构成了空间，所以说全部向量的集合是空间。	
						     如何描述空间：用基本的向量，来表示其他所有的向量，这最少的有互相关系的向量就是基。因为空间中的向量是不可数的---即其中向量不能一一和实数对应。
						    ---------反过来，有基就有空间。一组基就可以表示出一个空间，这个空间中每个元素的坐标---就是这个元素用空间的基来表示时的系数。线性空间，那么基之间通过线性组合就可以表示出空间中其他所有的元素（点），系数就是元素的“坐标”。
							推论1：泰勒级数基---全部阶的系数为1的多项式可以作为一组基，通过线性组合可以表示所有的函数--任何的函数，这些系数就构成了函数用泰勒基来表示的坐标，所以全部一维变量函数构成了一个函数空间，这个函数空间的基就是泰勒基。同理：傅里叶基也是函数空间的基。当然维度可以扩展，二维函数，同样用全部组合阶多项式表示的方式，可以表示出全部的二维函数，从而形成二维函数空间，基就是多变量泰勒基。扩展到n维函数，也自然是可以的。
							推论2：某某空间，就是任意的某某，可以用一组基本的东西通过线性组合的方式来表达。这组东西就是空间的基。（正交基，就是基互相垂直，乘积是0）
							推论3：空间的基的选择，有好有坏。越轻松就可以表示，越好。
							推论4：对于一个空间，想看其中的元素，不要看这个元素的具体表示形式，要看这个元素在空间中的坐标（基的系数）。比如函数空间。
								因此对于函数，其实可以用一个坐标来表示（基上的坐标）,也就是可以用一个向量来表示。
					   函数的本质：f(x)开始：	
							> f是一种运算，是一种单变量运算，x是单变量，运算f对单变量x的运算结果表达为：f(x)
							> f是一种双变量运算，x,y是两个单变量，运算f对两个变量x,y的运算结果表达为：f(x,y)。也可以用图来表示，变量表示输入，运算表示系统，结果表示输出。
							> f,g都是一种单变量运算，x是一个单变量，记f和g的某种结合(即这是一种能将两个运算融合成一个新运算的东西),使得f和g结合之后变成另一种运算h，记为（表达为）h = f  g,由于f和g都是单变量运算，所以断定h也是单变量运算（f,g对同一个变量进行运算）,它对一个单变量的运算结果表达为：h(x),而h是由f和g结合的，所以h对x的运算结果，必然是f对x的运算结果和g对x的运算结果的结合，而这种结合方式假设是乘法，即可表达为：h(x) = f(x) * g(x),等效的另一种即：(f  g)(x) = f(x) * g(x)。念作：单变量运算f和单变量运算g的结合对单变量x的运算结果等于f对x的运算结果乘以g对x的运算结果。如果f和g不是对同一个变量运算，那么表达为：(f  g)(x,y) = f(x) * g(y)
							> f,g都是双变量运算，那么照上述推理，有f和g的某种结合形成的新运算h，也是一个双变量运算，且假设这种组合出的双变量运算对两个变量的运算结果是构成它的两个双变量运算分别对着两个变量的运算结果之积，那么表达为：(f  g)(x,y) = f(x,y) * g(x,y)。同理，可以构造出更多更复杂的新运算出来。图可以清楚展现出来运算过程。
							>推论1：如果f和g都是线性映射，那么f  g就是双线性映射，是一种张量。此时，从运算角度看,f(x) 就是 x(f),,因为任意的x和f能够确定唯一的结果。
							>推论2：其实f(x)可以表示：f是x的映射（映射到实数空间）
					   内积空间：（可以考虑角度问题）
					   函数空间：	      
					   线性泛函：是一种算子。
						     定义域是向量（空间），值域是实数（空间）。
						     是一种映射。把向量映射到实数的映射。（给向量映射一个实数，在某种场景下是有描述意义的,是这个向量的度量，或者在坐标系上的度量）
						     是函数的函数。（函数的本质是映射、运算、算子）
						     是函数空间中的点（向量）.。
						     在一个空间上定义线性泛函，可以在向量空间上定义一个线性泛函，即一个向量（它自然对向量空间中的所有元素都有作用力），则所有的线性泛函构成的空间是线性泛函空间|向量空间（函数空间）,是原向量空间的对偶空间(两个空间中的元素是n对n的关系),对偶空间的基---显然就是原向量空间的基（纯坐标系角度看）。
						     本质是：向量。
					   最速降线问题：
					   变分法：
					   傅里叶变换：
					   对偶空间：若函数和向量一一映射，那么函数空间就是向量空间的对偶空间。向量的函数空间就是向量的对偶空间。
						     >推论1：如果f是x的映射（即f能把x映射到实数空间R，记作映射： x --f--> R,或者等式：f * x = R），x也是f的映射（即x也能把f映射到实数空间R），因为x和f的任意性，那么x所在的空间和f所在的空间就是对偶空间。
						     >推论2：变量空间的对偶空间是函数空间，函数空间的对偶空间是变量空间。而函数空间和向量空间都有基底，那么双方的基底之间的映射结果应该是很基础的的，记作：δ<i,j>
						     >推论3：什么让什么映射到实数空间R,第一个什么就是张量，第二个什么就是自变量（定义域）,第一个什么构成的空间就是张量空间，第二个什么构成的空间就是定义域。
							     因为矢量空间一员能让函数空间一员映射到实数空间，所以矢量空间实现了：V<*> --> R的映射，所以矢量空间是(1,0)张量。
							     同理函数空间是(0,1)张量。
						     >推论4：映射表达式和等式的关系：互相等价推导的关系。
							     例如 x--->R, f --- >R 都可以找到对应的等式：f * x = R , x * f = R， 其中等式中左边的运算符号，是一种广义的乘法，是一种映射乘法，映射乘法运算。是为了方便看出规律，否则写成f(x) = R, x(f) = R就不那么明显了。
							     而对于向量：因为矩阵可以对向量作线性变换，二变换之后通过函数空间的映射可以到实数：这个映射对应的表达式为：（AX） * f = R ,广义乘法展开为：A * X * f = R, 从而类似上述，可以看作：矩阵A实现了映射： X * f ---> R （这个比较难想象，这里的乘法表达式不能直接相乘--因为这个表达式是作为一个因子存在的，它要和左边的因子相乘，且从左到右的计算，没有交换律。可以认为2 * 3 * 4三维数组是一个2个坐标系，每个坐标系的向量的长度是4，每个坐标系有3个基底，而一个1 * 4的向量在各个坐标系上的变换结果是两个 1 * 3的向量,而这两个向量可以张成一个2 * 3的坐标系，此坐标系每个基底长度是3，共2个基底，可以对一个1 * 3的向量进行坐标变换-线性变换，变换结果是一个1 * 2的向量，看到整个过程的线性映射是把向量的长度不断降低了,从高维度映射到了低纬度空间,而一个1 * 2的函数空间的任意一个元素都能让这个向量映射到实数，即这2 * 3 * 4 维度的数组是一个 (2,1)张量）,所以认为矩阵A是(1,1)张量，矩阵空间是(1,1)张量空间。
							     而对于向量：因为三维数组就相当于与n个坐标系，被一个向量乘以，那么结果就变为一个坐标系（矩阵），矩阵是(1,1)张量，所以三维数组是(1,2)张量。
						     >推论5：n维数组，就是一个n = r + s阶的张量。
						     >推论6：函数也有坐标系，所以函数也能在坐标系之间变换，函数的坐标系（基底）所构成的产物也是矩阵（这个类似于由向量构成的矩阵，或者说向量空间的基底构成的矩阵），同理也有n个函数矩阵构成的产物（三维数组），此时这里的函数矩阵和三维数组就分别是(1,1)张量和(2,1)张量。拓展到四维空间，或者用五维空间来分析，那么可以达到的张量是(2,2)张量，(3,1)张量，(2,3)张量，(1,4)张量。
							---------映射的加法和数乘。
							函数和向量结合可以对应到一个值，是为函数值。函数可以和其他向量结合，向量可以和其他函数结合。结合是一种运算。函数是一种对象，向量自然是一种对象，对象即运算对象。
					   张量的本质2：多线性映射。
							什么东西能实现某某映射（谁到谁的映射）（实现方式就是和自己相乘），什么东西就是张量 V* V --> R	
							>张量是矢量（的基底）张成的量（矢量的基底和矢量对偶空间的基底的线性组合），所以叫张量。
							>向量的表示方法：可以用元素表示法，且不是把每个元素写出来的方式，而是用任意某个量的方式，比如v(μ),μ=1,2,...n。就可以表示一个n维向量了。
							>矩阵的基底，不是向量的基底。向量的基底，是一组基本的向量，可以表示全部的向量。同理，矩阵的基底，就是一组矩阵，可以表示所有的矩阵。根据矩阵乘法中有乘法结合律，向量也有。 V --> R 这个映射可以等效表示为：(ax1 + bx2 + cx3) ---> R，因为任意一个向量可以表示为向量空间的基底的线性组合。同理 V<*> * V ---> R 的这个映射可以等效表示为：(le1 + me2 + ne3) * (ax1 + bx2 + cx3) --> R ，可以得到的一个推论就是：坐标的表达式在展开的情形下就是任意一个矩阵的表示形式：即9张基本矩阵的线性组合，从而任意一个矩阵的基本矩阵的线性组合的系数就是它的在矩阵空间的坐标，就是它的每个元素值。
							>矢量的张量表示方法：矢量的张量基底的表示方法：	
								>预备：基本必然道理：1）一个向量空间或者函数空间由一组基底（这组基底的长度可以任意，但是每个基底长度一样，一个具体的函数空间或者向量空间必然确定了长度）,
										     2）不同空间的基底,本质上是表示用的，不是和其他空间元素进行运算用的,尽管它们可以运算，产生新空间即张量空间的基底，包括不同维度的基本空间之间也可以，因为基本空间的组合（从基底的乘积看出）的空间是矩阵空间,矩阵空间可以进一步和其他空间组合--满足矩阵乘法\任意维度数组的乘法。比如前述的2 * 3 * 4维度的数组作为一个在（2,1）张量，是V<*> * V<*> * V ---> R是映射，三个空间的基底是 1 * 4, 1 * 3, 1 * 2的向量（基底的个数分别是4,3,2个）, 三个空间的组合的结果空间的基底就是 4 * 3 * 2 维度的数组，个数有 4 * 3 * 2 个。
										     3）向量之间三种乘法：点乘、叉乘、矩阵乘。
										     4）一个坐标系可以映射为另一个坐标系，只需要有一个中间坐标系。
								>推论1：任意一个(2,1)张量的张量表示法：T<a,b;c> = T<μ,υ;σ> * e<a;μ>  e<b;υ>  e<c;σ>     其中，a,b,c是三个空间的基底的长度，μ,υ;σ是三个空间的某个基底的系数，e是基本空间基底， 是矩阵乘法。
					  		>看作：输入r个向量，输出一个数。这个机器就是张量。
							>张量对向量的运算结果的表示：u,v是向量，T是张量，它对两个向量的运算结果是T(u,v) ，且T(u,v) = u * T<;i,j> * v；；；是张量的计算
					   纤维丛：所有的(k,l)张量构成一个纤维丛。									
					   张量空间：
						   >每一个线性映射f都是一个(0,1)张量。对偶空间中每一个元素是线性映射，对偶空间是一个(0,1)张量空间。
							（所谓矢量空间，最好的理解都是从其中任意一个矢量来看）
							  （映射当作一种运算）
						    映射f-->R是(1,0)张量，如向量，就能对求模长这种线性映射 映射到R，这里的f是线性映射。
						    线性映射V-->R是(0,1)张量，如进行求模长的运算
						    线性映射V-->V是(1,1)张量，如进行乘矩阵运算，这种表达了线性变换的线性映射。
						
						----同时，须知，标量、向量的具体值都是相对的，相对于具体的坐标系的，坐标系不同，值不同。比如在高速运动的坐标系和静止的坐标系中（相对于地球），看到的同一个物体的长度、速度是不一样的。
					  	（所以说度量都是相对于某一个坐标系的，哪个坐标系都可以，但不是都现实需要）
					   		推论#：两个向量的线性组合，就是两个向量的点乘，就是一个向量在另一个向量上的投影乘以另一个向量的模长。 
							推论  
					   张量的逆变：
					   张量的协变：	
			>卷积层：
				如果输入是图片的每个像素，而若干个像素（小方格那么多，且对应位置）进行线性组合--权值明确得到一个输出，那么类似上述，这种组合有输入像素那么多个，每种组合对应到一个中间节点，那么这些中间节点就构成的就是中间层----卷积层。
				（另外，卷积，为什么要用卷这个字,积是乘积的意思，卷就是历史的输出结果不丢弃，卷起来，叠加起来，作为输出的一部分。。就像卷积云，在末端，云不走-走不掉，不断来的云就叠加在一起，向上卷-挤开了，所以某个时刻来看，这个结果云就是一个历史结果积累的卷起来的结果）
			>卷积核：上述小方格。
				计算|训练卷积核：
			>池化：就是压缩，就是特殊的小方格（只有一个系数是1，其他为0），也特殊的运动---每次滑动不止一个像素距离而是比如小方格个距离。
			>均值化：类似上面，小方格是均值系数--0.25，也是特殊的运动---滑动小方格个距离。
			>多层卷积：类似多层神经网络，多层，每层都学习上层的特征，一层层学习，特征就会越来越高级。
			
		>深度学习：
		>卷积神经网络：		
			     >预知：生成和提取
				# 生成是利用已经发现的（提取出来的）规则和逻辑（程序，流程）构造出新东西出来。
				# 提取是直接对现实中的原生事物、活动、现象提取特征、规律、和关系，并进一步分类（层层分类，层层抽象，类型越来越少，直到分到一类上去）、预测模拟（规律,从而可以互相确定,也是关系，只是表面的关系）、计算（关系，函数关系，逻辑关系）。
			     >问题描述：
				>对于分类问题，两类：一是要分类的对象之间没有关系，输入一个，输出一个分类，两个输入之间没有关系。
						     二是要分类的对象之间有关系，即上一时刻的隐状态的值会作为下一时刻的隐藏层的隐状态值计算的输入分量---另一分量自然是外部输入。如果把输入表示为x(t),输出表示为o(t),隐状态值为h(t),上一刻隐状态值为h(t - 1)，输入、上一刻隐状态、隐状态的权重分别是U,W,V。那么输入和输出的关系式：
							1) h(t) = f(U * x(t) + W * h(t - 1))
							2) o(t) = g(V * h(t))
							问题在于U,V，W的估算。
							
		>递归神经网络：
		>量子计算机：

 	>预知：
	     >神经元：树突为输入，轴突为输出。细胞质为计算。能计算（变换），也能存储（保持输出一段时间）















http://www.cnblogs.com/subconscious/p/5058741.html（由浅入深，由低到高）
http://blog.csdn.net/zzwu/article/details/574931/（科普，大量实例）
https://www.zhihu.com/question/22553761（知乎，神经网络对分类的作用）
http://blog.csdn.net/yunpiao123456/article/details/52437794（卷积神经网络，非常好）
http://blog.csdn.net/zouxy09/article/details/9993371（卷积神经网络的具体数学计算）
https://www.zhihu.com/question/39022858（知乎经典，资料充分）
https://www.zhihu.com/question/20695804（张量）			
http://www.csdn.net/article/2015-08-28/2825569（递归神经网络）
http://blog.csdn.net/aws3217150/article/details/50768453（递归神经网络）