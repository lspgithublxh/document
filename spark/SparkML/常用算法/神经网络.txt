1.问题描述：
	>条件--问题：
		   对于样本，样本有多个属性，有些为特征属性，这些值都知道--容易采集，而有的为目标属性，这个一般只有样本有。而目标属性和特征属性之间有线性关系或者其他明确的关系，但是关系中的系数是不明确的，利用样本求这些系数。
		  >神经网络对于分类的处理：
					>对于超平面，直线的分类，可以直接形象看作，超平面的参数。
					>对于曲线的分类，可以用更高维度线性组合表达式来描述，此时各个线性参数已经不是同一层次的超平面/直线的参数（因为同一层次的超平面、直线的参数远少于此时目标的超平面参数的个数），而是同一层次二项式变量组合（这组合容易想到就是任意曲线）的系数,从而以线性的方式（更多参数而已）等效表达了低层次的超平面上的任意曲线。
		  			>输入节点个数代表变量个数（输入向量的维度），中间处理层节点个数代表超平面变量的个数（也代表了超平面的系数的个数），输出节点个数代表分类曲线的个数（也代表了类别的个数）。。
					>分类方法：通过调整中间处理层各个节点输出和输出的参数和节点个数（一般是增加），可以获得任意多个不同的分类曲线----从而对各种维度的数据进行分类。
		  			>关键在于：如何调整参数（使得对所有的样本输入，输出都是符合预期的）（粒度和方向）
						  >训练：因为对于一个样本，输入值知道，希望输出值也知道，而 实际输出值也知道。
							 计算损失：差的平方和。损失最小那么参数才最优，使用最小二乘法求最小损失时的各个参数值，可以一轮一轮的进行，每轮有参数个样本，第一轮的参数是随机的，第二轮的参数是使用第一轮的参数和输入值-实际输出值-希望输出值计算出来的，下一轮的参数基于上一轮的参数-输入值-实际输出值-希望输出值而根据最小损失使用最小二乘法计算出来的，同理下一轮，...已知到后面的很多轮。
							 直至结果误差非常小为止。
							 -----梯度下降法也可以（沿着梯度的反方向前进-步进，比如梯度为正那么向梯度不断减小的方向步进，梯度为负则向梯度增大至0的方向前进，总之就是向0靠近（各个分量都是））----这是一种纯粹的计算机方法（完全不使用数学公式，而最小二乘法非常依赖于公式，是根据公式直接计算出目标在哪里，梯度下降法是纯计算机方式一次次逼近出来的）。
							另外:对于神经网络，因为有三个参数矩阵，需要估计三次，所以从后往前进行估计。（反向传播算法）
						  >梯度：函数f在各个方向（维度）上的分导数。
		  			>神经网络结构的描述：各层的节点的个数。
 	>预知：
	     >神经元：树突为输入，轴突为输出。细胞质为计算。能计算（变换），也能存储（保持输出一段时间）















http://www.cnblogs.com/subconscious/p/5058741.html（由浅入深，由低到高）
http://blog.csdn.net/zzwu/article/details/574931/（科普）
https://www.leiphone.com/news/201505/t3T1XQy2g3spCUdd.html（专家：spark）
https://www.zhihu.com/question/22553761（知乎，神经网络对分类的作用）

			

