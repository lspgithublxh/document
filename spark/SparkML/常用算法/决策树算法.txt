1.决策树算法：
  
  》条件-问题表述：
    >前提：就像集合套集合一样的分类关系，或者流程、多叉树从根走到叶子节点。根据条件从根构造到所有叶子结点。
    >对象有多个属性，每个属性有对应的值，
    >要求可以用多个属性参与的逻辑关系式来表达：  A&B&C&D&E|F  。。。其中同级运算有先后。
     要求也可以用文字语言：什么的什么的什么的什么的什么的什么的...来描述。
    >问题：从一组对象中找出各个属性值符合要求的一小组对象
	构造一个树，命名为决策树。   

  》必须认识到的本质、性质、关系、规律：
   >可逆过程：平衡状态（过程的每个极短步骤内，系统保持平衡状态--（没有形成变化梯度，变化加速度为0，就不需要外部作用-也就没有从外部吸收任何能量-外界没有对系统做功，也没有从系统吸收能量。。无耗散无吸收））
   >微观状态：每个原子由位置和动量决定。
   >玻尔兹曼公式：当每种微观状态出现概率相等时，即每种组合出现的概率相等，熵的定义就可以用微观状态总数来决定，具体运算就是玻尔兹曼公式。
                  而系统在元素分布最均匀的时候（即最多的组，每组元素个数都最均匀），微观状态是最多的，熵就最大。（同时，认为微观状态越多，即取值情况越多，系统越混乱：不确定性越大。。这是显然的，可能状态的数目少了，具体状态的确定性就大了）
                  ---均匀分布是系统的一种取值类别。这种类别的特征就是其中所有的取值情况的取值概率都是一样的。
		  ---理想气体只有一种取值类别。只是气体的类别并不是一开始就是理想气体，但是一个初始类别不是理想气体的气体会变为类别为理想气体的气体，因为理想气体的熵最大--或者说因为属于理想气体类别的气体情况可能数最多）（同理掷骰子，掷硬币,最终看到熵最大的那种类别的情形最有可能）。
			（可以看做趋于稳定，趋于均衡状态，趋于常态）（每种情况出现的概率相同，都是总情况数分之一）
                  ---任意气体（任意类别的气体）的熵如何计算，这时用香农定理可以计算，信息熵来表达。热熵。
   >排列：
	  0)方法：分解（分解为子部分，相乘;先排一部分，再排另一部分元素）；分类（整体分解为不同情形，相加；先排一类情形，再排另一类情形）；根本的一个位置一个位置从左到右确定--第一排为位置&第二排为树状表示的可能性（通法）；转化为标准形式的求排列模型Ank（可以使用集合方法，四则运算）；
                  基于映射种数的等效转化（第一种等效，不同元素对不同位置的映射，和不同位置对不同元素的映射是一样多的，等效的，因为映射是相互的）（第二，映射双方所在组，分别进行内部组合，后和对方组合进行一一映射，本质上即因为：双方互相一个一个的组合出一种情况又一种情况，等效于双方内部先自行组合出一种又一种情况，然后双方的各种情况进行一一映射，映射出的全部情况中去除相同的情况（对元素来说相同），得到的最终情况就是全部可能的情况，即两种方式组合的情况种数是一样多的，后面这种计算方式简便很多，清晰很多。可xx）；
          1)最基本、最简单的：第一类：n个不同的元素，选出k个进行排列。有Ank种。（后面的所有排列，应当转换为这种方式表示，或者说转化为这种排列），
          		      第二类：有相同的元素：甚至有好几组，排列的结果，从数字上看，可以是多个Ank的加减乘除构成的表达式
          2)经典复杂案例：##1 10个元素，5个1,5个0，求取出它们10进行排列，总共有的排列可能数（用映射方式的第二种等效转化方法，确定是C<10,5>）
			  ##2 同上，如果5个1，剩下的是3,4,5,6,7五个不同的数，那么总共的排列数（仍用映射方式第二种等效转化方法，确定是C<10,5> * A<5,5>）
                          ##3 同上，5个1,2个0，剩下3个不同的数：2,3,4,那么总共有的排列数（同上方法，有C<10,5> * C<5,2> * A<3,3>）.如果剩下的三个数是相同的另一个数如2，那么共有排列数：C<10,5> * C<5,2>
			      ----可以导出这种有若干组相同元素的排列问题的通项公式：。
			      当上述问题退化为n个组，每个组只有一个相同元素，结果C<5,1> * C<4,1>* C<3,1>* C<2,1>* C<1,1> = A<5,5>,转化为了最基本的排列方式了。所以排列可以用组合的方式计算表达出来.
			      ----结论1：分组越多，排列种类越多。
			      ----结论2：如果分组数一定，那么每类的元素个数越均匀，排列种类越多。（比如两个组，C<10,5>是C<10,k>（0<k<11）中最大的）
			                推论：排列数越多，那么对应系统状态出现的概率就越大。即系统在不断变化的话，那么出现这种状态的概率是最大的。这种状态就是其中元素的排列数是最多的：每种排列是一种情况.
                                             如何排列数最多，从数学上看，就是因子的值互相接近，意思即是分组中每个分组都要有，且一个一个的取，很平均的出元素，直到出完为止，这样得到的系统状态的元素排列数是最多的，即系统出现这种状态是最可能的。
			      ----结论3：
          3）具体思想：把相同的元素聚集到一块。
          4）排列不能算交换，排列数就是所有可能情况数。交换算同一种元素。为什么不能算交换？---因为这种交换前后，出现的排列是同一个排列（按照这个属性值）。。。。。所以说，排列，是按照属性值的排列。
              --既然排列是按照属性值的排列，所以说10个硬币的正反面值的所有可能排列是2的10次方，而不是A<10,10>。也说明排列一定要阐明排列的属性值--即依据值。排列是按照属性值的排列。
             
   >组合：
	0)经典问题：分组问题：10个相同元素，最多分成3组,且每组前后有序，求所有可能的分组数。（ps:当10个元素不同时，分组可能没有简便计算方法，比较复杂，没有什么规律，所以没有什么讨论价值。关键结果也没有意义？）
		   >分析：元素之间补充上空格，可以把分组形象化，看到所有具体分组的统一规律（或者分类下来有几种规律）,分为3组则等效转化为9个空格位置上选出2个的所有组合数：C<9,2>,分为2组则等效转化为9个空格位置上选出一个来放分割符的所有位置数C<9,1>，分为1组共有1个。共有C<9,2> + C<9,1> + 1  = 46种。（1或者是C<9,0>）
		   >推论：n个元素，分成k组，每组前后有序，共有C<n-1, k-1>种分法。最多分成k组，其他条件不变，则是连加表达式了。∑C<n-1,j>  0<= j < k
		   >推论2：上述问题等价于：苹果和梨和西瓜若干个，取三种水果组合，要求总共n个水果，求有几种组合方法，此时不同水果之间没有顺序。
                   >拓展1：10个相同元素，分为3组，每组前后无序，求分组数。等价于：将整数10分成3个可相等的正整数相加，有几种分法：
			   分析：假设有序，按照有序算，则对于无序则有重复，对于有两个元素相等的情况，重复了3次，对于三个元素都不相同的情况，重复了A<3,3>次。而其中两个元素相等，2n + k = 10 ,那么k能取4个值，所以有4个组合重复了3次，共占了3 * 4次，而有序的组合有C<9,2> = 36种，(36 - 3 * 4) / A<3,3> = 4次，所以共有无序组合4 + 4 = 8 种。
			   利用：无序可以分类情况，而一类情况中的每个具体情况发生概率（概率是一个具体情况相对于所有类的所有情况的比例）是一样的--所以这类情况的熵容易计算。
				 （求整数分割就是在求分类，求分类就可以简化整体的熵的计算）
   >概率论：
          0）定义概率和计算概率要明确的认识：
	      ##1 概率就是系统未来出现某一种情况的可能性，称系统出现该种情况的概率
	      ##4 该种情况的准确描述：使得其他情况也是明确可以知道的。
	      ##3 概率性事件：随机事件，根据目前有的不充分信息（包括事件发生的必要因素）来推算、来关联出全部可能的情况的比例，有说服力的比例，有根有据的比例。
	      ##2 计算概率：概率的影响因素：系统所有可能的情况总类数、每类情况的现实决定因素（决定条件）
			    概率的计算方法：可以直接实验；根据影响因素进行比例计算。按照1 * 1 * 1...分解：(0.3 + 0.5 + 0.2 ) * (0.3 + 0.1 + 0.6) * ...
              ##5 概率和总数的关系：
		   总数分之一就是平均情况下每种情况的概率。
	  1) 某种排列的总数和这种排列的概率，乘积就是这种排列出现的概率：
		>用0.6 + 0.4正 + 反概率的硬币两枚，出现一正一反的排列数是2，每个这种排列的概率是0.6*0.6 = 0.36，所以这种排列的总出现概率就是 0. 36 * 2 = 0.72
                >同理，前述的10枚硬币，0.6 + 0.4 正反概率，这种5上5下的概率就是：0.6 ^ 5 * 0.4 ^ 5 * C<10,5> = 0.02（概率不如全部朝上的0.047,6个朝上的0.25,7个朝上0.21）；特别的，概率是0.5 + 0.5时，5上5下的概率是：0.5^5 * 0.5^5 * C<10,5> = 0.253
	    
   >熵：熵出现的原因：实际中有想比较或者区分两件事情的结果的不确定程度谁大谁小？其中每件事情的结果都有多种可能，且每种可能都有一定的比例，或大或小，但两件事情的结果可能数不一样、每种可能的概率也分别不一样。
		      此时就定义一件事情的结果的不确定程度的量度：熵。
		      熵可以看作是对象的一件事情的结果的可以为小数的可能情况总数。
   >信息熵：是对概率空间中所有样本的不确定度的度量，是样本分布情况的度量（越均匀，排列数越多，不确定度越大。。即不确定哪些属于什么类），分组数就是微观状态数，数据集的一个分组就是理想气体的一个微观状态。数据集在A度量下的考察分组，好比气体在理想状态下的考察微观状态。
	    一个数据集有信息熵。
            >因为一个数据集有原始信息熵，而且在A度量下，还对数据集进行了分组，每个分组也是一个数据集--也分别有信息熵，利用这些分组的信息熵可以求出A度量下的信息熵。
   	    >###系统所有可能的取值类别（每个元素，相应取值，；每种取值类别下，都有若干个取值情况，每种取值情况就是所有元素的确切取值的集合）和对应的类别概率
            >###系统的两种取值情况是一样的：，认为是等效的，则者两种取值情况认为是同一种类别。
 		 取值情况一样的判定：认为系统中的元素是不同的，但是当两种系统取值情况的差别仅仅是情况中有两个元素交换了量度值（比如，两个原子交换了位置，两个骰子交换了位置，两个硬币交换了位置，进行了不同的排列），那么认为系统的这两种取值情况是同一个取值类别，或者说属于同一个取值类别。
	    >取值概率（取值比例）、取值情况（取值类别数，和类别情况数）和信息熵之间的关系。
              @@如何描述不同的类别，确定不同的类别，找到所有的类别，把什么认为是类别？类别与熵。类别和信息熵，	
	      @@分类是人为进行的，人为专门考察而指定的，一个类里面包含了若干个情况，
	      @@熵是一个类的熵，即一个类也有这个类的熵。而一个对象集合有很多种类情况，一个情况类包含着一些情况（同类情况）(那么就有相对这类情况，某种情况出现的比例概率。用这种相对类的概率算出的熵就是这个类的概率)。
			推论：只要某件事情会概率性的比例性的出现各种情况，那么这件事情就有熵（就有信息熵）。换种表达方式，对象，对象在某种事情上的结果是不确定的概率性的比例性的多种结果（多种情形，多种具体情形，多种情况），那么说这个对象在这种事情上的结果上有信息熵这个量度，就是这个结果的不确定度。
				（熵是什么熵？熵是对象的事情的结果的信息熵。信息不确定度，不确定程度（即结果有多种可能，而每种可能都有一定的概率，概率有大有小。。如何比较、区分两件事情的结果的不确定程度谁大谁小？）。）
				进一步推论：---实际上表明，结果的可能数越多，每种可能的概率越互相接近即越平均，那么不确定程度就越大，即结果的不确定程度只与可能情形的数目和每种情形的概率相关。加一些条件因素，可以推知结果的不确定程度的量度定义成熵的计算形式--香农形式是很好的--很一般的--很有代表性，与事实符合的很好。
				             上一句话提到的“其他一些因素”，就是现实实际上，不确定程度具有可加性，即一个整体结果的不确定度是其每个分类结果各自的不确定度的和（f(x,y) = f(x) + f(y)即等式两边要保持同一种形式。那么f一定是一个对数函数。且左边的逗号运算就是一个乘法运算），如果任意一个分类结果内部的每种情况的概率都相等--那么每个分类结果的熵是一个特殊的形式，而且自然而然每个熵相加出来的值，和独立整体不分类来计算这个整体的熵的值，是相等的（这也说明香农信息熵有可加性：这是在特殊情况下的证明）。

			运用：##1 掷10枚硬币，5上5下这类情况（这种事情，这件事情）的结果的不确定度（即到底是那种排列的5上5下）即结果的熵是：In252
                              ##2 A班和B班比赛这件事情的结果（不确定，但知有两种，A胜或者B胜，但到底是那种不确定）（A班胜的概率是50%）的不确定度即结果的熵是：In2
                    
                              推论三：概率是描述事件发生的比例的，熵是描述事情的结果的可能数的（不够准确--只是形象和正相关，最好混乱数，不确定程度,只是这样不形象）。
				
	    >样本集合，样本的某个属性的值域，值域中各个值的取值概率（比例），
	     结论1：多个样本的某个属性的某个值的取值概率确定了，那么这个样本集合的这个属性的信息熵也确定了（即实际情况出现的比例）。
            >已知样本集合，求下一个样本集合出现某种情况的信息熵：
			##1由样本集合，可以计算域值中每个值出现的概率（对于离散的属性）；可以认为是下一个样本集合中各个值出现的概率，从而求某种结果（如k个value1,k1个value2,k2个value3,...可以知道该类情况的实际概率--每种情况的概率乘以情况总数）信息熵--可以算的还是只与总情况数和每种情况的概率相关；同理其他种结果的信息熵也可以算；每种结果的信息熵之和就是全部结果的信息熵（从10硬币抛掷5上5下，即其他所有的情况，加起来的信息熵刚好是ln1024,而5上5下的信息熵是0.252ln1024）
			   当然可以直接根据全部结果的每个情况的概率来算全部结果的信息熵，且这是正确的，只是实际算时发现其中会出现一组又一组的相同的信息熵，而每一组相同的信息熵对应相同的情况概率而对应的情况刚好是属于同一类情况即同一种结果，且每组的信息熵之和正好是每组即每类结果的信息熵--显然直接找到所有类情况后专门算这些类情况的信息熵，就可以更快地计算出全部结果的信息熵了。（这也是为什么要算每类结果的信息熵的原因）
			（样本的价值：在于提供了值域中每个值的取值概率，当然，一般样本数少，值域未定，值域中也不一定每个值都取到。。但是足以估计下一组样本集合的信息熵了，无论集合的样本有多少，可多可少。。且一轮轮样本的输入，而一次次优化估计样本集合的信息熵（因为每种情况优化了概率））
	             >例子：比如10个样本，每个样本有一个属性，该属性的可能值是3个中的一个。
			    已经知道了这10个样本的该属性的具体值，求下10个样本的该属性的熵（信息熵）。	首先，三个值出现的概率可以计算出来了。	
			    从前面组合里的拓展分析，可知10个数，每个数可能是三个值中的一个，如果三个值至少有1个，那么总共有8种组合，而每种组合可以确定该组合的情况总数和每个情况的发生概率，从而每种组合的熵可以计算出来，从而整体的熵可以计算出来了。
			    如果三个值刚好出现两个，C<3,2> * （10 / 2） = 15 类，同样可以计算每类的情况总数和每个情况出现的概率，从而计算熵。同样，对于只有一个值的，3类情况，每个情况出现的概率也定了，熵也容易计算。
			    综合上述3种情形，三熵相加，可以知道下10个样本的熵（信息熵）。
  》分析：
	>必须要明确的事情：决策树要解决的问题是什么？
			   >对对象的某个量度的值的判断，已知影响该量度的若干个因素|变量|量度，以及若干组的各个因素具体取相应值时目标量度的值。
		           已知若干个“条件-结论”对（样本），求新来一个条件（样本），判断它的结论是什么？（或者说“问题-答案”对“条件-判读”）。（非常像数值拟合，已知若干点，求拟合函数。）
			   ##1即本质就是找到获取“条件”到认定结论的过程中的全部判断逻辑、完整判断流程。。。（即默认认为，根据条件推测出结论是一个判断流程）
			   ##1前提|假设1：根据条件推测出结论是一个判断流程。（即便是人来思考，也是在头脑中默认按这个流程走下来而得到结论的）
			      假设2：根据若干组条件-结论对，可以构造出这个判断流程。并且根据条件-结论对的增加而可以不断优化这个判断流程（其中判断条件、判断准则、判断根据、判断的量化指标）
			      假定3：影响该量度的所有因素已知。所有因素在各自的值域里任意的取值构成的组合，有若干个，每个组合对应的该目标量度的值也确定，并且目标量度的值是有限的，而不是每个组合都有唯一对应的目标量度的值--这样就没有分类的价值，即必须是多个组合对应同一个目标量度值。
			      假设4：由多个因素组合对应一个目标量度值，所以因素组合要分类，且即便对应了同一个目标量度值的若干个组合，也要建立分类，因为只有尽快根据样本建立了分类集合及其分类标准--根据各个因素的值而逐步推进最后判断出属于哪一个分类，而这个分类中已经有样本了，这个样本组合对应的目标量度值，也就是下一个到达这个分类的样本对应的该量度的值。
			      假设5: 将组合分类的思路之一：每个因素的取值的内部分布先内部分类，然后按照分类数从多到少，将分类数最多的属性作为待构造流程的起始判断即起始分类（因为不确定性最大，从而流程的深度更小的概率更大--当然可能更多冗余的判断：即某些因素在某些取值下可以直接得出目标属性值而不再参照其他因素），在每个分类下接着下一个次很不确定的因素作为进一步分类（进一步判断）的属性，
				     这样一直分类下去直到可以判断某个分类下的组合都对应到目标属性的某个值（有这种必然对应）,就不用再分类了，分到这里的组合就假设都是对应该目标属性的值了。
				     这样，所有组合（样本）经过最多属性次分组，至少在样本范围内，就会分到对应的深度类|宽度类下了：
			      进一步要解决的问题：如何判断哪个属性的不确定度|可能情况数更高，即作一个排序？ ---把该属性的值当做结果，求该结果的信息熵。
						  每个属性的值如何分类？---按照之前的k-means方法或者自己的那种遍历方式，最差就是每个值都作为一类（实际上可以合并很多分支：因为下面的分支都是一样的分类）。 在分类之前，要看这个分支上的所有样本对应的目标属性的值，是否已经相同（如果不想构造太多分支，又已经相同,那么此时可以不对这个分支进行分类了）。（假定：每个分支点有：属于该分支点的组合列表，该分支的子分支列表）
					          判断分类结束和分支发展即某分支节点再分化出新的分支（叶子节点和非叶子节点都可以）（甚至重构）?  -- 属性到最后一个属性必然结束了，但是按照某分支下所有组合都对应一个目标属性的值，就可以直接结束了。
											  一轮样本，初步构建了一个判断流程（树）（分类树，不循环分类树），如果再来一轮样本，当到某个非叶子节点后，发现某种分类下还没有元素，所以构造这个分支（也可以在开始就构造了这个分支，到这时直接就加进去了，但是这样就产生了大量的空分支），如果这是叶子节点，发现本样本的目标属性值不同于里面已有样本的目标属性值，那么
											  自然加新分支，将该样本放入该新分支中。。
                                                  根据这个算法，可以输入一个样本，一个样本的自然而然一点一点一个节点一个节点一层一层构造出了一颗判断树-决策树，到最后决策树基本不在变化。
			      最保险的方法：穷值构造树：甚至使得每个独立的样本都有独立的一个叶子节点。毕竟因为完全不知道规律，不知道值域，不知道哪些值域范围内一定对应到目标属性的某个值。
					    
			   ##2 认为1：
			

	>I: 一件事情的结果，没有决策树，新样本的信息熵是所有分类的信息熵的和，有信息熵，新样本的信息熵是决策树叶子节点的信息熵的平均值。
	>多个属性的联合信息熵：
	   一种新的信息熵：一种结果，它与多个因素属性的取值相关，求这种结果的信息熵，各个子结果的信息熵（也可以单个属性的信息熵，然后是属性联合的信息熵），什么结果？就是答案、目标属性的值。
	  ##1 属性的每类值的取到的概率，和 该类值下对应到的目标属性的熵（就是该类值下的样本中有几种目标属性的值，如果只有一种，那么目标属性的信息熵是ln1,两种，则是ln2,依次类推---当然这个熵与目标属性的总值数相关但是实际上还是看到这里的样本对应到几个不同的目标属性的值，x个则信息熵是lnx，非常好算），，，，
	      到叶子节点时，由于这里的样本对应的目标属性值只有一个，所以它的信息熵为ln1 = 0，如果对应目标属性值有多个，那么信息熵不是0了，就要分类了---按照下一个因素属性。
	      @@某个值（某类值）的取值概率（相对其他值）和取值熵（对应到的目标属性的结果值）
		取值概率的计算：一个属性的各个值的取值概率，对于一个样本集合，就计算各个值的出现概率。（因素属性也有熵，即样本的这个属性出现的结果的熵，但是这个熵对于构造决策树没有用。只有值出现的概率有用---而用这些概率计算的熵对构造树没意义）
	        取值熵：一个属性的取的某个值，在样本集合中，应该出现了多次，即有多个样本都取的那个值，而每个样本都对应着目标属性的某个值，那么多个样本可能对应到目标属性的值也有多种，正好计算此时目标属性的熵（即看各个值出现的概率，来计算出熵）。。
		属性的信息熵（而不是看属性的取值熵）：首先，属性的取值熵，可以用属性的各个值的熵相加而得，但是这样没体现属性 的各个值的取值概率，产生的问题是，如果一个值概率很大，一个很小，而对应目标属性的熵可以都是0，ln2等，这样就看不出真实的混乱情况-不确定情况。所以需要定义一个新的量度，
						    这个量度要把取值概率和取值熵联系起来，乘积是好的，所以在二者相乘为因子上求全部值的这个和，就是取值熵的加权平均值，作为属性的信息熵。
		系统的信息熵：样本集合定了，目标属性的取值也定了，那么目标属性的各个取值概率就定了，从而可以计算出信息熵。
		信息增益：上一层的信息熵 - 本层的信息熵 = 信息增益值。
		推论1：树的判断起点属性：属性信息熵最小的那个属性。
		推论2：构造分支的方法：按照取值熵进行，值熵为0的不再构造分支。可能出现最好的情况是：在属性个深度内，各个叶子结点尤其最深层的叶子节点的信息熵为0，为纯的；最坏的情况就是：深度达到了属性数目个，而最深层的叶子节点还是不纯的（即多个样本在这里对应到的目标属性值是多个）。
	        推论3：将多个无关因素-目标属性的取值集合作为样本，那么也能构造出一个决策树，但是这个树基本只对样本有效，对于新的样本其他样本，则基本错误，因为这个树就是在猜，和猜的概率其实是一样的。
			如果既有相关因素又有无关因素来作为样本的参照属性|关联属性，那么这个树也有一定的猜的时刻。。所以在选取属性时，一定要判断是相关因素。。（不能是50个股票分析师的案例）
		推论4：条件要求：@@不能是噪声式样本，即取值和实际有相当大的误差。@@不能用无关因素作为相关属性带进决策树，即不能考虑去看这个属性的取值。
				@@不能全部是一种样本，分布不均匀，过于集中，没有代表性。
		剪枝：（推论5）：这个是继承推论4的观点，但不是条件要求，而是树的要求：即不能是一个不好的树，比如取值概率很小的一个节点，它这里的样本很少，但是样本的目标属性信息熵不是0，按常理这里应该也作为剩下因素属性的分支起点，但是发展的及其缓慢，或者里面的样本实在太少。所以建议减掉这个取值概率极低的树头，而改成一个叶子节点就可以了。
				 或者上层节点直接将本层取值概率最大的那个取值作为唯一一个节点，且是叶子节点。
			
	>贪心算法：
	>随机森林：
		   
  》结论：


---也是筛选，也是分类

参考资料：http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html
http://blog.csdn.net/nieson2012/article/details/51314873（详细）
http://www.cnblogs.com/ShaneZhang/p/3970176.html（基本概念）
http://www.cnblogs.com/bourneli/archive/2013/03/15/2961568.html(简单生动)
https://sanwen8.cn/p/1ebzUR3.html（简单，且构造了决策树）