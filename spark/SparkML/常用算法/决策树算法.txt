1.决策树算法：
  
  》条件-问题表述：
    >前提：就像集合套集合一样的分类关系，或者流程、多叉树从根走到叶子节点。根据条件从根构造到所有叶子结点。
    >对象有多个属性，每个属性有对应的值，
    >要求可以用多个属性参与的逻辑关系式来表达：  A&B&C&D&E|F  。。。其中同级运算有先后。
     要求也可以用文字语言：什么的什么的什么的什么的什么的什么的...来描述。
    >问题：从一组对象中找出各个属性值符合要求的一小组对象
	构造一个树，命名为决策树。   

  》必须认识到的本质、性质、关系、规律：
   >可逆过程：平衡状态（过程的每个极短步骤内，系统保持平衡状态--（没有形成变化梯度，变化加速度为0，就不需要外部作用-也就没有从外部吸收任何能量-外界没有对系统做功，也没有从系统吸收能量。。无耗散无吸收））
   >微观状态：每个原子由位置和动量决定。
   >玻尔兹曼公式：当每种微观状态出现概率相等时，即每种组合出现的概率相等，熵的定义就可以用微观状态总数来决定，具体运算就是玻尔兹曼公式。
                  而系统在元素分布最均匀的时候（即最多的组，每组元素个数都最均匀），微观状态是最多的，熵就最大。（同时，认为微观状态越多，即取值情况越多，系统越混乱：不确定性越大。。这是显然的，可能状态的数目少了，具体状态的确定性就大了）
                  ---均匀分布是系统的一种取值类别。这种类别的特征就是其中所有的取值情况的取值概率都是一样的。
		  ---理想气体只有一种取值类别。只是气体的类别并不是一开始就是理想气体，但是一个初始类别不是理想气体的气体会变为类别为理想气体的气体，因为理想气体的熵最大--或者说因为属于理想气体类别的气体情况可能数最多）（同理掷骰子，掷硬币,最终看到熵最大的那种类别的情形最有可能）。
			（可以看做趋于稳定，趋于均衡状态，趋于常态）（每种情况出现的概率相同，都是总情况数分之一）
                  ---任意气体（任意类别的气体）的熵如何计算，这时用香农定理可以计算，信息熵来表达。热熵。
   >排列：
	  0)方法：分解（分解为子部分，相乘;先排一部分，再排另一部分元素）；分类（整体分解为不同情形，相加；先排一类情形，再排另一类情形）；根本的一个位置一个位置从左到右确定--第一排为位置&第二排为树状表示的可能性（通法）；转化为标准形式的求排列模型Ank（可以使用集合方法，四则运算）；
                  基于映射种数的等效转化（第一种等效，不同元素对不同位置的映射，和不同位置对不同元素的映射是一样多的，等效的，因为映射是相互的）（第二，映射双方所在组，分别进行内部组合，后和对方组合进行一一映射，本质上即因为：双方互相一个一个的组合出一种情况又一种情况，等效于双方内部先自行组合出一种又一种情况，然后双方的各种情况进行一一映射，映射出的全部情况中去除相同的情况（对元素来说相同），得到的最终情况就是全部可能的情况，即两种方式组合的情况种数是一样多的，后面这种计算方式简便很多，清晰很多。可xx）；
          1)最基本、最简单的：第一类：n个不同的元素，选出k个进行排列。有Ank种。（后面的所有排列，应当转换为这种方式表示，或者说转化为这种排列），
          		      第二类：有相同的元素：甚至有好几组，排列的结果，从数字上看，可以是多个Ank的加减乘除构成的表达式
          2)经典复杂案例：##1 10个元素，5个1,5个0，求取出它们10进行排列，总共有的排列可能数（用映射方式的第二种等效转化方法，确定是C<10,5>）
			  ##2 同上，如果5个1，剩下的是3,4,5,6,7五个不同的数，那么总共的排列数（仍用映射方式第二种等效转化方法，确定是C<10,5> * A<5,5>）
                          ##3 同上，5个1,2个0，剩下3个不同的数：2,3,4,那么总共有的排列数（同上方法，有C<10,5> * C<5,2> * A<3,3>）.如果剩下的三个数是相同的另一个数如2，那么共有排列数：C<10,5> * C<5,2>
			      ----可以导出这种有若干组相同元素的排列问题的通项公式：。
			      当上述问题退化为n个组，每个组只有一个相同元素，结果C<5,1> * C<4,1>* C<3,1>* C<2,1>* C<1,1> = A<5,5>,转化为了最基本的排列方式了。所以排列可以用组合的方式计算表达出来.
			      ----结论1：分组越多，排列种类越多。
			      ----结论2：如果分组数一定，那么每类的元素个数越均匀，排列种类越多。（比如两个组，C<10,5>是C<10,k>（0<k<11）中最大的）
			                推论：排列数越多，那么对应系统状态出现的概率就越大。即系统在不断变化的话，那么出现这种状态的概率是最大的。这种状态就是其中元素的排列数是最多的：每种排列是一种情况.
                                             如何排列数最多，从数学上看，就是因子的值互相接近，意思即是分组中每个分组都要有，且一个一个的取，很平均的出元素，直到出完为止，这样得到的系统状态的元素排列数是最多的，即系统出现这种状态是最可能的。
			      ----结论3：
          3）具体思想：把相同的元素聚集到一块。
          4）排列不能算交换，排列数就是所有可能情况数。交换算同一种元素。为什么不能算交换？---因为这种交换前后，出现的排列是同一个排列（按照这个属性值）。。。。。所以说，排列，是按照属性值的排列。
              --既然排列是按照属性值的排列，所以说10个硬币的正反面值的所有可能排列是2的10次方，而不是A<10,10>。也说明排列一定要阐明排列的属性值--即依据值。排列是按照属性值的排列。
             
   >组合：
	0)经典问题：分组问题：10个相同元素，最多分成3组,且每组前后有序，求所有可能的分组数。（ps:当10个元素不同时，分组可能没有简便计算方法，比较复杂，没有什么规律，所以没有什么讨论价值。关键结果也没有意义？）
		   >分析：元素之间补充上空格，可以把分组形象化，看到所有具体分组的统一规律（或者分类下来有几种规律）,分为3组则等效转化为9个空格位置上选出2个的所有组合数：C<9,2>,分为2组则等效转化为9个空格位置上选出一个来放分割符的所有位置数C<9,1>，分为1组共有1个。共有C<9,2> + C<9,1> + 1  = 46种。（1或者是C<9,0>）
		   >推论：n个元素，分成k组，每组前后有序，共有C<n-1, k-1>种分法。最多分成k组，其他条件不变，则是连加表达式了。∑C<n-1,j>  0<= j < k
		   >推论2：上述问题等价于：苹果和梨和西瓜若干个，取三种水果组合，要求总共n个水果，求有几种组合方法，此时不同水果之间没有顺序。
                   >拓展1：10个相同元素，分为3组，每组前后无序，求分组数。等价于：将整数10分成3个可相等的正整数相加，有几种分法：
			   分析：假设有序，按照有序算，则对于无序则有重复，对于有两个元素相等的情况，重复了3次，对于三个元素都不相同的情况，重复了A<3,3>次。而其中两个元素相等，2n + k = 10 ,那么k能取4个值，所以有4个组合重复了3次，共占了3 * 4次，而有序的组合有C<9,2> = 36种，(36 - 3 * 4) / A<3,3> = 4次，所以共有无序组合4 + 4 = 8 种。
			   利用：无序可以分类情况，而一类情况中的每个具体情况发生概率（概率是一个具体情况相对于所有类的所有情况的比例）是一样的--所以这类情况的熵容易计算。
				 （求整数分割就是在求分类，求分类就可以简化整体的熵的计算）
   >概率论：
          0）定义概率和计算概率要明确的认识：
	      ##1 概率就是系统未来出现某一种情况的可能性，称系统出现该种情况的概率
	      ##4 该种情况的准确描述：使得其他情况也是明确可以知道的。
	      ##3 概率性事件：随机事件，根据目前有的不充分信息（包括事件发生的必要因素）来推算、来关联出全部可能的情况的比例，有说服力的比例，有根有据的比例。
	      ##2 计算概率：概率的影响因素：系统所有可能的情况总类数、每类情况的现实决定因素（决定条件）
			    概率的计算方法：可以直接实验；根据影响因素进行比例计算。按照1 * 1 * 1...分解：(0.3 + 0.5 + 0.2 ) * (0.3 + 0.1 + 0.6) * ...
              ##5 概率和总数的关系：
		   总数分之一就是平均情况下每种情况的概率。
	  1) 某种排列的总数和这种排列的概率，乘积就是这种排列出现的概率：
		>用0.6 + 0.4正 + 反概率的硬币两枚，出现一正一反的排列数是2，每个这种排列的概率是0.6*0.6 = 0.36，所以这种排列的总出现概率就是 0. 36 * 2 = 0.72
                >同理，前述的10枚硬币，0.6 + 0.4 正反概率，这种5上5下的概率就是：0.6 ^ 5 * 0.4 ^ 5 * C<10,5> = 0.02（概率不如全部朝上的0.047,6个朝上的0.25,7个朝上0.21）；特别的，概率是0.5 + 0.5时，5上5下的概率是：0.5^5 * 0.5^5 * C<10,5> = 0.253
	    
   >熵：熵出现的原因：实际中有想比较或者区分两件事情的结果的不确定程度谁大谁小？其中每件事情的结果都有多种可能，且每种可能都有一定的比例，或大或小，但两件事情的结果可能数不一样、每种可能的概率也分别不一样。
		      此时就定义一件事情的结果的不确定程度的量度：熵。
   >信息熵：是对概率空间中所有样本的不确定度的度量，是样本分布情况的度量（越均匀，排列数越多，不确定度越大。。即不确定哪些属于什么类），分组数就是微观状态数，数据集的一个分组就是理想气体的一个微观状态。数据集在A度量下的考察分组，好比气体在理想状态下的考察微观状态。
	    一个数据集有信息熵。
            >因为一个数据集有原始信息熵，而且在A度量下，还对数据集进行了分组，每个分组也是一个数据集--也分别有信息熵，利用这些分组的信息熵可以求出A度量下的信息熵。
   	    >###系统所有可能的取值类别（每个元素，相应取值，；每种取值类别下，都有若干个取值情况，每种取值情况就是所有元素的确切取值的集合）和对应的类别概率
            >###系统的两种取值情况是一样的：，认为是等效的，则者两种取值情况认为是同一种类别。
 		 取值情况一样的判定：认为系统中的元素是不同的，但是当两种系统取值情况的差别仅仅是情况中有两个元素交换了量度值（比如，两个原子交换了位置，两个骰子交换了位置，两个硬币交换了位置，进行了不同的排列），那么认为系统的这两种取值情况是同一个取值类别，或者说属于同一个取值类别。
	    >取值概率（取值比例）、取值情况（取值类别数，和类别情况数）和信息熵之间的关系。
              @@如何描述不同的类别，确定不同的类别，找到所有的类别，把什么认为是类别？类别与熵。类别和信息熵，	
	      @@分类是人为进行的，人为专门考察而指定的，一个类里面包含了若干个情况，
	      @@熵是一个类的熵，即一个类也有这个类的熵。而一个对象集合有很多种类情况，一个情况类包含着一些情况（同类情况）(那么就有相对这类情况，某种情况出现的比例概率。用这种相对类的概率算出的熵就是这个类的概率)。
			推论：只要某件事情会概率性的比例性的出现各种情况，那么这件事情就有熵（就有信息熵）。换种表达方式，对象，对象在某种事情上的结果是不确定的概率性的比例性的多种结果（多种情形，多种具体情形，多种情况），那么说这个对象在这种事情上的结果上有信息熵这个量度，就是这个结果的不确定度。
				（熵是什么熵？熵是对象的事情的结果的信息熵。信息不确定度，不确定程度（即结果有多种可能，而每种可能都有一定的概率，概率有大有小。。如何比较、区分两件事情的结果的不确定程度谁大谁小？）。）
				进一步推论：---实际上表明，结果的可能数越多，每种可能的概率越互相接近即越平均，那么不确定程度就越大，即结果的不确定程度只与可能情形的数目和每种情形的概率相关。加一些条件因素，可以推知结果的不确定程度的量度定义成熵的计算形式--香农形式是很好的--很一般的--很有代表性，与事实符合的很好。
				             上一句话提到的“其他一些因素”，就是现实实际上，不确定程度具有可加性，即一个整体结果的不确定度是其每个分类结果各自的不确定度的和（f(x,y) = f(x) + f(y)即等式两边要保持同一种形式。那么f一定是一个对数函数。且左边的逗号运算就是一个乘法运算），如果任意一个分类结果内部的每种情况的概率都相等--那么每个分类结果的熵是一个特殊的形式，而且自然而然每个熵相加出来的值，和独立整体不分类来计算这个整体的熵的值，是相等的（这也说明香农信息熵有可加性：这是在特殊情况下的证明）。

			运用：##1 掷10枚硬币，5上5下这类情况（这种事情，这件事情）的结果的不确定度（即到底是那种排列的5上5下）即结果的熵是：In252
                              ##2 A班和B班比赛这件事情的结果（不确定，但知有两种，A胜或者B胜，但到底是那种不确定）（A班胜的概率是50%）的不确定度即结果的熵是：In2
                    
                              推论三：概率是描述事件发生的比例的，熵是描述事情的结果的可能数的（不够准确--只是形象和正相关，最好混乱数，不确定程度,只是这样不形象）。
				
	    >样本集合，样本的某个属性的值域，值域中各个值的取值概率（比例），
	     结论1：多个样本的某个属性的某个值的取值概率确定了，那么这个样本集合的这个属性的信息熵也确定了（即实际情况出现的比例）。
            >已知样本集合，求下一个样本集合出现某种情况的信息熵：
			##1由样本集合，可以计算域值中每个值出现的概率（对于离散的属性）；可以认为是下一个样本集合中各个值出现的概率，从而求某种结果（如k个value1,k1个value2,k2个value3,...可以知道该类情况的实际概率--每种情况的概率乘以情况总数）信息熵--可以算的还是只与总情况数和每种情况的概率相关；同理其他种结果的信息熵也可以算；每种结果的信息熵之和就是全部结果的信息熵（从10硬币抛掷5上5下，即其他所有的情况，加起来的信息熵刚好是ln1024,而5上5下的信息熵是0.252ln1024）
			   当然可以直接根据全部结果的每个情况的概率来算全部结果的信息熵，且这是正确的，只是实际算时发现其中会出现一组又一组的相同的信息熵，而每一组相同的信息熵对应相同的情况概率而对应的情况刚好是属于同一类情况即同一种结果，且每组的信息熵之和正好是每组即每类结果的信息熵--显然直接找到所有类情况后专门算这些类情况的信息熵，就可以更快地计算出全部结果的信息熵了。（这也是为什么要算每类结果的信息熵的原因）
			（样本的价值：在于提供了值域中每个值的取值概率，当然，一般样本数少，值域未定，值域中也不一定每个值都取到。。但是足以估计下一组样本集合的信息熵了，无论集合的样本有多少，可多可少。。且一轮轮样本的输入，而一次次优化估计样本集合的信息熵（因为每种情况优化了概率））
	   		

  》分析：
  》结论：


---也是筛选，也是分类

参考资料：http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html
http://blog.csdn.net/nieson2012/article/details/51314873（详细）
http://www.cnblogs.com/ShaneZhang/p/3970176.html（基本概念）
