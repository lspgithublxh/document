参考资料：http://blog.csdn.net/dai451954706/article/details/46966165 （有浏览器图可以看）
	http://www.chinacloud.cn/show.aspx?id=23371&cid=12（ubuntu，最可靠）
	http://www.tuicool.com/articles/MbuaUv
1.先安装hadoop集群--运行良好，并且启动spark之前，要先启动hadoop。安装scala
  》解压文件，配置环境变量：/etc/profile  $SPARK_HOME
	tar -zcvf spark-1.6.1-bin-hadoop2.6.tar.gz
  》配置目录下conf文件夹下的slaves文件、spark-env.sh文件（启动参数、环境变量，4个HOME，3个PATH.....）、spark-defaults.conf文件（spark提交任务时默认读取的文件）
  》创建日志和工作的logs、worker文件夹：在spark的安装目录下：mkdir logs（这个可以建立---也可以不建立--会自动建立这个文件夹,worker可不建立）（但是分布式文件需要建立：./hadoop fs -mkdir /sparkHistoryLogs  即建立配置中的日志文件，不建立会报SparkException:exitCode:15错）
  》启动spark进程和历史任务进程（sbin/start-all.sh , sbin/start-history-server.sh）
  》启动实例， ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster --driver-memory 1G --executor-memory 1G --executor-cores 1 examples/jars/spark-examples_2.11-2.1.1.jar 40
  》启动自己上传的任务： ./bin/spark-submit --class com.spark.test.main.WordsCount --executor-memory 1G --total-executor-cores 1 ../javaSpark2.jar /user/words.txt
			./bin/spark-submit --class com.spark.test.main.WordsCount --master yarn --deploy-mode cluster --driver-memory 1G  --executor-memory 1G --total-executor-cores 1 ~/opt/javaSpark2.jar /user/words.txt 
			 ./bin/spark-submit --class com.spark.test.main.WordsCount --master yarn --deploy-mode cluster --driver-memory 1G  --executor-memory 1G --total-executor-cores 1 ../javaSpark-1.0-SNAPSHOT.jar /user/words.txt
1.1具体spark-env.sh配置：
  	export SPARK_MASTER_IP=10.206.131.48  # bind the master to a different IP address or hostname

	export SPARK_MASTER_PORT=7077 # SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master

	export SPARK_MASTER_WEBUI_PORT=8080 #
	export SPARK_WORKER_MERMORY=2G
	export SPARK_EXECUTOR_MEMORY=2500M
	       SPARK_LOCAL_DIRS=/home/fang/spark-1.6.1-bin-hadoop2.6
　	export SPARK_LIBARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/native
	export SPARK_DRIVER_MEMORY=2500M
	export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
　	export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop-2.5.1
	export SCALA_HOME=/usr/local/program/scala/scala-2.11.4  
   	export SPARK_HOME=/home/fang/spark-1.6.1-bin-hadoop2.6
 	export JAVA_HOME=/usr/local/program/jdk1.7  
	export HADOOP_HOME=/usr/local/program/hadoop-2.5.1    
	export SPARK_DAEMON_JAVA_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/home/spark/spark/logs -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+DisableExplicitGC -Xms1024m -Xmx2048m -XX:PermSize=128m -XX:MaxPermSize=256m"
	export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=7777 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://master:9000/sparkHistoryLogs -Dspark.yarn.historyServer.address=master:7788 -Dspark.history.fs.updateInterval=10" 
	export SPARK_JAR=/usr/local/program/spark/spark-1.1.0-bin-hadoop2.4/lib/spark-assembly-1.1.0-hadoop2.4.0.jar  
	export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SCALA_HOME/bin

  1.2具体spark-defaults.conf配置：
    	spark.master                     spark://master:7077
	spark.eventLog.enabled           true
	spark.eventLog.dir               hdfs://master:9000/sparkHistoryLogs
	spark.eventLog.compress          true
	spark.history.updateInterval     5
	spark.history.ui.port            7777
	spark.history.fs.logDirectory    hdfs://master:9000/sparkHistoryLogs
  1.3 hadoop上还要创建日志文件：hadoop fs -mkdir hdfs://master:9000/sparkHistoryLogs

2.测试spark集群
  》启动shell（即scala脚本）: bin/spark-shell --master spark://master:7077
  》输入scala脚本测试：如var data = Array(1,2,3,4,5,6,7,8,9,10)
			 var pdata = sc.parallelize(data)
                         pdata.reduce(_ + _)

3.浏览器查看集群节点信息：http://master:8080  ，历史任务信息：http://master:7777
  （hadoop集群信息的查看不再赘述）
	
  
4.问题：
  >没试过，但是还是只改配置文件---可以copy内容，不直接从windows复制文件过来覆盖。
