(严肃才能平静，否则会浮躁)
并发相关全知识：https://cloud.tencent.com/developer/news/313447 java并发编程的艺术
jvm技术规范：https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html#jvms-6.5.monitorenter
next work ,  sumerize every night!!new technology 
可以归结出几大技术点：(能力边界，消耗成本)(逻辑边界)
应用业务层次：---：

工具层次：------：数据流线角度来收拢聚集
#调用式编程：不要直接开始写代码---调用式，自顶向下编程；就是抽象通用方法编程，就是写思路，写算法。

底层技术-------：(动机范围)(基础引用<用什么牌>,运用策略(<该牌怎么打>) (普通方案，最优方案)
数据结构层次：
	#字符hash：hashmap的hash函数(h ^ (h >>>16) )\fst\nginx一致性hash(圈)
	
	#索引结构的增删查：	
	#hashmap数据结构的增删查：Node链表数组(hash-key-value-next), key的hashcode的hash()函数结果  & (n - 1)得到的数自然会是 < n-1的数(比如n是16,因为和高位0相与结果为0)(作为entry[]的索引)
								>当链表节点数 >= 8则会把链表先转为双向链表(且反序)，再转为红黑树！！(每插入一个元素就进行一次红黑平衡算法平衡红黑树)(LL型平衡旋转(最长路径一直往左下)--右短分支加进来而本节点放在左边；RR型平衡旋转(最长路径一直往右下)--左短分支加进来而本节点放在右边)；；当最长路径存在左转右转时，先层层向上转为单向。最终一步都是使最长分支的顶点向下旋转----从而最长分支的长度减1。
								>删除：左右子节点不为空--则使用左子树最大节点为新顶节点，并递归删除该节点。
								>resize(): 当node entry个数 >= 长度 * 负载因子loadFactor   则先初始化一个2倍长的table，在逐个赋值；在新table的位置---如果只有一个元素-则该元素的hash & (newlength - 1)， 如果有多个链表型元素---则将元素的hash按高位是否有1而分解为高位list和低位list分别加到扩2倍的新node array的两个位置j, j + oldCap   ....因为线程不安全，所以多线程扩容的时候：：或许会导致产生大量的新2倍数组产生----从而导致内存和cpu都巨大消耗。hashtable线程安全----但是对entry数组整体加锁---从而性能不好。如果是树节点TreeNode，则先把树变为一个链表，如果树的大小超过了新的load_factor*newCap, 那么就需要再次化为树，如果小于，则转为Node链表；放的位置依然是0或者j+oldcap
	#concurrentHashmap数据结构的增删查：Node链表数组，hash方法本质上一样，树也有next方法-所以统一，加节点时，如果数组i位置是null--则cas原子操作数组方法替换i位置为新的值，如果hash是-1会重新操作数组--transfer之类比较复杂，获取数组i位置的节点也是volatile方式缓存失效方式获取元素引用；最后在正常的有next的元素的put时---用这个新构造的Node进行synchronied同步----如果key存在则覆盖--key不存在next==null的位置加，如果是treeBin则相应的查找添加--甚至会在重构树时需要获取写锁--而使用到park(this)停车。//直接对Node加锁---而且用synchronized,且有用cas减少锁使用。扩容用transfer	
								>put方法类似hashmap。只是加了并发控制synchronized
	#红黑树：新加节点都是红色；不满足“根黑 红-黑 黑数同” 则需要处理-重新平衡：变色(对一个节点变色检查：该节点定是红色，1:父叔同红-变父叔为黑、祖父红 ，当前为祖，继续 ；2：父红叔不红(黑或空)&当前为右-令当前为父-新当前再左旋，3：父红叔不红(黑或空)&当前为左-变父黑祖红-令当前为祖-新当前右旋； 根节点结束)、左旋(当前节点向左下，而右子上位)、右旋(当前节点向右下，而左子上位)。。黑下可以为黑。父红祖不红，则父旋(父子-祖父不同向)或祖旋(父子-祖父同向)；父红叔红，则父叔变黑祖变红，当前为祖，继续。
		>删除：无子节点则直接删除、有单一子节点则删除而用新子节点替换、有2子节点则删除而用右子树最左节点替换。https://www.jianshu.com/p/0eaea4cc5619  https://blog.csdn.net/eson_15/article/details/51144079 https://www.jianshu.com/p/e136ec79235c(最佳)
			>是红色：且最多1个节点，直接用子树替换。
			  >2个子节点：
				>替代元素是红色：直接移除并替代
				>替代元素是黑色：调整顺序：兄红父左旋(数差不变)、兄左儿红兄右旋、兄右儿红父左旋、兄黑兄双儿黑
			>是黑色：(考虑黑数和高差改变)
				>替代元素是红色：直接移除并替代元素为黑色
				>替代元素是黑色：
		>增加：原则：红之黑父，互换色而反红旋。即如果父叔叔子节点有单一红，则这样操作几次。如果父叔叔子节点都没有红，则直接变父叔为红而祖为黑；继续祖。
			>
	#treeMap数据结构的增删查：底层数据结构为Entry为节点的二叉树。
	#BlockingArrayQueue(netty)数据结构的增删查:使用ReentrantLock进行同步、阻塞。扩容是也是create一个新的更长的数组。
	#ArrayBlockingQueue数据结构的增删查：使用ReentrantLock进行同步、阻塞。
	#CyclicBarrier类似CountDownLatch, 它功能为让并发的线程运行到相同位置。--先到的线程则阻塞。--直到num==0而开始都运行。
	#Semaphore信号量，类似synchronied、lock---但是不同之处是，可以多个线程同时获取到锁--即一个信号量，达到限额后获取才会阻塞。
	#Exchanger两个线程交换数据，使用Unsafe.park(false, time)方法进行先到线程等待后到线程，后线程Unsafe.unpark()来唤醒先到线程，Node节点里:item/match/parked/hash属性。其中item就是先到设置的，match是后到线程设置的，parked就是先到的停车线程。
	
	
	
线程方面：(多上下文方面)(数据+指令代码的临时环境内存环境)
	#同步：
	   >本地多线程同步(代码块)：同步代码块synchronized/Lock(可重入锁ReentrantLock/读写锁ReentrantReadWriteLock)/自旋锁(mcLock, clhLock)<利用了原子操作>/带Unsafe 利用了OS 的CAS原子操作来实现的同步 AtomicInteger, AtomicBoolean   (但实际上synchronized更简单---1.6以后它的jvm实现也是cas)
		  >CLH锁：节点对象：有一个next对象 以及 一个属性引用原子获取更新getAndSet()类对象(Unsafe类获得)， 取锁方法：首先n个线程原子获取更新next属性为自己的node对象，同时获得上一个线程设置的对象，由于每个都会串行执行，所以每个获取的都不同，并把这个上一个node.next指向自己。自己在这个node上自旋---退出条件是这个node被属于的那个线程设置它的lock属性为false-----即表明上一个线程释放了锁而自己得到了锁。
	     --使用锁性能问题的原因：线程上下文切换--用户态内核态切换(自旋则没有上下文切换)  ； 磁盘IO不同扇区引起盘片转动和读写磁头转动
		 --多线程缓存不一致的原因：A线程读取主存中的一个值，不是立即给cpu，而是给高速缓存，然后进行运算；且这两步都没有锁住总线---从而其他cpu可以访问主存中的该值；。。注：线程读取数据，具体到os执行级别，都是先看cpu缓存里有无，再定是否读主存；(每次因计算而更新了cpu缓存中的值则会保存到主存)
									>即某一个线程修改的变量仅仅更新到了对应内核的L1,L2高速缓存里而没有更新到主存，而此时其他内核之前读主存时读了含此变量的缓存行进来，从而它读的时候也只读了它对应的内核的L1/L2高速缓存，从而不一致。总之就是更新没有使得高速缓存失效。
		   多线程缓存不一致的解法：缓存锁定(总线锁定上述)：MESI协议：一个cpu在写主存中的一个值时，会发送RFO指令信号给其他cpu让内部缓存里的同一个主存地址的值失效(迫使其他cpu进行重读)(而其他cpu也会监听缓存状态-如果是M则要等待它刷到主存为止)。(一个cpu里多内核，各个内核也有自己的高速缓存)
																
		 --os提供的原子性操作：读写一个字节、Compare And Swap (利用了上述--总线锁定/缓存锁定) (乐观锁方式读取比较写入;相等则写入返回true-否则返回false) ---这串指令操作被封装到了Unsafe类的多个方法中，如Unsafe.compareAndSwapInt(实例，属性，期望值，更新值);更新失败可以重新读重写compareAndSwapInt();(在Unsafe.getAndAddInt(实例，属性，递增值)就是这样的重试机制)
	   >volatile: 只是保证在对变量更新后，立即从内核高速缓存回写到内存，而一般情况下没有volatile修饰的变量被修改不会立即被回写--而且很久一直都是从高速缓存读。称为变量有可见性。可见型变量，，无脏读变量。(非常重要，CopyOnwriteArrayList实现里就使用了--这个是先copy原数组并长度+1添加末尾元素后将数组指针赋值给原数组指针)
				>避免指令重排--所以volatile变量的读写 会加内存屏蔽(LoadLoad, StoreStore LoadStore StoreLoad) 指令。(重排序，在一个线程中执行的顺序指令，但是可能不是顺序的，导致另一个线程看到的值变化不是顺序变化的)(cpu汇编级别指令会重排-同类指令在一块执行-提高效率-减少来回切换)
	   >final: 也实现了可见性。毕竟不能被重新赋值---但可以第一次赋值---这个第一次赋值只能顺序执行中被赋值。
	   >happens-before:是操作之间的关系，用来辅助描述各种规则中的各操作的先后顺序。比如volatile变量的写happens-before该变量的读(多线程中)
	   >四种锁状态：以及升级关系：无锁->偏向锁(获取锁自己用，不存在释放锁,没有CAS也没有自旋，竞争时膨胀为轻量级锁)->轻量级锁(竞争失败时自旋或者CAS)->重量级锁(竞争失败时使用互斥量mutex阻塞当前线程-从而线程切换)     (锁对象：Mark word里会存获取本锁的线程信息、锁级别，线程cas方式获取和赋值)
					>一定是因为四种锁状态下“加锁和解锁”的底层操作指令不同、代价不同，所以在一种锁状态等待极限内仍然没有获取到锁--则会升级锁。----意义-应用：在竞争强烈的场景下，直接先关闭偏向锁功能；第二：设置轻量级锁自旋次数：-XX:PreBlockSpin；第三：升级轻量级锁为重量级锁可以避免浪费cpu资源(尽管线程切换、阻塞需要os内核态操作所以需要用户态-内核态切换又耗时)。
	   >sychronized: 1.8以后性能持平或者更优。且concurrentHashmap用synchronized而不用ReentrantLock	。。synchronized先偏向锁--->再轻量锁--->再重量级锁
	   >AQS:同步组件实现的关键部分,是基于CLH锁的FIFO同步器，本身提供“(取锁--即cas方式加到队列尾部tail)排队-循环阻塞-状态取赋更(可以导致退出阻塞)(释锁-激活下一个元素)”基本功能，比如使用Sync， CountDownLatch(利用sync实现自己的同步语义)就用了Sync。比如ReentrantLock(利用sync实现自己的同步语义)(sync的AQS的ConditionObject里的await()方法---根本是用了LockSupport的park()方法--而它则用了本地方法UNSAFE.park(false, 0L);方法来阻塞当前线程---其他线程则用UNSAFE.unpark(threadObject)来释放阻塞的线程)类里就用到。(AtomicInteger是用Unsafe, LockSupport也用Unsafe, concurrentHashMap 用lockSupport)
				>CLH锁中队列元素对应先后阻塞的线程，int整数代表同步状态(>0排队阻塞)(AQS自身不改变state的值)，AQS提供取、赋、CAS更新三个方法操作该状态。
				>实际继承AQS：常用来实现tryaquire  tryrelease 两个方法，实现中利用set/get state方法(有时会使用get/set ExclusiveOwnerThread来判断是否重入之类)(更新成功则得锁，没有自然就返回---从而可以不用阻塞)，而使用AQS自己实现的acquire release方法。
	   
	   >分布式同步：
	#线程模型：从任务特征上分类。
		>Runnalbe模型：无输入值无返回值直接异步执行任务 ,
		>Future模型：无输入值返回future并异步执行任务-父线程future.get()同步等待结果
		>Fork-join模型：有输入值有返回值(任务特征)，任务本身根据入参或者直接计算返回值或者分解出2个子任务异步执行并取它们的结果；父线程task1.fork(), task2.fork()异步执行,父线程task1.join(), task2.join()同步等待结果。
		>Actor模型：用信号signal异步发送woker请求request实例和response回调函数类实例给actor，无返回值。请求实例也会被异步执行---即如果在发送的时候做为循环发送n个req, 那么这个n个req会并行执行。(如果在回调里再发送，而外层只发送一个，则自然变成了顺序执行了。) 参考资料：http://www.agilewiki.org/projects/JActor2/1.0.0/JActor2RevisitedByExample.pdf

	#网络模型：nio的多线程selector为例
		>普通socket:阻塞模型，流数据模型。到则必须读，无则阻塞。写则直接写--不管端口状态。
		>NIO:端口四大事件通知模式--(linux上是epoll)，每个端口对应的通道四大事件有通知-无阻塞(相当于一个方法有值返回无值等待阻塞)，通道可以读出可以写入--根据实践类型而定----而这个通道读与写是同步的。Selector-->SocketChannel-->ByteBuffer
		>AIO:读与写是异步进行的Future线程模型
	#存储模型：副本为例
	
	
		
#内存方面：
	>内存管理：
	  >jvm内存管理：
		>各代划分和默认比例：
			>NewRadio: 新生代=1 年老代=n   默认为2
			>SurvivorRadio: Surivior0=1 Eden=m 默认是8  
			
		>回收器的标记回收过程与促进它的参数调整：
			>垃圾回收器：Serial  Parellel CMS G1
			 >回收方法1：stop the world 标记所有根节点(静态属性，常量)-可达对象-清理不可达-复制可达到survivor/old。。。Serial Parellel 
			 >回收方法2：stop the world 标记所有根节点 --> 并行标记 可达对象(trace算法记录在bitmap) ---> STW 重新标记 动态用户线程产生的新可达对象 --> 并行清理不可达对象。CMS优先保证stw的时间短。而G1好处在内存空间管理更精细，空间碎片更少，影响的线程更少。
			>回收器组合1：+UseConcMarkSweepGC +ParNewGC 来对年老代和新生代进行垃圾回收
			>回收器组合2：+UseG1GC +MaxGCPauseMills=200
			>G1回收器：物理分区：Region(1M-32M)(Xmx/2048个) 每个Region内存分配最小单位Card(512B)
				       被引用集合: points-into; RSET 一个RSET记录了一个Region中那些被其他对象(可在其他Region)引用的对象构成的一个个引用关系。在垃圾回收时查看可达对象就查看RSET而不是全表扫描
					   引用集合：points-out; Card Table。。CSET。
					   回收策略：先收集垃圾对象最多的分区
					   回收过程：可以认为同CMS的4阶段--但更多-如根分区扫描。仅仅是具体做法因为物理分区的不同而不同。
		>各代使用查看、回收查看、实例查看、线程状态查看(进程的线程数、进程的线程资源占用)：
			>关注：参考：《深入理解Java虚拟机》
			 >内存泄露？(不使用而被根引用到/长周期对象引用到的对象占用的空间无法回收;短周期对象即时关闭和置为null)内存垃圾old代回收次数太多、均次耗时大？(STW)(jmap -histo:live pid | sort -n -r -k 3 | head -10)(jstat -gcutil pid) 泄露查看工具leakcanary
			 >线程泄露？线程死锁？(ps -hH pid | wc -l) (jstack -l pid)系统线程数(cat /proc/pid/status)
			  >各方法执行次数\执行时间排序：
		>堆大小限制：32G以上，指针不能做压缩。
	  >零拷贝：使用DMA将磁盘数据放到内核态数据区后直接copy放到网络设备上，而不是先copy到用户态的数据区、切换到用户态、什么一般指令也没做就切换到内核态继续执行特权指令--将数据从用户态数据区cp到网络设备上。
	  >伪共享：cpu1更新了X但是没有再读X, 导致X所在的缓存行失效---而又因没读导致cpu1所在L3中的X所在的缓存行是失效的，---如果该缓存行里有Y变量被其他线程所在cpu2操作，那么cpu2就只能从主内存中取---导致效率不高。---伪共享问题。，和缓存行有关---cpu中L1L2缓存没有失效--没有因为其他cpu的更新而失效。	
		>问题产生：常常在多个long,int基础变量使用时，它们被放入了同一个缓存行。相反，如果每个变量单独放在一个缓存行，那么就互相不影响，因为只是一个缓存行的失效，不会影响其他缓存行失效--而重新去主存里拿。(disruptor这个内存消息队列就避免了伪共享--避免了重新去主存加载)
		
#网络方面：
	#https协议：http协议 + ssl/tls协议： 最佳总结https://www.jianshu.com/p/14cd2c9d2cd2
		>https请求过程：前奏：客户端把独立生成的对称加密密钥client key  加密传输给服务端，服务端解密后得到后，服务端客户端就可以用这个对称密钥来加密解密了。  加密：是从服务端获取的公钥，解密，是服务端自己的私钥。
				客户端： 请求服务端                     公钥合法(证书有效),则独立生成一个client key即密钥，并用公钥来加密这个client key , 将结果发送给服务端																																																														客户端用client key解密得到内容即可。										
				服务端：              返回公钥(证书) 																											使用私钥解密该结果，得到client key 。这个client key就是双方进行通信的对称加密密钥(保证通过网络也只有客户端和服务端知道) ::从而双方可以用这个client key进行加密通信了。即服务端就当作对称密钥来对称加密要发送回客户端的内容，将结果发送给客户端。

		>http通信模型：请求-响应：一个请求：用 url + 报头 + 报文 表达，  一个响应：用 报头 + 报文   。 报头说明请求类型、报文类型、请求参数、参数长度、cookie信息
		>http/2: 多流并用一个tcp连接。2015年后。
		>证明B收到的信息肯定来自于A(保证信息没有被修改)：则只需要A用自己的私钥加密信息传给B，而B用A的公钥解密。可以解开得出合理信息则说明肯定来自于A（因为中间人不可能知道A的私钥，所以解开后无法再加密）
			>但直接加密“信息”本身则耗时，一般是加密“信息”的hash值，得到的就叫做数字签名。同理，将“数字签名”解密之后，得到的就是hash值---和将收到的信息进行hash后的结果比较。则可以确定来自于A。(因为中间人无法生成正确的数字签名)(即保证信息没有被修改)。(但无法保证信息没有泄露)
		>证明B收到的信息没有泄露(没有被中间人看到)：只需要A传输时用B的公钥对内容进行加密。(中间人没有私钥，所以无法解开)
			>因此传输过程：1.用B的公钥加密(文件+数字证书)发送给B(则中间人不能解开-无B的私钥，也不能调包-无A的私钥)(但条件是AB双方事先知道对方的公钥)。。就可以保证信息未被修改未泄露。
			>公钥获取正确的解决办法：因为公钥允许泄露，保证确实是B的即可；B的公钥作为信息，用另一个私钥X加密，传递给A；A用X对应的公钥解开---来获得B的公钥。。。而A要事先知道的X对应的公钥---就是CA中心的公钥。仍然两个问题：数字证书如何被服务端获取到；CA中心的公钥如何被客户端获取到(客户端存CA中心的根证书，则可以验证所有分中心下级中心颁发的证书)(根证书可靠：windows也预置了50多个根证书)(各个具体软件.exe属性可以查看数字证书：即该软件所在公司从CA中心申请的证书)。
			--传自己私钥加密的内容：保证来源。(传第三方私钥加密的内容：也可以保证来源。比如传CA中心的私钥加密后的内容：数字证书)(自己私钥加密的内容：只叫数字签名)
			--传对方公钥加密的内容：保证不泄露。
			
			
	#tcp/ip三次连接而不是2次的原因：(提问式反问式 找设计的理由找设计的原因式理解，而不是认识式定义式理解；；设计的原因才能让人真正理解，，而不是只是知道是什么；更要知道为什么要这样设计为什么不这样设计---因为这才是真正最透彻深刻的理解；否则是肤浅的)(这么设计目的：一是为了实现功能、二是为了避免带来新问题新不确定、三是一种解决不可避免的恶劣情况导致的功能故障的方案(悲观考虑必须、意外情况必须--不能只考虑正常情况的处理----意外情况往往就能改变设计思路局部甚至整体---导致设计的结果直观上不太容易理解--毕竟离正常情况已经有点远了-不纯粹只是为了处理正常情况而设计--这个毕竟也是不强大的))
		>服务端不能收到信息就确认建立新连接，而开始接收处理实际真正要传的数据。因为客户端可能会多次请求--没有回应就会重试。所以需要客户端确认才可以。即：客户端发起连接请求(标记为X: SYN=1 Seq=X)->服务端响应(标记为X+Y: SYN=1 ACK=X+1 Seq=Y)返回-->客户端确认(Y+Z: ACK=Y+1 Seq=Z)发送-->此时服务端才确认建立连接(ACK确认的是上一次的Seq)(说明：1.当服务端返回时--客户端就可以确认网络通了，而服务端收到客户端的确认时--就可以确认客户端也收到“我的确认”了；过程非常类似https请求--加密传递密钥)(客户端：经历“发-收”；服务端：经历“发-收”。双端都经历“发-收”(说明双端确认网络“可发可收”)连接建立)。(3次的目的：双端确认网络可以“可发可收”；服务端可以忽略建立后释放前的重试请求---免得浪费资源；客户端也可以忽略建立后释放前的反复确认--同样避免了浪费资源)
		>
	#tcp/ip四次释放连接而不是2次的原因：首先释放连接的原因：--本方已经接收数据完毕--不用再连接了；这个消息要让对方知道-所以要发送结束报文(FIN=1 ACK=Z Seq=X)给对方，当然也要得到对方的收到确认(ACK=X+1 Seq=Z)(否则认为对方没有收到而继续发-此为需要接收确认的重要性)；同理，对方接收数据完毕，而需要告知对方(否则对方可能认为连接中断而重试重发)(发送FIN=1 ACK=X Seq=Y) , 也要接受到对方的确认为止(对方发送ACK=Y Seq=X)(否则认为对方没收到而继续发送)。
		>

	#DNS过程：从host到本地DNS服务器到代理DNS服务器到询问13台根服务器
	#ip地址划分：
		>子网划分：
		>路由算法：
#网络设备单点问题：(单点问题，不可用)
	>集群：
	>主备：
	>主从：(从节点宕机不可用；检测，下线)(主节点宕机：检测，重选)
	  >主的选举：一个选举周期内:  对于选的非自己---收：增、同  ； 发：广、定   。。对于选的是自己---收：增、定 ； 发： 广、同。
		>zk的类似fast-paxos算法：关键在第二阶段的时候，如果同时有n个节点进入了第二阶段的主发，那么收到消息的accepter节点原子更新自己的select和自增zid, 从而只有一个主发节点成功，其他主发节点则失败，收到超过一般的二阶段返回则更新自己为Leading开始广播---没有超过一般的则重新开始--显然二阶段结束后只有一个主发超过一半，其他则又从第一阶段开始；显然这个时候成功节点已经广播leading消息过来了---收到者全部更新自己的leader为它。(一阶段同时成功者可以小一半，但二阶段成功者却只有一个)
		>paxos算法：2pc两阶段提交。K; maxN  , accpectN acceptV ;  一：向超过半数发送K(不必全部), 超半数则区结果中最大N的V ,二：构造K-V发送，收到 超半数，成功选举。接受方改AcceptN=K, AcceptV=V
	  >有效读：zk可以从follower读取，但kafka只能从leader读取。
	  >有效写：同步数据时，从节点至少一半返回成功。
	>分布式系统理论：
		>一致性问题：
			>CAP理论：分布式系统的数据一致性、可用性、分区性；分区性肯定保留，一致性要求节点之间要通过网络进行同步保持一致-显然阻塞耗时-，可用性要求系统快速响应-可用性不关心节点数据是否一致，从而一致性和可用性是矛盾的、反相关关系的。
			>BASE方案：是承认上述矛盾而采取的分布式数据方案，来满足需求--这种需求就是更重可用性和分区性。基本可用，软状态。是一种对分布式系统CAP特性现实的一种权衡方案，是在承认CAP之间的制约矛盾下，系统基本可用、可分区而牺牲节点数据短暂不一致但确保最终一致的方案， 即存在不一致的状态，但时间极端。
		>保护：
			>缓存、降级、限流
	
#文件方面：
	#软连接：相当于快捷方式：并不复制内容，只是保持一个连接。ln -s 源文件名 软连接文件名   。。。只要源文件名是完整路径，就可以移动。
	#硬连接：相当于cp -p 文件 + 同步更新，；连同属性都复制，然后源文件更改硬连接文件会同步被修改，硬连接文件被更改源文件也会被更改。ln 源文件名 硬连接文件名。
		>源文件删除：快捷方式自然不可用。而硬连接文件可用。因为硬连接文件和源文件共用一个i节点号(i节点就是文件信息结构体)。linux删除文件只是删除i节点号与源文件这条映射、源文件(磁盘上的一个数据块)。而i节点号-硬连接文件依然存在。而展现上：目录下有：文件名-i节点信息
		>tomcat下的catalina.out存在着同步文件。
#中间件
 >nginx:

数据库方面：
	>数据库一致性问题(渐进)和对应的隔离性级别解法
	 事务隔离级别		数据库一致性问题
				脏读	不可重复读	幻读
	 读未提交    x                         (值为是否解决，x未解决， v为解决)
	 读已提交    v         x
	 可重复读    v         v    	x		(锁不了新加列)
	  序列化     v		   v		v
	  
	>事务的ACID:
		>A:事务中的所有操作要么都成功、要么失败回滚使得数据库回到操作前状态。没有中间态。
		>C:事务修改使数据库从一个一致状态转变到另一个一致性状态，用户感知不到数据库的中间变化。
		>I:事务都有自己的数据空间，互相不影响。即时操作同一个数据也有自己的数据空间。更新也是从事务数据空间/临时表空间到数据库存储空间的更新。
		>D:事务一旦成功，对数据库数据的修改就是永久性的，被永久性的保存了下来，即使崩溃重启也是修改后的数据。
	>mysql:
	  >索引的存储数据结构与增删查：B+树
		>B树：平衡多叉树，关键字分布在非叶子节点上，数据在叶子节点上，可以在非叶子节点上命中。非叶子容器K个元素K个儿子；叶子容器K2个元素。从添加、删除过程看，构造出整颗树的过程看，K>=M/2 且K<M(<M/2 >=M会重新平衡). K2>=L/2 且K2<L同样在这个范围之外会重新平衡。
		 >搜索性能：按最少算：=查找次数= 每层的查找次数(都一样) * 层数(跟总数、单容器节点数有关) = log2(M/2) *  logM/2(N/(M/2)) 其中N / (M/2)自然就是容器个数，而层数与容器个数X的关系就是：M/2^(h) - 1 = X 所以认为h = logM/2(X)...化简后近似=log2(N)
		 
		>B+树：平衡多叉树。不能在非叶子节点命中，叶子节点形成链表-前后引用-自然从左到右是有序的。
		 >缺点：叶子节点的分裂，使得逻辑上连续的数据块在磁盘上并不顺序存储，使得读数据需要大量的随机读(磁头频繁移动，磁盘频繁旋转)---从而性能低下。
		>B*树：平衡多叉树。B+树变种。同层非叶子非根节点也前后互相引用。
			>单容器数据结构：从树的形成过程中可以看出，同层最左容器才可能有左子节点，其他元素都是只有右子节点---即子节点中元素最小值>=自己的值
			>增加一个元素：后，叶子容器个数<L不动，>=L 自底向上分裂，即叶子容器分裂为两个，父容器增加一个节点-值为分裂的右半容器的最小值；然后判断父容器是否要分裂。如此下去，直到根节点。
			>删除一个元素：后，叶子容器个数>=L/2不动，否则 看右容器元素有多少---如果>L/2则借一个补充；<=L/2则合并右容器，并把右容器的父节点的值改为合并容器的最小节点值，同时删除左容器。然后看父容器节点个数是否足够--进行相应借或者合并直到根节点。
			
	  >数据item的存储结构和增删查：
	  >单点问题的解决方案：主备切换、主从同步
		>主备同步：
		 >binLog: 日志格式mixed: 如果不会引起不一致，则用statement格式语句写到binlog, 如果会引起不一致，则用row格式(直接变化的记录)。delete:记录删除的每一行。update:记录前后的数据。insert记录本身。所以row格式方便恢复数据。
		  >通过binlog恢复数据库：即只需要mysql事先开启了binlog, 则输入binlog起始位置和删除数据之前的位置重新执行一遍即可(更新数据自然也可以binlog来恢复)：mysqlbinlog --start-position=4183 --stop-position=4592 /opt/lampp/var/mysql/mysql-bin.000001 | mysql -u root -p
		 >备库动作：将主库的操作同步过来 本地执行，来保持数据一致。
		 >过程：用户update语句-->主库undolog-->redolog准备-->binlog--->redolog commit提交--->返回客户端ok。。另外，写到binlog后，还会另一条线：和备库保持连接的dump_thread将按照备库传过来的binlog偏移量 读取binlog相应位置开始的内容发送给B，B本身启动了io_thread	和 sql_thread, io_thread接收到A发来的binlog日志后写到relay log中转文件里，sql_thread线程则从relay log里读取出来执行其中的sql命令。
		 >双M结构：不用修改主备关系，两个都是主都是备；而发送binlog的日志时日志在接收用户写入时已经加了serverid, 发送给备，备又会原样返回，此时serverid是自己，会忽略这个消息。从而结束。
		 >主备延迟：seconds_behind_master 主库执行完事务写binlog开始到备库执行完事务结束这个时间差，大半部分是备库从relay log里读数据构成的。
		 >主备切换策略：参考https://blog.csdn.net/hanpeiyu1995/article/details/89499550
		  >可靠性优先：当备库的seconds_behind_master 足够小比如<5s时，说明基本要同步完了，可以切换了，于是将主库的readonly设置为true, 等seconds_behind_master 等于0的时候，把备库的readonly设置为false, 再把业务请求切到备库上即可。这样，仍然有时长为seconds_behind_master的时间不可写只能读。
		  >可用性优先：切换时直接切换到备库，备库的readonly立即设置为false,同时因为binlog_format=row 从而会binlog传递的是记录，那么主库未完成的binlog传递的条数就是冲突的最多个数----此时共有此数*2条数据未插入。所以丢失还是严重。
		  >个人方案：也是立即切换，但是备库在写的时候仅仅写到undolog里，暂时不实际的执行。等到主库binlog写完之后备库也执行完毕之后，才开始实际执行undolog--------此时undolog里有T3-T1这个落后时长的积累的数据，但是没有遗漏也没有丢失数据-----只是将会暂时不可用---卡住--否则暂时不可读。
		 >+keepalived:
	  >集群cluster的读写过程、节点加入移除和故障的检测恢复：redis为例 参考https://my.oschina.net/u/2600078/blog/1923696
	    >redis cluster基本架构：三主三从 + 若干sentinal构成的集合。
	    >hash slot:供16384个(2^18)，每个master分得若干个slot。从key得到对应的slot从而找到存储此key的master的算法：hash slot = crc16(key) mod 16384 。。这样，集群中，不同master存储的就是不同的key
			>crc16:循环效验码 参考https://blog.csdn.net/xinyuan510214/article/details/80104356
			 >发送k位数据M: 先生成n位冗余码，然后k+n发送出去。
			 >冗余码生成规则：  M<<n 异或除 P(len=n+1)(收发端事先商定固定的除数) 得到一个余数FCS , 从而FCS + M<<n就是最终要发送的数据。异或除：每轮的被除数共n+1位，如果首位是0，则后面的位落下直至构成首位非0的n+1位数，继续和P异或运算，直至最后得出一个n位的余数。  
			 >冗余码接收端差错校验：直接将接收的数据 L  异或除 P 看结果如果是0则正确！！！
		>redis cluster读写过程：参考https://redis.io/topics/replication 避免master自动重启 http://redisdoc.com/topic/replication.html
			>读：slave转给master
			>写：master写之后，直接返回，非阻塞的，而异步的同步命令给slave。。master上有backlog， 同时接收slave的offset来发送剩下的指令。
			   >同步过程：slave发送psync命令给master, master验证runid未通过则发送RDB文件给slave, 验证通过则根据offset同步部分数据给slave---即发送缓存区的所有命令.
		>故障转移：master down之后集群会将master的一个slave晋升为新master。。自动化实现机制：sentinel架构。哨兵模式高可用架构。
		 >sentinel节点：互相心跳检测，也检测主从节点。也是一个redis实例。
		 >工作过程：当一个sentinel检测到某个master在指定时间内无心跳返回，则开始询问其他sentinel节点是否真的故障，其他sentinel则也去访问该master并且返回给询问sentinel结果，如果询问sentinel收到超过半数的确认认为故障，则这个询问sentinel开始故障转移工作：即从master所在节点组选择一个slaveA 发送slaveof on one 让它升级称为新的master，然后向其他slave节点发送 slaveof slaveA的ip:port 命令，从而其他slave开始设置新的master节点。另外，sentinel会继续监控老master,如果恢复了--则发送slaveof xxx同样的给它。
		 	
		>水平拆分：
		>水平扩展：增加一组master-slave实例，或者物理上的水平扩展(master slave交叉部署在多台物理机器上--更高可用)。
		>分片迁移：slot的迁出迁入
		>拓扑结构(集群结构)：按负责的slot分为多个 节点组 --->  每个节点组:master1 + slave多个 (master读写服务，slave只读服务)
	  >高并发的使用策略：读写分离、分库分表、sql优化
	>hbase: 自顶向下、职责划分
	  >设计理念：存储海量，通过rowkey把相关数据存储在一起---第一可以避免join-第二可以按rowkey分割存储在不同服务器--可扩展性强, 提供基于行的原子性操作。
	   >cell: 每一个cell存储了：rowkey --> colomn family --> column --> version/timestamp --> value  这也是每个cell值的完整索引。版本个数默认3， 超过则删除老的。
	   >Meta table: 保存集群region信息(rowkey范围-->region id-->region server)  通过B树组织。
	  >物理上：三大服务器集群：zk(主从)(服务器状态监控通知) , HMaster(主从--通过zk创建临时顺序节点时间), HRegionServer(集群)(和hdfs datanode同台部署)
	   >zk:
	    >一致性算法：
	  >读：客户端从本地或zk上读取所在MetaTable信息--->找到所在RegionServer--->发送给RegionServer读取请求--->RegionServer定位到HRegion-HStore, 依次Block Cache--> MemStore--->磁盘上HFile的索引/bloomFilter找到相应的cell读出
	   >读放大效应：memStore多次刷数据导致多个Hfile,导致需要读取多个HFile来
	    >措施：minor compaction: 自动将一些小HFile合并到几个大HFile中。
			   major compaction: 定时将一个cf下的所有HFile整理已经删除的或过期的cell并合并为一个HFile。因为会读大量HFile, 所以有写放大效应。且如果是在HRegion分裂后一个Region被转移到另一台服务器上，那么还会远程读取HFile到本地。
	  >写：客户端同上找到所在RegionServer--->发送给RegionServer---->RegionServer先append顺序添加写入WAL，再写入memFile, 然后返回写入成功；而再异步的判断memFile是否足够大而需要刷入磁盘形成一个新的Hfile/StoreFile(每条数据顺序刷，减少寻址时间所以高效)
	  >删除：给cell打上删除标签---使得不可读。
	  >备份：WAL和HFile都会在HDFS上有备份。
	  >HRegionServer宕机：zk通知HMaster，HMaster重新分配宕机机器上的Region给其他机器，同时将宕机机器上的WAL分配给对应其他机器，其他机器就可以顺序读取执行WAL中的数据操作产生memStore---当满了则flush到HFile
	  >HMaster: 多实例部署，zk负责进行leader选举.
		>对Region管理: Region 分裂合并后重新分配，调整Region分布，HRegionServer挂了之后迁移上面的Region到其他HRegionServer
		>对HRegionServer: 负载均衡，状态监听--通过zk通知
		>对用户：接收增删改查，数据库的创建删除
	  >HRegionServer: 多实例部署, HMaster负责对多实例负载均衡。
	   >对客户端写入：进行IO，存储数据到HDFS；读取也可以。
	   >对Region: 存储1000 个Region(可以来自不同表格，可以来自同一个CF的不同rowkey段)
	   >本地文件：
	     >WAL--日志文件-故障后恢复数据用，写数据到来时先写到WAL日志里。
		 >Block Cache 读缓存---访问次数最高的n个数据的缓存
		 >MemStore 写缓存---累积n个、排序后再刷到磁盘，形成一个Hfile/StoreFile
	  >HRegion: 默认大小：1GB
	   >对HStore: 存储多个HStore
	   >和family关系：一一对应
	  >HStore:
	   >对storeFile: 存储多个StoreFile, 组织方式：LSM树
	   >对memFile/memstore: 一个。
	    >LSM树：日志结构合并树。读弱写强。大多数NoSQL数据库采用。日志结构合并树，优化措施：bloomfilter判断小树有无，小树合并为大树提高查询效率。参考https://www.cnblogs.com/bonelee/p/6244810.html
		 >写强原因：日志顺序写磁盘，实际数据先在内存中用多颗B+树有序缓存--数据量达到一定时刷入磁盘(磁盘中是有序数据)。移动磁头、旋转磁盘、读取头标和数据；即由柱面号 移动臂移动到指定柱面，根据盘面号确定盘面，把块号指定的磁道段移动到磁头下，磁头读取数据传到内存。(移动臂移动自己--查找时间，移动磁道段--等待时间，磁头读写--传输时间))。(磁盘一般2-3个盘片)每一个盘片的两个盘面上都有一个读写磁头， 每个盘面1024个同心圆称为磁道(最外为0)，一个磁道上分多个圆弧称为扇区；所有盘面上同一个磁道构成一个柱面(每个柱面最上的磁头编号为0)(写数据按照同一个柱面最上磁道开始，一直写到最下磁道---电子切换；然后才移动磁头到里面的柱面---机械移动), 一个磁道多个扇区/块，一个块里分扇区头标(盘柱扇号+CRC)和数据(数据512B+ECC)两部分
		 >读弱原因：内存中小树读时间=N/m *log2(m) (使用bloomfilter会更少)， 没有则读磁盘已经写入的树文件(定期树文件合并merge为大树，减少磁盘读另外还减少了读时间---磁盘文件的m变大)，。(?合并的最大文件量是一个磁盘块--即一页8k?)
		 >内存小树写到磁盘后：为减少磁盘随机读(多文件)，多个storeFile 会合并为一颗大树(因为有序，所以合并简单)文件。这个叫小树文件compact。因为查询时间= N/m * log2(m) , m为每棵小树的数据量，从而m增大，查询时间会减少，当m=N时，达到log2N
		 >读取一个数据的过程：先从一棵小树上二分查找，没有则下一颗小树中找....
		  >ECC: 纠正一个bit错误和检测2个bit错误。参考https://blog.csdn.net/liaoyaonline/article/details/80166133
		   >q元码：(n,k,d) n为码字长度，k为码字数量，d为码字之间的最小汉明距离。
		   >汉明距离: v=(v1,v2,...vi) u = (u1,u2,...ui) v的汉明重量=w(v)=1的个数， v,u的汉明距离=d(v,u)=w(v-u)其中-号是异或。。。根据空间三角形，有d(u,v)<=d(u,w) + d(w,v)
		   >一个信号x的ECC编码：奇偶校验编码：x后面加上一位--x的1的个数(偶校验码), 如果x后面加上一位--x的1的个数的非(奇校验码)..总之，整个编码1的个数是偶数--偶效验码，个数是奇数-几效验码
						海明码：数据位N, 校验位r ， 则r位校验位可以组合出的最大的数2^r - 1， 也是能够检测的最大位，更高的位则不能检测；而在最大位以内又有r位是校验码，所以只剩2^r - 1 - r位来表示数据, 所有数据位N<= 2^r - 1 - r。
								r位校验位的位置：2^k, k=0,...r-1
								第i位校验码新增校验的全部位置：2^i * 奇数  (全体整数={2^n *奇数}其中n=0,...n)。所以全部位置都会被校验到。而且第i位刚好未知其他位都知道---从而确定第i位的奇偶校验值。比如对于偶校验，0位上的校验码值=所有奇数位的1数
								第i位校验码值确定：取i+1位隔i+1位再取i+1位...得到的数作为奇偶校验的输入信息。参考https://blog.csdn.net/lycb_gz/article/details/8214961
		  >局域性原理：一个数据被使用时，附近的数据也会被马上使用。如程序运行中，需要的数据不在主存，则触发缺页异常，系统向磁盘发读取信号，磁盘会找到数据起始位置并连续读一页或几页载入主存，异常返回程序继续执行。
		  >操作系统管理存储器的逻辑块：称为页。8k字节(16个扇区)。
		  >磁盘碎片产生的原因：文件在磁盘上连续存储，而文件新增则在其他地方，文件删除从而就会留下许多空隙，文件读取就会多次寻道而耗时更多。参考https://www.cnblogs.com/cxzdy/p/5379570.html
		 >跳表：多层链表。搜索时自顶向下搜索。每层链表有尾指针Nil 。
		  >搜索：查找元素为C, 在第i层链表中某个节点N处发现N<C，如果下一个节点是Nil, 则找N的下一层链表的元素M,继续比较。直到最底层，如果没有找到则没有。
		  >添加：同查找，直到最底层，D1<C<D2 也没有发现C，则在D1,D2之间插入一个节点值为C, 然后随机决定要不要在上一层垂直上方添加一个元素--值也为C。
		 >bloom filter: 初始化集合到 bloom内部数据结构：输入信息m, 用k个hash函数处理m得出ki, i=1,...m 标记bit[ki] = 1 ；形成过滤器需要的数据结构。查询时，只要input的k个hash值对应bit[]上的值有一个是0，则input一定不在集合里。但如果都是1，则大概率在集合里--也可能不在。
		 >bitmap: 本质上是长度为n的bit数组bit[n]来表示元素最大不超过n的k个自然数构成的集合。集合中有的自然数m则在bit[m]=1, 没有的自然数c则bit[c]=0。。。bit[n]的初始化，只需要遍历一遍集合就可以得出来；而bit[n]是有序的，再遍历一遍bit[n]就得出了排序后的自然数集合---海量数据排序。如果用byte[]数组来表示bit[],则 一个整数M在byte[]中的位置= byte[M/8]中的M%8下标位设置为1 byte[M/8] = byte[M/8] | 0x01<<(M%8) 
		 >Level DB: 顺序写磁盘日志Append Only; 应用到内存有序表SkipList(满了之后打包成key有序文件sst(顺序写到磁盘，追加而不是重写旧数据))(读则利用table cache, block cache, bloomfilter)
	   >对memFile: 存储一个memFile
	  >StoreFile：即刷入磁盘的一颗小树(B+树)
	   >数据结构：一个多层索引系统 + 一个个64kB数据块顺序构成，每个数据块可以当作B+树的叶子容器：包括--叶子容器的所有cell数据升序排列作为节点、叶索引、bloomfilter.。而多层索引系统就是B+树的根索引 + 中间索引+叶子索引；这个多层索引系统读到HStore中就是block cache
	  >memFile:	即LSM中的一颗小树(B+树)
	   >保存的每一行就是一个rowkey的一个列簇的所有cell信息(rowkey-cf-co-version-value),同时保存了最后一次写操作的序号---这样保证HFiles之间是通过序号有序组织的。
	   >保存新加入和更新的行：
	  >BlockCache: 最近读取过的n行
	  >Region: 分裂合并
	   >Region大小大于设定值时，平分分裂为两个，HMaster负责分配新产生的HRegion的分配。分配到其他HRegionServer上的Region会在它下一次的major compaction时将HFile文件下载到本地。
	>elasticsearch: 不用关系型数据库来实现全文检索。高扩展性、高可用性的实时数据分析。扩展至上百台，PB级数据。
	  >分词：将输入的搜索词 分解为 多个 关键词/字。。经过分词器Tokenizer得到Token词元。
	  >倒排索引：浏览数据而形成的 “关键字---文档id集合” 称为倒排索引，词典Term Directory(词与词频) --> 文档列表Postings List；但量太大而不会装载都jvm heap里， 所以对Term词典做了一个前缀索引Term Index, 用FST实现占用空间小--从而全量加载到内存heap； 从而先查内存中的前缀索引Term Index 再查磁盘上的Term Directory。es为每个column都使用索引--倒排索引(而不是一般关系型数据库的B树索引)。。参考http://www.360doc.com/content/16/0106/17/17572791_525951877.shtml
	   >词典前缀索引数据结构：FST树 参考http://www.shenyanchao.cn/blog/2018/12/04/lucene-fst/
	    >FST树：有限状态转移机(输入相应的信息--状态进行相应的转移：转移的起止路径对应输入的信息集合)：开始节点：单词最大前缀匹配， 后不够再增加边，直到 结束节点。形成一个有向无环图。
	     >作为key-value数据结构：压缩率有3倍-20倍，省内存---相比于hashMap/treemap
		>Trie树：存储字符串的链表，一个节点一个字符。子节点则最多26个(可以在父节点中用hashmap,array[26], bitmap表达)。达到其中一个节点必然只有一种路径---所以其中一个节点有标记--是否是单词(尾字符)。
		 >add: 从头开始查找，没有的字符则新增该字符节点，然后继续。
		 >query: 从头开始查找，没有的字符则在Trie树中不存在，直到最后Trie树的结尾/或者字符串的结尾，为判断的终止。
		 >delete: isWord=false或者删除整个都是false的分支---且只从下往上删到中间节点有分支为止。
		 ---排序：先序遍历。
		 ---词频统计：单词尾记录出现次数, 最后遍历一遍单词。
		 ---字符串存在检索/去重：相当于查询一次Trie树---效果比hash之类好是因为避免了和不必要的字符串比较---因为每个字符都是精准匹配的。参考https://segmentfault.com/a/1190000008877595
		>bit-wise Trie: 不是存字符，而是存0\1的 前缀树。/地址分配/路由管理.。。参考https://www.cnblogs.com/justinh/p/7716421.html
	  >全文检索：搜索词 分词 后去查 倒排索引 得到的 文档id集合 再去 查相应的文档 并返回，就是一个全文检索的过程。
	  >Lucene: 单机模式, java开发；索引、检索功能。ES的存储引擎，ES在之上增加了分布式接口而已--依赖Luncene；即建立一个分布式框架，而节点底层依赖Luncene进行实质操作；查询时并行查节点，结果合并；索引时，各节点独立索引自己节点上的数据。索引树：FST树---每个单词对应着的值就是docid集合。
	    >es喂给/传输给lucene的：已经是非常规范化的数据；一个Lucene实例也不用关心其他Lucene实例。
		>Lucene倒排索引生成过程：先在内存生成Inverted Index , 再定期以段文件segment file形式刷到磁盘上的，而且不再修改，更新会写到新的文件。所以一个段segment就是一个完整的倒排索引。而segment memory就是倒排索引词典的前缀索引。
		>开箱即用：不用配置。
		>jvm参数配置：jvm堆大小不超过os内存一半--以便os缓存磁盘数据。
	  >Solr: 数据格式更强：不止json,html,excel也可以。但实时性差。
	  >数据组织层次：索引--类型--文档--字段
	   >物理上：集群-->多个es实例节点-->
	            索引-->切分为多个分片--->每个分片多个 副本分片 --> 每个分片多个类型--->每个类型有一个mapping: 字段类型说明 --->每个类型下多个文档:row---->每个row多个field：column
				-----:分配：分片分配给节点。
				      gateway: es索引快照存储方式：一般是本地磁盘，也可以是hdfs,云。存储发生的时间：es先将索引存在内存，达到一定大小就调用gateway持久化到本地/hdfs/云。
	  >对用户提供：Transport交互方式:默认是tcp,但也提供了http: RESTFUL API ：curl 和java接口
	  >索引过程：全量/增量
	   >增量：/修改：修改也会把文档视作新文档而添加到新的segment里，分钟级别可以被重新索引(因为刷磁盘的fsync操作)。
	  >搜索过程：将Query转换为Lucene Query , 在所有的segment中计算
	    >具体过程：词法分析找出关键词；语法分析形成语法树 --> 关键词语言处理器处理规范化---> 从倒排索引中找出关键词的文档列表来交集/差集，---> 对文档排序(根据词对文档的重要性---词权重/词频数/词在的文档数，得到文档的词频向量，而查询语句也是一个词频向量，从而计算两个向量余弦值--作为相关性的打分，分值大的就排在前面: VSM向量空间模型算法)
		>极好参考：https://blog.csdn.net/guoyuguang0/article/details/76769184/
	  >添加数据：新的segment会先缓存到内核中，然后才flush到磁盘。来提高性能。而本身在文档修改,写入es节点时，会首先append写到translog里持久化，方便内存中的segment崩溃后从最近的commit point的数据恢复。
		>segment合并：同hbase HFile的合并，segment也会合并为新的更大的segment，同时整理了segment里的被标记为删除的数据。
		>segment内容：Inverted Index、Stored Fields 、Document Values
		>具体过程：数据生成索引存入内存Buffer, 同时写入translog, 内存中的数据每隔一秒以segment格式存入系统缓存，系统缓存中的segment定期刷入磁盘/同时清除translog中的记录。
		 >数据生成索引：分词得到词元-->语言处理组件将词规范化还原-->词传给索引组建 创建字典、排序、合并相同的词、形成文档倒排 链表
	  >删除数据：segment中的数据删除--仅仅是一个标记；下次会被索引到--但是返回给用户时过滤了。每一个提交点有一个.del文件--记录哪个segment的哪个docid被删除了。
	             文档删除---也仅仅是标记为删除.
	  >分片重新分配：当es节点宕机后，上面的分片会被重新分配到其他节点。
	  >p2p系统：广播和多播
	  >ES节点自动发现：Discovery.zen 相当于每台节点排序广播得到的网络中的serverid， 字典序第一个的作为leader，如果一个节点获得了超过半数票--则只会有一个--从而它就是新的leader
		>参考：https://blog.csdn.net/qq_17864929/article/details/54923720
缓存方面：(三大问题：key总是没有(穿透了)，突然没有-一个热key或者大量key(击穿了，雪崩了))
	>缓存穿透：key数据库不存在，缓存也不存在。导致对数据库不断的查询--给数据库带来压力。(解法：也存到缓存，但是值为特殊的)
	>缓存雪崩：缓存的同时缓存了大量的数据，失效时间也同，集中过期，(或者缓存服务器节点宕机、断网)，导致突然给数据库很大的访问压力--而且是周期性的。(解法：冷门访问缓存时间少，热门长；缓存时间加随机因子。)
	>缓存击穿：热点数据太热，在失效瞬间给数据库很大压力。

#微服务方面：
	>分布式系统的保护：缓存、降级、限流
		>降级：服务舍弃一些突发不可用功能确保整体可用，子服务不可用、超时，整体服务还是要能用，即时因而是有损的。(N次请求m次超时失败, 称为可用率m/N * 100%)
		>限流：一个服务限制调用方对它的调用速率。
	
消息队列方面：kafka为例子(独特数据结构+算法方面)
	>主题-队列组织结构：一个主题下多个分区，每个分区主从组织,每个分区的消息量。producer和主题的每个分区的leader保持socket通信，并且hash方式均匀发送到各个分区leader
	>acks级别：收到、收到且刷入本地队列、刷到从节点、从节点写入成功
	>多个consumer读取一个主题：如果在同一个group，则读取的消息唯一，所以此时comsumer也需要一个均衡算法
	>数据的存储和获取：本地磁盘的顺序批量操作(先buffer再flush到磁盘)，较少磁盘IO和磁盘旋转磁头运动，  
	>zero-copy机制:
	>消费模型：pull模型，comsumer控制消费速率，手动确认是否重新消费。
	>网络模型：nio的多线程selector
	>存储模型：副本机制
	>partion日志分段：每个消息都会append log, 每个段都有index和log两个文件；一个partition日志分段为了方便二分查找日志--先找到端再找到内容，从而方便随机读；而顺序读，利用os预加载page cache页缓存，所以快。
	>leader所在broker故障而消息不丢失的原因机制是什么：HW高水位  
	>消息发送成功三种语义保证：at least once , at most once , exactly once 。最后一种的实现：可以不丢失消息还保证消息顺序，一个producer一个broker,broker对消息进行对齐；消息格式为"序号+消息"，序号小认为重复序号大1以上认为消息有丢失
	>事务：producer对消息队列一次操作的集合。  
	  
	  
日志采集方面：flume为例子(selector->channel->sink->kafka/hdfs/agent)
	>日志+处理模式：比单纯的日志有更多的处理：一是拦截器链(使得不一定存到channel/memorychannel/filechnnel/jdbchannel)，二是写到了channel里(channnel看作一个队列，但有很多接口方便sink调用)，三是有sink异步的独立的调用channel而获取里面的数据，进一步的处理可以为rpc发送到另一个agent；这个agent再将event数据调用source写入channel，而另一方面sink调用channel读取event写入jdbc或者文件等。
	>数据单位：event是一个字节数组，有header头信息。
	>多级流：nginx, tomcat, log4j, syslog日志混合在一起的日志流，可以在在下一级Source时分开发送到不同的channel里，再sink到不同的地点。
	>负载均衡：一个channel可以配置多个sink,并且均衡的从channel里取得event(或者说channel均衡发送event到不同的sink---当然是取好--速率由接收方决定)
    >source: spoolsource监控spool目录，有新日志文件就会读取数据(利用log4j配置为精确到分钟级别)，完毕后文件后缀变为.COMPLETED  而 Exec Source可以实时采集日志
	>本地日志log4j2:
		>distrupor: 高效低延时的消息组件-读写数组(队列)。高性能有界内存队列。(CAS和缓存行补齐方法性能提升)
			>数据结构：数组(环形)，长度：2^n ， index只需要递增，下一个元素位置就是 index & (2^n - 1)按位与就可以了。
			>读写一致性保证：CAS保证写的同步从而一致。即会先申请位置，然后CAS方式写入。(实际上：可以先读取index, 再计算出下一个位置，再CAS写入，失败重读index)
			>缓存行补齐：避免伪共享问题。一个缓存行有64字节，一个缓存行就是cpu L1L2缓存失效的基本单位,而缓存失效是其他cpu发出的失效命令导致的---即改cpu要更新数据写到主存(同时加内存屏障)。
			>avalibleBuffer: 写入后更新多个位置读avalible，读出后更新那些位置写avalible。从而下一次写之前读位置时就有位置可以读了。	
			
	
大数据统计分析方面：Hive为例子
	>
大数据数据导出方面：sqoop为例子
	>

架构设计与技术选型：系统设计/方案设计评审	
	>系统设计的几个主题：参考：https://blog.csdn.net/u013007900/article/details/79049961

软件工程方面：
	>系统建模：数据流图、架构图、时序图、组件图。。。。参考：https://www.jianshu.com/p/4c9f795da7ea
	>
	
#开发方面：
	>能提前评审发现隐藏的潜在的问题：也发现该技术的能力边界。 
	
	
Spring方面：
   >IOC和AOP的实现：
	>IOC：bean交给spring来实例化，只需要配置该bean的属性，对于对象类型的属性-甚至可以不必配置--直接默认也交给spring注入，。(控制反转，依赖注入)
    >AOP: 方法的代理 ，代理方法的内容就是切面编程(方法的前后两个切面)。
	>spring做的事：
		>1.浏览包，对每一个带注解的类/接口 对应生成一个继承/实现它的子类/实现类(字节码)(代理类、动态代理类)--这个类对父类方法全部重写-内容都是调用一个回调类的方法(如果动态代理是cglib，就是实现了MethodInterceptor的类的intercept()方法;javax.tools包则根据类字符串而动态字节码生成)(这个回调类方法里，spring的处理是先获取方法的所有注解，先执行在被代理方法invoke之前要执行的注解的实现类的before方法(以及显示匹配本方法的切面bean的before方法)，再invoke父类方法，再执行之后注解实现类要执行的after方法(和显式匹配到本方法的切面bean的after方法))，将此代理类反射实例化，放入容器(map)。(常规的@Controller @Service @POST @RequestParam; @Aespect)。。属性的注入体现了spring做的依赖注入IOC，方法的代理体现了spring做的切面编程AOP(编写一个切面)。
		>2.执行bean初始化方法：init-method
   >WF实现的IOC和AOP:
  >事务的实现：依赖数据库的事务：打开连接-打开事务-执行操作CURD-提交事务(故障则回滚事务)-关闭连接。。可见执行操作CURD可以被切面编程，而前后的动作都是固定的，所以spring只需要直接写一个事务切面就可以了--用户则只需要使用这个事务切面的注解即可。       参考资料：https://www.jianshu.com/p/2449cd914e3c
	>事务的隔离级别：5种，上述。事务之间互相影响的结果/程度/大小。
	>事务的传播行为：决定新建一个事务，或者加入一个已经有的事务(使用同一个连接)(已有的事务是什么？---即保存在事务ThreadLocal里的当前线程的值)(此外，还有连接ConnectionHolder， 也是当前线程保存的连接)。
		>产生此概念的原因：A方法上有事务，B方法上有事务，而A方法又调用了B方法。
		>行为1：PROPAGATION_REQUIRED 保持一个事务。
		>行为2：PROPAGATION_SUPPORTS 支持当前事务，没有就也不新建。
		>行为3：PROPAGATION_MANDATORY 支持当前事务，但没有要抛出异常。
		>行为4：PROPAGATION_REQUIRES_NEW 当前有事务，则挂起。建立另一个事务执行完再说。
		>行为5：PROPAGATION_NOT_SUPPORTED 不使用事务，当前有也不使用--把它挂起。
		>行为6：PROPAGATION_NEVER 不使用事务，但没有要抛出异常。
		>行为7：PROPAGATION_NESTED 当前有事务，则嵌套一个事务；当前没有事务，则新建一个事务执行。
	>事务的保存点：回滚到保存点。保存点之前的不用回滚，确认已经ok。
	>mysql 命令控制事务：
		>设置数据库、会话的事务隔离级别： set session transaction isolation level read committed; set global transaction isolation level repeatable read;
		>设置不自动提交：set autocommit = 0;
		>开启事务：start transaction; begin;
		>回滚事务：rollback;
		>提交事务：commit;
		>在一个事务中设置多个保存点：savepoint tx1;   则xxx之后，可以 rollback to tx1;//回滚到保存点
		>开启只读：在作用结束前本会话也读不到其他会话/事务插入的内容：SET TRANSACTION READ ONLY;  结束只读：commit;
